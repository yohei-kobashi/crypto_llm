
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0153
    HostName g0153
    Port 2222
    StrictHostKeyChecking no

Host g0154
    HostName g0154
    Port 2222
    StrictHostKeyChecking no

Host g0156
    HostName g0156
    Port 2222
    StrictHostKeyChecking no

Host g0159
    HostName g0159
    Port 2222
    StrictHostKeyChecking no

Host g0161
    HostName g0161
    Port 2222
    StrictHostKeyChecking no

Host g0163
    HostName g0163
    Port 2222
    StrictHostKeyChecking no

Host g0164
    HostName g0164
    Port 2222
    StrictHostKeyChecking no

Host g0165
    HostName g0165
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42829711
g0153 slots=4
g0154 slots=4
g0156 slots=4
g0159 slots=4
g0161 slots=4
g0163 slots=4
g0164 slots=4
g0165 slots=4

[2024-08-12 03:39:49,780] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-12 03:39:55,628] [INFO] [runner.py:463:main] Using IP address of 10.1.5.17 for node g0153
[2024-08-12 03:39:55,631] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0153,g0154,g0156,g0159,g0161,g0163,g0164,g0165
[2024-08-12 03:39:55,631] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0153,g0154,g0156,g0159,g0161,g0163,g0164,g0165 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDE1MyI6IFswLCAxLCAyLCAzXSwgImcwMTU0IjogWzAsIDEsIDIsIDNdLCAiZzAxNTYiOiBbMCwgMSwgMiwgM10sICJnMDE1OSI6IFswLCAxLCAyLCAzXSwgImcwMTYxIjogWzAsIDEsIDIsIDNdLCAiZzAxNjMiOiBbMCwgMSwgMiwgM10sICJnMDE2NCI6IFswLCAxLCAyLCAzXSwgImcwMTY1IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.5.17 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '13631488000' --train-samples '6656000' --lr '2.0e-4' --min-lr '1.0e-5' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True' --log-optimizer-states-to-tensorboard --train-data-exact-num-epochs --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True' --wandb_tag 'other_gpu'
g0153: [2024-08-12 03:39:59,153] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0153: [2024-08-12 03:40:01,408] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0153: [2024-08-12 03:40:01,408] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0153': [0, 1, 2, 3], 'g0154': [0, 1, 2, 3], 'g0156': [0, 1, 2, 3], 'g0159': [0, 1, 2, 3], 'g0161': [0, 1, 2, 3], 'g0163': [0, 1, 2, 3], 'g0164': [0, 1, 2, 3], 'g0165': [0, 1, 2, 3]}
g0153: [2024-08-12 03:40:01,408] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0153: [2024-08-12 03:40:01,408] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0153': [0, 1, 2, 3], 'g0154': [4, 5, 6, 7], 'g0156': [8, 9, 10, 11], 'g0159': [12, 13, 14, 15], 'g0161': [16, 17, 18, 19], 'g0163': [20, 21, 22, 23], 'g0164': [24, 25, 26, 27], 'g0165': [28, 29, 30, 31]})
g0153: [2024-08-12 03:40:01,408] [INFO] [launch.py:163:main] dist_world_size=32
g0153: [2024-08-12 03:40:01,408] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0153: [2024-08-12 03:40:04,540] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0153: [2024-08-12 03:40:04,541] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0153: [2024-08-12 03:40:04,557] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0153: [2024-08-12 03:40:04,709] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0156: [2024-08-12 03:40:05,349] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0165: [2024-08-12 03:40:06,742] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0161: [2024-08-12 03:40:07,102] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0159: [2024-08-12 03:40:07,156] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0164: [2024-08-12 03:40:07,156] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0154: [2024-08-12 03:40:07,231] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0163: [2024-08-12 03:40:07,263] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0153: --------------------------------------------------
g0153: DeepSpeed C++/CUDA extension op report
g0153: --------------------------------------------------
g0153: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0153:       runtime if needed. Op compatibility means that your system
g0153:       meet the required dependencies to JIT install the op.
g0153: --------------------------------------------------
g0153: JIT compiled ops requires ninja
g0153: --------------------------------------------------
g0153: DeepSpeed C++/CUDA extension op report
g0153: --------------------------------------------------
g0153: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0153:       runtime if needed. Op compatibility means that your system
g0153:       meet the required dependencies to JIT install the op.
g0153: --------------------------------------------------
g0153: JIT compiled ops requires ninja
g0153: --------------------------------------------------
g0153: DeepSpeed C++/CUDA extension op report
g0153: --------------------------------------------------
g0153: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0153:       runtime if needed. Op compatibility means that your system
g0153:       meet the required dependencies to JIT install the op.
g0153: --------------------------------------------------
g0153: JIT compiled ops requires ninja
g0153: --------------------------------------------------
g0153: DeepSpeed C++/CUDA extension op report
g0153: --------------------------------------------------
g0153: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0153:       runtime if needed. Op compatibility means that your system
g0153:       meet the required dependencies to JIT install the op.
g0153: --------------------------------------------------
g0153: JIT compiled ops requires ninja
g0153: ninjaninjaninjaninja   .................. .................................... ..................  [92m[OKAY][0m [92m[OKAY][0m[92m[OKAY][0m
g0153: [92m[OKAY][0m
g0153: 
g0153: 
g0153: ----------------------------------------------------------------------------------------------------
g0153: ----------------------------------------------------------------------------------------------------
g0153: 
g0153: op name
g0153: op nameop name  op name................ ................  ................ installed................ installed  installed installed ......    compatiblecompatible..compatible
g0153: 
g0153: 
g0153:  ----------------------------------------------------------------------------------------------------compatible--------------------------------------------------
g0153: 
g0153: 
g0153: 
g0153: --------------------------------------------------
g0153: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0153: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0153: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0153: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0153: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0153: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0153: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0153: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0153: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0153: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0153: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0153: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0153: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0153: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0153: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0153: cpu_lion ............... [92m[YES][0masync_io  .....................  [92m[OKAY][0m[92m[YES][0m
g0153:  ...... [92m[OKAY][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0153: 
g0153: evoformer_attn ......... [93m[NO][0mfused_adam  ....................  [93m[NO][0m[92m[YES][0m
g0153:  ...... [92m[OKAY][0mfused_lamb
g0153:  ............. [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0153: [92m[YES][0m ...... [92m[OKAY][0m
g0153: cpu_adagradfused_lion  .........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0153: 
g0153: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0153: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0153: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0153: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0153: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0153: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0minference_core_ops
g0153:  ..... [92m[YES][0m ...... [92m[OKAY][0m
g0153: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0153: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0153: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: cutlass_ops ............ [92m[YES][0mcutlass_ops  ..................  [92m[OKAY][0m[92m[YES][0m
g0153:  ...... quantizer[92m[OKAY][0m 
g0153: .............. [92m[YES][0m quantizer......  ..............[92m[OKAY][0m 
g0153: [92m[YES][0m ...... [92m[OKAY][0m
g0153: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0153: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0153: ragged_device_ops ...... [92m[YES][0m ragged_device_ops......  [92m[OKAY][0m......
g0153:  [92m[YES][0m ...... [92m[OKAY][0m
g0153: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0153: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0153: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0153: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0153: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0153: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0153: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0153: ragged_ops ............. [92m[YES][0m ragged_ops......  .............[92m[OKAY][0m 
g0153: [92m[YES][0m ...... random_ltd[92m[OKAY][0m 
g0153: ............. [92m[YES][0m random_ltd......  .............[92m[OKAY][0m 
g0153: [92m[YES][0m ...... [92m[OKAY][0m
g0153: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0153: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0153: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0153: 
g0153: sparse_attnsparse_attn  ........................  [93m[NO][0m[93m[NO][0m  ..............  [93m[NO][0m[93m[NO][0m
g0153: 
g0153: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0153: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0153: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0153: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0153: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0153: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0153: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0153: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0153: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0153: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0153: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0153: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0153: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0153: --------------------------------------------------
g0153: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0153: --------------------------------------------------
g0153: transformer_inferencetransformer_inference  ....  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0153: 
g0153: ----------------------------------------------------------------------------------------------------
g0153: 
g0153: DeepSpeed general environment info:
g0153: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0153: torch version .................... 2.0.1+cu118
g0153: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0153: deepspeed info ................... 0.12.4, unknown, unknown
g0153: torch cuda version ............... 11.8
g0153: torch hip version ................ None
g0153: nvcc version ..................... 11.8
g0153: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0153: shared memory (/dev/shm) size .... 188.13 GB
g0153: DeepSpeed general environment info:
g0153: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0153: torch version .................... 2.0.1+cu118
g0153: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0153: deepspeed info ................... 0.12.4, unknown, unknown
g0153: torch cuda version ............... 11.8
g0153: torch hip version ................ None
g0153: nvcc version ..................... 11.8
g0153: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0153: shared memory (/dev/shm) size .... 188.13 GB
g0153: DeepSpeed general environment info:
g0153: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0153: torch version .................... 2.0.1+cu118
g0153: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0153: deepspeed info ................... 0.12.4, unknown, unknown
g0153: torch cuda version ............... 11.8
g0153: torch hip version ................ None
g0153: nvcc version ..................... 11.8
g0153: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0153: shared memory (/dev/shm) size .... 188.13 GB
g0153: DeepSpeed general environment info:
g0153: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0153: torch version .................... 2.0.1+cu118
g0153: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0153: deepspeed info ................... 0.12.4, unknown, unknown
g0153: torch cuda version ............... 11.8
g0153: torch hip version ................ None
g0153: nvcc version ..................... 11.8
g0153: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0153: shared memory (/dev/shm) size .... 188.13 GB
g0153: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0153: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0153: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0153: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0153: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0153:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0153:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0153:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0153:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0153:                        [--expert-interval EXPERT_INTERVAL]
g0153:                        [--hidden-size HIDDEN_SIZE]
g0153:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0153:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0153:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0153:                        [--kv-channels KV_CHANNELS]
g0153:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0153:                        [--use-rotary-position-embeddings]
g0153:                        [--rotary-percent ROTARY_PERCENT]
g0153:                        [--no-position-embedding]
g0153:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0153:                        [--normalization {layernorm,rmsnorm}]
g0153:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0153:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0153:                        [--apply-residual-connection-post-layernorm]
g0153:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0153:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0153:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0153:                        [--untie-embeddings-and-output-weights]
g0153:                        [--embedding-weights-in-fp32]
g0153:                        [--attention-dropout ATTENTION_DROPOUT]
g0153:                        [--hidden-dropout HIDDEN_DROPOUT]
g0153:                        [--weight-decay WEIGHT_DECAY]
g0153:                        [--start-weight-decay START_WEIGHT_DECAY]
g0153:                        [--end-weight-decay END_WEIGHT_DECAY]
g0153:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0153:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0153:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0153:                        [--sgd-momentum SGD_MOMENTUM]
g0153:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0153:                        [--batch-size BATCH_SIZE]
g0153:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0153:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0153:                        [--recompute-activations]
g0153:                        [--recompute-granularity {full,selective}]
g0153:                        [--distribute-saved-activations]
g0153:                        [--recompute-method {uniform,block}]
g0153:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0153:                        [--checkpoint-activations]
g0153:                        [--distribute-checkpointed-activations]
g0153:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0153:                        [--train-iters TRAIN_ITERS]
g0153:                        [--train-samples TRAIN_SAMPLES]
g0153:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0153:                        [--log-interval LOG_INTERVAL]
g0153:                        [--exit-interval EXIT_INTERVAL]
g0153:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0153:                        [--exit-signal-handler]
g0153:                        [--tensorboard-dir TENSORBOARD_DIR]
g0153:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0153:                        [--no-bias-dropout-fusion]
g0153:                        [--disable-moe-token-dropping]
g0153:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0153:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0153:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0153:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0153:                        [--create-moe-param-group] [--use-flash-attn]
g0153:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0153:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0153:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0153:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0153:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0153:                        [--no-async-tensor-model-parallel-allreduce]
g0153:                        [--no-persist-layer-norm] [--sequence-parallel]
g0153:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0153:                        [--force-ds-sequence-parallel]
g0153:                        [--no-gradient-accumulation-fusion]
g0153:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0153:                        [--data-parallel-random-init]
g0153:                        [--init-method-std INIT_METHOD_STD]
g0153:                        [--init-method-xavier-uniform] [--lr LR]
g0153:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0153:                        [--lr-decay-iters LR_DECAY_ITERS]
g0153:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0153:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0153:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0153:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0153:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0153:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0153:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0153:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0153:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0153:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0153:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0153:                        [--no-initialization] [--use-checkpoint-args]
g0153:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0153:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0153:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0153:                        [--min-loss-scale MIN_LOSS_SCALE]
g0153:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0153:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0153:                        [--no-query-key-layer-scaling]
g0153:                        [--attention-softmax-in-fp32]
g0153:                        [--accumulate-allreduce-grads-in-fp32]
g0153:                        [--fp16-lm-cross-entropy]
g0153:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0153:                        [--enable-expert-tensor-parallelism]
g0153:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0153:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0153:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0153:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0153:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0153:                        [--overlap-p2p-communication]
g0153:                        [--distributed-backend {nccl,gloo,ccl}]
g0153:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0153:                        [--DDP-impl {local,torch,FSDP}]
g0153:                        [--no-contiguous-buffers-in-local-ddp]
g0153:                        [--no-scatter-gather-tensors-in-pipeline]
g0153:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0153:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0153:                        [--use-cpu-initialization]
g0153:                        [--empty-unused-memory-level {0,1,2}]
g0153:                        [--standalone-embedding-stage]
g0153:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0153:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0153:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0153:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0153:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0153:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0153:                        [--test-data-path [TEST_DATA_PATH ...]]
g0153:                        [--data-cache-path DATA_CACHE_PATH]
g0153:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0153:                        [--merge-file MERGE_FILE]
g0153:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0153:                        [--seq-length SEQ_LENGTH]
g0153:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0153:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0153:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0153:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0153:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0153:                        [--num-workers NUM_WORKERS]
g0153:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0153:                        [--tokenizer-model TOKENIZER_MODEL]
g0153:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0153:                        [--reset-attention-mask] [--eod-mask-loss]
g0153:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0153:                        [--return-data-index]
g0153:                        [--data-efficiency-curriculum-learning]
g0153:                        [--train-idx-path TRAIN_IDX_PATH]
g0153:                        [--train-desc-path TRAIN_DESC_PATH]
g0153:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0153:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0153:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0153:                        [--repeated-dataloader] [--adlr-autoresume]
g0153:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0153:                        [--ict-head-size ICT_HEAD_SIZE]
g0153:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0153:                        [--biencoder-shared-query-context-model]
g0153:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0153:                        [--titles-data-path TITLES_DATA_PATH]
g0153:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0153:                        [--use-one-sent-docs]
g0153:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0153:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0153:                        [--retriever-score-scaling]
g0153:                        [--block-data-path BLOCK_DATA_PATH]
g0153:                        [--embedding-path EMBEDDING_PATH]
g0153:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0153:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0153:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0153:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0153:                        [--patch-dim PATCH_DIM]
g0153:                        [--classes-fraction CLASSES_FRACTION]
g0153:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0153:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0153:                        [--vision-pretraining]
g0153:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0153:                        [--vision-backbone-type {vit,mit,swin}]
g0153:                        [--swin-backbone-type {tiny,base,h3}]
g0153:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0153:                        [--iter-per-epoch ITER_PER_EPOCH]
g0153:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0153:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0153:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0153:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0153:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0153:                        [--dino-norm-last-layer]
g0153:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0153:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0153:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0153:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0153:                        [--timing-log-level {0,1,2}]
g0153:                        [--no-barrier-with-level-1-timing]
g0153:                        [--timing-log-option {max,minmax,all}]
g0153:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0153:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0153:                        [--log-timers-to-tensorboard]
g0153:                        [--log-batch-size-to-tensorboard]
g0153:                        [--no-log-learnig-rate-to-tensorboard]
g0153:                        [--no-log-loss-scale-to-tensorboard]
g0153:                        [--log-validation-ppl-to-tensorboard]
g0153:                        [--log-optimizer-states-to-tensorboard]
g0153:                        [--log-memory-to-tensorboard]
g0153:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0153:                        [--wandb-entity WANDB_ENTITY]
g0153:                        [--wandb-project WANDB_PROJECT]
g0153:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0153:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0153:                        [--zero-contigious-gradients]
g0153:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0153:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0153:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0153:                        [--scattered-embeddings] [--split-transformers]
g0153:                        [--memory-centric-tiled-linear]
g0153:                        [--tile-factor TILE_FACTOR]
g0153:                        [--deepspeed-activation-checkpointing]
g0153:                        [--partition-activations] [--contigious-checkpointing]
g0153:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0153:                        [--profile-backward]
g0153:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0153:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0153:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0153:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0153:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0153:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0153:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0153:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0153:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0153:                        [--output-bert-embeddings]
g0153:                        [--bert-embedder-type {megatron,huggingface}]
g0153:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0153:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0153:                        [--transformer-impl {local,transformer_engine}]
g0153:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0153:                        [--fp8-amax-compute-algo {most_recent,max}]
g0153:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0153:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0153:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0153:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0153:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0153:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0153:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0153:                        [--retro-return-doc-ids] [--deepspeed]
g0153:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0153:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0153: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0153: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0153:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0153:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0153:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0153:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0153:                        [--expert-interval EXPERT_INTERVAL]
g0153:                        [--hidden-size HIDDEN_SIZE]
g0153:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0153:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0153:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0153:                        [--kv-channels KV_CHANNELS]
g0153:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0153:                        [--use-rotary-position-embeddings]
g0153:                        [--rotary-percent ROTARY_PERCENT]
g0153:                        [--no-position-embedding]
g0153:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0153:                        [--normalization {layernorm,rmsnorm}]
g0153:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0153:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0153:                        [--apply-residual-connection-post-layernorm]
g0153:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0153:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0153:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0153:                        [--untie-embeddings-and-output-weights]
g0153:                        [--embedding-weights-in-fp32]
g0153:                        [--attention-dropout ATTENTION_DROPOUT]
g0153:                        [--hidden-dropout HIDDEN_DROPOUT]
g0153:                        [--weight-decay WEIGHT_DECAY]
g0153:                        [--start-weight-decay START_WEIGHT_DECAY]
g0153:                        [--end-weight-decay END_WEIGHT_DECAY]
g0153:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0153:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0153:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0153:                        [--sgd-momentum SGD_MOMENTUM]
g0153:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0153:                        [--batch-size BATCH_SIZE]
g0153:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0153:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0153:                        [--recompute-activations]
g0153:                        [--recompute-granularity {full,selective}]
g0153:                        [--distribute-saved-activations]
g0153:                        [--recompute-method {uniform,block}]
g0153:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0153:                        [--checkpoint-activations]
g0153:                        [--distribute-checkpointed-activations]
g0153:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0153:                        [--train-iters TRAIN_ITERS]
g0153:                        [--train-samples TRAIN_SAMPLES]
g0153:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0153:                        [--log-interval LOG_INTERVAL]
g0153:                        [--exit-interval EXIT_INTERVAL]
g0153:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0153:                        [--exit-signal-handler]
g0153:                        [--tensorboard-dir TENSORBOARD_DIR]
g0153:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0153:                        [--no-bias-dropout-fusion]
g0153:                        [--disable-moe-token-dropping]
g0153:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0153:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0153:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0153:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0153:                        [--create-moe-param-group] [--use-flash-attn]
g0153:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0153:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0153:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0153:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0153:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0153:                        [--no-async-tensor-model-parallel-allreduce]
g0153:                        [--no-persist-layer-norm] [--sequence-parallel]
g0153:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0153:                        [--force-ds-sequence-parallel]
g0153:                        [--no-gradient-accumulation-fusion]
g0153:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0153:                        [--data-parallel-random-init]
g0153:                        [--init-method-std INIT_METHOD_STD]
g0153:                        [--init-method-xavier-uniform] [--lr LR]
g0153:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0153:                        [--lr-decay-iters LR_DECAY_ITERS]
g0153:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0153:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0153:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0153:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0153:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0153:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0153:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0153:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0153:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0153:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0153:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0153:                        [--no-initialization] [--use-checkpoint-args]
g0153:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0153:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0153:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0153:                        [--min-loss-scale MIN_LOSS_SCALE]
g0153:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0153:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0153:                        [--no-query-key-layer-scaling]
g0153:                        [--attention-softmax-in-fp32]
g0153:                        [--accumulate-allreduce-grads-in-fp32]
g0153:                        [--fp16-lm-cross-entropy]
g0153:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0153:                        [--enable-expert-tensor-parallelism]
g0153:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0153:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0153:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0153:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0153:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0153:                        [--overlap-p2p-communication]
g0153:                        [--distributed-backend {nccl,gloo,ccl}]
g0153:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0153:                        [--DDP-impl {local,torch,FSDP}]
g0153:                        [--no-contiguous-buffers-in-local-ddp]
g0153:                        [--no-scatter-gather-tensors-in-pipeline]
g0153:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0153:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0153:                        [--use-cpu-initialization]
g0153:                        [--empty-unused-memory-level {0,1,2}]
g0153:                        [--standalone-embedding-stage]
g0153:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0153:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0153:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0153:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0153:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0153:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0153:                        [--test-data-path [TEST_DATA_PATH ...]]
g0153:                        [--data-cache-path DATA_CACHE_PATH]
g0153:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0153:                        [--merge-file MERGE_FILE]
g0153:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0153:                        [--seq-length SEQ_LENGTH]
g0153:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0153:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0153:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0153:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0153:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0153:                        [--num-workers NUM_WORKERS]
g0153:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0153:                        [--tokenizer-model TOKENIZER_MODEL]
g0153:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0153:                        [--reset-attention-mask] [--eod-mask-loss]
g0153:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0153:                        [--return-data-index]
g0153:                        [--data-efficiency-curriculum-learning]
g0153:                        [--train-idx-path TRAIN_IDX_PATH]
g0153:                        [--train-desc-path TRAIN_DESC_PATH]
g0153:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0153:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0153:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0153:                        [--repeated-dataloader] [--adlr-autoresume]
g0153:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0153:                        [--ict-head-size ICT_HEAD_SIZE]
g0153:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0153:                        [--biencoder-shared-query-context-model]
g0153:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0153:                        [--titles-data-path TITLES_DATA_PATH]
g0153:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0153:                        [--use-one-sent-docs]
g0153:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0153:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0153:                        [--retriever-score-scaling]
g0153:                        [--block-data-path BLOCK_DATA_PATH]
g0153:                        [--embedding-path EMBEDDING_PATH]
g0153:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0153:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0153:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0153:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0153:                        [--patch-dim PATCH_DIM]
g0153:                        [--classes-fraction CLASSES_FRACTION]
g0153:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0153:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0153:                        [--vision-pretraining]
g0153:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0153:                        [--vision-backbone-type {vit,mit,swin}]
g0153:                        [--swin-backbone-type {tiny,base,h3}]
g0153:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0153:                        [--iter-per-epoch ITER_PER_EPOCH]
g0153:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0153:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0153:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0153:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0153:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0153:                        [--dino-norm-last-layer]
g0153:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0153:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0153:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0153:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0153:                        [--timing-log-level {0,1,2}]
g0153:                        [--no-barrier-with-level-1-timing]
g0153:                        [--timing-log-option {max,minmax,all}]
g0153:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0153:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0153:                        [--log-timers-to-tensorboard]
g0153:                        [--log-batch-size-to-tensorboard]
g0153:                        [--no-log-learnig-rate-to-tensorboard]
g0153:                        [--no-log-loss-scale-to-tensorboard]
g0153:                        [--log-validation-ppl-to-tensorboard]
g0153:                        [--log-optimizer-states-to-tensorboard]
g0153:                        [--log-memory-to-tensorboard]
g0153:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0153:                        [--wandb-entity WANDB_ENTITY]
g0153:                        [--wandb-project WANDB_PROJECT]
g0153:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0153:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0153:                        [--zero-contigious-gradients]
g0153:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0153:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0153:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0153:                        [--scattered-embeddings] [--split-transformers]
g0153:                        [--memory-centric-tiled-linear]
g0153:                        [--tile-factor TILE_FACTOR]
g0153:                        [--deepspeed-activation-checkpointing]
g0153:                        [--partition-activations] [--contigious-checkpointing]
g0153:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0153:                        [--profile-backward]
g0153:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0153:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0153:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0153:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0153:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0153:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0153:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0153:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0153:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0153:                        [--output-bert-embeddings]
g0153:                        [--bert-embedder-type {megatron,huggingface}]
g0153:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0153:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0153:                        [--transformer-impl {local,transformer_engine}]
g0153:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0153:                        [--fp8-amax-compute-algo {most_recent,max}]
g0153:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0153:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0153:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0153:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0153:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0153:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0153:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0153:                        [--retro-return-doc-ids] [--deepspeed]
g0153:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0153:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0153: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0153: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0153:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0153:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0153:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0153:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0153:                        [--expert-interval EXPERT_INTERVAL]
g0153:                        [--hidden-size HIDDEN_SIZE]
g0153:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0153:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0153:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0153:                        [--kv-channels KV_CHANNELS]
g0153:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0153:                        [--use-rotary-position-embeddings]
g0153:                        [--rotary-percent ROTARY_PERCENT]
g0153:                        [--no-position-embedding]
g0153:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0153:                        [--normalization {layernorm,rmsnorm}]
g0153:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0153:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0153:                        [--apply-residual-connection-post-layernorm]
g0153:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0153:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0153:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0153:                        [--untie-embeddings-and-output-weights]
g0153:                        [--embedding-weights-in-fp32]
g0153:                        [--attention-dropout ATTENTION_DROPOUT]
g0153:                        [--hidden-dropout HIDDEN_DROPOUT]
g0153:                        [--weight-decay WEIGHT_DECAY]
g0153:                        [--start-weight-decay START_WEIGHT_DECAY]
g0153:                        [--end-weight-decay END_WEIGHT_DECAY]
g0153:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0153:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0153:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0153:                        [--sgd-momentum SGD_MOMENTUM]
g0153:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0153:                        [--batch-size BATCH_SIZE]
g0153:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0153:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0153:                        [--recompute-activations]
g0153:                        [--recompute-granularity {full,selective}]
g0153:                        [--distribute-saved-activations]
g0153:                        [--recompute-method {uniform,block}]
g0153:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0153:                        [--checkpoint-activations]
g0153:                        [--distribute-checkpointed-activations]
g0153:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0153:                        [--train-iters TRAIN_ITERS]
g0153:                        [--train-samples TRAIN_SAMPLES]
g0153:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0153:                        [--log-interval LOG_INTERVAL]
g0153:                        [--exit-interval EXIT_INTERVAL]
g0153:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0153:                        [--exit-signal-handler]
g0153:                        [--tensorboard-dir TENSORBOARD_DIR]
g0153:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0153:                        [--no-bias-dropout-fusion]
g0153:                        [--disable-moe-token-dropping]
g0153:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0153:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0153:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0153:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0153:                        [--create-moe-param-group] [--use-flash-attn]
g0153:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0153:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0153:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0153:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0153:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0153:                        [--no-async-tensor-model-parallel-allreduce]
g0153:                        [--no-persist-layer-norm] [--sequence-parallel]
g0153:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0153:                        [--force-ds-sequence-parallel]
g0153:                        [--no-gradient-accumulation-fusion]
g0153:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0153:                        [--data-parallel-random-init]
g0153:                        [--init-method-std INIT_METHOD_STD]
g0153:                        [--init-method-xavier-uniform] [--lr LR]
g0153:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0153:                        [--lr-decay-iters LR_DECAY_ITERS]
g0153:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0153:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0153:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0153:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0153:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0153:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0153:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0153:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0153:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0153:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0153:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0153:                        [--no-initialization] [--use-checkpoint-args]
g0153:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0153:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0153:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0153:                        [--min-loss-scale MIN_LOSS_SCALE]
g0153:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0153:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0153:                        [--no-query-key-layer-scaling]
g0153:                        [--attention-softmax-in-fp32]
g0153:                        [--accumulate-allreduce-grads-in-fp32]
g0153:                        [--fp16-lm-cross-entropy]
g0153:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0153:                        [--enable-expert-tensor-parallelism]
g0153:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0153:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0153:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0153:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0153:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0153:                        [--overlap-p2p-communication]
g0153:                        [--distributed-backend {nccl,gloo,ccl}]
g0153:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0153:                        [--DDP-impl {local,torch,FSDP}]
g0153:                        [--no-contiguous-buffers-in-local-ddp]
g0153:                        [--no-scatter-gather-tensors-in-pipeline]
g0153:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0153:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0153:                        [--use-cpu-initialization]
g0153:                        [--empty-unused-memory-level {0,1,2}]
g0153:                        [--standalone-embedding-stage]
g0153:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0153:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0153:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0153:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0153:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0153:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0153:                        [--test-data-path [TEST_DATA_PATH ...]]
g0153:                        [--data-cache-path DATA_CACHE_PATH]
g0153:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0153:                        [--merge-file MERGE_FILE]
g0153:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0153:                        [--seq-length SEQ_LENGTH]
g0153:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0153:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0153:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0153:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0153:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0153:                        [--num-workers NUM_WORKERS]
g0153:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0153:                        [--tokenizer-model TOKENIZER_MODEL]
g0153:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0153:                        [--reset-attention-mask] [--eod-mask-loss]
g0153:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0153:                        [--return-data-index]
g0153:                        [--data-efficiency-curriculum-learning]
g0153:                        [--train-idx-path TRAIN_IDX_PATH]
g0153:                        [--train-desc-path TRAIN_DESC_PATH]
g0153:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0153:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0153:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0153:                        [--repeated-dataloader] [--adlr-autoresume]
g0153:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0153:                        [--ict-head-size ICT_HEAD_SIZE]
g0153:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0153:                        [--biencoder-shared-query-context-model]
g0153:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0153:                        [--titles-data-path TITLES_DATA_PATH]
g0153:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0153:                        [--use-one-sent-docs]
g0153:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0153:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0153:                        [--retriever-score-scaling]
g0153:                        [--block-data-path BLOCK_DATA_PATH]
g0153:                        [--embedding-path EMBEDDING_PATH]
g0153:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0153:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0153:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0153:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0153:                        [--patch-dim PATCH_DIM]
g0153:                        [--classes-fraction CLASSES_FRACTION]
g0153:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0153:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0153:                        [--vision-pretraining]
g0153:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0153:                        [--vision-backbone-type {vit,mit,swin}]
g0153:                        [--swin-backbone-type {tiny,base,h3}]
g0153:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0153:                        [--iter-per-epoch ITER_PER_EPOCH]
g0153:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0153:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0153:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0153:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0153:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0153:                        [--dino-norm-last-layer]
g0153:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0153:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0153:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0153:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0153:                        [--timing-log-level {0,1,2}]
g0153:                        [--no-barrier-with-level-1-timing]
g0153:                        [--timing-log-option {max,minmax,all}]
g0153:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0153:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0153:                        [--log-timers-to-tensorboard]
g0153:                        [--log-batch-size-to-tensorboard]
g0153:                        [--no-log-learnig-rate-to-tensorboard]
g0153:                        [--no-log-loss-scale-to-tensorboard]
g0153:                        [--log-validation-ppl-to-tensorboard]
g0153:                        [--log-optimizer-states-to-tensorboard]
g0153:                        [--log-memory-to-tensorboard]
g0153:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0153:                        [--wandb-entity WANDB_ENTITY]
g0153:                        [--wandb-project WANDB_PROJECT]
g0153:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0153:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0153:                        [--zero-contigious-gradients]
g0153:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0153:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0153:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0153:                        [--scattered-embeddings] [--split-transformers]
g0153:                        [--memory-centric-tiled-linear]
g0153:                        [--tile-factor TILE_FACTOR]
g0153:                        [--deepspeed-activation-checkpointing]
g0153:                        [--partition-activations] [--contigious-checkpointing]
g0153:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0153:                        [--profile-backward]
g0153:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0153:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0153:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0153:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0153:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0153:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0153:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0153:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0153:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0153:                        [--output-bert-embeddings]
g0153:                        [--bert-embedder-type {megatron,huggingface}]
g0153:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0153:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0153:                        [--transformer-impl {local,transformer_engine}]
g0153:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0153:                        [--fp8-amax-compute-algo {most_recent,max}]
g0153:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0153:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0153:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0153:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0153:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0153:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0153:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0153:                        [--retro-return-doc-ids] [--deepspeed]
g0153:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0153:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0153: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0153: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0153:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0153:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0153:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0153:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0153:                        [--expert-interval EXPERT_INTERVAL]
g0153:                        [--hidden-size HIDDEN_SIZE]
g0153:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0153:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0153:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0153:                        [--kv-channels KV_CHANNELS]
g0153:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0153:                        [--use-rotary-position-embeddings]
g0153:                        [--rotary-percent ROTARY_PERCENT]
g0153:                        [--no-position-embedding]
g0153:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0153:                        [--normalization {layernorm,rmsnorm}]
g0153:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0153:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0153:                        [--apply-residual-connection-post-layernorm]
g0153:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0153:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0153:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0153:                        [--untie-embeddings-and-output-weights]
g0153:                        [--embedding-weights-in-fp32]
g0153:                        [--attention-dropout ATTENTION_DROPOUT]
g0153:                        [--hidden-dropout HIDDEN_DROPOUT]
g0153:                        [--weight-decay WEIGHT_DECAY]
g0153:                        [--start-weight-decay START_WEIGHT_DECAY]
g0153:                        [--end-weight-decay END_WEIGHT_DECAY]
g0153:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0153:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0153:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0153:                        [--sgd-momentum SGD_MOMENTUM]
g0153:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0153:                        [--batch-size BATCH_SIZE]
g0153:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0153:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0153:                        [--recompute-activations]
g0153:                        [--recompute-granularity {full,selective}]
g0153:                        [--distribute-saved-activations]
g0153:                        [--recompute-method {uniform,block}]
g0153:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0153:                        [--checkpoint-activations]
g0153:                        [--distribute-checkpointed-activations]
g0153:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0153:                        [--train-iters TRAIN_ITERS]
g0153:                        [--train-samples TRAIN_SAMPLES]
g0153:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0153:                        [--log-interval LOG_INTERVAL]
g0153:                        [--exit-interval EXIT_INTERVAL]
g0153:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0153:                        [--exit-signal-handler]
g0153:                        [--tensorboard-dir TENSORBOARD_DIR]
g0153:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0153:                        [--no-bias-dropout-fusion]
g0153:                        [--disable-moe-token-dropping]
g0153:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0153:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0153:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0153:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0153:                        [--create-moe-param-group] [--use-flash-attn]
g0153:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0153:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0153:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0153:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0153:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0153:                        [--no-async-tensor-model-parallel-allreduce]
g0153:                        [--no-persist-layer-norm] [--sequence-parallel]
g0153:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0153:                        [--force-ds-sequence-parallel]
g0153:                        [--no-gradient-accumulation-fusion]
g0153:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0153:                        [--data-parallel-random-init]
g0153:                        [--init-method-std INIT_METHOD_STD]
g0153:                        [--init-method-xavier-uniform] [--lr LR]
g0153:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0153:                        [--lr-decay-iters LR_DECAY_ITERS]
g0153:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0153:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0153:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0153:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0153:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0153:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0153:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0153:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0153:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0153:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0153:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0153:                        [--no-initialization] [--use-checkpoint-args]
g0153:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0153:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0153:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0153:                        [--min-loss-scale MIN_LOSS_SCALE]
g0153:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0153:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0153:                        [--no-query-key-layer-scaling]
g0153:                        [--attention-softmax-in-fp32]
g0153:                        [--accumulate-allreduce-grads-in-fp32]
g0153:                        [--fp16-lm-cross-entropy]
g0153:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0153:                        [--enable-expert-tensor-parallelism]
g0153:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0153:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0153:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0153:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0153:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0153:                        [--overlap-p2p-communication]
g0153:                        [--distributed-backend {nccl,gloo,ccl}]
g0153:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0153:                        [--DDP-impl {local,torch,FSDP}]
g0153:                        [--no-contiguous-buffers-in-local-ddp]
g0153:                        [--no-scatter-gather-tensors-in-pipeline]
g0153:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0153:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0153:                        [--use-cpu-initialization]
g0153:                        [--empty-unused-memory-level {0,1,2}]
g0153:                        [--standalone-embedding-stage]
g0153:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0153:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0153:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0153:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0153:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0153:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0153:                        [--test-data-path [TEST_DATA_PATH ...]]
g0153:                        [--data-cache-path DATA_CACHE_PATH]
g0153:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0153:                        [--merge-file MERGE_FILE]
g0153:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0153:                        [--seq-length SEQ_LENGTH]
g0153:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0153:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0153:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0153:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0153:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0153:                        [--num-workers NUM_WORKERS]
g0153:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0153:                        [--tokenizer-model TOKENIZER_MODEL]
g0153:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0153:                        [--reset-attention-mask] [--eod-mask-loss]
g0153:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0153:                        [--return-data-index]
g0153:                        [--data-efficiency-curriculum-learning]
g0153:                        [--train-idx-path TRAIN_IDX_PATH]
g0153:                        [--train-desc-path TRAIN_DESC_PATH]
g0153:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0153:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0153:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0153:                        [--repeated-dataloader] [--adlr-autoresume]
g0153:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0153:                        [--ict-head-size ICT_HEAD_SIZE]
g0153:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0153:                        [--biencoder-shared-query-context-model]
g0153:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0153:                        [--titles-data-path TITLES_DATA_PATH]
g0153:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0153:                        [--use-one-sent-docs]
g0153:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0153:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0153:                        [--retriever-score-scaling]
g0153:                        [--block-data-path BLOCK_DATA_PATH]
g0153:                        [--embedding-path EMBEDDING_PATH]
g0153:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0153:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0153:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0153:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0153:                        [--patch-dim PATCH_DIM]
g0153:                        [--classes-fraction CLASSES_FRACTION]
g0153:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0153:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0153:                        [--vision-pretraining]
g0153:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0153:                        [--vision-backbone-type {vit,mit,swin}]
g0153:                        [--swin-backbone-type {tiny,base,h3}]
g0153:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0153:                        [--iter-per-epoch ITER_PER_EPOCH]
g0153:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0153:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0153:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0153:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0153:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0153:                        [--dino-norm-last-layer]
g0153:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0153:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0153:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0153:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0153:                        [--timing-log-level {0,1,2}]
g0153:                        [--no-barrier-with-level-1-timing]
g0153:                        [--timing-log-option {max,minmax,all}]
g0153:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0153:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0153:                        [--log-timers-to-tensorboard]
g0153:                        [--log-batch-size-to-tensorboard]
g0153:                        [--no-log-learnig-rate-to-tensorboard]
g0153:                        [--no-log-loss-scale-to-tensorboard]
g0153:                        [--log-validation-ppl-to-tensorboard]
g0153:                        [--log-optimizer-states-to-tensorboard]
g0153:                        [--log-memory-to-tensorboard]
g0153:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0153:                        [--wandb-entity WANDB_ENTITY]
g0153:                        [--wandb-project WANDB_PROJECT]
g0153:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0153:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0153:                        [--zero-contigious-gradients]
g0153:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0153:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0153:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0153:                        [--scattered-embeddings] [--split-transformers]
g0153:                        [--memory-centric-tiled-linear]
g0153:                        [--tile-factor TILE_FACTOR]
g0153:                        [--deepspeed-activation-checkpointing]
g0153:                        [--partition-activations] [--contigious-checkpointing]
g0153:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0153:                        [--profile-backward]
g0153:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0153:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0153:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0153:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0153:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0153:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0153:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0153:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0153:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0153:                        [--output-bert-embeddings]
g0153:                        [--bert-embedder-type {megatron,huggingface}]
g0153:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0153:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0153:                        [--transformer-impl {local,transformer_engine}]
g0153:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0153:                        [--fp8-amax-compute-algo {most_recent,max}]
g0153:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0153:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0153:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0153:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0153:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0153:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0153:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0153:                        [--retro-return-doc-ids] [--deepspeed]
g0153:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0153:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0153: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0156: [2024-08-12 03:40:09,536] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0156: [2024-08-12 03:40:09,536] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0153': [0, 1, 2, 3], 'g0154': [0, 1, 2, 3], 'g0156': [0, 1, 2, 3], 'g0159': [0, 1, 2, 3], 'g0161': [0, 1, 2, 3], 'g0163': [0, 1, 2, 3], 'g0164': [0, 1, 2, 3], 'g0165': [0, 1, 2, 3]}
g0156: [2024-08-12 03:40:09,536] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0156: [2024-08-12 03:40:09,536] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0153': [0, 1, 2, 3], 'g0154': [4, 5, 6, 7], 'g0156': [8, 9, 10, 11], 'g0159': [12, 13, 14, 15], 'g0161': [16, 17, 18, 19], 'g0163': [20, 21, 22, 23], 'g0164': [24, 25, 26, 27], 'g0165': [28, 29, 30, 31]})
g0156: [2024-08-12 03:40:09,536] [INFO] [launch.py:163:main] dist_world_size=32
g0156: [2024-08-12 03:40:09,536] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0153: [2024-08-12 03:40:10,413] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2777595
g0153: [2024-08-12 03:40:10,413] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2777596
g0153: [2024-08-12 03:40:10,431] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2777597
g0153: [2024-08-12 03:40:10,446] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2777598
g0153: [2024-08-12 03:40:10,461] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True', '--log-optimizer-states-to-tensorboard', '--train-data-exact-num-epochs', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 2
g0165: [2024-08-12 03:40:10,802] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0165: [2024-08-12 03:40:10,803] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0153': [0, 1, 2, 3], 'g0154': [0, 1, 2, 3], 'g0156': [0, 1, 2, 3], 'g0159': [0, 1, 2, 3], 'g0161': [0, 1, 2, 3], 'g0163': [0, 1, 2, 3], 'g0164': [0, 1, 2, 3], 'g0165': [0, 1, 2, 3]}
g0165: [2024-08-12 03:40:10,803] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0165: [2024-08-12 03:40:10,803] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0153': [0, 1, 2, 3], 'g0154': [4, 5, 6, 7], 'g0156': [8, 9, 10, 11], 'g0159': [12, 13, 14, 15], 'g0161': [16, 17, 18, 19], 'g0163': [20, 21, 22, 23], 'g0164': [24, 25, 26, 27], 'g0165': [28, 29, 30, 31]})
g0165: [2024-08-12 03:40:10,803] [INFO] [launch.py:163:main] dist_world_size=32
g0165: [2024-08-12 03:40:10,803] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
pdsh@g0153: g0153: ssh exited with exit code 2
g0164: [2024-08-12 03:40:11,191] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0164: [2024-08-12 03:40:11,191] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0153': [0, 1, 2, 3], 'g0154': [0, 1, 2, 3], 'g0156': [0, 1, 2, 3], 'g0159': [0, 1, 2, 3], 'g0161': [0, 1, 2, 3], 'g0163': [0, 1, 2, 3], 'g0164': [0, 1, 2, 3], 'g0165': [0, 1, 2, 3]}
g0164: [2024-08-12 03:40:11,191] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0164: [2024-08-12 03:40:11,191] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0153': [0, 1, 2, 3], 'g0154': [4, 5, 6, 7], 'g0156': [8, 9, 10, 11], 'g0159': [12, 13, 14, 15], 'g0161': [16, 17, 18, 19], 'g0163': [20, 21, 22, 23], 'g0164': [24, 25, 26, 27], 'g0165': [28, 29, 30, 31]})
g0164: [2024-08-12 03:40:11,191] [INFO] [launch.py:163:main] dist_world_size=32
g0164: [2024-08-12 03:40:11,191] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0161: [2024-08-12 03:40:11,226] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0161: [2024-08-12 03:40:11,226] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0153': [0, 1, 2, 3], 'g0154': [0, 1, 2, 3], 'g0156': [0, 1, 2, 3], 'g0159': [0, 1, 2, 3], 'g0161': [0, 1, 2, 3], 'g0163': [0, 1, 2, 3], 'g0164': [0, 1, 2, 3], 'g0165': [0, 1, 2, 3]}
g0161: [2024-08-12 03:40:11,226] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0161: [2024-08-12 03:40:11,226] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0153': [0, 1, 2, 3], 'g0154': [4, 5, 6, 7], 'g0156': [8, 9, 10, 11], 'g0159': [12, 13, 14, 15], 'g0161': [16, 17, 18, 19], 'g0163': [20, 21, 22, 23], 'g0164': [24, 25, 26, 27], 'g0165': [28, 29, 30, 31]})
g0161: [2024-08-12 03:40:11,226] [INFO] [launch.py:163:main] dist_world_size=32
g0161: [2024-08-12 03:40:11,226] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0159: [2024-08-12 03:40:11,274] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0159: [2024-08-12 03:40:11,274] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0153': [0, 1, 2, 3], 'g0154': [0, 1, 2, 3], 'g0156': [0, 1, 2, 3], 'g0159': [0, 1, 2, 3], 'g0161': [0, 1, 2, 3], 'g0163': [0, 1, 2, 3], 'g0164': [0, 1, 2, 3], 'g0165': [0, 1, 2, 3]}
g0159: [2024-08-12 03:40:11,274] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0159: [2024-08-12 03:40:11,274] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0153': [0, 1, 2, 3], 'g0154': [4, 5, 6, 7], 'g0156': [8, 9, 10, 11], 'g0159': [12, 13, 14, 15], 'g0161': [16, 17, 18, 19], 'g0163': [20, 21, 22, 23], 'g0164': [24, 25, 26, 27], 'g0165': [28, 29, 30, 31]})
g0159: [2024-08-12 03:40:11,275] [INFO] [launch.py:163:main] dist_world_size=32
g0159: [2024-08-12 03:40:11,275] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0163: [2024-08-12 03:40:11,343] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0163: [2024-08-12 03:40:11,343] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0153': [0, 1, 2, 3], 'g0154': [0, 1, 2, 3], 'g0156': [0, 1, 2, 3], 'g0159': [0, 1, 2, 3], 'g0161': [0, 1, 2, 3], 'g0163': [0, 1, 2, 3], 'g0164': [0, 1, 2, 3], 'g0165': [0, 1, 2, 3]}
g0163: [2024-08-12 03:40:11,343] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0163: [2024-08-12 03:40:11,343] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0153': [0, 1, 2, 3], 'g0154': [4, 5, 6, 7], 'g0156': [8, 9, 10, 11], 'g0159': [12, 13, 14, 15], 'g0161': [16, 17, 18, 19], 'g0163': [20, 21, 22, 23], 'g0164': [24, 25, 26, 27], 'g0165': [28, 29, 30, 31]})
g0163: [2024-08-12 03:40:11,343] [INFO] [launch.py:163:main] dist_world_size=32
g0163: [2024-08-12 03:40:11,343] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0154: [2024-08-12 03:40:11,397] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0154: [2024-08-12 03:40:11,397] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0153': [0, 1, 2, 3], 'g0154': [0, 1, 2, 3], 'g0156': [0, 1, 2, 3], 'g0159': [0, 1, 2, 3], 'g0161': [0, 1, 2, 3], 'g0163': [0, 1, 2, 3], 'g0164': [0, 1, 2, 3], 'g0165': [0, 1, 2, 3]}
g0154: [2024-08-12 03:40:11,397] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0154: [2024-08-12 03:40:11,397] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0153': [0, 1, 2, 3], 'g0154': [4, 5, 6, 7], 'g0156': [8, 9, 10, 11], 'g0159': [12, 13, 14, 15], 'g0161': [16, 17, 18, 19], 'g0163': [20, 21, 22, 23], 'g0164': [24, 25, 26, 27], 'g0165': [28, 29, 30, 31]})
g0154: [2024-08-12 03:40:11,397] [INFO] [launch.py:163:main] dist_world_size=32
g0154: [2024-08-12 03:40:11,397] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0156: [2024-08-12 03:40:12,687] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0156: [2024-08-12 03:40:12,687] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0156: [2024-08-12 03:40:12,697] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0156: [2024-08-12 03:40:12,984] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0165: [2024-08-12 03:40:13,912] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0165: [2024-08-12 03:40:13,912] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0165: [2024-08-12 03:40:13,937] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0165: [2024-08-12 03:40:14,071] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0164: [2024-08-12 03:40:14,301] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0164: [2024-08-12 03:40:14,302] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0161: [2024-08-12 03:40:14,337] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0161: [2024-08-12 03:40:14,337] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0161: [2024-08-12 03:40:14,337] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0164: [2024-08-12 03:40:14,385] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0159: [2024-08-12 03:40:14,391] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0159: [2024-08-12 03:40:14,391] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0164: [2024-08-12 03:40:14,438] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0159: [2024-08-12 03:40:14,481] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0163: [2024-08-12 03:40:14,500] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0163: [2024-08-12 03:40:14,501] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0159: [2024-08-12 03:40:14,508] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0163: [2024-08-12 03:40:14,510] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0161: [2024-08-12 03:40:14,539] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0154: [2024-08-12 03:40:14,544] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0154: [2024-08-12 03:40:14,544] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0154: [2024-08-12 03:40:14,563] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0154: [2024-08-12 03:40:14,646] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0163: [2024-08-12 03:40:14,691] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0156: --------------------------------------------------
g0156: DeepSpeed C++/CUDA extension op report
g0156: --------------------------------------------------
g0156: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0156:       runtime if needed. Op compatibility means that your system
g0156:       meet the required dependencies to JIT install the op.
g0156: --------------------------------------------------
g0156: JIT compiled ops requires ninja
g0156: --------------------------------------------------
g0156: DeepSpeed C++/CUDA extension op report
g0156: --------------------------------------------------
g0156: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0156:       runtime if needed. Op compatibility means that your system
g0156:       meet the required dependencies to JIT install the op.
g0156: --------------------------------------------------
g0156: JIT compiled ops requires ninja
g0156: --------------------------------------------------
g0156: DeepSpeed C++/CUDA extension op report
g0156: --------------------------------------------------
g0156: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0156:       runtime if needed. Op compatibility means that your system
g0156:       meet the required dependencies to JIT install the op.
g0156: --------------------------------------------------
g0156: JIT compiled ops requires ninja
g0156: ninjaninja ninja..................  .................. [92m[OKAY][0m ..................
g0156: [92m[OKAY][0m 
g0156: [92m[OKAY][0m--------------------------------------------------
g0156: 
g0156: --------------------------------------------------
g0156: op name--------------------------------------------------op name
g0156:   ................op name................   installed................installed   ..installed..   compatible..compatible
g0156:  
g0156: compatible----------------------------------------------------------------------------------------------------
g0156: 
g0156: 
g0156: --------------------------------------------------
g0156: --------------------------------------------------
g0156: DeepSpeed C++/CUDA extension op report
g0156: --------------------------------------------------
g0156: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0156:       runtime if needed. Op compatibility means that your system
g0156:       meet the required dependencies to JIT install the op.
g0156: --------------------------------------------------
g0156: JIT compiled ops requires ninja
g0156: ninja .................. [92m[OKAY][0m
g0156: --------------------------------------------------
g0156: op name ................ installed .. compatible
g0156: --------------------------------------------------
g0156: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0156: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0156: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0156: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0156: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0156: evoformer_attn ......... async_io[93m[NO][0m  ......................  [93m[NO][0m[92m[YES][0m
g0156:  ...... [92m[OKAY][0mfused_lamb
g0156:  ............. [92m[YES][0m ...... fused_adam[92m[OKAY][0m 
g0156: ............. [92m[YES][0m async_io......  [92m[OKAY][0m
g0156: ...............fused_lion  [92m[YES][0m............. cpu_adam ...... [92m[YES][0m ...............  ......[92m[OKAY][0m[92m[YES][0m 
g0156:  [92m[OKAY][0m......
g0156:  [92m[OKAY][0m
g0156: fused_adamcpu_adagrad  .........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0156: 
g0156: cpu_adamcpu_lion  ..............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0156: 
g0156: cpu_adagrad ............ [92m[YES][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......
g0156:  async_io[92m[OKAY][0m
g0156: evoformer_attn  ........................cpu_lion   [92m[YES][0m[93m[NO][0m...............   .............[92m[YES][0m   [93m[NO][0m[92m[OKAY][0m......
g0156:  
g0156: [92m[OKAY][0m
g0156: fused_lamb ............. [92m[YES][0m fused_adam......  [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH.............[92m[OKAY][0m
g0156:  
g0156: [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0156:  .......fused_lion  [93m[NO][0mcpu_adam.............
g0156:   ...............[92m[YES][0m  fused_lamb[92m[YES][0m......   ...................[92m[OKAY][0m  
g0156: [92m[YES][0m[92m[OKAY][0m 
g0156: ...... [92m[OKAY][0mcpu_adagrad
g0156:  ............ [92m[YES][0m ...... [92m[OKAY][0m
g0156: fused_lion .............cpu_lion  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m......
g0156:  [92m[OKAY][0m
g0156: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0156: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0156: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............ inference_core_ops [92m[OKAY][0m[92m[OKAY][0m 
g0156: 
g0156: ..... [92m[YES][0m ...... [92m[OKAY][0m
g0156: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0156: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0156: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0156: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0156: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0156: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0156: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0156: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0156: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0156: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0156: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0156: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0156: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0156: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: random_ltd ............. [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible[92m[YES][0m 
g0156: ...... [92m[OKAY][0msparse_attn
g0156:  ............ [93m[NO][0m ....... [93m[NO][0m
g0156: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0156: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0156: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0156: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0156: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0156: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0156: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0156: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0156: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0156: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0156: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0156: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0156: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0156: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0156: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0156: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0156: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0156: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0156: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0156: --------------------------------------------------
g0156: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0156: --------------------------------------------------
g0156: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0156: --------------------------------------------------
g0156: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0156: --------------------------------------------------
g0156: DeepSpeed general environment info:
g0156: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0156: torch version .................... 2.0.1+cu118
g0156: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0156: deepspeed info ................... 0.12.4, unknown, unknown
g0156: torch cuda version ............... 11.8
g0156: torch hip version ................ None
g0156: nvcc version ..................... 11.8
g0156: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0156: shared memory (/dev/shm) size .... 188.13 GB
g0156: DeepSpeed general environment info:
g0156: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0156: torch version .................... 2.0.1+cu118
g0156: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0156: deepspeed info ................... 0.12.4, unknown, unknown
g0156: torch cuda version ............... 11.8
g0156: torch hip version ................ None
g0156: nvcc version ..................... 11.8
g0156: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0156: shared memory (/dev/shm) size .... 188.13 GB
g0156: DeepSpeed general environment info:
g0156: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0156: torch version .................... 2.0.1+cu118
g0156: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0156: deepspeed info ................... 0.12.4, unknown, unknown
g0156: torch cuda version ............... 11.8
g0156: torch hip version ................ None
g0156: nvcc version ..................... 11.8
g0156: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0156: shared memory (/dev/shm) size .... 188.13 GB
g0156: DeepSpeed general environment info:
g0156: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0156: torch version .................... 2.0.1+cu118
g0156: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0156: deepspeed info ................... 0.12.4, unknown, unknown
g0156: torch cuda version ............... 11.8
g0156: torch hip version ................ None
g0156: nvcc version ..................... 11.8
g0156: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0156: shared memory (/dev/shm) size .... 188.13 GB
g0156: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0156: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ******** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0156: 
g0156: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0156: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0156:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0156:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0156:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0156:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0156:                        [--expert-interval EXPERT_INTERVAL]
g0156:                        [--hidden-size HIDDEN_SIZE]
g0156:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0156:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0156:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0156:                        [--kv-channels KV_CHANNELS]
g0156:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0156:                        [--use-rotary-position-embeddings]
g0156:                        [--rotary-percent ROTARY_PERCENT]
g0156:                        [--no-position-embedding]
g0156:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0156:                        [--normalization {layernorm,rmsnorm}]
g0156:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0156:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0156:                        [--apply-residual-connection-post-layernorm]
g0156:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0156:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0156:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0156:                        [--untie-embeddings-and-output-weights]
g0156:                        [--embedding-weights-in-fp32]
g0156:                        [--attention-dropout ATTENTION_DROPOUT]
g0156:                        [--hidden-dropout HIDDEN_DROPOUT]
g0156:                        [--weight-decay WEIGHT_DECAY]
g0156:                        [--start-weight-decay START_WEIGHT_DECAY]
g0156:                        [--end-weight-decay END_WEIGHT_DECAY]
g0156:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0156:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0156:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0156:                        [--sgd-momentum SGD_MOMENTUM]
g0156:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0156:                        [--batch-size BATCH_SIZE]
g0156:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0156:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0156:                        [--recompute-activations]
g0156:                        [--recompute-granularity {full,selective}]
g0156:                        [--distribute-saved-activations]
g0156:                        [--recompute-method {uniform,block}]
g0156:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0156:                        [--checkpoint-activations]
g0156:                        [--distribute-checkpointed-activations]
g0156:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0156:                        [--train-iters TRAIN_ITERS]
g0156:                        [--train-samples TRAIN_SAMPLES]
g0156:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0156:                        [--log-interval LOG_INTERVAL]
g0156:                        [--exit-interval EXIT_INTERVAL]
g0156:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0156:                        [--exit-signal-handler]
g0156:                        [--tensorboard-dir TENSORBOARD_DIR]
g0156:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0156:                        [--no-bias-dropout-fusion]
g0156:                        [--disable-moe-token-dropping]
g0156:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0156:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0156:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0156:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0156:                        [--create-moe-param-group] [--use-flash-attn]
g0156:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0156:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0156:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0156:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0156:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0156:                        [--no-async-tensor-model-parallel-allreduce]
g0156:                        [--no-persist-layer-norm] [--sequence-parallel]
g0156:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0156:                        [--force-ds-sequence-parallel]
g0156:                        [--no-gradient-accumulation-fusion]
g0156:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0156:                        [--data-parallel-random-init]
g0156:                        [--init-method-std INIT_METHOD_STD]
g0156:                        [--init-method-xavier-uniform] [--lr LR]
g0156:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0156:                        [--lr-decay-iters LR_DECAY_ITERS]
g0156:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0156:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0156:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0156:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0156:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0156:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0156:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0156:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0156:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0156:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0156:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0156:                        [--no-initialization] [--use-checkpoint-args]
g0156:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0156:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0156:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0156:                        [--min-loss-scale MIN_LOSS_SCALE]
g0156:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0156:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0156:                        [--no-query-key-layer-scaling]
g0156:                        [--attention-softmax-in-fp32]
g0156:                        [--accumulate-allreduce-grads-in-fp32]
g0156:                        [--fp16-lm-cross-entropy]
g0156:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0156:                        [--enable-expert-tensor-parallelism]
g0156:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0156:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0156:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0156:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0156:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0156:                        [--overlap-p2p-communication]
g0156:                        [--distributed-backend {nccl,gloo,ccl}]
g0156:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0156:                        [--DDP-impl {local,torch,FSDP}]
g0156:                        [--no-contiguous-buffers-in-local-ddp]
g0156:                        [--no-scatter-gather-tensors-in-pipeline]
g0156:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0156:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0156:                        [--use-cpu-initialization]
g0156:                        [--empty-unused-memory-level {0,1,2}]
g0156:                        [--standalone-embedding-stage]
g0156:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0156:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0156:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0156:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0156:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0156:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0156:                        [--test-data-path [TEST_DATA_PATH ...]]
g0156:                        [--data-cache-path DATA_CACHE_PATH]
g0156:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0156:                        [--merge-file MERGE_FILE]
g0156:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0156:                        [--seq-length SEQ_LENGTH]
g0156:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0156:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0156:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0156:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0156:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0156:                        [--num-workers NUM_WORKERS]
g0156:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0156:                        [--tokenizer-model TOKENIZER_MODEL]
g0156:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0156:                        [--reset-attention-mask] [--eod-mask-loss]
g0156:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0156:                        [--return-data-index]
g0156:                        [--data-efficiency-curriculum-learning]
g0156:                        [--train-idx-path TRAIN_IDX_PATH]
g0156:                        [--train-desc-path TRAIN_DESC_PATH]
g0156:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0156:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0156:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0156:                        [--repeated-dataloader] [--adlr-autoresume]
g0156:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0156:                        [--ict-head-size ICT_HEAD_SIZE]
g0156:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0156:                        [--biencoder-shared-query-context-model]
g0156:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0156:                        [--titles-data-path TITLES_DATA_PATH]
g0156:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0156:                        [--use-one-sent-docs]
g0156:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0156:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0156:                        [--retriever-score-scaling]
g0156:                        [--block-data-path BLOCK_DATA_PATH]
g0156:                        [--embedding-path EMBEDDING_PATH]
g0156:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0156:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0156:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0156:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0156:                        [--patch-dim PATCH_DIM]
g0156:                        [--classes-fraction CLASSES_FRACTION]
g0156:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0156:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0156:                        [--vision-pretraining]
g0156:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0156:                        [--vision-backbone-type {vit,mit,swin}]
g0156:                        [--swin-backbone-type {tiny,base,h3}]
g0156:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0156:                        [--iter-per-epoch ITER_PER_EPOCH]
g0156:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0156:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0156:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0156:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0156:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0156:                        [--dino-norm-last-layer]
g0156:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0156:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0156:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0156:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0156:                        [--timing-log-level {0,1,2}]
g0156:                        [--no-barrier-with-level-1-timing]
g0156:                        [--timing-log-option {max,minmax,all}]
g0156:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0156:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0156:                        [--log-timers-to-tensorboard]
g0156:                        [--log-batch-size-to-tensorboard]
g0156:                        [--no-log-learnig-rate-to-tensorboard]
g0156:                        [--no-log-loss-scale-to-tensorboard]
g0156:                        [--log-validation-ppl-to-tensorboard]
g0156:                        [--log-optimizer-states-to-tensorboard]
g0156:                        [--log-memory-to-tensorboard]
g0156:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0156:                        [--wandb-entity WANDB_ENTITY]
g0156:                        [--wandb-project WANDB_PROJECT]
g0156:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0156:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0156:                        [--zero-contigious-gradients]
g0156:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0156:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0156:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0156:                        [--scattered-embeddings] [--split-transformers]
g0156:                        [--memory-centric-tiled-linear]
g0156:                        [--tile-factor TILE_FACTOR]
g0156:                        [--deepspeed-activation-checkpointing]
g0156:                        [--partition-activations] [--contigious-checkpointing]
g0156:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0156:                        [--profile-backward]
g0156:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0156:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0156:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0156:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0156:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0156:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0156:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0156:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0156:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0156:                        [--output-bert-embeddings]
g0156:                        [--bert-embedder-type {megatron,huggingface}]
g0156:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0156:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0156:                        [--transformer-impl {local,transformer_engine}]
g0156:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0156:                        [--fp8-amax-compute-algo {most_recent,max}]
g0156:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0156:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0156:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0156:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0156:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0156:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0156:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0156:                        [--retro-return-doc-ids] [--deepspeed]
g0156:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0156:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0156: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0156: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0156:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0156:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0156:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0156:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0156:                        [--expert-interval EXPERT_INTERVAL]
g0156:                        [--hidden-size HIDDEN_SIZE]
g0156:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0156:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0156:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0156:                        [--kv-channels KV_CHANNELS]
g0156:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0156:                        [--use-rotary-position-embeddings]
g0156:                        [--rotary-percent ROTARY_PERCENT]
g0156:                        [--no-position-embedding]
g0156:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0156:                        [--normalization {layernorm,rmsnorm}]
g0156:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0156:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0156:                        [--apply-residual-connection-post-layernorm]
g0156:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0156:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0156:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0156:                        [--untie-embeddings-and-output-weights]
g0156:                        [--embedding-weights-in-fp32]
g0156:                        [--attention-dropout ATTENTION_DROPOUT]
g0156:                        [--hidden-dropout HIDDEN_DROPOUT]
g0156:                        [--weight-decay WEIGHT_DECAY]
g0156:                        [--start-weight-decay START_WEIGHT_DECAY]
g0156:                        [--end-weight-decay END_WEIGHT_DECAY]
g0156:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0156:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0156:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0156:                        [--sgd-momentum SGD_MOMENTUM]
g0156:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0156:                        [--batch-size BATCH_SIZE]
g0156:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0156:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0156:                        [--recompute-activations]
g0156:                        [--recompute-granularity {full,selective}]
g0156:                        [--distribute-saved-activations]
g0156:                        [--recompute-method {uniform,block}]
g0156:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0156:                        [--checkpoint-activations]
g0156:                        [--distribute-checkpointed-activations]
g0156:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0156:                        [--train-iters TRAIN_ITERS]
g0156:                        [--train-samples TRAIN_SAMPLES]
g0156:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0156:                        [--log-interval LOG_INTERVAL]
g0156:                        [--exit-interval EXIT_INTERVAL]
g0156:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0156:                        [--exit-signal-handler]
g0156:                        [--tensorboard-dir TENSORBOARD_DIR]
g0156:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0156:                        [--no-bias-dropout-fusion]
g0156:                        [--disable-moe-token-dropping]
g0156:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0156:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0156:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0156:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0156:                        [--create-moe-param-group] [--use-flash-attn]
g0156:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0156:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0156:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0156:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0156:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0156:                        [--no-async-tensor-model-parallel-allreduce]
g0156:                        [--no-persist-layer-norm] [--sequence-parallel]
g0156:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0156:                        [--force-ds-sequence-parallel]
g0156:                        [--no-gradient-accumulation-fusion]
g0156:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0156:                        [--data-parallel-random-init]
g0156:                        [--init-method-std INIT_METHOD_STD]
g0156:                        [--init-method-xavier-uniform] [--lr LR]
g0156:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0156:                        [--lr-decay-iters LR_DECAY_ITERS]
g0156:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0156:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0156:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0156:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0156:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0156:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0156:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0156:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0156:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0156:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0156:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0156:                        [--no-initialization] [--use-checkpoint-args]
g0156:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0156:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0156:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0156:                        [--min-loss-scale MIN_LOSS_SCALE]
g0156:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0156:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0156:                        [--no-query-key-layer-scaling]
g0156:                        [--attention-softmax-in-fp32]
g0156:                        [--accumulate-allreduce-grads-in-fp32]
g0156:                        [--fp16-lm-cross-entropy]
g0156:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0156:                        [--enable-expert-tensor-parallelism]
g0156:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0156:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0156:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0156:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0156:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0156:                        [--overlap-p2p-communication]
g0156:                        [--distributed-backend {nccl,gloo,ccl}]
g0156:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0156:                        [--DDP-impl {local,torch,FSDP}]
g0156:                        [--no-contiguous-buffers-in-local-ddp]
g0156:                        [--no-scatter-gather-tensors-in-pipeline]
g0156:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0156:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0156:                        [--use-cpu-initialization]
g0156:                        [--empty-unused-memory-level {0,1,2}]
g0156:                        [--standalone-embedding-stage]
g0156:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0156:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0156:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0156:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0156:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0156:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0156:                        [--test-data-path [TEST_DATA_PATH ...]]
g0156:                        [--data-cache-path DATA_CACHE_PATH]
g0156:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0156:                        [--merge-file MERGE_FILE]
g0156:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0156:                        [--seq-length SEQ_LENGTH]
g0156:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0156:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0156:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0156:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0156:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0156:                        [--num-workers NUM_WORKERS]
g0156:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0156:                        [--tokenizer-model TOKENIZER_MODEL]
g0156:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0156:                        [--reset-attention-mask] [--eod-mask-loss]
g0156:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0156:                        [--return-data-index]
g0156:                        [--data-efficiency-curriculum-learning]
g0156:                        [--train-idx-path TRAIN_IDX_PATH]
g0156:                        [--train-desc-path TRAIN_DESC_PATH]
g0156:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0156:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0156:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0156:                        [--repeated-dataloader] [--adlr-autoresume]
g0156:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0156:                        [--ict-head-size ICT_HEAD_SIZE]
g0156:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0156:                        [--biencoder-shared-query-context-model]
g0156:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0156:                        [--titles-data-path TITLES_DATA_PATH]
g0156:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0156:                        [--use-one-sent-docs]
g0156:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0156:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0156:                        [--retriever-score-scaling]
g0156:                        [--block-data-path BLOCK_DATA_PATH]
g0156:                        [--embedding-path EMBEDDING_PATH]
g0156:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0156:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0156:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0156:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0156:                        [--patch-dim PATCH_DIM]
g0156:                        [--classes-fraction CLASSES_FRACTION]
g0156:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0156:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0156:                        [--vision-pretraining]
g0156:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0156:                        [--vision-backbone-type {vit,mit,swin}]
g0156:                        [--swin-backbone-type {tiny,base,h3}]
g0156:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0156:                        [--iter-per-epoch ITER_PER_EPOCH]
g0156:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0156:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0156:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0156:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0156:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0156:                        [--dino-norm-last-layer]
g0156:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0156:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0156:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0156:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0156:                        [--timing-log-level {0,1,2}]
g0156:                        [--no-barrier-with-level-1-timing]
g0156:                        [--timing-log-option {max,minmax,all}]
g0156:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0156:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0156:                        [--log-timers-to-tensorboard]
g0156:                        [--log-batch-size-to-tensorboard]
g0156:                        [--no-log-learnig-rate-to-tensorboard]
g0156:                        [--no-log-loss-scale-to-tensorboard]
g0156:                        [--log-validation-ppl-to-tensorboard]
g0156:                        [--log-optimizer-states-to-tensorboard]
g0156:                        [--log-memory-to-tensorboard]
g0156:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0156:                        [--wandb-entity WANDB_ENTITY]
g0156:                        [--wandb-project WANDB_PROJECT]
g0156:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0156:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0156:                        [--zero-contigious-gradients]
g0156:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0156:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0156:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0156:                        [--scattered-embeddings] [--split-transformers]
g0156:                        [--memory-centric-tiled-linear]
g0156:                        [--tile-factor TILE_FACTOR]
g0156:                        [--deepspeed-activation-checkpointing]
g0156:                        [--partition-activations] [--contigious-checkpointing]
g0156:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0156:                        [--profile-backward]
g0156:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0156:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0156:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0156:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0156:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0156:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0156:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0156:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0156:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0156:                        [--output-bert-embeddings]
g0156:                        [--bert-embedder-type {megatron,huggingface}]
g0156:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0156:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0156:                        [--transformer-impl {local,transformer_engine}]
g0156:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0156:                        [--fp8-amax-compute-algo {most_recent,max}]
g0156:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0156:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0156:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0156:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0156:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0156:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0156:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0156:                        [--retro-return-doc-ids] [--deepspeed]
g0156:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0156:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0156: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0156: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0156:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0156:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0156:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0156:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0156:                        [--expert-interval EXPERT_INTERVAL]
g0156:                        [--hidden-size HIDDEN_SIZE]
g0156:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0156:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0156:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0156:                        [--kv-channels KV_CHANNELS]
g0156:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0156:                        [--use-rotary-position-embeddings]
g0156:                        [--rotary-percent ROTARY_PERCENT]
g0156:                        [--no-position-embedding]
g0156:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0156:                        [--normalization {layernorm,rmsnorm}]
g0156:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0156:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0156:                        [--apply-residual-connection-post-layernorm]
g0156:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0156:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0156:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0156:                        [--untie-embeddings-and-output-weights]
g0156:                        [--embedding-weights-in-fp32]
g0156:                        [--attention-dropout ATTENTION_DROPOUT]
g0156:                        [--hidden-dropout HIDDEN_DROPOUT]
g0156:                        [--weight-decay WEIGHT_DECAY]
g0156:                        [--start-weight-decay START_WEIGHT_DECAY]
g0156:                        [--end-weight-decay END_WEIGHT_DECAY]
g0156:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0156:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0156:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0156:                        [--sgd-momentum SGD_MOMENTUM]
g0156:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0156:                        [--batch-size BATCH_SIZE]
g0156:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0156:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0156:                        [--recompute-activations]
g0156:                        [--recompute-granularity {full,selective}]
g0156:                        [--distribute-saved-activations]
g0156:                        [--recompute-method {uniform,block}]
g0156:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0156:                        [--checkpoint-activations]
g0156:                        [--distribute-checkpointed-activations]
g0156:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0156:                        [--train-iters TRAIN_ITERS]
g0156:                        [--train-samples TRAIN_SAMPLES]
g0156:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0156:                        [--log-interval LOG_INTERVAL]
g0156:                        [--exit-interval EXIT_INTERVAL]
g0156:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0156:                        [--exit-signal-handler]
g0156:                        [--tensorboard-dir TENSORBOARD_DIR]
g0156:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0156:                        [--no-bias-dropout-fusion]
g0156:                        [--disable-moe-token-dropping]
g0156:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0156:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0156:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0156:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0156:                        [--create-moe-param-group] [--use-flash-attn]
g0156:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0156:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0156:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0156:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0156:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0156:                        [--no-async-tensor-model-parallel-allreduce]
g0156:                        [--no-persist-layer-norm] [--sequence-parallel]
g0156:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0156:                        [--force-ds-sequence-parallel]
g0156:                        [--no-gradient-accumulation-fusion]
g0156:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0156:                        [--data-parallel-random-init]
g0156:                        [--init-method-std INIT_METHOD_STD]
g0156:                        [--init-method-xavier-uniform] [--lr LR]
g0156:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0156:                        [--lr-decay-iters LR_DECAY_ITERS]
g0156:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0156:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0156:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0156:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0156:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0156:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0156:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0156:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0156:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0156:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0156:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0156:                        [--no-initialization] [--use-checkpoint-args]
g0156:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0156:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0156:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0156:                        [--min-loss-scale MIN_LOSS_SCALE]
g0156:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0156:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0156:                        [--no-query-key-layer-scaling]
g0156:                        [--attention-softmax-in-fp32]
g0156:                        [--accumulate-allreduce-grads-in-fp32]
g0156:                        [--fp16-lm-cross-entropy]
g0156:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0156:                        [--enable-expert-tensor-parallelism]
g0156:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0156:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0156:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0156:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0156:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0156:                        [--overlap-p2p-communication]
g0156:                        [--distributed-backend {nccl,gloo,ccl}]
g0156:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0156:                        [--DDP-impl {local,torch,FSDP}]
g0156:                        [--no-contiguous-buffers-in-local-ddp]
g0156:                        [--no-scatter-gather-tensors-in-pipeline]
g0156:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0156:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0156:                        [--use-cpu-initialization]
g0156:                        [--empty-unused-memory-level {0,1,2}]
g0156:                        [--standalone-embedding-stage]
g0156:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0156:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0156:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0156:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0156:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0156:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0156:                        [--test-data-path [TEST_DATA_PATH ...]]
g0156:                        [--data-cache-path DATA_CACHE_PATH]
g0156:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0156:                        [--merge-file MERGE_FILE]
g0156:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0156:                        [--seq-length SEQ_LENGTH]
g0156:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0156:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0156:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0156:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0156:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0156:                        [--num-workers NUM_WORKERS]
g0156:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0156:                        [--tokenizer-model TOKENIZER_MODEL]
g0156:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0156:                        [--reset-attention-mask] [--eod-mask-loss]
g0156:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0156:                        [--return-data-index]
g0156:                        [--data-efficiency-curriculum-learning]
g0156:                        [--train-idx-path TRAIN_IDX_PATH]
g0156:                        [--train-desc-path TRAIN_DESC_PATH]
g0156:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0156:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0156:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0156:                        [--repeated-dataloader] [--adlr-autoresume]
g0156:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0156:                        [--ict-head-size ICT_HEAD_SIZE]
g0156:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0156:                        [--biencoder-shared-query-context-model]
g0156:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0156:                        [--titles-data-path TITLES_DATA_PATH]
g0156:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0156:                        [--use-one-sent-docs]
g0156:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0156:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0156:                        [--retriever-score-scaling]
g0156:                        [--block-data-path BLOCK_DATA_PATH]
g0156:                        [--embedding-path EMBEDDING_PATH]
g0156:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0156:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0156:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0156:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0156:                        [--patch-dim PATCH_DIM]
g0156:                        [--classes-fraction CLASSES_FRACTION]
g0156:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0156:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0156:                        [--vision-pretraining]
g0156:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0156:                        [--vision-backbone-type {vit,mit,swin}]
g0156:                        [--swin-backbone-type {tiny,base,h3}]
g0156:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0156:                        [--iter-per-epoch ITER_PER_EPOCH]
g0156:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0156:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0156:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0156:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0156:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0156:                        [--dino-norm-last-layer]
g0156:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0156:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0156:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0156:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0156:                        [--timing-log-level {0,1,2}]
g0156:                        [--no-barrier-with-level-1-timing]
g0156:                        [--timing-log-option {max,minmax,all}]
g0156:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0156:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0156:                        [--log-timers-to-tensorboard]
g0156:                        [--log-batch-size-to-tensorboard]
g0156:                        [--no-log-learnig-rate-to-tensorboard]
g0156:                        [--no-log-loss-scale-to-tensorboard]
g0156:                        [--log-validation-ppl-to-tensorboard]
g0156:                        [--log-optimizer-states-to-tensorboard]
g0156:                        [--log-memory-to-tensorboard]
g0156:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0156:                        [--wandb-entity WANDB_ENTITY]
g0156:                        [--wandb-project WANDB_PROJECT]
g0156:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0156:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0156:                        [--zero-contigious-gradients]
g0156:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0156:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0156:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0156:                        [--scattered-embeddings] [--split-transformers]
g0156:                        [--memory-centric-tiled-linear]
g0156:                        [--tile-factor TILE_FACTOR]
g0156:                        [--deepspeed-activation-checkpointing]
g0156:                        [--partition-activations] [--contigious-checkpointing]
g0156:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0156:                        [--profile-backward]
g0156:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0156:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0156:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0156:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0156:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0156:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0156:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0156:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0156:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0156:                        [--output-bert-embeddings]
g0156:                        [--bert-embedder-type {megatron,huggingface}]
g0156:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0156:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0156:                        [--transformer-impl {local,transformer_engine}]
g0156:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0156:                        [--fp8-amax-compute-algo {most_recent,max}]
g0156:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0156:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0156:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0156:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0156:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0156:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0156:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0156:                        [--retro-return-doc-ids] [--deepspeed]
g0156:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0156:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0156: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0156: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0156:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0156:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0156:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0156:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0156:                        [--expert-interval EXPERT_INTERVAL]
g0156:                        [--hidden-size HIDDEN_SIZE]
g0156:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0156:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0156:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0156:                        [--kv-channels KV_CHANNELS]
g0156:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0156:                        [--use-rotary-position-embeddings]
g0156:                        [--rotary-percent ROTARY_PERCENT]
g0156:                        [--no-position-embedding]
g0156:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0156:                        [--normalization {layernorm,rmsnorm}]
g0156:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0156:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0156:                        [--apply-residual-connection-post-layernorm]
g0156:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0156:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0156:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0156:                        [--untie-embeddings-and-output-weights]
g0156:                        [--embedding-weights-in-fp32]
g0156:                        [--attention-dropout ATTENTION_DROPOUT]
g0156:                        [--hidden-dropout HIDDEN_DROPOUT]
g0156:                        [--weight-decay WEIGHT_DECAY]
g0156:                        [--start-weight-decay START_WEIGHT_DECAY]
g0156:                        [--end-weight-decay END_WEIGHT_DECAY]
g0156:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0156:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0156:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0156:                        [--sgd-momentum SGD_MOMENTUM]
g0156:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0156:                        [--batch-size BATCH_SIZE]
g0156:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0156:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0156:                        [--recompute-activations]
g0156:                        [--recompute-granularity {full,selective}]
g0156:                        [--distribute-saved-activations]
g0156:                        [--recompute-method {uniform,block}]
g0156:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0156:                        [--checkpoint-activations]
g0156:                        [--distribute-checkpointed-activations]
g0156:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0156:                        [--train-iters TRAIN_ITERS]
g0156:                        [--train-samples TRAIN_SAMPLES]
g0156:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0156:                        [--log-interval LOG_INTERVAL]
g0156:                        [--exit-interval EXIT_INTERVAL]
g0156:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0156:                        [--exit-signal-handler]
g0156:                        [--tensorboard-dir TENSORBOARD_DIR]
g0156:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0156:                        [--no-bias-dropout-fusion]
g0156:                        [--disable-moe-token-dropping]
g0156:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0156:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0156:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0156:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0156:                        [--create-moe-param-group] [--use-flash-attn]
g0156:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0156:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0156:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0156:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0156:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0156:                        [--no-async-tensor-model-parallel-allreduce]
g0156:                        [--no-persist-layer-norm] [--sequence-parallel]
g0156:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0156:                        [--force-ds-sequence-parallel]
g0156:                        [--no-gradient-accumulation-fusion]
g0156:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0156:                        [--data-parallel-random-init]
g0156:                        [--init-method-std INIT_METHOD_STD]
g0156:                        [--init-method-xavier-uniform] [--lr LR]
g0156:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0156:                        [--lr-decay-iters LR_DECAY_ITERS]
g0156:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0156:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0156:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0156:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0156:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0156:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0156:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0156:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0156:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0156:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0156:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0156:                        [--no-initialization] [--use-checkpoint-args]
g0156:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0156:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0156:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0156:                        [--min-loss-scale MIN_LOSS_SCALE]
g0156:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0156:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0156:                        [--no-query-key-layer-scaling]
g0156:                        [--attention-softmax-in-fp32]
g0156:                        [--accumulate-allreduce-grads-in-fp32]
g0156:                        [--fp16-lm-cross-entropy]
g0156:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0156:                        [--enable-expert-tensor-parallelism]
g0156:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0156:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0156:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0156:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0156:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0156:                        [--overlap-p2p-communication]
g0156:                        [--distributed-backend {nccl,gloo,ccl}]
g0156:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0156:                        [--DDP-impl {local,torch,FSDP}]
g0156:                        [--no-contiguous-buffers-in-local-ddp]
g0156:                        [--no-scatter-gather-tensors-in-pipeline]
g0156:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0156:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0156:                        [--use-cpu-initialization]
g0156:                        [--empty-unused-memory-level {0,1,2}]
g0156:                        [--standalone-embedding-stage]
g0156:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0156:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0156:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0156:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0156:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0156:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0156:                        [--test-data-path [TEST_DATA_PATH ...]]
g0156:                        [--data-cache-path DATA_CACHE_PATH]
g0156:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0156:                        [--merge-file MERGE_FILE]
g0156:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0156:                        [--seq-length SEQ_LENGTH]
g0156:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0156:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0156:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0156:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0156:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0156:                        [--num-workers NUM_WORKERS]
g0156:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0156:                        [--tokenizer-model TOKENIZER_MODEL]
g0156:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0156:                        [--reset-attention-mask] [--eod-mask-loss]
g0156:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0156:                        [--return-data-index]
g0156:                        [--data-efficiency-curriculum-learning]
g0156:                        [--train-idx-path TRAIN_IDX_PATH]
g0156:                        [--train-desc-path TRAIN_DESC_PATH]
g0156:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0156:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0156:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0156:                        [--repeated-dataloader] [--adlr-autoresume]
g0156:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0156:                        [--ict-head-size ICT_HEAD_SIZE]
g0156:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0156:                        [--biencoder-shared-query-context-model]
g0156:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0156:                        [--titles-data-path TITLES_DATA_PATH]
g0156:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0156:                        [--use-one-sent-docs]
g0156:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0156:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0156:                        [--retriever-score-scaling]
g0156:                        [--block-data-path BLOCK_DATA_PATH]
g0156:                        [--embedding-path EMBEDDING_PATH]
g0156:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0156:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0156:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0156:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0156:                        [--patch-dim PATCH_DIM]
g0156:                        [--classes-fraction CLASSES_FRACTION]
g0156:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0156:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0156:                        [--vision-pretraining]
g0156:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0156:                        [--vision-backbone-type {vit,mit,swin}]
g0156:                        [--swin-backbone-type {tiny,base,h3}]
g0156:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0156:                        [--iter-per-epoch ITER_PER_EPOCH]
g0156:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0156:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0156:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0156:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0156:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0156:                        [--dino-norm-last-layer]
g0156:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0156:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0156:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0156:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0156:                        [--timing-log-level {0,1,2}]
g0156:                        [--no-barrier-with-level-1-timing]
g0156:                        [--timing-log-option {max,minmax,all}]
g0156:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0156:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0156:                        [--log-timers-to-tensorboard]
g0156:                        [--log-batch-size-to-tensorboard]
g0156:                        [--no-log-learnig-rate-to-tensorboard]
g0156:                        [--no-log-loss-scale-to-tensorboard]
g0156:                        [--log-validation-ppl-to-tensorboard]
g0156:                        [--log-optimizer-states-to-tensorboard]
g0156:                        [--log-memory-to-tensorboard]
g0156:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0156:                        [--wandb-entity WANDB_ENTITY]
g0156:                        [--wandb-project WANDB_PROJECT]
g0156:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0156:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0156:                        [--zero-contigious-gradients]
g0156:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0156:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0156:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0156:                        [--scattered-embeddings] [--split-transformers]
g0156:                        [--memory-centric-tiled-linear]
g0156:                        [--tile-factor TILE_FACTOR]
g0156:                        [--deepspeed-activation-checkpointing]
g0156:                        [--partition-activations] [--contigious-checkpointing]
g0156:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0156:                        [--profile-backward]
g0156:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0156:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0156:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0156:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0156:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0156:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0156:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0156:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0156:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0156:                        [--output-bert-embeddings]
g0156:                        [--bert-embedder-type {megatron,huggingface}]
g0156:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0156:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0156:                        [--transformer-impl {local,transformer_engine}]
g0156:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0156:                        [--fp8-amax-compute-algo {most_recent,max}]
g0156:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0156:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0156:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0156:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0156:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0156:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0156:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0156:                        [--retro-return-doc-ids] [--deepspeed]
g0156:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0156:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0156: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0165: --------------------------------------------------
g0165: DeepSpeed C++/CUDA extension op report
g0165: --------------------------------------------------
g0165: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0165:       runtime if needed. Op compatibility means that your system
g0165:       meet the required dependencies to JIT install the op.
g0165: --------------------------------------------------
g0165: JIT compiled ops requires ninja
g0165: --------------------------------------------------
g0165: DeepSpeed C++/CUDA extension op report
g0165: --------------------------------------------------
g0165: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0165:       runtime if needed. Op compatibility means that your system
g0165:       meet the required dependencies to JIT install the op.
g0165: --------------------------------------------------
g0165: JIT compiled ops requires ninja
g0165: --------------------------------------------------
g0165: DeepSpeed C++/CUDA extension op report
g0165: --------------------------------------------------
g0165: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0165:       runtime if needed. Op compatibility means that your system
g0165:       meet the required dependencies to JIT install the op.
g0165: --------------------------------------------------
g0165: JIT compiled ops requires ninja
g0165: ninjaninja ninja..................  .................. [92m[OKAY][0m ..................
g0165: [92m[OKAY][0m 
g0165: [92m[OKAY][0m--------------------------------------------------
g0165: 
g0165: --------------------------------------------------
g0165: op name-------------------------------------------------- 
g0165: op name................  op name................installed   ................installed..   installed..compatible  
g0165: ..compatible --------------------------------------------------
g0165: compatible
g0165: 
g0165: --------------------------------------------------
g0165: --------------------------------------------------
g0165: --------------------------------------------------
g0165: DeepSpeed C++/CUDA extension op report
g0165: --------------------------------------------------
g0165: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0165:       runtime if needed. Op compatibility means that your system
g0165:       meet the required dependencies to JIT install the op.
g0165: --------------------------------------------------
g0165: JIT compiled ops requires ninja
g0165: ninja .................. [92m[OKAY][0m
g0165: --------------------------------------------------
g0165: op name ................ installed .. compatible
g0165: --------------------------------------------------
g0165: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0165: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0165: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0165: cpu_lion ...............async_io [92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0165: ...... [92m[OKAY][0m
g0165: fused_adam [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH.............
g0165:  [92m[YES][0m evoformer_attn......  .........[92m[OKAY][0m 
g0165: [93m[NO][0m ....... cpu_adam[93m[NO][0m 
g0165: ............... [92m[YES][0mfused_lamb  ...................  [92m[OKAY][0m[92m[YES][0m
g0165:  ...... cpu_adagrad[92m[OKAY][0m 
g0165: ............ [92m[YES][0m ...... [92m[OKAY][0m
g0165: fused_lioncpu_lion  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0165: 
g0165: async_io ............... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0165:  ......evoformer_attn  [92m[OKAY][0m.........
g0165:  [93m[NO][0m ....... [93m[NO][0m
g0165: fused_adam .............fused_lamb  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0165:  [92m[OKAY][0m
g0165: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0mfused_lion
g0165:  ............. [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0m[92m[YES][0m
g0165:  ...... [92m[OKAY][0m
g0165: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0165: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0165: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0165: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0165: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0165: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0165: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0165: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0165: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0165: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0165: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0165: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0165: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0165: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0165: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0165: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0165: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0165: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0165: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0165: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0165: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0165: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0165: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0165: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0165: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0165: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0165: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0165: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0165: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0165: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0165: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0165: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0165: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0165: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0165: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0165: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0165: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0165: --------------------------------------------------
g0165: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0165: --------------------------------------------------
g0165: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0165: --------------------------------------------------
g0165: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0165: DeepSpeed general environment info:
g0165: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0165: torch version .................... 2.0.1+cu118
g0165: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0165: deepspeed info ................... 0.12.4, unknown, unknown
g0165: torch cuda version ............... 11.8
g0165: torch hip version ................ None
g0165: nvcc version ..................... 11.8
g0165: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0165: shared memory (/dev/shm) size .... 188.13 GB
g0165: DeepSpeed general environment info:
g0165: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0165: torch version .................... 2.0.1+cu118
g0165: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0165: DeepSpeed general environment info:deepspeed info 
g0165: ................... 0.12.4, unknown, unknowntorch install path
g0165:  ...............torch cuda version  ............... 11.8['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0165: 
g0165: torch hip version ................torch version  None....................
g0165:  nvcc version2.0.1+cu118 
g0165: .....................deepspeed install path  11.8...........
g0165:  ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0165: deepspeed wheel compiled w.deepspeed info  .........................  torch 2.0, cuda 11.80.12.4, unknown, unknown
g0165: 
g0165: shared memory (/dev/shm) sizetorch cuda version  ...................  188.13 GB11.8
g0165: 
g0165: torch hip version ................ None
g0165: nvcc version ..................... 11.8
g0165: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0165: shared memory (/dev/shm) size .... 188.13 GB
g0165: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0165: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0165: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0165: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0165: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0165: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0165: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0165: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0165: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0165: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0165: --------------------------------------------------
g0165: DeepSpeed general environment info:
g0165: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0165: torch version .................... 2.0.1+cu118
g0165: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0165: deepspeed info ................... 0.12.4, unknown, unknown
g0165: torch cuda version ............... 11.8
g0165: torch hip version ................ None
g0165: nvcc version ..................... 11.8
g0165: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0165: shared memory (/dev/shm) size .... 188.13 GB
g0165: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0165: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0165: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0165: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0165: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0165:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0165:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0165:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0165:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0165:                        [--expert-interval EXPERT_INTERVAL]
g0165:                        [--hidden-size HIDDEN_SIZE]
g0165:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0165:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0165:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0165:                        [--kv-channels KV_CHANNELS]
g0165:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0165:                        [--use-rotary-position-embeddings]
g0165:                        [--rotary-percent ROTARY_PERCENT]
g0165:                        [--no-position-embedding]
g0165:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0165:                        [--normalization {layernorm,rmsnorm}]
g0165:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0165:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0165:                        [--apply-residual-connection-post-layernorm]
g0165:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0165:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0165:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0165:                        [--untie-embeddings-and-output-weights]
g0165:                        [--embedding-weights-in-fp32]
g0165:                        [--attention-dropout ATTENTION_DROPOUT]
g0165:                        [--hidden-dropout HIDDEN_DROPOUT]
g0165:                        [--weight-decay WEIGHT_DECAY]
g0165:                        [--start-weight-decay START_WEIGHT_DECAY]
g0165:                        [--end-weight-decay END_WEIGHT_DECAY]
g0165:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0165:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0165:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0165:                        [--sgd-momentum SGD_MOMENTUM]
g0165:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0165:                        [--batch-size BATCH_SIZE]
g0165:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0165:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0165:                        [--recompute-activations]
g0165:                        [--recompute-granularity {full,selective}]
g0165:                        [--distribute-saved-activations]
g0165:                        [--recompute-method {uniform,block}]
g0165:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0165:                        [--checkpoint-activations]
g0165:                        [--distribute-checkpointed-activations]
g0165:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0165:                        [--train-iters TRAIN_ITERS]
g0165:                        [--train-samples TRAIN_SAMPLES]
g0165:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0165:                        [--log-interval LOG_INTERVAL]
g0165:                        [--exit-interval EXIT_INTERVAL]
g0165:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0165:                        [--exit-signal-handler]
g0165:                        [--tensorboard-dir TENSORBOARD_DIR]
g0165:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0165:                        [--no-bias-dropout-fusion]
g0165:                        [--disable-moe-token-dropping]
g0165:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0165:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0165:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0165:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0165:                        [--create-moe-param-group] [--use-flash-attn]
g0165:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0165:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0165:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0165:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0165:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0165:                        [--no-async-tensor-model-parallel-allreduce]
g0165:                        [--no-persist-layer-norm] [--sequence-parallel]
g0165:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0165:                        [--force-ds-sequence-parallel]
g0165:                        [--no-gradient-accumulation-fusion]
g0165:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0165:                        [--data-parallel-random-init]
g0165:                        [--init-method-std INIT_METHOD_STD]
g0165:                        [--init-method-xavier-uniform] [--lr LR]
g0165:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0165:                        [--lr-decay-iters LR_DECAY_ITERS]
g0165:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0165:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0165:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0165:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0165:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0165:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0165:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0165:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0165:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0165:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0165:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0165:                        [--no-initialization] [--use-checkpoint-args]
g0165:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0165:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0165:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0165:                        [--min-loss-scale MIN_LOSS_SCALE]
g0165:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0165:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0165:                        [--no-query-key-layer-scaling]
g0165:                        [--attention-softmax-in-fp32]
g0165:                        [--accumulate-allreduce-grads-in-fp32]
g0165:                        [--fp16-lm-cross-entropy]
g0165:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0165:                        [--enable-expert-tensor-parallelism]
g0165:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0165:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0165:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0165:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0165:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0165:                        [--overlap-p2p-communication]
g0165:                        [--distributed-backend {nccl,gloo,ccl}]
g0165:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0165:                        [--DDP-impl {local,torch,FSDP}]
g0165:                        [--no-contiguous-buffers-in-local-ddp]
g0165:                        [--no-scatter-gather-tensors-in-pipeline]
g0165:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0165:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0165:                        [--use-cpu-initialization]
g0165:                        [--empty-unused-memory-level {0,1,2}]
g0165:                        [--standalone-embedding-stage]
g0165:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0165:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0165:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0165:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0165:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0165:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0165:                        [--test-data-path [TEST_DATA_PATH ...]]
g0165:                        [--data-cache-path DATA_CACHE_PATH]
g0165:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0165:                        [--merge-file MERGE_FILE]
g0165:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0165:                        [--seq-length SEQ_LENGTH]
g0165:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0165:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0165:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0165:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0165:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0165:                        [--num-workers NUM_WORKERS]
g0165:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0165:                        [--tokenizer-model TOKENIZER_MODEL]
g0165:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0165:                        [--reset-attention-mask] [--eod-mask-loss]
g0165:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0165:                        [--return-data-index]
g0165:                        [--data-efficiency-curriculum-learning]
g0165:                        [--train-idx-path TRAIN_IDX_PATH]
g0165:                        [--train-desc-path TRAIN_DESC_PATH]
g0165:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0165:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0165:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0165:                        [--repeated-dataloader] [--adlr-autoresume]
g0165:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0165:                        [--ict-head-size ICT_HEAD_SIZE]
g0165:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0165:                        [--biencoder-shared-query-context-model]
g0165:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0165:                        [--titles-data-path TITLES_DATA_PATH]
g0165:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0165:                        [--use-one-sent-docs]
g0165:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0165:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0165:                        [--retriever-score-scaling]
g0165:                        [--block-data-path BLOCK_DATA_PATH]
g0165:                        [--embedding-path EMBEDDING_PATH]
g0165:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0165:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0165:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0165:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0165:                        [--patch-dim PATCH_DIM]
g0165:                        [--classes-fraction CLASSES_FRACTION]
g0165:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0165:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0165:                        [--vision-pretraining]
g0165:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0165:                        [--vision-backbone-type {vit,mit,swin}]
g0165:                        [--swin-backbone-type {tiny,base,h3}]
g0165:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0165:                        [--iter-per-epoch ITER_PER_EPOCH]
g0165:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0165:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0165:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0165:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0165:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0165:                        [--dino-norm-last-layer]
g0165:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0165:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0165:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0165:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0165:                        [--timing-log-level {0,1,2}]
g0165:                        [--no-barrier-with-level-1-timing]
g0165:                        [--timing-log-option {max,minmax,all}]
g0165:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0165:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0165:                        [--log-timers-to-tensorboard]
g0165:                        [--log-batch-size-to-tensorboard]
g0165:                        [--no-log-learnig-rate-to-tensorboard]
g0165:                        [--no-log-loss-scale-to-tensorboard]
g0165:                        [--log-validation-ppl-to-tensorboard]
g0165:                        [--log-optimizer-states-to-tensorboard]
g0165:                        [--log-memory-to-tensorboard]
g0165:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0165:                        [--wandb-entity WANDB_ENTITY]
g0165:                        [--wandb-project WANDB_PROJECT]
g0165:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0165:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0165:                        [--zero-contigious-gradients]
g0165:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0165:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0165:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0165:                        [--scattered-embeddings] [--split-transformers]
g0165:                        [--memory-centric-tiled-linear]
g0165:                        [--tile-factor TILE_FACTOR]
g0165:                        [--deepspeed-activation-checkpointing]
g0165:                        [--partition-activations] [--contigious-checkpointing]
g0165:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0165:                        [--profile-backward]
g0165:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0165:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0165:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0165:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0165:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0165:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0165:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0165:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0165:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0165:                        [--output-bert-embeddings]
g0165:                        [--bert-embedder-type {megatron,huggingface}]
g0165:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0165:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0165:                        [--transformer-impl {local,transformer_engine}]
g0165:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0165:                        [--fp8-amax-compute-algo {most_recent,max}]
g0165:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0165:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0165:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0165:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0165:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0165:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0165:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0165:                        [--retro-return-doc-ids] [--deepspeed]
g0165:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0165:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0165: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0165:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0165:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0165:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0165:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0165:                        [--expert-interval EXPERT_INTERVAL]
g0165:                        [--hidden-size HIDDEN_SIZE]
g0165:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0165:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0165:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0165:                        [--kv-channels KV_CHANNELS]
g0165:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0165:                        [--use-rotary-position-embeddings]
g0165:                        [--rotary-percent ROTARY_PERCENT]
g0165:                        [--no-position-embedding]
g0165:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0165:                        [--normalization {layernorm,rmsnorm}]
g0165:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0165:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0165:                        [--apply-residual-connection-post-layernorm]
g0165:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0165:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0165:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0165:                        [--untie-embeddings-and-output-weights]
g0165:                        [--embedding-weights-in-fp32]
g0165:                        [--attention-dropout ATTENTION_DROPOUT]
g0165:                        [--hidden-dropout HIDDEN_DROPOUT]
g0165:                        [--weight-decay WEIGHT_DECAY]
g0165:                        [--start-weight-decay START_WEIGHT_DECAY]
g0165:                        [--end-weight-decay END_WEIGHT_DECAY]
g0165:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0165:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0165:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0165:                        [--sgd-momentum SGD_MOMENTUM]
g0165:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0165:                        [--batch-size BATCH_SIZE]
g0165:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0165:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0165:                        [--recompute-activations]
g0165:                        [--recompute-granularity {full,selective}]
g0165:                        [--distribute-saved-activations]
g0165:                        [--recompute-method {uniform,block}]
g0165:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0165:                        [--checkpoint-activations]
g0165:                        [--distribute-checkpointed-activations]
g0165:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0165:                        [--train-iters TRAIN_ITERS]
g0165:                        [--train-samples TRAIN_SAMPLES]
g0165:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0165:                        [--log-interval LOG_INTERVAL]
g0165:                        [--exit-interval EXIT_INTERVAL]
g0165:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0165:                        [--exit-signal-handler]
g0165:                        [--tensorboard-dir TENSORBOARD_DIR]
g0165:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0165:                        [--no-bias-dropout-fusion]
g0165:                        [--disable-moe-token-dropping]
g0165:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0165:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0165:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0165:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0165:                        [--create-moe-param-group] [--use-flash-attn]
g0165:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0165:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0165:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0165:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0165:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0165:                        [--no-async-tensor-model-parallel-allreduce]
g0165:                        [--no-persist-layer-norm] [--sequence-parallel]
g0165:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0165:                        [--force-ds-sequence-parallel]
g0165:                        [--no-gradient-accumulation-fusion]
g0165:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0165:                        [--data-parallel-random-init]
g0165:                        [--init-method-std INIT_METHOD_STD]
g0165:                        [--init-method-xavier-uniform] [--lr LR]
g0165:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0165:                        [--lr-decay-iters LR_DECAY_ITERS]
g0165:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0165:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0165:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0165:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0165:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0165:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0165:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0165:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0165:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0165:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0165:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0165:                        [--no-initialization] [--use-checkpoint-args]
g0165:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0165:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0165:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0165:                        [--min-loss-scale MIN_LOSS_SCALE]
g0165:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0165:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0165:                        [--no-query-key-layer-scaling]
g0165:                        [--attention-softmax-in-fp32]
g0165:                        [--accumulate-allreduce-grads-in-fp32]
g0165:                        [--fp16-lm-cross-entropy]
g0165:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0165:                        [--enable-expert-tensor-parallelism]
g0165:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0165:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0165:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0165:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0165:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0165:                        [--overlap-p2p-communication]
g0165:                        [--distributed-backend {nccl,gloo,ccl}]
g0165:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0165:                        [--DDP-impl {local,torch,FSDP}]
g0165:                        [--no-contiguous-buffers-in-local-ddp]
g0165:                        [--no-scatter-gather-tensors-in-pipeline]
g0165:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0165:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0165:                        [--use-cpu-initialization]
g0165:                        [--empty-unused-memory-level {0,1,2}]
g0165:                        [--standalone-embedding-stage]
g0165:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0165:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0165:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0165:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0165:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0165:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0165:                        [--test-data-path [TEST_DATA_PATH ...]]
g0165:                        [--data-cache-path DATA_CACHE_PATH]
g0165:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0165:                        [--merge-file MERGE_FILE]
g0165:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0165:                        [--seq-length SEQ_LENGTH]
g0165:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0165:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0165:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0165:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0165:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0165:                        [--num-workers NUM_WORKERS]
g0165:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0165:                        [--tokenizer-model TOKENIZER_MODEL]
g0165:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0165:                        [--reset-attention-mask] [--eod-mask-loss]
g0165:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0165:                        [--return-data-index]
g0165:                        [--data-efficiency-curriculum-learning]
g0165:                        [--train-idx-path TRAIN_IDX_PATH]
g0165:                        [--train-desc-path TRAIN_DESC_PATH]
g0165:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0165:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0165:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0165:                        [--repeated-dataloader] [--adlr-autoresume]
g0165:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0165:                        [--ict-head-size ICT_HEAD_SIZE]
g0165:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0165:                        [--biencoder-shared-query-context-model]
g0165:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0165:                        [--titles-data-path TITLES_DATA_PATH]
g0165:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0165:                        [--use-one-sent-docs]
g0165:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0165:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0165:                        [--retriever-score-scaling]
g0165:                        [--block-data-path BLOCK_DATA_PATH]
g0165:                        [--embedding-path EMBEDDING_PATH]
g0165:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0165:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0165:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0165:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0165:                        [--patch-dim PATCH_DIM]
g0165:                        [--classes-fraction CLASSES_FRACTION]
g0165:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0165:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0165:                        [--vision-pretraining]
g0165:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0165:                        [--vision-backbone-type {vit,mit,swin}]
g0165:                        [--swin-backbone-type {tiny,base,h3}]
g0165:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0165:                        [--iter-per-epoch ITER_PER_EPOCH]
g0165:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0165:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0165:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0165:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0165:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0165:                        [--dino-norm-last-layer]
g0165:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0165:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0165:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0165:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0165:                        [--timing-log-level {0,1,2}]
g0165:                        [--no-barrier-with-level-1-timing]
g0165:                        [--timing-log-option {max,minmax,all}]
g0165:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0165:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0165:                        [--log-timers-to-tensorboard]
g0165:                        [--log-batch-size-to-tensorboard]
g0165:                        [--no-log-learnig-rate-to-tensorboard]
g0165:                        [--no-log-loss-scale-to-tensorboard]
g0165:                        [--log-validation-ppl-to-tensorboard]
g0165:                        [--log-optimizer-states-to-tensorboard]
g0165:                        [--log-memory-to-tensorboard]
g0165:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0165:                        [--wandb-entity WANDB_ENTITY]
g0165:                        [--wandb-project WANDB_PROJECT]
g0165:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0165:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0165:                        [--zero-contigious-gradients]
g0165:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0165:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0165:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0165:                        [--scattered-embeddings] [--split-transformers]
g0165:                        [--memory-centric-tiled-linear]
g0165:                        [--tile-factor TILE_FACTOR]
g0165:                        [--deepspeed-activation-checkpointing]
g0165:                        [--partition-activations] [--contigious-checkpointing]
g0165:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0165:                        [--profile-backward]
g0165:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0165:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0165:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0165:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0165:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0165:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0165:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0165:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0165:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0165:                        [--output-bert-embeddings]
g0165:                        [--bert-embedder-type {megatron,huggingface}]
g0165:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0165:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0165:                        [--transformer-impl {local,transformer_engine}]
g0165:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0165:                        [--fp8-amax-compute-algo {most_recent,max}]
g0165:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0165:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0165:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0165:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0165:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0165:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0165:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0165:                        [--retro-return-doc-ids] [--deepspeed]
g0165:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0165:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0165: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0165: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0165: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0165:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0165:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0165:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0165:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0165:                        [--expert-interval EXPERT_INTERVAL]
g0165:                        [--hidden-size HIDDEN_SIZE]
g0165:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0165:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0165:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0165:                        [--kv-channels KV_CHANNELS]
g0165:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0165:                        [--use-rotary-position-embeddings]
g0165:                        [--rotary-percent ROTARY_PERCENT]
g0165:                        [--no-position-embedding]
g0165:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0165:                        [--normalization {layernorm,rmsnorm}]
g0165:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0165:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0165:                        [--apply-residual-connection-post-layernorm]
g0165:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0165:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0165:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0165:                        [--untie-embeddings-and-output-weights]
g0165:                        [--embedding-weights-in-fp32]
g0165:                        [--attention-dropout ATTENTION_DROPOUT]
g0165:                        [--hidden-dropout HIDDEN_DROPOUT]
g0165:                        [--weight-decay WEIGHT_DECAY]
g0165:                        [--start-weight-decay START_WEIGHT_DECAY]
g0165:                        [--end-weight-decay END_WEIGHT_DECAY]
g0165:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0165:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0165:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0165:                        [--sgd-momentum SGD_MOMENTUM]
g0165:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0165:                        [--batch-size BATCH_SIZE]
g0165:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0165:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0165:                        [--recompute-activations]
g0165:                        [--recompute-granularity {full,selective}]
g0165:                        [--distribute-saved-activations]
g0165:                        [--recompute-method {uniform,block}]
g0165:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0165:                        [--checkpoint-activations]
g0165:                        [--distribute-checkpointed-activations]
g0165:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0165:                        [--train-iters TRAIN_ITERS]
g0165:                        [--train-samples TRAIN_SAMPLES]
g0165:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0165:                        [--log-interval LOG_INTERVAL]
g0165:                        [--exit-interval EXIT_INTERVAL]
g0165:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0165:                        [--exit-signal-handler]
g0165:                        [--tensorboard-dir TENSORBOARD_DIR]
g0165:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0165:                        [--no-bias-dropout-fusion]
g0165:                        [--disable-moe-token-dropping]
g0165:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0165:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0165:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0165:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0165:                        [--create-moe-param-group] [--use-flash-attn]
g0165:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0165:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0165:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0165:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0165:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0165:                        [--no-async-tensor-model-parallel-allreduce]
g0165:                        [--no-persist-layer-norm] [--sequence-parallel]
g0165:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0165:                        [--force-ds-sequence-parallel]
g0165:                        [--no-gradient-accumulation-fusion]
g0165:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0165:                        [--data-parallel-random-init]
g0165:                        [--init-method-std INIT_METHOD_STD]
g0165:                        [--init-method-xavier-uniform] [--lr LR]
g0165:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0165:                        [--lr-decay-iters LR_DECAY_ITERS]
g0165:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0165:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0165:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0165:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0165:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0165:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0165:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0165:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0165:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0165:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0165:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0165:                        [--no-initialization] [--use-checkpoint-args]
g0165:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0165:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0165:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0165:                        [--min-loss-scale MIN_LOSS_SCALE]
g0165:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0165:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0165:                        [--no-query-key-layer-scaling]
g0165:                        [--attention-softmax-in-fp32]
g0165:                        [--accumulate-allreduce-grads-in-fp32]
g0165:                        [--fp16-lm-cross-entropy]
g0165:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0165:                        [--enable-expert-tensor-parallelism]
g0165:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0165:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0165:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0165:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0165:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0165:                        [--overlap-p2p-communication]
g0165:                        [--distributed-backend {nccl,gloo,ccl}]
g0165:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0165:                        [--DDP-impl {local,torch,FSDP}]
g0165:                        [--no-contiguous-buffers-in-local-ddp]
g0165:                        [--no-scatter-gather-tensors-in-pipeline]
g0165:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0165:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0165:                        [--use-cpu-initialization]
g0165:                        [--empty-unused-memory-level {0,1,2}]
g0165:                        [--standalone-embedding-stage]
g0165:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0165:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0165:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0165:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0165:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0165:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0165:                        [--test-data-path [TEST_DATA_PATH ...]]
g0165:                        [--data-cache-path DATA_CACHE_PATH]
g0165:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0165:                        [--merge-file MERGE_FILE]
g0165:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0165:                        [--seq-length SEQ_LENGTH]
g0165:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0165:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0165:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0165:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0165:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0165:                        [--num-workers NUM_WORKERS]
g0165:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0165:                        [--tokenizer-model TOKENIZER_MODEL]
g0165:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0165:                        [--reset-attention-mask] [--eod-mask-loss]
g0165:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0165:                        [--return-data-index]
g0165:                        [--data-efficiency-curriculum-learning]
g0165:                        [--train-idx-path TRAIN_IDX_PATH]
g0165:                        [--train-desc-path TRAIN_DESC_PATH]
g0165:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0165:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0165:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0165:                        [--repeated-dataloader] [--adlr-autoresume]
g0165:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0165:                        [--ict-head-size ICT_HEAD_SIZE]
g0165:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0165:                        [--biencoder-shared-query-context-model]
g0165:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0165:                        [--titles-data-path TITLES_DATA_PATH]
g0165:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0165:                        [--use-one-sent-docs]
g0165:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0165:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0165:                        [--retriever-score-scaling]
g0165:                        [--block-data-path BLOCK_DATA_PATH]
g0165:                        [--embedding-path EMBEDDING_PATH]
g0165:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0165:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0165:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0165:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0165:                        [--patch-dim PATCH_DIM]
g0165:                        [--classes-fraction CLASSES_FRACTION]
g0165:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0165:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0165:                        [--vision-pretraining]
g0165:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0165:                        [--vision-backbone-type {vit,mit,swin}]
g0165:                        [--swin-backbone-type {tiny,base,h3}]
g0165:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0165:                        [--iter-per-epoch ITER_PER_EPOCH]
g0165:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0165:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0165:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0165:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0165:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0165:                        [--dino-norm-last-layer]
g0165:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0165:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0165:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0165:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0165:                        [--timing-log-level {0,1,2}]
g0165:                        [--no-barrier-with-level-1-timing]
g0165:                        [--timing-log-option {max,minmax,all}]
g0165:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0165:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0165:                        [--log-timers-to-tensorboard]
g0165:                        [--log-batch-size-to-tensorboard]
g0165:                        [--no-log-learnig-rate-to-tensorboard]
g0165:                        [--no-log-loss-scale-to-tensorboard]
g0165:                        [--log-validation-ppl-to-tensorboard]
g0165:                        [--log-optimizer-states-to-tensorboard]
g0165:                        [--log-memory-to-tensorboard]
g0165:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0165:                        [--wandb-entity WANDB_ENTITY]
g0165:                        [--wandb-project WANDB_PROJECT]
g0165:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0165:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0165:                        [--zero-contigious-gradients]
g0165:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0165:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0165:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0165:                        [--scattered-embeddings] [--split-transformers]
g0165:                        [--memory-centric-tiled-linear]
g0165:                        [--tile-factor TILE_FACTOR]
g0165:                        [--deepspeed-activation-checkpointing]
g0165:                        [--partition-activations] [--contigious-checkpointing]
g0165:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0165:                        [--profile-backward]
g0165:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0165:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0165:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0165:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0165:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0165:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0165:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0165:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0165:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0165:                        [--output-bert-embeddings]
g0165:                        [--bert-embedder-type {megatron,huggingface}]
g0165:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0165:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0165:                        [--transformer-impl {local,transformer_engine}]
g0165:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0165:                        [--fp8-amax-compute-algo {most_recent,max}]
g0165:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0165:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0165:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0165:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0165:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0165:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0165:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0165:                        [--retro-return-doc-ids] [--deepspeed]
g0165:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0165:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0165: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0165: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0165:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0165:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0165:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0165:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0165:                        [--expert-interval EXPERT_INTERVAL]
g0165:                        [--hidden-size HIDDEN_SIZE]
g0165:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0165:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0165:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0165:                        [--kv-channels KV_CHANNELS]
g0165:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0165:                        [--use-rotary-position-embeddings]
g0165:                        [--rotary-percent ROTARY_PERCENT]
g0165:                        [--no-position-embedding]
g0165:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0165:                        [--normalization {layernorm,rmsnorm}]
g0165:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0165:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0165:                        [--apply-residual-connection-post-layernorm]
g0165:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0165:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0165:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0165:                        [--untie-embeddings-and-output-weights]
g0165:                        [--embedding-weights-in-fp32]
g0165:                        [--attention-dropout ATTENTION_DROPOUT]
g0165:                        [--hidden-dropout HIDDEN_DROPOUT]
g0165:                        [--weight-decay WEIGHT_DECAY]
g0165:                        [--start-weight-decay START_WEIGHT_DECAY]
g0165:                        [--end-weight-decay END_WEIGHT_DECAY]
g0165:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0165:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0165:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0165:                        [--sgd-momentum SGD_MOMENTUM]
g0165:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0165:                        [--batch-size BATCH_SIZE]
g0165:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0165:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0165:                        [--recompute-activations]
g0165:                        [--recompute-granularity {full,selective}]
g0165:                        [--distribute-saved-activations]
g0165:                        [--recompute-method {uniform,block}]
g0165:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0165:                        [--checkpoint-activations]
g0165:                        [--distribute-checkpointed-activations]
g0165:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0165:                        [--train-iters TRAIN_ITERS]
g0165:                        [--train-samples TRAIN_SAMPLES]
g0165:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0165:                        [--log-interval LOG_INTERVAL]
g0165:                        [--exit-interval EXIT_INTERVAL]
g0165:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0165:                        [--exit-signal-handler]
g0165:                        [--tensorboard-dir TENSORBOARD_DIR]
g0165:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0165:                        [--no-bias-dropout-fusion]
g0165:                        [--disable-moe-token-dropping]
g0165:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0165:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0165:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0165:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0165:                        [--create-moe-param-group] [--use-flash-attn]
g0165:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0165:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0165:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0165:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0165:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0165:                        [--no-async-tensor-model-parallel-allreduce]
g0165:                        [--no-persist-layer-norm] [--sequence-parallel]
g0165:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0165:                        [--force-ds-sequence-parallel]
g0165:                        [--no-gradient-accumulation-fusion]
g0165:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0165:                        [--data-parallel-random-init]
g0165:                        [--init-method-std INIT_METHOD_STD]
g0165:                        [--init-method-xavier-uniform] [--lr LR]
g0165:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0165:                        [--lr-decay-iters LR_DECAY_ITERS]
g0165:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0165:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0165:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0165:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0165:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0165:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0165:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0165:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0165:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0165:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0165:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0165:                        [--no-initialization] [--use-checkpoint-args]
g0165:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0165:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0165:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0165:                        [--min-loss-scale MIN_LOSS_SCALE]
g0165:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0165:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0165:                        [--no-query-key-layer-scaling]
g0165:                        [--attention-softmax-in-fp32]
g0165:                        [--accumulate-allreduce-grads-in-fp32]
g0165:                        [--fp16-lm-cross-entropy]
g0165:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0165:                        [--enable-expert-tensor-parallelism]
g0165:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0165:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0165:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0165:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0165:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0165:                        [--overlap-p2p-communication]
g0165:                        [--distributed-backend {nccl,gloo,ccl}]
g0165:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0165:                        [--DDP-impl {local,torch,FSDP}]
g0165:                        [--no-contiguous-buffers-in-local-ddp]
g0165:                        [--no-scatter-gather-tensors-in-pipeline]
g0165:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0165:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0165:                        [--use-cpu-initialization]
g0165:                        [--empty-unused-memory-level {0,1,2}]
g0165:                        [--standalone-embedding-stage]
g0165:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0165:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0165:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0165:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0165:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0165:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0165:                        [--test-data-path [TEST_DATA_PATH ...]]
g0165:                        [--data-cache-path DATA_CACHE_PATH]
g0165:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0165:                        [--merge-file MERGE_FILE]
g0165:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0165:                        [--seq-length SEQ_LENGTH]
g0165:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0165:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0165:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0165:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0165:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0165:                        [--num-workers NUM_WORKERS]
g0165:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0165:                        [--tokenizer-model TOKENIZER_MODEL]
g0165:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0165:                        [--reset-attention-mask] [--eod-mask-loss]
g0165:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0165:                        [--return-data-index]
g0165:                        [--data-efficiency-curriculum-learning]
g0165:                        [--train-idx-path TRAIN_IDX_PATH]
g0165:                        [--train-desc-path TRAIN_DESC_PATH]
g0165:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0165:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0165:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0165:                        [--repeated-dataloader] [--adlr-autoresume]
g0165:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0165:                        [--ict-head-size ICT_HEAD_SIZE]
g0165:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0165:                        [--biencoder-shared-query-context-model]
g0165:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0165:                        [--titles-data-path TITLES_DATA_PATH]
g0165:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0165:                        [--use-one-sent-docs]
g0165:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0165:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0165:                        [--retriever-score-scaling]
g0165:                        [--block-data-path BLOCK_DATA_PATH]
g0165:                        [--embedding-path EMBEDDING_PATH]
g0165:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0165:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0165:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0165:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0165:                        [--patch-dim PATCH_DIM]
g0165:                        [--classes-fraction CLASSES_FRACTION]
g0165:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0165:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0165:                        [--vision-pretraining]
g0165:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0165:                        [--vision-backbone-type {vit,mit,swin}]
g0165:                        [--swin-backbone-type {tiny,base,h3}]
g0165:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0165:                        [--iter-per-epoch ITER_PER_EPOCH]
g0165:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0165:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0165:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0165:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0165:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0165:                        [--dino-norm-last-layer]
g0165:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0165:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0165:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0165:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0165:                        [--timing-log-level {0,1,2}]
g0165:                        [--no-barrier-with-level-1-timing]
g0165:                        [--timing-log-option {max,minmax,all}]
g0165:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0165:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0165:                        [--log-timers-to-tensorboard]
g0165:                        [--log-batch-size-to-tensorboard]
g0165:                        [--no-log-learnig-rate-to-tensorboard]
g0165:                        [--no-log-loss-scale-to-tensorboard]
g0165:                        [--log-validation-ppl-to-tensorboard]
g0165:                        [--log-optimizer-states-to-tensorboard]
g0165:                        [--log-memory-to-tensorboard]
g0165:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0165:                        [--wandb-entity WANDB_ENTITY]
g0165:                        [--wandb-project WANDB_PROJECT]
g0165:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0165:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0165:                        [--zero-contigious-gradients]
g0165:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0165:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0165:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0165:                        [--scattered-embeddings] [--split-transformers]
g0165:                        [--memory-centric-tiled-linear]
g0165:                        [--tile-factor TILE_FACTOR]
g0165:                        [--deepspeed-activation-checkpointing]
g0165:                        [--partition-activations] [--contigious-checkpointing]
g0165:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0165:                        [--profile-backward]
g0165:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0165:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0165:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0165:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0165:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0165:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0165:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0165:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0165:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0165:                        [--output-bert-embeddings]
g0165:                        [--bert-embedder-type {megatron,huggingface}]
g0165:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0165:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0165:                        [--transformer-impl {local,transformer_engine}]
g0165:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0165:                        [--fp8-amax-compute-algo {most_recent,max}]
g0165:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0165:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0165:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0165:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0165:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0165:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0165:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0165:                        [--retro-return-doc-ids] [--deepspeed]
g0165:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0165:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0165: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0159: --------------------------------------------------
g0159: DeepSpeed C++/CUDA extension op report
g0159: --------------------------------------------------
g0159: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0159:       runtime if needed. Op compatibility means that your system
g0159:       meet the required dependencies to JIT install the op.
g0159: --------------------------------------------------
g0159: JIT compiled ops requires ninja
g0159: --------------------------------------------------
g0159: DeepSpeed C++/CUDA extension op report
g0159: --------------------------------------------------
g0159: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0159:       runtime if needed. Op compatibility means that your system
g0159:       meet the required dependencies to JIT install the op.
g0159: --------------------------------------------------
g0159: JIT compiled ops requires ninja
g0159: --------------------------------------------------
g0159: DeepSpeed C++/CUDA extension op report
g0159: --------------------------------------------------
g0159: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0159:       runtime if needed. Op compatibility means that your system
g0159:       meet the required dependencies to JIT install the op.
g0159: --------------------------------------------------
g0159: JIT compiled ops requires ninja
g0159: --------------------------------------------------
g0159: DeepSpeed C++/CUDA extension op report
g0159: --------------------------------------------------
g0159: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0159:       runtime if needed. Op compatibility means that your system
g0159:       meet the required dependencies to JIT install the op.
g0159: --------------------------------------------------
g0159: JIT compiled ops requires ninja
g0159: ninjaninjaninjaninja   .................. .................................... ..................  [92m[OKAY][0m [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0159: 
g0159: 
g0159: 
g0159: ----------------------------------------------------------------------------------------------------
g0159: ----------------------------------------------------------------------------------------------------
g0159: 
g0159: 
g0159: op nameop name op name op name................ ................  ................ ................installed installed installedinstalled    ...... ..  compatible compatiblecompatible
g0159: compatible
g0159: 
g0159: 
g0159: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0159: --------------------------------------------------
g0159: 
g0159: 
g0159: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0159: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0159: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0159: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0159: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0159: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0159: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0159: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: async_iocpu_lion  ..............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0159: 
g0159: fused_adam .............[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0159: [92m[YES][0m ......evoformer_attn  [92m[OKAY][0m.........
g0159:  [93m[NO][0m .......cpu_adam  [93m[NO][0m...............
g0159:  [92m[YES][0m fused_lamb......  .............[92m[OKAY][0m 
g0159: [92m[YES][0m ...... cpu_adagrad[92m[OKAY][0m 
g0159: ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: fused_lioncpu_lion  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0159: 
g0159: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0159: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0159: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0159: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0159: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0159: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0159: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0159: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0159: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0159: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0159: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0159: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0159: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0159: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0159: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0159: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0159: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0159: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0159: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0159: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0159: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0159: sparse_attn ............ [93m[NO][0m[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible 
g0159: ....... [93m[NO][0msparse_attn
g0159:  ............ [93m[NO][0m ....... [93m[NO][0m
g0159: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0159: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0159: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0159: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0159: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0159: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0159: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0159: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0159: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0159: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0159: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0159: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0159: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0159: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0159: --------------------------------------------------
g0159: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0159: --------------------------------------------------
g0159: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0159: --------------------------------------------------
g0159: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0159: --------------------------------------------------
g0159: DeepSpeed general environment info:
g0159: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0159: torch version .................... 2.0.1+cu118
g0159: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0159: deepspeed info ................... 0.12.4, unknown, unknown
g0159: torch cuda version ............... 11.8
g0159: torch hip version ................ None
g0159: nvcc version ..................... 11.8
g0159: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0159: shared memory (/dev/shm) size .... 188.13 GB
g0164: --------------------------------------------------
g0164: DeepSpeed C++/CUDA extension op report
g0164: --------------------------------------------------
g0164: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0164:       runtime if needed. Op compatibility means that your system
g0164:       meet the required dependencies to JIT install the op.
g0164: --------------------------------------------------
g0164: JIT compiled ops requires ninja
g0164: --------------------------------------------------
g0164: DeepSpeed C++/CUDA extension op report
g0164: --------------------------------------------------
g0164: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0164:       runtime if needed. Op compatibility means that your system
g0164:       meet the required dependencies to JIT install the op.
g0164: --------------------------------------------------
g0164: JIT compiled ops requires ninja
g0164: --------------------------------------------------
g0164: DeepSpeed C++/CUDA extension op report
g0164: --------------------------------------------------
g0164: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0164:       runtime if needed. Op compatibility means that your system
g0164:       meet the required dependencies to JIT install the op.
g0164: --------------------------------------------------
g0164: JIT compiled ops requires ninja
g0164: --------------------------------------------------
g0164: DeepSpeed C++/CUDA extension op report
g0164: --------------------------------------------------
g0164: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0164:       runtime if needed. Op compatibility means that your system
g0164:       meet the required dependencies to JIT install the op.
g0164: --------------------------------------------------
g0164: JIT compiled ops requires ninja
g0159: DeepSpeed general environment info:
g0159: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0159: torch version .................... 2.0.1+cu118
g0159: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0159: deepspeed info ................... 0.12.4, unknown, unknown
g0159: torch cuda version ............... 11.8
g0159: torch hip version ................ None
g0159: nvcc version ..................... 11.8
g0159: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0159: shared memory (/dev/shm) size .... 188.13 GB
g0159: DeepSpeed general environment info:
g0159: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0159: torch version .................... 2.0.1+cu118
g0159: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0159: deepspeed info ................... 0.12.4, unknown, unknown
g0159: torch cuda version ............... 11.8
g0159: torch hip version ................ None
g0159: nvcc version ..................... 11.8
g0159: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0159: shared memory (/dev/shm) size .... 188.13 GB
g0159: DeepSpeed general environment info:
g0159: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0159: torch version .................... 2.0.1+cu118
g0159: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0159: deepspeed info ................... 0.12.4, unknown, unknown
g0159: torch cuda version ............... 11.8
g0159: torch hip version ................ None
g0159: nvcc version ..................... 11.8
g0159: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0159: shared memory (/dev/shm) size .... 188.13 GB
g0164: ninjaninja ninjaninja ....................................    [92m[OKAY][0m..................[92m[OKAY][0m.................. 
g0164: 
g0164: [92m[OKAY][0m 
g0164: ----------------------------------------------------------------------------------------------------[92m[OKAY][0m
g0164: 
g0164: --------------------------------------------------
g0164: 
g0164: op nameop name  op name--------------------------------------------------................................   ................
g0164: installed installed installed op name....   .. compatible ................compatible
g0164: compatible 
g0164: 
g0164: --------------------------------------------------installed--------------------------------------------------
g0164: -------------------------------------------------- 
g0164: 
g0164: .. compatible
g0164: --------------------------------------------------
g0163: --------------------------------------------------
g0163: DeepSpeed C++/CUDA extension op report
g0163: --------------------------------------------------
g0163: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0163:       runtime if needed. Op compatibility means that your system
g0163:       meet the required dependencies to JIT install the op.
g0163: --------------------------------------------------
g0163: JIT compiled ops requires ninja
g0161: --------------------------------------------------
g0161: DeepSpeed C++/CUDA extension op report
g0161: --------------------------------------------------
g0161: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0161:       runtime if needed. Op compatibility means that your system
g0161:       meet the required dependencies to JIT install the op.
g0161: --------------------------------------------------
g0161: JIT compiled ops requires ninja
g0161: --------------------------------------------------
g0161: DeepSpeed C++/CUDA extension op report
g0161: --------------------------------------------------
g0161: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0161:       runtime if needed. Op compatibility means that your system
g0161:       meet the required dependencies to JIT install the op.
g0161: --------------------------------------------------
g0161: JIT compiled ops requires ninja
g0161: --------------------------------------------------
g0161: DeepSpeed C++/CUDA extension op report
g0161: --------------------------------------------------
g0161: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0161:       runtime if needed. Op compatibility means that your system
g0161:       meet the required dependencies to JIT install the op.
g0161: --------------------------------------------------
g0161: JIT compiled ops requires ninja
g0163: --------------------------------------------------
g0163: DeepSpeed C++/CUDA extension op report
g0163: --------------------------------------------------
g0163: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0163:       runtime if needed. Op compatibility means that your system
g0163:       meet the required dependencies to JIT install the op.
g0163: --------------------------------------------------
g0163: JIT compiled ops requires ninja
g0163: --------------------------------------------------
g0163: DeepSpeed C++/CUDA extension op report
g0163: --------------------------------------------------
g0163: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0163:       runtime if needed. Op compatibility means that your system
g0163:       meet the required dependencies to JIT install the op.
g0163: --------------------------------------------------
g0163: JIT compiled ops requires ninja
g0163: --------------------------------------------------
g0163: DeepSpeed C++/CUDA extension op report
g0163: --------------------------------------------------
g0163: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0163:       runtime if needed. Op compatibility means that your system
g0163:       meet the required dependencies to JIT install the op.
g0163: --------------------------------------------------
g0163: JIT compiled ops requires ninja
g0161: ninjaninjaninja  ....................................   ..................[92m[OKAY][0m[92m[OKAY][0m 
g0161: 
g0161: [92m[OKAY][0m--------------------------------------------------
g0161: --------------------------------------------------
g0161: 
g0161: --------------------------------------------------op nameop name
g0161:   ................................op name   installedinstalled................   ....installed   compatiblecompatible..
g0161:  
g0161: --------------------------------------------------compatible--------------------------------------------------
g0161: 
g0161: 
g0161: --------------------------------------------------
g0159: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0159: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0159: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0159: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0163: ninja .................. [92m[OKAY][0m
g0163: --------------------------------------------------
g0163: op name ................ installed .. compatible
g0163: --------------------------------------------------
g0163: ninjaninja  ....................................  [92m[OKAY][0m[92m[OKAY][0m
g0163: 
g0163: ----------------------------------------------------------------------------------------------------
g0163: 
g0163: op nameop name  ................................  installedinstalled  ....  compatiblecompatible
g0163: 
g0163: ----------------------------------------------------------------------------------------------------
g0163: 
g0163: ninja .................. [92m[OKAY][0m
g0163: --------------------------------------------------
g0163: op name ................ installed .. compatible
g0163: --------------------------------------------------
g0156: [2024-08-12 03:40:18,540] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 175867
g0159: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0159:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0159:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0159:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0159:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0159:                        [--expert-interval EXPERT_INTERVAL]
g0159:                        [--hidden-size HIDDEN_SIZE]
g0159:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0159:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0159:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0159:                        [--kv-channels KV_CHANNELS]
g0159:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0159:                        [--use-rotary-position-embeddings]
g0159:                        [--rotary-percent ROTARY_PERCENT]
g0159:                        [--no-position-embedding]
g0159:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0159:                        [--normalization {layernorm,rmsnorm}]
g0159:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0159:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0159:                        [--apply-residual-connection-post-layernorm]
g0159:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0159:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0159:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0159:                        [--untie-embeddings-and-output-weights]
g0159:                        [--embedding-weights-in-fp32]
g0159:                        [--attention-dropout ATTENTION_DROPOUT]
g0159:                        [--hidden-dropout HIDDEN_DROPOUT]
g0159:                        [--weight-decay WEIGHT_DECAY]
g0159:                        [--start-weight-decay START_WEIGHT_DECAY]
g0159:                        [--end-weight-decay END_WEIGHT_DECAY]
g0159:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0159:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0159:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0159:                        [--sgd-momentum SGD_MOMENTUM]
g0159:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0159:                        [--batch-size BATCH_SIZE]
g0159:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0159:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0159:                        [--recompute-activations]
g0159:                        [--recompute-granularity {full,selective}]
g0159:                        [--distribute-saved-activations]
g0159:                        [--recompute-method {uniform,block}]
g0159:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0159:                        [--checkpoint-activations]
g0159:                        [--distribute-checkpointed-activations]
g0159:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0159:                        [--train-iters TRAIN_ITERS]
g0159:                        [--train-samples TRAIN_SAMPLES]
g0159:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0159:                        [--log-interval LOG_INTERVAL]
g0159:                        [--exit-interval EXIT_INTERVAL]
g0159:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0159:                        [--exit-signal-handler]
g0159:                        [--tensorboard-dir TENSORBOARD_DIR]
g0159:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0159:                        [--no-bias-dropout-fusion]
g0159:                        [--disable-moe-token-dropping]
g0159:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0159:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0159:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0159:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0159:                        [--create-moe-param-group] [--use-flash-attn]
g0159:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0159:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0159:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0159:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0159:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0159:                        [--no-async-tensor-model-parallel-allreduce]
g0159:                        [--no-persist-layer-norm] [--sequence-parallel]
g0159:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0159:                        [--force-ds-sequence-parallel]
g0159:                        [--no-gradient-accumulation-fusion]
g0159:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0159:                        [--data-parallel-random-init]
g0159:                        [--init-method-std INIT_METHOD_STD]
g0159:                        [--init-method-xavier-uniform] [--lr LR]
g0159:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0159:                        [--lr-decay-iters LR_DECAY_ITERS]
g0159:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0159:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0159:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0159:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0159:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0159:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0159:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0159:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0159:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0159:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0159:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0159:                        [--no-initialization] [--use-checkpoint-args]
g0159:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0159:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0159:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0159:                        [--min-loss-scale MIN_LOSS_SCALE]
g0159:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0159:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0159:                        [--no-query-key-layer-scaling]
g0159:                        [--attention-softmax-in-fp32]
g0159:                        [--accumulate-allreduce-grads-in-fp32]
g0159:                        [--fp16-lm-cross-entropy]
g0159:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0159:                        [--enable-expert-tensor-parallelism]
g0159:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0159:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0159:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0159:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0159:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0159:                        [--overlap-p2p-communication]
g0159:                        [--distributed-backend {nccl,gloo,ccl}]
g0159:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0159:                        [--DDP-impl {local,torch,FSDP}]
g0159:                        [--no-contiguous-buffers-in-local-ddp]
g0159:                        [--no-scatter-gather-tensors-in-pipeline]
g0159:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0159:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0159:                        [--use-cpu-initialization]
g0159:                        [--empty-unused-memory-level {0,1,2}]
g0159:                        [--standalone-embedding-stage]
g0159:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0159:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0159:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0159:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0159:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0159:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0159:                        [--test-data-path [TEST_DATA_PATH ...]]
g0159:                        [--data-cache-path DATA_CACHE_PATH]
g0159:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0159:                        [--merge-file MERGE_FILE]
g0159:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0159:                        [--seq-length SEQ_LENGTH]
g0159:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0159:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0159:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0159:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0159:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0159:                        [--num-workers NUM_WORKERS]
g0159:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0159:                        [--tokenizer-model TOKENIZER_MODEL]
g0159:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0159:                        [--reset-attention-mask] [--eod-mask-loss]
g0159:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0159:                        [--return-data-index]
g0159:                        [--data-efficiency-curriculum-learning]
g0159:                        [--train-idx-path TRAIN_IDX_PATH]
g0159:                        [--train-desc-path TRAIN_DESC_PATH]
g0159:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0159:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0159:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0159:                        [--repeated-dataloader] [--adlr-autoresume]
g0159:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0159:                        [--ict-head-size ICT_HEAD_SIZE]
g0159:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0159:                        [--biencoder-shared-query-context-model]
g0159:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0159:                        [--titles-data-path TITLES_DATA_PATH]
g0159:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0159:                        [--use-one-sent-docs]
g0159:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0159:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0159:                        [--retriever-score-scaling]
g0159:                        [--block-data-path BLOCK_DATA_PATH]
g0159:                        [--embedding-path EMBEDDING_PATH]
g0159:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0159:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0159:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0159:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0159:                        [--patch-dim PATCH_DIM]
g0159:                        [--classes-fraction CLASSES_FRACTION]
g0159:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0159:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0159:                        [--vision-pretraining]
g0159:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0159:                        [--vision-backbone-type {vit,mit,swin}]
g0159:                        [--swin-backbone-type {tiny,base,h3}]
g0159:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0159:                        [--iter-per-epoch ITER_PER_EPOCH]
g0159:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0159:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0159:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0159:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0159:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0159:                        [--dino-norm-last-layer]
g0159:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0159:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0159:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0159:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0159:                        [--timing-log-level {0,1,2}]
g0159:                        [--no-barrier-with-level-1-timing]
g0159:                        [--timing-log-option {max,minmax,all}]
g0159:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0159:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0159:                        [--log-timers-to-tensorboard]
g0159:                        [--log-batch-size-to-tensorboard]
g0159:                        [--no-log-learnig-rate-to-tensorboard]
g0159:                        [--no-log-loss-scale-to-tensorboard]
g0159:                        [--log-validation-ppl-to-tensorboard]
g0159:                        [--log-optimizer-states-to-tensorboard]
g0159:                        [--log-memory-to-tensorboard]
g0159:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0159:                        [--wandb-entity WANDB_ENTITY]
g0159:                        [--wandb-project WANDB_PROJECT]
g0159:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0159:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0159:                        [--zero-contigious-gradients]
g0159:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0159:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0159:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0159:                        [--scattered-embeddings] [--split-transformers]
g0159:                        [--memory-centric-tiled-linear]
g0159:                        [--tile-factor TILE_FACTOR]
g0159:                        [--deepspeed-activation-checkpointing]
g0159:                        [--partition-activations] [--contigious-checkpointing]
g0159:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0159:                        [--profile-backward]
g0159:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0159:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0159:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0159:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0159:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0159:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0159:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0159:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0159:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0159:                        [--output-bert-embeddings]
g0159:                        [--bert-embedder-type {megatron,huggingface}]
g0159:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0159:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0159:                        [--transformer-impl {local,transformer_engine}]
g0159:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0159:                        [--fp8-amax-compute-algo {most_recent,max}]
g0159:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0159:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0159:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0159:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0159:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0159:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0159:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0159:                        [--retro-return-doc-ids] [--deepspeed]
g0159:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0159:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0159: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0159: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0159:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0159:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0159:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0159:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0159:                        [--expert-interval EXPERT_INTERVAL]
g0159:                        [--hidden-size HIDDEN_SIZE]
g0159:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0159:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0159:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0159:                        [--kv-channels KV_CHANNELS]
g0159:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0159:                        [--use-rotary-position-embeddings]
g0159:                        [--rotary-percent ROTARY_PERCENT]
g0159:                        [--no-position-embedding]
g0159:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0159:                        [--normalization {layernorm,rmsnorm}]
g0159:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0159:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0159:                        [--apply-residual-connection-post-layernorm]
g0159:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0159:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0159:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0159:                        [--untie-embeddings-and-output-weights]
g0159:                        [--embedding-weights-in-fp32]
g0159:                        [--attention-dropout ATTENTION_DROPOUT]
g0159:                        [--hidden-dropout HIDDEN_DROPOUT]
g0159:                        [--weight-decay WEIGHT_DECAY]
g0159:                        [--start-weight-decay START_WEIGHT_DECAY]
g0159:                        [--end-weight-decay END_WEIGHT_DECAY]
g0159:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0159:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0159:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0159:                        [--sgd-momentum SGD_MOMENTUM]
g0159:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0159:                        [--batch-size BATCH_SIZE]
g0159:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0159:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0159:                        [--recompute-activations]
g0159:                        [--recompute-granularity {full,selective}]
g0159:                        [--distribute-saved-activations]
g0159:                        [--recompute-method {uniform,block}]
g0159:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0159:                        [--checkpoint-activations]
g0159:                        [--distribute-checkpointed-activations]
g0159:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0159:                        [--train-iters TRAIN_ITERS]
g0159:                        [--train-samples TRAIN_SAMPLES]
g0159:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0159:                        [--log-interval LOG_INTERVAL]
g0159:                        [--exit-interval EXIT_INTERVAL]
g0159:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0159:                        [--exit-signal-handler]
g0159:                        [--tensorboard-dir TENSORBOARD_DIR]
g0159:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0159:                        [--no-bias-dropout-fusion]
g0159:                        [--disable-moe-token-dropping]
g0159:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0159:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0159:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0159:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0159:                        [--create-moe-param-group] [--use-flash-attn]
g0159:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0159:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0159:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0159:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0159:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0159:                        [--no-async-tensor-model-parallel-allreduce]
g0159:                        [--no-persist-layer-norm] [--sequence-parallel]
g0159:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0159:                        [--force-ds-sequence-parallel]
g0159:                        [--no-gradient-accumulation-fusion]
g0159:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0159:                        [--data-parallel-random-init]
g0159:                        [--init-method-std INIT_METHOD_STD]
g0159:                        [--init-method-xavier-uniform] [--lr LR]
g0159:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0159:                        [--lr-decay-iters LR_DECAY_ITERS]
g0159:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0159:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0159:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0159:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0159:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0159:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0159:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0159:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0159:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0159:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0159:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0159:                        [--no-initialization] [--use-checkpoint-args]
g0159:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0159:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0159:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0159:                        [--min-loss-scale MIN_LOSS_SCALE]
g0159:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0159:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0159:                        [--no-query-key-layer-scaling]
g0159:                        [--attention-softmax-in-fp32]
g0159:                        [--accumulate-allreduce-grads-in-fp32]
g0159:                        [--fp16-lm-cross-entropy]
g0159:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0159:                        [--enable-expert-tensor-parallelism]
g0159:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0159:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0159:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0159:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0159:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0159:                        [--overlap-p2p-communication]
g0159:                        [--distributed-backend {nccl,gloo,ccl}]
g0159:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0159:                        [--DDP-impl {local,torch,FSDP}]
g0159:                        [--no-contiguous-buffers-in-local-ddp]
g0159:                        [--no-scatter-gather-tensors-in-pipeline]
g0159:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0159:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0159:                        [--use-cpu-initialization]
g0159:                        [--empty-unused-memory-level {0,1,2}]
g0159:                        [--standalone-embedding-stage]
g0159:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0159:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0159:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0159:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0159:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0159:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0159:                        [--test-data-path [TEST_DATA_PATH ...]]
g0159:                        [--data-cache-path DATA_CACHE_PATH]
g0159:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0159:                        [--merge-file MERGE_FILE]
g0159:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0159:                        [--seq-length SEQ_LENGTH]
g0159:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0159:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0159:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0159:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0159:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0159:                        [--num-workers NUM_WORKERS]
g0159:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0159:                        [--tokenizer-model TOKENIZER_MODEL]
g0159:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0159:                        [--reset-attention-mask] [--eod-mask-loss]
g0159:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0159:                        [--return-data-index]
g0159:                        [--data-efficiency-curriculum-learning]
g0159:                        [--train-idx-path TRAIN_IDX_PATH]
g0159:                        [--train-desc-path TRAIN_DESC_PATH]
g0159:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0159:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0159:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0159:                        [--repeated-dataloader] [--adlr-autoresume]
g0159:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0159:                        [--ict-head-size ICT_HEAD_SIZE]
g0159:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0159:                        [--biencoder-shared-query-context-model]
g0159:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0159:                        [--titles-data-path TITLES_DATA_PATH]
g0159:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0159:                        [--use-one-sent-docs]
g0159:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0159:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0159:                        [--retriever-score-scaling]
g0159:                        [--block-data-path BLOCK_DATA_PATH]
g0159:                        [--embedding-path EMBEDDING_PATH]
g0159:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0159:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0159:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0159:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0159:                        [--patch-dim PATCH_DIM]
g0159:                        [--classes-fraction CLASSES_FRACTION]
g0159:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0159:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0159:                        [--vision-pretraining]
g0159:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0159:                        [--vision-backbone-type {vit,mit,swin}]
g0159:                        [--swin-backbone-type {tiny,base,h3}]
g0159:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0159:                        [--iter-per-epoch ITER_PER_EPOCH]
g0159:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0159:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0159:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0159:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0159:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0159:                        [--dino-norm-last-layer]
g0159:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0159:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0159:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0159:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0159:                        [--timing-log-level {0,1,2}]
g0159:                        [--no-barrier-with-level-1-timing]
g0159:                        [--timing-log-option {max,minmax,all}]
g0159:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0159:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0159:                        [--log-timers-to-tensorboard]
g0159:                        [--log-batch-size-to-tensorboard]
g0159:                        [--no-log-learnig-rate-to-tensorboard]
g0159:                        [--no-log-loss-scale-to-tensorboard]
g0159:                        [--log-validation-ppl-to-tensorboard]
g0159:                        [--log-optimizer-states-to-tensorboard]
g0159:                        [--log-memory-to-tensorboard]
g0159:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0159:                        [--wandb-entity WANDB_ENTITY]
g0159:                        [--wandb-project WANDB_PROJECT]
g0159:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0159:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0159:                        [--zero-contigious-gradients]
g0159:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0159:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0159:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0159:                        [--scattered-embeddings] [--split-transformers]
g0159:                        [--memory-centric-tiled-linear]
g0159:                        [--tile-factor TILE_FACTOR]
g0159:                        [--deepspeed-activation-checkpointing]
g0159:                        [--partition-activations] [--contigious-checkpointing]
g0159:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0159:                        [--profile-backward]
g0159:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0159:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0159:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0159:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0159:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0159:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0159:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0159:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0159:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0159:                        [--output-bert-embeddings]
g0159:                        [--bert-embedder-type {megatron,huggingface}]
g0159:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0159:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0159:                        [--transformer-impl {local,transformer_engine}]
g0159:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0159:                        [--fp8-amax-compute-algo {most_recent,max}]
g0159:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0159:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0159:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0159:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0159:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0159:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0159:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0159:                        [--retro-return-doc-ids] [--deepspeed]
g0159:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0159:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0159: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0159: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0159:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0159:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0159:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0159:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0159:                        [--expert-interval EXPERT_INTERVAL]
g0159:                        [--hidden-size HIDDEN_SIZE]
g0159:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0159:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0159:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0159:                        [--kv-channels KV_CHANNELS]
g0159:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0159:                        [--use-rotary-position-embeddings]
g0159:                        [--rotary-percent ROTARY_PERCENT]
g0159:                        [--no-position-embedding]
g0159:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0159:                        [--normalization {layernorm,rmsnorm}]
g0159:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0159:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0159:                        [--apply-residual-connection-post-layernorm]
g0159:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0159:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0159:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0159:                        [--untie-embeddings-and-output-weights]
g0159:                        [--embedding-weights-in-fp32]
g0159:                        [--attention-dropout ATTENTION_DROPOUT]
g0159:                        [--hidden-dropout HIDDEN_DROPOUT]
g0159:                        [--weight-decay WEIGHT_DECAY]
g0159:                        [--start-weight-decay START_WEIGHT_DECAY]
g0159:                        [--end-weight-decay END_WEIGHT_DECAY]
g0159:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0159:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0159:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0159:                        [--sgd-momentum SGD_MOMENTUM]
g0159:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0159:                        [--batch-size BATCH_SIZE]
g0159:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0159:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0159:                        [--recompute-activations]
g0159:                        [--recompute-granularity {full,selective}]
g0159:                        [--distribute-saved-activations]
g0159:                        [--recompute-method {uniform,block}]
g0159:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0159:                        [--checkpoint-activations]
g0159:                        [--distribute-checkpointed-activations]
g0159:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0159:                        [--train-iters TRAIN_ITERS]
g0159:                        [--train-samples TRAIN_SAMPLES]
g0159:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0159:                        [--log-interval LOG_INTERVAL]
g0159:                        [--exit-interval EXIT_INTERVAL]
g0159:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0159:                        [--exit-signal-handler]
g0159:                        [--tensorboard-dir TENSORBOARD_DIR]
g0159:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0159:                        [--no-bias-dropout-fusion]
g0159:                        [--disable-moe-token-dropping]
g0159:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0159:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0159:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0159:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0159:                        [--create-moe-param-group] [--use-flash-attn]
g0159:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0159:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0159:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0159:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0159:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0159:                        [--no-async-tensor-model-parallel-allreduce]
g0159:                        [--no-persist-layer-norm] [--sequence-parallel]
g0159:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0159:                        [--force-ds-sequence-parallel]
g0159:                        [--no-gradient-accumulation-fusion]
g0159:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0159:                        [--data-parallel-random-init]
g0159:                        [--init-method-std INIT_METHOD_STD]
g0159:                        [--init-method-xavier-uniform] [--lr LR]
g0159:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0159:                        [--lr-decay-iters LR_DECAY_ITERS]
g0159:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0159:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0159:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0159:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0159:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0159:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0159:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0159:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0159:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0159:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0159:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0159:                        [--no-initialization] [--use-checkpoint-args]
g0159:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0159:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0159:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0159:                        [--min-loss-scale MIN_LOSS_SCALE]
g0159:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0159:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0159:                        [--no-query-key-layer-scaling]
g0159:                        [--attention-softmax-in-fp32]
g0159:                        [--accumulate-allreduce-grads-in-fp32]
g0159:                        [--fp16-lm-cross-entropy]
g0159:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0159:                        [--enable-expert-tensor-parallelism]
g0159:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0159:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0159:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0159:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0159:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0159:                        [--overlap-p2p-communication]
g0159:                        [--distributed-backend {nccl,gloo,ccl}]
g0159:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0159:                        [--DDP-impl {local,torch,FSDP}]
g0159:                        [--no-contiguous-buffers-in-local-ddp]
g0159:                        [--no-scatter-gather-tensors-in-pipeline]
g0159:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0159:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0159:                        [--use-cpu-initialization]
g0159:                        [--empty-unused-memory-level {0,1,2}]
g0159:                        [--standalone-embedding-stage]
g0159:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0159:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0159:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0159:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0159:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0159:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0159:                        [--test-data-path [TEST_DATA_PATH ...]]
g0159:                        [--data-cache-path DATA_CACHE_PATH]
g0159:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0159:                        [--merge-file MERGE_FILE]
g0159:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0159:                        [--seq-length SEQ_LENGTH]
g0159:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0159:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0159:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0159:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0159:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0159:                        [--num-workers NUM_WORKERS]
g0159:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0159:                        [--tokenizer-model TOKENIZER_MODEL]
g0159:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0159:                        [--reset-attention-mask] [--eod-mask-loss]
g0159:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0159:                        [--return-data-index]
g0159:                        [--data-efficiency-curriculum-learning]
g0159:                        [--train-idx-path TRAIN_IDX_PATH]
g0159:                        [--train-desc-path TRAIN_DESC_PATH]
g0159:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0159:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0159:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0159:                        [--repeated-dataloader] [--adlr-autoresume]
g0159:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0159:                        [--ict-head-size ICT_HEAD_SIZE]
g0159:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0159:                        [--biencoder-shared-query-context-model]
g0159:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0159:                        [--titles-data-path TITLES_DATA_PATH]
g0159:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0159:                        [--use-one-sent-docs]
g0159:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0159:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0159:                        [--retriever-score-scaling]
g0159:                        [--block-data-path BLOCK_DATA_PATH]
g0159:                        [--embedding-path EMBEDDING_PATH]
g0159:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0159:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0159:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0159:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0159:                        [--patch-dim PATCH_DIM]
g0159:                        [--classes-fraction CLASSES_FRACTION]
g0159:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0159:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0159:                        [--vision-pretraining]
g0159:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0159:                        [--vision-backbone-type {vit,mit,swin}]
g0159:                        [--swin-backbone-type {tiny,base,h3}]
g0159:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0159:                        [--iter-per-epoch ITER_PER_EPOCH]
g0159:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0159:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0159:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0159:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0159:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0159:                        [--dino-norm-last-layer]
g0159:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0159:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0159:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0159:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0159:                        [--timing-log-level {0,1,2}]
g0159:                        [--no-barrier-with-level-1-timing]
g0159:                        [--timing-log-option {max,minmax,all}]
g0159:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0159:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0159:                        [--log-timers-to-tensorboard]
g0159:                        [--log-batch-size-to-tensorboard]
g0159:                        [--no-log-learnig-rate-to-tensorboard]
g0159:                        [--no-log-loss-scale-to-tensorboard]
g0159:                        [--log-validation-ppl-to-tensorboard]
g0159:                        [--log-optimizer-states-to-tensorboard]
g0159:                        [--log-memory-to-tensorboard]
g0159:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0159:                        [--wandb-entity WANDB_ENTITY]
g0159:                        [--wandb-project WANDB_PROJECT]
g0159:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0159:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0159:                        [--zero-contigious-gradients]
g0159:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0159:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0159:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0159:                        [--scattered-embeddings] [--split-transformers]
g0159:                        [--memory-centric-tiled-linear]
g0159:                        [--tile-factor TILE_FACTOR]
g0159:                        [--deepspeed-activation-checkpointing]
g0159:                        [--partition-activations] [--contigious-checkpointing]
g0159:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0159:                        [--profile-backward]
g0159:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0159:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0159:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0159:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0159:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0159:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0159:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0159:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0159:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0159:                        [--output-bert-embeddings]
g0159:                        [--bert-embedder-type {megatron,huggingface}]
g0159:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0159:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0159:                        [--transformer-impl {local,transformer_engine}]
g0159:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0159:                        [--fp8-amax-compute-algo {most_recent,max}]
g0159:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0159:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0159:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0159:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0159:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0159:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0159:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0159:                        [--retro-return-doc-ids] [--deepspeed]
g0159:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0159:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0159: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0159: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0159:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0159:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0159:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0159:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0159:                        [--expert-interval EXPERT_INTERVAL]
g0159:                        [--hidden-size HIDDEN_SIZE]
g0159:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0159:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0159:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0159:                        [--kv-channels KV_CHANNELS]
g0159:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0159:                        [--use-rotary-position-embeddings]
g0159:                        [--rotary-percent ROTARY_PERCENT]
g0159:                        [--no-position-embedding]
g0159:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0159:                        [--normalization {layernorm,rmsnorm}]
g0159:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0159:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0159:                        [--apply-residual-connection-post-layernorm]
g0159:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0159:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0159:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0159:                        [--untie-embeddings-and-output-weights]
g0159:                        [--embedding-weights-in-fp32]
g0159:                        [--attention-dropout ATTENTION_DROPOUT]
g0159:                        [--hidden-dropout HIDDEN_DROPOUT]
g0159:                        [--weight-decay WEIGHT_DECAY]
g0159:                        [--start-weight-decay START_WEIGHT_DECAY]
g0159:                        [--end-weight-decay END_WEIGHT_DECAY]
g0159:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0159:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0159:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0159:                        [--sgd-momentum SGD_MOMENTUM]
g0159:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0159:                        [--batch-size BATCH_SIZE]
g0159:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0159:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0159:                        [--recompute-activations]
g0159:                        [--recompute-granularity {full,selective}]
g0159:                        [--distribute-saved-activations]
g0159:                        [--recompute-method {uniform,block}]
g0159:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0159:                        [--checkpoint-activations]
g0159:                        [--distribute-checkpointed-activations]
g0159:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0159:                        [--train-iters TRAIN_ITERS]
g0159:                        [--train-samples TRAIN_SAMPLES]
g0159:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0159:                        [--log-interval LOG_INTERVAL]
g0159:                        [--exit-interval EXIT_INTERVAL]
g0159:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0159:                        [--exit-signal-handler]
g0159:                        [--tensorboard-dir TENSORBOARD_DIR]
g0159:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0159:                        [--no-bias-dropout-fusion]
g0159:                        [--disable-moe-token-dropping]
g0159:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0159:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0159:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0159:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0159:                        [--create-moe-param-group] [--use-flash-attn]
g0159:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0159:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0159:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0159:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0159:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0159:                        [--no-async-tensor-model-parallel-allreduce]
g0159:                        [--no-persist-layer-norm] [--sequence-parallel]
g0159:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0159:                        [--force-ds-sequence-parallel]
g0159:                        [--no-gradient-accumulation-fusion]
g0159:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0159:                        [--data-parallel-random-init]
g0159:                        [--init-method-std INIT_METHOD_STD]
g0159:                        [--init-method-xavier-uniform] [--lr LR]
g0159:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0159:                        [--lr-decay-iters LR_DECAY_ITERS]
g0159:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0159:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0159:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0159:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0159:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0159:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0159:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0159:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0159:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0159:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0159:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0159:                        [--no-initialization] [--use-checkpoint-args]
g0159:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0159:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0159:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0159:                        [--min-loss-scale MIN_LOSS_SCALE]
g0159:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0159:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0159:                        [--no-query-key-layer-scaling]
g0159:                        [--attention-softmax-in-fp32]
g0159:                        [--accumulate-allreduce-grads-in-fp32]
g0159:                        [--fp16-lm-cross-entropy]
g0159:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0159:                        [--enable-expert-tensor-parallelism]
g0159:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0159:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0159:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0159:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0159:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0159:                        [--overlap-p2p-communication]
g0159:                        [--distributed-backend {nccl,gloo,ccl}]
g0159:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0159:                        [--DDP-impl {local,torch,FSDP}]
g0159:                        [--no-contiguous-buffers-in-local-ddp]
g0159:                        [--no-scatter-gather-tensors-in-pipeline]
g0159:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0159:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0159:                        [--use-cpu-initialization]
g0159:                        [--empty-unused-memory-level {0,1,2}]
g0159:                        [--standalone-embedding-stage]
g0159:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0159:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0159:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0159:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0159:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0159:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0159:                        [--test-data-path [TEST_DATA_PATH ...]]
g0159:                        [--data-cache-path DATA_CACHE_PATH]
g0159:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0159:                        [--merge-file MERGE_FILE]
g0159:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0159:                        [--seq-length SEQ_LENGTH]
g0159:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0159:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0159:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0159:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0159:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0159:                        [--num-workers NUM_WORKERS]
g0159:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0159:                        [--tokenizer-model TOKENIZER_MODEL]
g0159:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0159:                        [--reset-attention-mask] [--eod-mask-loss]
g0159:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0159:                        [--return-data-index]
g0159:                        [--data-efficiency-curriculum-learning]
g0159:                        [--train-idx-path TRAIN_IDX_PATH]
g0159:                        [--train-desc-path TRAIN_DESC_PATH]
g0159:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0159:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0159:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0159:                        [--repeated-dataloader] [--adlr-autoresume]
g0159:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0159:                        [--ict-head-size ICT_HEAD_SIZE]
g0159:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0159:                        [--biencoder-shared-query-context-model]
g0159:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0159:                        [--titles-data-path TITLES_DATA_PATH]
g0159:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0159:                        [--use-one-sent-docs]
g0159:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0159:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0159:                        [--retriever-score-scaling]
g0159:                        [--block-data-path BLOCK_DATA_PATH]
g0159:                        [--embedding-path EMBEDDING_PATH]
g0159:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0159:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0159:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0159:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0159:                        [--patch-dim PATCH_DIM]
g0159:                        [--classes-fraction CLASSES_FRACTION]
g0159:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0159:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0159:                        [--vision-pretraining]
g0159:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0159:                        [--vision-backbone-type {vit,mit,swin}]
g0159:                        [--swin-backbone-type {tiny,base,h3}]
g0159:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0159:                        [--iter-per-epoch ITER_PER_EPOCH]
g0159:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0159:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0159:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0159:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0159:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0159:                        [--dino-norm-last-layer]
g0159:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0159:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0159:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0159:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0159:                        [--timing-log-level {0,1,2}]
g0159:                        [--no-barrier-with-level-1-timing]
g0159:                        [--timing-log-option {max,minmax,all}]
g0159:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0159:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0159:                        [--log-timers-to-tensorboard]
g0159:                        [--log-batch-size-to-tensorboard]
g0159:                        [--no-log-learnig-rate-to-tensorboard]
g0159:                        [--no-log-loss-scale-to-tensorboard]
g0159:                        [--log-validation-ppl-to-tensorboard]
g0159:                        [--log-optimizer-states-to-tensorboard]
g0159:                        [--log-memory-to-tensorboard]
g0159:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0159:                        [--wandb-entity WANDB_ENTITY]
g0159:                        [--wandb-project WANDB_PROJECT]
g0159:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0159:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0159:                        [--zero-contigious-gradients]
g0159:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0159:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0159:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0159:                        [--scattered-embeddings] [--split-transformers]
g0159:                        [--memory-centric-tiled-linear]
g0159:                        [--tile-factor TILE_FACTOR]
g0159:                        [--deepspeed-activation-checkpointing]
g0159:                        [--partition-activations] [--contigious-checkpointing]
g0159:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0159:                        [--profile-backward]
g0159:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0159:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0159:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0159:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0159:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0159:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0159:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0159:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0159:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0159:                        [--output-bert-embeddings]
g0159:                        [--bert-embedder-type {megatron,huggingface}]
g0159:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0159:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0159:                        [--transformer-impl {local,transformer_engine}]
g0159:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0159:                        [--fp8-amax-compute-algo {most_recent,max}]
g0159:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0159:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0159:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0159:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0159:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0159:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0159:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0159:                        [--retro-return-doc-ids] [--deepspeed]
g0159:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0159:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0159: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0161: --------------------------------------------------
g0161: DeepSpeed C++/CUDA extension op report
g0161: --------------------------------------------------
g0161: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0161:       runtime if needed. Op compatibility means that your system
g0161:       meet the required dependencies to JIT install the op.
g0161: --------------------------------------------------
g0161: JIT compiled ops requires ninja
g0161: ninja .................. [92m[OKAY][0m
g0161: --------------------------------------------------
g0161: op name ................ installed .. compatible
g0161: --------------------------------------------------
g0156: [2024-08-12 03:40:18,558] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 175868
g0164: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0164: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0164: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0164: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0164: async_io[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0164: ............... [92m[YES][0mevoformer_attn  ...... .........[92m[OKAY][0m 
g0164: [93m[NO][0m ....... fused_adam[93m[NO][0m 
g0164: ............. [92m[YES][0m ...... fused_lamb[92m[OKAY][0m 
g0164: ............. [92m[YES][0mcpu_adam  ..................... [92m[YES][0m  ......[92m[OKAY][0m 
g0164: [92m[OKAY][0m
g0164: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0164: fused_lion cpu_lion............. ...............  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m
g0164: [92m[OKAY][0m
g0164: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0164: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0164: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0164: async_io ............... [92m[YES][0mfused_adam  ...................  [92m[OKAY][0m[92m[YES][0m
g0164:  ...... [92m[OKAY][0m
g0164: cpu_adam fused_adam...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0164: [92m[OKAY][0m
g0164: cpu_adagrad cpu_adam............  ...............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0164: [92m[OKAY][0m
g0164: cpu_lion cpu_adagrad...............  ............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0164: [92m[OKAY][0m
g0164: cpu_lion ............... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0164:  ......evoformer_attn  [92m[OKAY][0m.........
g0164:  [93m[NO][0m ....... [93m[NO][0m
g0164: fused_lamb ............. [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0164:  ...... evoformer_attn[92m[OKAY][0m 
g0164: ......... [93m[NO][0m ....... [93m[NO][0m
g0164: fused_lion fused_lamb.............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0164: [92m[OKAY][0m
g0164: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0156: [2024-08-12 03:40:18,573] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 175869
g0156: [2024-08-12 03:40:18,573] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 175870
g0164: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0164: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0164: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0164: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0164: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0164: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0164: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0164: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0164: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0164: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0164: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0164: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0164: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0164: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0164: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0156: [2024-08-12 03:40:18,587] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True', '--log-optimizer-states-to-tensorboard', '--train-data-exact-num-epochs', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 2
g0164: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: random_ltd ............. [92m[YES][0m ragged_ops......  .............[92m[OKAY][0m [92m[YES][0m
g0164:  ...... [92m[OKAY][0m
g0164: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0164: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0164: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0164: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0164: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0164: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0164: sparse_attn [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible............
g0164:  [93m[NO][0m sparse_attn.......  ............[93m[NO][0m 
g0164: [93m[NO][0m ....... [93m[NO][0m
g0164: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0164: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0164: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0164: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0164: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0164: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0164: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0164: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0164: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0164: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0164: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0164: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0164: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0164: --------------------------------------------------
g0164: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0164: --------------------------------------------------
g0164: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0164: --------------------------------------------------
g0164: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0164: --------------------------------------------------
g0161: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0161: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0161: async_iocpu_adagrad  ...........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0161: 
g0161: cpu_lion ............... [92m[YES][0mfused_adam  ...................  [92m[OKAY][0m[92m[YES][0m
g0161:  ...... [92m[OKAY][0m
g0161: cpu_adam[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0161: ............... [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0161:  ....... cpu_adagrad[93m[NO][0m 
g0161: ............ [92m[YES][0m fused_lamb......  .............[92m[OKAY][0m 
g0161: [92m[YES][0m ...... cpu_lion[92m[OKAY][0m 
g0161: ............... [92m[YES][0m ...... [92m[OKAY][0m
g0161: fused_lion ............. [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0161: [92m[OKAY][0m
g0161: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0161: async_io fused_lamb...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0161: [92m[OKAY][0m
g0161: fused_adam ............. fused_lion[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0161: ...... [92m[OKAY][0m
g0161: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0161: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0161: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0161: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0161: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0161: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: DeepSpeed general environment info:
g0164: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0164: torch version .................... 2.0.1+cu118
g0164: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0164: deepspeed info ................... 0.12.4, unknown, unknown
g0164: torch cuda version ............... 11.8
g0164: torch hip version ................ None
g0164: nvcc version ..................... 11.8
g0164: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0164: shared memory (/dev/shm) size .... 188.13 GB
g0163: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0163: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0163: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0masync_io
g0163:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0163: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0163: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0163: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0163: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0163: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0163: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0163: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: DeepSpeed general environment info:
g0164: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0164: torch version .................... 2.0.1+cu118
g0164: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0164: deepspeed info ................... 0.12.4, unknown, unknown
g0164: torch cuda version ............... 11.8
g0164: torch hip version ................ None
g0164: nvcc version ..................... 11.8
g0164: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0164: shared memory (/dev/shm) size .... 188.13 GB
g0164: DeepSpeed general environment info:
g0164: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0164: torch version .................... 2.0.1+cu118
g0164: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0164: deepspeed info ................... 0.12.4, unknown, unknown
g0164: torch cuda version ............... 11.8
g0164: torch hip version ................ None
g0164: nvcc version ..................... 11.8
g0164: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0164: shared memory (/dev/shm) size .... 188.13 GB
g0164: DeepSpeed general environment info:
g0164: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0164: torch version .................... 2.0.1+cu118
g0164: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0164: deepspeed info ................... 0.12.4, unknown, unknown
g0164: torch cuda version ............... 11.8
g0164: torch hip version ................ None
g0164: nvcc version ..................... 11.8
g0164: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0164: shared memory (/dev/shm) size .... 188.13 GB
g0161: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0161: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0161: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0161: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0161: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0161: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0161: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0161: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0161: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0161: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0161: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0161: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0161: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0161: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0161: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0161: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0161: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0161: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0161: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0161: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0161: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0161: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0161: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0161: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0161: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0161: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0161: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0163: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0163: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0163: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0161: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0161: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0161: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0161: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0161: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0161: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0161: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0161: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0161: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0161: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0163: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0161: --------------------------------------------------
g0161: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0161: --------------------------------------------------
g0161: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0161: --------------------------------------------------
g0161: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0161: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0161: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0161: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0163: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0163: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0163: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0163: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0161: DeepSpeed general environment info:
g0161: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0161: torch version .................... 2.0.1+cu118
g0161: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0161: deepspeed info ................... 0.12.4, unknown, unknown
g0161: torch cuda version ............... 11.8
g0161: torch hip version ................ None
g0161: nvcc version ..................... 11.8
g0161: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0161: shared memory (/dev/shm) size .... 188.13 GB
g0161: DeepSpeed general environment info:
g0161: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0161: torch version .................... 2.0.1+cu118
g0161: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0161: deepspeed info ................... 0.12.4, unknown, unknown
g0161: torch cuda version ............... 11.8
g0161: torch hip version ................ None
g0161: nvcc version ..................... 11.8
g0161: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0161: shared memory (/dev/shm) size .... 188.13 GB
g0161: DeepSpeed general environment info:
g0161: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0161: torch version .................... 2.0.1+cu118
g0161: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0161: deepspeed info ................... 0.12.4, unknown, unknown
g0161: torch cuda version ............... 11.8
g0161: torch hip version ................ None
g0161: nvcc version ..................... 11.8
g0161: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0161: shared memory (/dev/shm) size .... 188.13 GB
g0161: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0161: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0161: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0163: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0164: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0163: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0164: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0163: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0163: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0163: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0164: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0163: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0163: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0163: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0163: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0163: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0163: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0163: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0163: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0163: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0163: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0161: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0161: --------------------------------------------------
g0163: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0163: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0163: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0163: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0163: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0163: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0163: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0163: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0163: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0161: DeepSpeed general environment info:
g0161: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0161: torch version .................... 2.0.1+cu118
g0161: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0161: deepspeed info ................... 0.12.4, unknown, unknown
g0161: torch cuda version ............... 11.8
g0161: torch hip version ................ None
g0161: nvcc version ..................... 11.8
g0161: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0161: shared memory (/dev/shm) size .... 188.13 GB
g0163: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0163: --------------------------------------------------
g0163: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0163: --------------------------------------------------
g0163: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0163: --------------------------------------------------
g0163: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0163: --------------------------------------------------
g0164: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0164:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0164:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0164:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0164:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0164:                        [--expert-interval EXPERT_INTERVAL]
g0164:                        [--hidden-size HIDDEN_SIZE]
g0164:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0164:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0164:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0164:                        [--kv-channels KV_CHANNELS]
g0164:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0164:                        [--use-rotary-position-embeddings]
g0164:                        [--rotary-percent ROTARY_PERCENT]
g0164:                        [--no-position-embedding]
g0164:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0164:                        [--normalization {layernorm,rmsnorm}]
g0164:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0164:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0164:                        [--apply-residual-connection-post-layernorm]
g0164:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0164:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0164:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0164:                        [--untie-embeddings-and-output-weights]
g0164:                        [--embedding-weights-in-fp32]
g0164:                        [--attention-dropout ATTENTION_DROPOUT]
g0164:                        [--hidden-dropout HIDDEN_DROPOUT]
g0164:                        [--weight-decay WEIGHT_DECAY]
g0164:                        [--start-weight-decay START_WEIGHT_DECAY]
g0164:                        [--end-weight-decay END_WEIGHT_DECAY]
g0164:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0164:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0164:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0164:                        [--sgd-momentum SGD_MOMENTUM]
g0164:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0164:                        [--batch-size BATCH_SIZE]
g0164:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0164:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0164:                        [--recompute-activations]
g0164:                        [--recompute-granularity {full,selective}]
g0164:                        [--distribute-saved-activations]
g0164:                        [--recompute-method {uniform,block}]
g0164:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0164:                        [--checkpoint-activations]
g0164:                        [--distribute-checkpointed-activations]
g0164:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0164:                        [--train-iters TRAIN_ITERS]
g0164:                        [--train-samples TRAIN_SAMPLES]
g0164:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0164:                        [--log-interval LOG_INTERVAL]
g0164:                        [--exit-interval EXIT_INTERVAL]
g0164:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0164:                        [--exit-signal-handler]
g0164:                        [--tensorboard-dir TENSORBOARD_DIR]
g0164:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0164:                        [--no-bias-dropout-fusion]
g0164:                        [--disable-moe-token-dropping]
g0164:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0164:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0164:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0164:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0164:                        [--create-moe-param-group] [--use-flash-attn]
g0164:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0164:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0164:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0164:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0164:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0164:                        [--no-async-tensor-model-parallel-allreduce]
g0164:                        [--no-persist-layer-norm] [--sequence-parallel]
g0164:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0164:                        [--force-ds-sequence-parallel]
g0164:                        [--no-gradient-accumulation-fusion]
g0164:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0164:                        [--data-parallel-random-init]
g0164:                        [--init-method-std INIT_METHOD_STD]
g0164:                        [--init-method-xavier-uniform] [--lr LR]
g0164:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0164:                        [--lr-decay-iters LR_DECAY_ITERS]
g0164:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0164:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0164:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0164:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0164:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0164:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0164:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0164:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0164:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0164:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0164:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0164:                        [--no-initialization] [--use-checkpoint-args]
g0164:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0164:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0164:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0164:                        [--min-loss-scale MIN_LOSS_SCALE]
g0164:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0164:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0164:                        [--no-query-key-layer-scaling]
g0164:                        [--attention-softmax-in-fp32]
g0164:                        [--accumulate-allreduce-grads-in-fp32]
g0164:                        [--fp16-lm-cross-entropy]
g0164:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0164:                        [--enable-expert-tensor-parallelism]
g0164:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0164:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0164:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0164:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0164:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0164:                        [--overlap-p2p-communication]
g0164:                        [--distributed-backend {nccl,gloo,ccl}]
g0164:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0164:                        [--DDP-impl {local,torch,FSDP}]
g0164:                        [--no-contiguous-buffers-in-local-ddp]
g0164:                        [--no-scatter-gather-tensors-in-pipeline]
g0164:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0164:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0164:                        [--use-cpu-initialization]
g0164:                        [--empty-unused-memory-level {0,1,2}]
g0164:                        [--standalone-embedding-stage]
g0164:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0164:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0164:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0164:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0164:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0164:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0164:                        [--test-data-path [TEST_DATA_PATH ...]]
g0164:                        [--data-cache-path DATA_CACHE_PATH]
g0164:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0164:                        [--merge-file MERGE_FILE]
g0164:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0164:                        [--seq-length SEQ_LENGTH]
g0164:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0164:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0164:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0164:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0164:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0164:                        [--num-workers NUM_WORKERS]
g0164:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0164:                        [--tokenizer-model TOKENIZER_MODEL]
g0164:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0164:                        [--reset-attention-mask] [--eod-mask-loss]
g0164:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0164:                        [--return-data-index]
g0164:                        [--data-efficiency-curriculum-learning]
g0164:                        [--train-idx-path TRAIN_IDX_PATH]
g0164:                        [--train-desc-path TRAIN_DESC_PATH]
g0164:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0164:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0164:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0164:                        [--repeated-dataloader] [--adlr-autoresume]
g0164:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0164:                        [--ict-head-size ICT_HEAD_SIZE]
g0164:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0164:                        [--biencoder-shared-query-context-model]
g0164:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0164:                        [--titles-data-path TITLES_DATA_PATH]
g0164:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0164:                        [--use-one-sent-docs]
g0164:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0164:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0164:                        [--retriever-score-scaling]
g0164:                        [--block-data-path BLOCK_DATA_PATH]
g0164:                        [--embedding-path EMBEDDING_PATH]
g0164:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0164:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0164:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0164:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0164:                        [--patch-dim PATCH_DIM]
g0164:                        [--classes-fraction CLASSES_FRACTION]
g0164:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0164:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0164:                        [--vision-pretraining]
g0164:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0164:                        [--vision-backbone-type {vit,mit,swin}]
g0164:                        [--swin-backbone-type {tiny,base,h3}]
g0164:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0164:                        [--iter-per-epoch ITER_PER_EPOCH]
g0164:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0164:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0164:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0164:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0164:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0164:                        [--dino-norm-last-layer]
g0164:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0164:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0164:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0164:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0164:                        [--timing-log-level {0,1,2}]
g0164:                        [--no-barrier-with-level-1-timing]
g0164:                        [--timing-log-option {max,minmax,all}]
g0164:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0164:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0164:                        [--log-timers-to-tensorboard]
g0164:                        [--log-batch-size-to-tensorboard]
g0164:                        [--no-log-learnig-rate-to-tensorboard]
g0164:                        [--no-log-loss-scale-to-tensorboard]
g0164:                        [--log-validation-ppl-to-tensorboard]
g0164:                        [--log-optimizer-states-to-tensorboard]
g0164:                        [--log-memory-to-tensorboard]
g0164:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0164:                        [--wandb-entity WANDB_ENTITY]
g0164:                        [--wandb-project WANDB_PROJECT]
g0164:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0164:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0164:                        [--zero-contigious-gradients]
g0164:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0164:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0164:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0164:                        [--scattered-embeddings] [--split-transformers]
g0164:                        [--memory-centric-tiled-linear]
g0164:                        [--tile-factor TILE_FACTOR]
g0164:                        [--deepspeed-activation-checkpointing]
g0164:                        [--partition-activations] [--contigious-checkpointing]
g0164:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0164:                        [--profile-backward]
g0164:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0164:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0164:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0164:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0164:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0164:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0164:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0164:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0164:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0164:                        [--output-bert-embeddings]
g0164:                        [--bert-embedder-type {megatron,huggingface}]
g0164:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0164:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0164:                        [--transformer-impl {local,transformer_engine}]
g0164:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0164:                        [--fp8-amax-compute-algo {most_recent,max}]
g0164:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0164:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0164:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0164:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0164:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0164:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0164:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0164:                        [--retro-return-doc-ids] [--deepspeed]
g0164:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0164:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0164: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0164:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0164:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0164:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0164:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0164:                        [--expert-interval EXPERT_INTERVAL]
g0164:                        [--hidden-size HIDDEN_SIZE]
g0164:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0164:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0164:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0164:                        [--kv-channels KV_CHANNELS]
g0164:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0164:                        [--use-rotary-position-embeddings]
g0164:                        [--rotary-percent ROTARY_PERCENT]
g0164:                        [--no-position-embedding]
g0164:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0164:                        [--normalization {layernorm,rmsnorm}]
g0164:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0164:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0164:                        [--apply-residual-connection-post-layernorm]
g0164:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0164:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0164:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0164:                        [--untie-embeddings-and-output-weights]
g0164:                        [--embedding-weights-in-fp32]
g0164:                        [--attention-dropout ATTENTION_DROPOUT]
g0164:                        [--hidden-dropout HIDDEN_DROPOUT]
g0164:                        [--weight-decay WEIGHT_DECAY]
g0164:                        [--start-weight-decay START_WEIGHT_DECAY]
g0164:                        [--end-weight-decay END_WEIGHT_DECAY]
g0164:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0164:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0164:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0164:                        [--sgd-momentum SGD_MOMENTUM]
g0164:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0164:                        [--batch-size BATCH_SIZE]
g0164:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0164:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0164:                        [--recompute-activations]
g0164:                        [--recompute-granularity {full,selective}]
g0164:                        [--distribute-saved-activations]
g0164:                        [--recompute-method {uniform,block}]
g0164:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0164:                        [--checkpoint-activations]
g0164:                        [--distribute-checkpointed-activations]
g0164:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0164:                        [--train-iters TRAIN_ITERS]
g0164:                        [--train-samples TRAIN_SAMPLES]
g0164:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0164:                        [--log-interval LOG_INTERVAL]
g0164:                        [--exit-interval EXIT_INTERVAL]
g0164:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0164:                        [--exit-signal-handler]
g0164:                        [--tensorboard-dir TENSORBOARD_DIR]
g0164:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0164:                        [--no-bias-dropout-fusion]
g0164:                        [--disable-moe-token-dropping]
g0164:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0164:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0164:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0164:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0164:                        [--create-moe-param-group] [--use-flash-attn]
g0164:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0164:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0164:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0164:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0164:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0164:                        [--no-async-tensor-model-parallel-allreduce]
g0164:                        [--no-persist-layer-norm] [--sequence-parallel]
g0164:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0164:                        [--force-ds-sequence-parallel]
g0164:                        [--no-gradient-accumulation-fusion]
g0164:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0164:                        [--data-parallel-random-init]
g0164:                        [--init-method-std INIT_METHOD_STD]
g0164:                        [--init-method-xavier-uniform] [--lr LR]
g0164:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0164:                        [--lr-decay-iters LR_DECAY_ITERS]
g0164:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0164:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0164:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0164:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0164:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0164:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0164:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0164:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0164:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0164:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0164:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0164:                        [--no-initialization] [--use-checkpoint-args]
g0164:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0164:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0164:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0164:                        [--min-loss-scale MIN_LOSS_SCALE]
g0164:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0164:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0164:                        [--no-query-key-layer-scaling]
g0164:                        [--attention-softmax-in-fp32]
g0164:                        [--accumulate-allreduce-grads-in-fp32]
g0164:                        [--fp16-lm-cross-entropy]
g0164:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0164:                        [--enable-expert-tensor-parallelism]
g0164:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0164:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0164:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0164:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0164:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0164:                        [--overlap-p2p-communication]
g0164:                        [--distributed-backend {nccl,gloo,ccl}]
g0164:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0164:                        [--DDP-impl {local,torch,FSDP}]
g0164:                        [--no-contiguous-buffers-in-local-ddp]
g0164:                        [--no-scatter-gather-tensors-in-pipeline]
g0164:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0164:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0164:                        [--use-cpu-initialization]
g0164:                        [--empty-unused-memory-level {0,1,2}]
g0164:                        [--standalone-embedding-stage]
g0164:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0164:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0164:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0164:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0164:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0164:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0164:                        [--test-data-path [TEST_DATA_PATH ...]]
g0164:                        [--data-cache-path DATA_CACHE_PATH]
g0164:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0164:                        [--merge-file MERGE_FILE]
g0164:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0164:                        [--seq-length SEQ_LENGTH]
g0164:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0164:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0164:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0164:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0164:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0164:                        [--num-workers NUM_WORKERS]
g0164:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0164:                        [--tokenizer-model TOKENIZER_MODEL]
g0164:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0164:                        [--reset-attention-mask] [--eod-mask-loss]
g0164:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0164:                        [--return-data-index]
g0164:                        [--data-efficiency-curriculum-learning]
g0164:                        [--train-idx-path TRAIN_IDX_PATH]
g0164:                        [--train-desc-path TRAIN_DESC_PATH]
g0164:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0164:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0164:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0164:                        [--repeated-dataloader] [--adlr-autoresume]
g0164:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0164:                        [--ict-head-size ICT_HEAD_SIZE]
g0164:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0164:                        [--biencoder-shared-query-context-model]
g0164:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0164:                        [--titles-data-path TITLES_DATA_PATH]
g0164:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0164:                        [--use-one-sent-docs]
g0164:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0164:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0164:                        [--retriever-score-scaling]
g0164:                        [--block-data-path BLOCK_DATA_PATH]
g0164:                        [--embedding-path EMBEDDING_PATH]
g0164:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0164:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0164:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0164:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0164:                        [--patch-dim PATCH_DIM]
g0164:                        [--classes-fraction CLASSES_FRACTION]
g0164:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0164:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0164:                        [--vision-pretraining]
g0164:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0164:                        [--vision-backbone-type {vit,mit,swin}]
g0164:                        [--swin-backbone-type {tiny,base,h3}]
g0164:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0164:                        [--iter-per-epoch ITER_PER_EPOCH]
g0164:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0164:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0164:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0164:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0164:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0164:                        [--dino-norm-last-layer]
g0164:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0164:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0164:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0164:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0164:                        [--timing-log-level {0,1,2}]
g0164:                        [--no-barrier-with-level-1-timing]
g0164:                        [--timing-log-option {max,minmax,all}]
g0164:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0164:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0164:                        [--log-timers-to-tensorboard]
g0164:                        [--log-batch-size-to-tensorboard]
g0164:                        [--no-log-learnig-rate-to-tensorboard]
g0164:                        [--no-log-loss-scale-to-tensorboard]
g0164:                        [--log-validation-ppl-to-tensorboard]
g0164:                        [--log-optimizer-states-to-tensorboard]
g0164:                        [--log-memory-to-tensorboard]
g0164:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0164:                        [--wandb-entity WANDB_ENTITY]
g0164:                        [--wandb-project WANDB_PROJECT]
g0164:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0164:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0164:                        [--zero-contigious-gradients]
g0164:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0164:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0164:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0164:                        [--scattered-embeddings] [--split-transformers]
g0164:                        [--memory-centric-tiled-linear]
g0164:                        [--tile-factor TILE_FACTOR]
g0164:                        [--deepspeed-activation-checkpointing]
g0164:                        [--partition-activations] [--contigious-checkpointing]
g0164:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0164:                        [--profile-backward]
g0164:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0164:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0164:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0164:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0164:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0164:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0164:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0164:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0164:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0164:                        [--output-bert-embeddings]
g0164:                        [--bert-embedder-type {megatron,huggingface}]
g0164:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0164:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0164:                        [--transformer-impl {local,transformer_engine}]
g0164:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0164:                        [--fp8-amax-compute-algo {most_recent,max}]
g0164:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0164:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0164:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0164:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0164:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0164:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0164:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0164:                        [--retro-return-doc-ids] [--deepspeed]
g0164:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0164:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0164: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0164: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0163: DeepSpeed general environment info:
g0163: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0163: torch version .................... 2.0.1+cu118
g0163: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0163: deepspeed info ................... 0.12.4, unknown, unknown
g0163: torch cuda version ............... 11.8
g0163: torch hip version ................ None
g0163: nvcc version ..................... 11.8
g0163: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0163: shared memory (/dev/shm) size .... 188.13 GB
g0163: DeepSpeed general environment info:
g0163: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0163: DeepSpeed general environment info:torch version
g0163:  .................... torch install path2.0.1+cu118 
g0163: ............... deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0163: ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0163: torch versiondeepspeed info  .......................................  0.12.4, unknown, unknown2.0.1+cu118
g0163: 
g0163: torch cuda versiondeepspeed install path  ..........................  11.8['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0163: 
g0163: torch hip versiondeepspeed info  ...................................  None0.12.4, unknown, unknown
g0163: 
g0163: nvcc versiontorch cuda version  ....................................  11.811.8
g0163: 
g0163: torch hip version ................ deepspeed wheel compiled w.None 
g0163: ...... nvcc versiontorch 2.0, cuda 11.8 
g0163: ..................... shared memory (/dev/shm) size11.8 
g0163: .... 188.13 GB
g0163: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0163: shared memory (/dev/shm) size .... 188.13 GB
g0163: DeepSpeed general environment info:
g0163: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0163: torch version .................... 2.0.1+cu118
g0163: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0163: deepspeed info ................... 0.12.4, unknown, unknown
g0163: torch cuda version ............... 11.8
g0163: torch hip version ................ None
g0163: nvcc version ..................... 11.8
g0163: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0163: shared memory (/dev/shm) size .... 188.13 GB
g0164: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0164:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0164:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0164:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0164:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0164:                        [--expert-interval EXPERT_INTERVAL]
g0164:                        [--hidden-size HIDDEN_SIZE]
g0164:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0164:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0164:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0164:                        [--kv-channels KV_CHANNELS]
g0164:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0164:                        [--use-rotary-position-embeddings]
g0164:                        [--rotary-percent ROTARY_PERCENT]
g0164:                        [--no-position-embedding]
g0164:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0164:                        [--normalization {layernorm,rmsnorm}]
g0164:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0164:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0164:                        [--apply-residual-connection-post-layernorm]
g0164:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0164:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0164:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0164:                        [--untie-embeddings-and-output-weights]
g0164:                        [--embedding-weights-in-fp32]
g0164:                        [--attention-dropout ATTENTION_DROPOUT]
g0164:                        [--hidden-dropout HIDDEN_DROPOUT]
g0164:                        [--weight-decay WEIGHT_DECAY]
g0164:                        [--start-weight-decay START_WEIGHT_DECAY]
g0164:                        [--end-weight-decay END_WEIGHT_DECAY]
g0164:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0164:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0164:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0164:                        [--sgd-momentum SGD_MOMENTUM]
g0164:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0164:                        [--batch-size BATCH_SIZE]
g0164:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0164:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0164:                        [--recompute-activations]
g0164:                        [--recompute-granularity {full,selective}]
g0164:                        [--distribute-saved-activations]
g0164:                        [--recompute-method {uniform,block}]
g0164:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0164:                        [--checkpoint-activations]
g0164:                        [--distribute-checkpointed-activations]
g0164:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0164:                        [--train-iters TRAIN_ITERS]
g0164:                        [--train-samples TRAIN_SAMPLES]
g0164:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0164:                        [--log-interval LOG_INTERVAL]
g0164:                        [--exit-interval EXIT_INTERVAL]
g0164:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0164:                        [--exit-signal-handler]
g0164:                        [--tensorboard-dir TENSORBOARD_DIR]
g0164:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0164:                        [--no-bias-dropout-fusion]
g0164:                        [--disable-moe-token-dropping]
g0164:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0164:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0164:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0164:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0164:                        [--create-moe-param-group] [--use-flash-attn]
g0164:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0164:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0164:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0164:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0164:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0164:                        [--no-async-tensor-model-parallel-allreduce]
g0164:                        [--no-persist-layer-norm] [--sequence-parallel]
g0164:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0164:                        [--force-ds-sequence-parallel]
g0164:                        [--no-gradient-accumulation-fusion]
g0164:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0164:                        [--data-parallel-random-init]
g0164:                        [--init-method-std INIT_METHOD_STD]
g0164:                        [--init-method-xavier-uniform] [--lr LR]
g0164:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0164:                        [--lr-decay-iters LR_DECAY_ITERS]
g0164:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0164:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0164:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0164:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0164:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0164:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0164:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0164:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0164:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0164:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0164:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0164:                        [--no-initialization] [--use-checkpoint-args]
g0164:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0164:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0164:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0164:                        [--min-loss-scale MIN_LOSS_SCALE]
g0164:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0164:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0164:                        [--no-query-key-layer-scaling]
g0164:                        [--attention-softmax-in-fp32]
g0164:                        [--accumulate-allreduce-grads-in-fp32]
g0164:                        [--fp16-lm-cross-entropy]
g0164:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0164:                        [--enable-expert-tensor-parallelism]
g0164:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0164:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0164:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0164:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0164:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0164:                        [--overlap-p2p-communication]
g0164:                        [--distributed-backend {nccl,gloo,ccl}]
g0164:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0164:                        [--DDP-impl {local,torch,FSDP}]
g0164:                        [--no-contiguous-buffers-in-local-ddp]
g0164:                        [--no-scatter-gather-tensors-in-pipeline]
g0164:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0164:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0164:                        [--use-cpu-initialization]
g0164:                        [--empty-unused-memory-level {0,1,2}]
g0164:                        [--standalone-embedding-stage]
g0164:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0164:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0164:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0164:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0164:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0164:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0164:                        [--test-data-path [TEST_DATA_PATH ...]]
g0164:                        [--data-cache-path DATA_CACHE_PATH]
g0164:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0164:                        [--merge-file MERGE_FILE]
g0164:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0164:                        [--seq-length SEQ_LENGTH]
g0164:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0164:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0164:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0164:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0164:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0164:                        [--num-workers NUM_WORKERS]
g0164:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0164:                        [--tokenizer-model TOKENIZER_MODEL]
g0164:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0164:                        [--reset-attention-mask] [--eod-mask-loss]
g0164:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0164:                        [--return-data-index]
g0164:                        [--data-efficiency-curriculum-learning]
g0164:                        [--train-idx-path TRAIN_IDX_PATH]
g0164:                        [--train-desc-path TRAIN_DESC_PATH]
g0164:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0164:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0164:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0164:                        [--repeated-dataloader] [--adlr-autoresume]
g0164:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0164:                        [--ict-head-size ICT_HEAD_SIZE]
g0164:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0164:                        [--biencoder-shared-query-context-model]
g0164:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0164:                        [--titles-data-path TITLES_DATA_PATH]
g0164:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0164:                        [--use-one-sent-docs]
g0164:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0164:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0164:                        [--retriever-score-scaling]
g0164:                        [--block-data-path BLOCK_DATA_PATH]
g0164:                        [--embedding-path EMBEDDING_PATH]
g0164:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0164:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0164:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0164:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0164:                        [--patch-dim PATCH_DIM]
g0164:                        [--classes-fraction CLASSES_FRACTION]
g0164:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0164:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0164:                        [--vision-pretraining]
g0164:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0164:                        [--vision-backbone-type {vit,mit,swin}]
g0164:                        [--swin-backbone-type {tiny,base,h3}]
g0164:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0164:                        [--iter-per-epoch ITER_PER_EPOCH]
g0164:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0164:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0164:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0164:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0164:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0164:                        [--dino-norm-last-layer]
g0164:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0164:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0164:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0164:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0164:                        [--timing-log-level {0,1,2}]
g0164:                        [--no-barrier-with-level-1-timing]
g0164:                        [--timing-log-option {max,minmax,all}]
g0164:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0164:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0164:                        [--log-timers-to-tensorboard]
g0164:                        [--log-batch-size-to-tensorboard]
g0164:                        [--no-log-learnig-rate-to-tensorboard]
g0164:                        [--no-log-loss-scale-to-tensorboard]
g0164:                        [--log-validation-ppl-to-tensorboard]
g0164:                        [--log-optimizer-states-to-tensorboard]
g0164:                        [--log-memory-to-tensorboard]
g0164:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0164:                        [--wandb-entity WANDB_ENTITY]
g0164:                        [--wandb-project WANDB_PROJECT]
g0164:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0164:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0164:                        [--zero-contigious-gradients]
g0164:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0164:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0164:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0164:                        [--scattered-embeddings] [--split-transformers]
g0164:                        [--memory-centric-tiled-linear]
g0164:                        [--tile-factor TILE_FACTOR]
g0164:                        [--deepspeed-activation-checkpointing]
g0164:                        [--partition-activations] [--contigious-checkpointing]
g0164:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0164:                        [--profile-backward]
g0164:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0164:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0164:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0164:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0164:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0164:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0164:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0164:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0164:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0164:                        [--output-bert-embeddings]
g0164:                        [--bert-embedder-type {megatron,huggingface}]
g0164:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0164:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0164:                        [--transformer-impl {local,transformer_engine}]
g0164:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0164:                        [--fp8-amax-compute-algo {most_recent,max}]
g0164:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0164:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0164:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0164:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0164:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0164:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0164:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0164:                        [--retro-return-doc-ids] [--deepspeed]
g0164:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0164:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0164: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0164:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0164:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0164:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0164:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0164:                        [--expert-interval EXPERT_INTERVAL]
g0164:                        [--hidden-size HIDDEN_SIZE]
g0164:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0164:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0164:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0164:                        [--kv-channels KV_CHANNELS]
g0164:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0164:                        [--use-rotary-position-embeddings]
g0164:                        [--rotary-percent ROTARY_PERCENT]
g0164:                        [--no-position-embedding]
g0164:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0164:                        [--normalization {layernorm,rmsnorm}]
g0164:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0164:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0164:                        [--apply-residual-connection-post-layernorm]
g0164:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0164:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0164:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0164:                        [--untie-embeddings-and-output-weights]
g0164:                        [--embedding-weights-in-fp32]
g0164:                        [--attention-dropout ATTENTION_DROPOUT]
g0164:                        [--hidden-dropout HIDDEN_DROPOUT]
g0164:                        [--weight-decay WEIGHT_DECAY]
g0164:                        [--start-weight-decay START_WEIGHT_DECAY]
g0164:                        [--end-weight-decay END_WEIGHT_DECAY]
g0164:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0164:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0164:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0164:                        [--sgd-momentum SGD_MOMENTUM]
g0164:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0164:                        [--batch-size BATCH_SIZE]
g0164:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0164:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0164:                        [--recompute-activations]
g0164:                        [--recompute-granularity {full,selective}]
g0164:                        [--distribute-saved-activations]
g0164:                        [--recompute-method {uniform,block}]
g0164:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0164:                        [--checkpoint-activations]
g0164:                        [--distribute-checkpointed-activations]
g0164:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0164:                        [--train-iters TRAIN_ITERS]
g0164:                        [--train-samples TRAIN_SAMPLES]
g0164:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0164:                        [--log-interval LOG_INTERVAL]
g0164:                        [--exit-interval EXIT_INTERVAL]
g0164:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0164:                        [--exit-signal-handler]
g0164:                        [--tensorboard-dir TENSORBOARD_DIR]
g0164:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0164:                        [--no-bias-dropout-fusion]
g0164:                        [--disable-moe-token-dropping]
g0164:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0164:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0164:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0164:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0164:                        [--create-moe-param-group] [--use-flash-attn]
g0164:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0164:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0164:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0164:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0164:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0164:                        [--no-async-tensor-model-parallel-allreduce]
g0164:                        [--no-persist-layer-norm] [--sequence-parallel]
g0164:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0164:                        [--force-ds-sequence-parallel]
g0164:                        [--no-gradient-accumulation-fusion]
g0164:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0164:                        [--data-parallel-random-init]
g0164:                        [--init-method-std INIT_METHOD_STD]
g0164:                        [--init-method-xavier-uniform] [--lr LR]
g0164:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0164:                        [--lr-decay-iters LR_DECAY_ITERS]
g0164:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0164:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0164:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0164:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0164:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0164:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0164:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0164:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0164:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0164:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0164:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0164:                        [--no-initialization] [--use-checkpoint-args]
g0164:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0164:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0164:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0164:                        [--min-loss-scale MIN_LOSS_SCALE]
g0164:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0164:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0164:                        [--no-query-key-layer-scaling]
g0164:                        [--attention-softmax-in-fp32]
g0164:                        [--accumulate-allreduce-grads-in-fp32]
g0164:                        [--fp16-lm-cross-entropy]
g0164:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0164:                        [--enable-expert-tensor-parallelism]
g0164:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0164:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0164:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0164:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0164:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0164:                        [--overlap-p2p-communication]
g0164:                        [--distributed-backend {nccl,gloo,ccl}]
g0164:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0164:                        [--DDP-impl {local,torch,FSDP}]
g0164:                        [--no-contiguous-buffers-in-local-ddp]
g0164:                        [--no-scatter-gather-tensors-in-pipeline]
g0164:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0164:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0164:                        [--use-cpu-initialization]
g0164:                        [--empty-unused-memory-level {0,1,2}]
g0164:                        [--standalone-embedding-stage]
g0164:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0164:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0164:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0164:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0164:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0164:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0164:                        [--test-data-path [TEST_DATA_PATH ...]]
g0164:                        [--data-cache-path DATA_CACHE_PATH]
g0164:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0164:                        [--merge-file MERGE_FILE]
g0164:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0164:                        [--seq-length SEQ_LENGTH]
g0164:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0164:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0164:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0164:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0164:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0164:                        [--num-workers NUM_WORKERS]
g0164:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0164:                        [--tokenizer-model TOKENIZER_MODEL]
g0164:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0164:                        [--reset-attention-mask] [--eod-mask-loss]
g0164:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0164:                        [--return-data-index]
g0164:                        [--data-efficiency-curriculum-learning]
g0164:                        [--train-idx-path TRAIN_IDX_PATH]
g0164:                        [--train-desc-path TRAIN_DESC_PATH]
g0164:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0164:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0164:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0164:                        [--repeated-dataloader] [--adlr-autoresume]
g0164:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0164:                        [--ict-head-size ICT_HEAD_SIZE]
g0164:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0164:                        [--biencoder-shared-query-context-model]
g0164:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0164:                        [--titles-data-path TITLES_DATA_PATH]
g0164:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0164:                        [--use-one-sent-docs]
g0164:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0164:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0164:                        [--retriever-score-scaling]
g0164:                        [--block-data-path BLOCK_DATA_PATH]
g0164:                        [--embedding-path EMBEDDING_PATH]
g0164:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0164:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0164:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0164:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0164:                        [--patch-dim PATCH_DIM]
g0164:                        [--classes-fraction CLASSES_FRACTION]
g0164:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0164:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0164:                        [--vision-pretraining]
g0164:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0164:                        [--vision-backbone-type {vit,mit,swin}]
g0164:                        [--swin-backbone-type {tiny,base,h3}]
g0164:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0164:                        [--iter-per-epoch ITER_PER_EPOCH]
g0164:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0164:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0164:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0164:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0164:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0164:                        [--dino-norm-last-layer]
g0164:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0164:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0164:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0164:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0164:                        [--timing-log-level {0,1,2}]
g0164:                        [--no-barrier-with-level-1-timing]
g0164:                        [--timing-log-option {max,minmax,all}]
g0164:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0164:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0164:                        [--log-timers-to-tensorboard]
g0164:                        [--log-batch-size-to-tensorboard]
g0164:                        [--no-log-learnig-rate-to-tensorboard]
g0164:                        [--no-log-loss-scale-to-tensorboard]
g0164:                        [--log-validation-ppl-to-tensorboard]
g0164:                        [--log-optimizer-states-to-tensorboard]
g0164:                        [--log-memory-to-tensorboard]
g0164:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0164:                        [--wandb-entity WANDB_ENTITY]
g0164:                        [--wandb-project WANDB_PROJECT]
g0164:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0164:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0164:                        [--zero-contigious-gradients]
g0164:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0164:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0164:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0164:                        [--scattered-embeddings] [--split-transformers]
g0164:                        [--memory-centric-tiled-linear]
g0164:                        [--tile-factor TILE_FACTOR]
g0164:                        [--deepspeed-activation-checkpointing]
g0164:                        [--partition-activations] [--contigious-checkpointing]
g0164:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0164:                        [--profile-backward]
g0164:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0164:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0164:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0164:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0164:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0164:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0164:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0164:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0164:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0164:                        [--output-bert-embeddings]
g0164:                        [--bert-embedder-type {megatron,huggingface}]
g0164:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0164:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0164:                        [--transformer-impl {local,transformer_engine}]
g0164:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0164:                        [--fp8-amax-compute-algo {most_recent,max}]
g0164:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0164:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0164:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0164:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0164:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0164:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0164:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0164:                        [--retro-return-doc-ids] [--deepspeed]
g0164:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0164:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0164: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0164: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0161: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0161: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0161: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0161: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0161: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0161:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0161:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0161:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0161:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0161:                        [--expert-interval EXPERT_INTERVAL]
g0161:                        [--hidden-size HIDDEN_SIZE]
g0161:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0161:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0161:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0161:                        [--kv-channels KV_CHANNELS]
g0161:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0161:                        [--use-rotary-position-embeddings]
g0161:                        [--rotary-percent ROTARY_PERCENT]
g0161:                        [--no-position-embedding]
g0161:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0161:                        [--normalization {layernorm,rmsnorm}]
g0161:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0161:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0161:                        [--apply-residual-connection-post-layernorm]
g0161:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0161:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0161:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0161:                        [--untie-embeddings-and-output-weights]
g0161:                        [--embedding-weights-in-fp32]
g0161:                        [--attention-dropout ATTENTION_DROPOUT]
g0161:                        [--hidden-dropout HIDDEN_DROPOUT]
g0161:                        [--weight-decay WEIGHT_DECAY]
g0161:                        [--start-weight-decay START_WEIGHT_DECAY]
g0161:                        [--end-weight-decay END_WEIGHT_DECAY]
g0161:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0161:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0161:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0161:                        [--sgd-momentum SGD_MOMENTUM]
g0161:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0161:                        [--batch-size BATCH_SIZE]
g0161:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0161:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0161:                        [--recompute-activations]
g0161:                        [--recompute-granularity {full,selective}]
g0161:                        [--distribute-saved-activations]
g0161:                        [--recompute-method {uniform,block}]
g0161:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0161:                        [--checkpoint-activations]
g0161:                        [--distribute-checkpointed-activations]
g0161:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0161:                        [--train-iters TRAIN_ITERS]
g0161:                        [--train-samples TRAIN_SAMPLES]
g0161:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0161:                        [--log-interval LOG_INTERVAL]
g0161:                        [--exit-interval EXIT_INTERVAL]
g0161:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0161:                        [--exit-signal-handler]
g0161:                        [--tensorboard-dir TENSORBOARD_DIR]
g0161:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0161:                        [--no-bias-dropout-fusion]
g0161:                        [--disable-moe-token-dropping]
g0161:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0161:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0161:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0161:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0161:                        [--create-moe-param-group] [--use-flash-attn]
g0161:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0161:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0161:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0161:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0161:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0161:                        [--no-async-tensor-model-parallel-allreduce]
g0161:                        [--no-persist-layer-norm] [--sequence-parallel]
g0161:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0161:                        [--force-ds-sequence-parallel]
g0161:                        [--no-gradient-accumulation-fusion]
g0161:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0161:                        [--data-parallel-random-init]
g0161:                        [--init-method-std INIT_METHOD_STD]
g0161:                        [--init-method-xavier-uniform] [--lr LR]
g0161:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0161:                        [--lr-decay-iters LR_DECAY_ITERS]
g0161:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0161:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0161:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0161:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0161:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0161:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0161:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0161:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0161:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0161:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0161:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0161:                        [--no-initialization] [--use-checkpoint-args]
g0161:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0161:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0161:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0161:                        [--min-loss-scale MIN_LOSS_SCALE]
g0161:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0161:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0161:                        [--no-query-key-layer-scaling]
g0161:                        [--attention-softmax-in-fp32]
g0161:                        [--accumulate-allreduce-grads-in-fp32]
g0161:                        [--fp16-lm-cross-entropy]
g0161:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0161:                        [--enable-expert-tensor-parallelism]
g0161:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0161:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0161:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0161:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0161:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0161:                        [--overlap-p2p-communication]
g0161:                        [--distributed-backend {nccl,gloo,ccl}]
g0161:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0161:                        [--DDP-impl {local,torch,FSDP}]
g0161:                        [--no-contiguous-buffers-in-local-ddp]
g0161:                        [--no-scatter-gather-tensors-in-pipeline]
g0161:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0161:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0161:                        [--use-cpu-initialization]
g0161:                        [--empty-unused-memory-level {0,1,2}]
g0161:                        [--standalone-embedding-stage]
g0161:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0161:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0161:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0161:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0161:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0161:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0161:                        [--test-data-path [TEST_DATA_PATH ...]]
g0161:                        [--data-cache-path DATA_CACHE_PATH]
g0161:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0161:                        [--merge-file MERGE_FILE]
g0161:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0161:                        [--seq-length SEQ_LENGTH]
g0161:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0161:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0161:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0161:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0161:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0161:                        [--num-workers NUM_WORKERS]
g0161:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0161:                        [--tokenizer-model TOKENIZER_MODEL]
g0161:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0161:                        [--reset-attention-mask] [--eod-mask-loss]
g0161:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0161:                        [--return-data-index]
g0161:                        [--data-efficiency-curriculum-learning]
g0161:                        [--train-idx-path TRAIN_IDX_PATH]
g0161:                        [--train-desc-path TRAIN_DESC_PATH]
g0161:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0161:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0161:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0161:                        [--repeated-dataloader] [--adlr-autoresume]
g0161:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0161:                        [--ict-head-size ICT_HEAD_SIZE]
g0161:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0161:                        [--biencoder-shared-query-context-model]
g0161:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0161:                        [--titles-data-path TITLES_DATA_PATH]
g0161:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0161:                        [--use-one-sent-docs]
g0161:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0161:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0161:                        [--retriever-score-scaling]
g0161:                        [--block-data-path BLOCK_DATA_PATH]
g0161:                        [--embedding-path EMBEDDING_PATH]
g0161:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0161:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0161:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0161:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0161:                        [--patch-dim PATCH_DIM]
g0161:                        [--classes-fraction CLASSES_FRACTION]
g0161:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0161:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0161:                        [--vision-pretraining]
g0161:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0161:                        [--vision-backbone-type {vit,mit,swin}]
g0161:                        [--swin-backbone-type {tiny,base,h3}]
g0161:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0161:                        [--iter-per-epoch ITER_PER_EPOCH]
g0161:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0161:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0161:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0161:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0161:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0161:                        [--dino-norm-last-layer]
g0161:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0161:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0161:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0161:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0161:                        [--timing-log-level {0,1,2}]
g0161:                        [--no-barrier-with-level-1-timing]
g0161:                        [--timing-log-option {max,minmax,all}]
g0161:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0161:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0161:                        [--log-timers-to-tensorboard]
g0161:                        [--log-batch-size-to-tensorboard]
g0161:                        [--no-log-learnig-rate-to-tensorboard]
g0161:                        [--no-log-loss-scale-to-tensorboard]
g0161:                        [--log-validation-ppl-to-tensorboard]
g0161:                        [--log-optimizer-states-to-tensorboard]
g0161:                        [--log-memory-to-tensorboard]
g0161:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0161:                        [--wandb-entity WANDB_ENTITY]
g0161:                        [--wandb-project WANDB_PROJECT]
g0161:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0161:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0161:                        [--zero-contigious-gradients]
g0161:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0161:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0161:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0161:                        [--scattered-embeddings] [--split-transformers]
g0161:                        [--memory-centric-tiled-linear]
g0161:                        [--tile-factor TILE_FACTOR]
g0161:                        [--deepspeed-activation-checkpointing]
g0161:                        [--partition-activations] [--contigious-checkpointing]
g0161:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0161:                        [--profile-backward]
g0161:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0161:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0161:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0161:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0161:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0161:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0161:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0161:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0161:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0161:                        [--output-bert-embeddings]
g0161:                        [--bert-embedder-type {megatron,huggingface}]
g0161:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0161:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0161:                        [--transformer-impl {local,transformer_engine}]
g0161:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0161:                        [--fp8-amax-compute-algo {most_recent,max}]
g0161:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0161:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0161:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0161:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0161:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0161:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0161:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0161:                        [--retro-return-doc-ids] [--deepspeed]
g0161:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0161:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0161: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0161:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0161:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0161:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0161:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0161:                        [--expert-interval EXPERT_INTERVAL]
g0161:                        [--hidden-size HIDDEN_SIZE]
g0161:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0161:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0161:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0161:                        [--kv-channels KV_CHANNELS]
g0161:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0161:                        [--use-rotary-position-embeddings]
g0161:                        [--rotary-percent ROTARY_PERCENT]
g0161:                        [--no-position-embedding]
g0161:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0161:                        [--normalization {layernorm,rmsnorm}]
g0161:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0161:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0161:                        [--apply-residual-connection-post-layernorm]
g0161:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0161:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0161:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0161:                        [--untie-embeddings-and-output-weights]
g0161:                        [--embedding-weights-in-fp32]
g0161:                        [--attention-dropout ATTENTION_DROPOUT]
g0161:                        [--hidden-dropout HIDDEN_DROPOUT]
g0161:                        [--weight-decay WEIGHT_DECAY]
g0161:                        [--start-weight-decay START_WEIGHT_DECAY]
g0161:                        [--end-weight-decay END_WEIGHT_DECAY]
g0161:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0161:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0161:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0161:                        [--sgd-momentum SGD_MOMENTUM]
g0161:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0161:                        [--batch-size BATCH_SIZE]
g0161:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0161:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0161:                        [--recompute-activations]
g0161:                        [--recompute-granularity {full,selective}]
g0161:                        [--distribute-saved-activations]
g0161:                        [--recompute-method {uniform,block}]
g0161:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0161:                        [--checkpoint-activations]
g0161:                        [--distribute-checkpointed-activations]
g0161:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0161:                        [--train-iters TRAIN_ITERS]
g0161:                        [--train-samples TRAIN_SAMPLES]
g0161:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0161:                        [--log-interval LOG_INTERVAL]
g0161:                        [--exit-interval EXIT_INTERVAL]
g0161:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0161:                        [--exit-signal-handler]
g0161:                        [--tensorboard-dir TENSORBOARD_DIR]
g0161:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0161:                        [--no-bias-dropout-fusion]
g0161:                        [--disable-moe-token-dropping]
g0161:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0161:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0161:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0161:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0161:                        [--create-moe-param-group] [--use-flash-attn]
g0161:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0161:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0161:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0161:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0161:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0161:                        [--no-async-tensor-model-parallel-allreduce]
g0161:                        [--no-persist-layer-norm] [--sequence-parallel]
g0161:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0161:                        [--force-ds-sequence-parallel]
g0161:                        [--no-gradient-accumulation-fusion]
g0161:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0161:                        [--data-parallel-random-init]
g0161:                        [--init-method-std INIT_METHOD_STD]
g0161:                        [--init-method-xavier-uniform] [--lr LR]
g0161:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0161:                        [--lr-decay-iters LR_DECAY_ITERS]
g0161:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0161:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0161:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0161:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0161:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0161:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0161:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0161:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0161:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0161:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0161:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0161:                        [--no-initialization] [--use-checkpoint-args]
g0161:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0161:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0161:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0161:                        [--min-loss-scale MIN_LOSS_SCALE]
g0161:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0161:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0161:                        [--no-query-key-layer-scaling]
g0161:                        [--attention-softmax-in-fp32]
g0161:                        [--accumulate-allreduce-grads-in-fp32]
g0161:                        [--fp16-lm-cross-entropy]
g0161:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0161:                        [--enable-expert-tensor-parallelism]
g0161:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0161:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0161:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0161:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0161:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0161:                        [--overlap-p2p-communication]
g0161:                        [--distributed-backend {nccl,gloo,ccl}]
g0161:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0161:                        [--DDP-impl {local,torch,FSDP}]
g0161:                        [--no-contiguous-buffers-in-local-ddp]
g0161:                        [--no-scatter-gather-tensors-in-pipeline]
g0161:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0161:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0161:                        [--use-cpu-initialization]
g0161:                        [--empty-unused-memory-level {0,1,2}]
g0161:                        [--standalone-embedding-stage]
g0161:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0161:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0161:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0161:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0161:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0161:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0161:                        [--test-data-path [TEST_DATA_PATH ...]]
g0161:                        [--data-cache-path DATA_CACHE_PATH]
g0161:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0161:                        [--merge-file MERGE_FILE]
g0161:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0161:                        [--seq-length SEQ_LENGTH]
g0161:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0161:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0161:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0161:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0161:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0161:                        [--num-workers NUM_WORKERS]
g0161:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0161:                        [--tokenizer-model TOKENIZER_MODEL]
g0161:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0161:                        [--reset-attention-mask] [--eod-mask-loss]
g0161:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0161:                        [--return-data-index]
g0161:                        [--data-efficiency-curriculum-learning]
g0161:                        [--train-idx-path TRAIN_IDX_PATH]
g0161:                        [--train-desc-path TRAIN_DESC_PATH]
g0161:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0161:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0161:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0161:                        [--repeated-dataloader] [--adlr-autoresume]
g0161:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0161:                        [--ict-head-size ICT_HEAD_SIZE]
g0161:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0161:                        [--biencoder-shared-query-context-model]
g0161:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0161:                        [--titles-data-path TITLES_DATA_PATH]
g0161:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0161:                        [--use-one-sent-docs]
g0161:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0161:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0161:                        [--retriever-score-scaling]
g0161:                        [--block-data-path BLOCK_DATA_PATH]
g0161:                        [--embedding-path EMBEDDING_PATH]
g0161:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0161:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0161:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0161:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0161:                        [--patch-dim PATCH_DIM]
g0161:                        [--classes-fraction CLASSES_FRACTION]
g0161:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0161:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0161:                        [--vision-pretraining]
g0161:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0161:                        [--vision-backbone-type {vit,mit,swin}]
g0161:                        [--swin-backbone-type {tiny,base,h3}]
g0161:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0161:                        [--iter-per-epoch ITER_PER_EPOCH]
g0161:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0161:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0161:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0161:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0161:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0161:                        [--dino-norm-last-layer]
g0161:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0161:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0161:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0161:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0161:                        [--timing-log-level {0,1,2}]
g0161:                        [--no-barrier-with-level-1-timing]
g0161:                        [--timing-log-option {max,minmax,all}]
g0161:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0161:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0161:                        [--log-timers-to-tensorboard]
g0161:                        [--log-batch-size-to-tensorboard]
g0161:                        [--no-log-learnig-rate-to-tensorboard]
g0161:                        [--no-log-loss-scale-to-tensorboard]
g0161:                        [--log-validation-ppl-to-tensorboard]
g0161:                        [--log-optimizer-states-to-tensorboard]
g0161:                        [--log-memory-to-tensorboard]
g0161:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0161:                        [--wandb-entity WANDB_ENTITY]
g0161:                        [--wandb-project WANDB_PROJECT]
g0161:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0161:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0161:                        [--zero-contigious-gradients]
g0161:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0161:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0161:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0161:                        [--scattered-embeddings] [--split-transformers]
g0161:                        [--memory-centric-tiled-linear]
g0161:                        [--tile-factor TILE_FACTOR]
g0161:                        [--deepspeed-activation-checkpointing]
g0161:                        [--partition-activations] [--contigious-checkpointing]
g0161:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0161:                        [--profile-backward]
g0161:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0161:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0161:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0161:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0161:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0161:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0161:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0161:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0161:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0161:                        [--output-bert-embeddings]
g0161:                        [--bert-embedder-type {megatron,huggingface}]
g0161:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0161:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0161:                        [--transformer-impl {local,transformer_engine}]
g0161:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0161:                        [--fp8-amax-compute-algo {most_recent,max}]
g0161:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0161:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0161:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0161:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0161:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0161:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0161:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0161:                        [--retro-return-doc-ids] [--deepspeed]
g0161:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0161:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0161: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0161: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0161:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0161:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0161:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0161:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0161:                        [--expert-interval EXPERT_INTERVAL]
g0161:                        [--hidden-size HIDDEN_SIZE]
g0161:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0161:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0161:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0161:                        [--kv-channels KV_CHANNELS]
g0161:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0161:                        [--use-rotary-position-embeddings]
g0161:                        [--rotary-percent ROTARY_PERCENT]
g0161:                        [--no-position-embedding]
g0161:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0161:                        [--normalization {layernorm,rmsnorm}]
g0161:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0161:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0161:                        [--apply-residual-connection-post-layernorm]
g0161:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0161:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0161:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0161:                        [--untie-embeddings-and-output-weights]
g0161:                        [--embedding-weights-in-fp32]
g0161:                        [--attention-dropout ATTENTION_DROPOUT]
g0161:                        [--hidden-dropout HIDDEN_DROPOUT]
g0161:                        [--weight-decay WEIGHT_DECAY]
g0161:                        [--start-weight-decay START_WEIGHT_DECAY]
g0161:                        [--end-weight-decay END_WEIGHT_DECAY]
g0161:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0161:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0161:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0161:                        [--sgd-momentum SGD_MOMENTUM]
g0161:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0161:                        [--batch-size BATCH_SIZE]
g0161:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0161:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0161:                        [--recompute-activations]
g0161:                        [--recompute-granularity {full,selective}]
g0161:                        [--distribute-saved-activations]
g0161:                        [--recompute-method {uniform,block}]
g0161:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0161:                        [--checkpoint-activations]
g0161:                        [--distribute-checkpointed-activations]
g0161:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0161:                        [--train-iters TRAIN_ITERS]
g0161:                        [--train-samples TRAIN_SAMPLES]
g0161:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0161:                        [--log-interval LOG_INTERVAL]
g0161:                        [--exit-interval EXIT_INTERVAL]
g0161:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0161:                        [--exit-signal-handler]
g0161:                        [--tensorboard-dir TENSORBOARD_DIR]
g0161:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0161:                        [--no-bias-dropout-fusion]
g0161:                        [--disable-moe-token-dropping]
g0161:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0161:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0161:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0161:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0161:                        [--create-moe-param-group] [--use-flash-attn]
g0161:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0161:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0161:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0161:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0161:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0161:                        [--no-async-tensor-model-parallel-allreduce]
g0161:                        [--no-persist-layer-norm] [--sequence-parallel]
g0161:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0161:                        [--force-ds-sequence-parallel]
g0161:                        [--no-gradient-accumulation-fusion]
g0161:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0161:                        [--data-parallel-random-init]
g0161:                        [--init-method-std INIT_METHOD_STD]
g0161:                        [--init-method-xavier-uniform] [--lr LR]
g0161:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0161:                        [--lr-decay-iters LR_DECAY_ITERS]
g0161:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0161:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0161:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0161:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0161:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0161:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0161:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0161:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0161:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0161:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0161:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0161:                        [--no-initialization] [--use-checkpoint-args]
g0161:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0161:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0161:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0161:                        [--min-loss-scale MIN_LOSS_SCALE]
g0161:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0161:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0161:                        [--no-query-key-layer-scaling]
g0161:                        [--attention-softmax-in-fp32]
g0161:                        [--accumulate-allreduce-grads-in-fp32]
g0161:                        [--fp16-lm-cross-entropy]
g0161:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0161:                        [--enable-expert-tensor-parallelism]
g0161:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0161:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0161:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0161:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0161:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0161:                        [--overlap-p2p-communication]
g0161:                        [--distributed-backend {nccl,gloo,ccl}]
g0161:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0161:                        [--DDP-impl {local,torch,FSDP}]
g0161:                        [--no-contiguous-buffers-in-local-ddp]
g0161:                        [--no-scatter-gather-tensors-in-pipeline]
g0161:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0161:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0161:                        [--use-cpu-initialization]
g0161:                        [--empty-unused-memory-level {0,1,2}]
g0161:                        [--standalone-embedding-stage]
g0161:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0161:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0161:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0161:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0161:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0161:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0161:                        [--test-data-path [TEST_DATA_PATH ...]]
g0161:                        [--data-cache-path DATA_CACHE_PATH]
g0161:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0161:                        [--merge-file MERGE_FILE]
g0161:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0161:                        [--seq-length SEQ_LENGTH]
g0161:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0161:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0161:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0161:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0161:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0161:                        [--num-workers NUM_WORKERS]
g0161:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0161:                        [--tokenizer-model TOKENIZER_MODEL]
g0161:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0161:                        [--reset-attention-mask] [--eod-mask-loss]
g0161:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0161:                        [--return-data-index]
g0161:                        [--data-efficiency-curriculum-learning]
g0161:                        [--train-idx-path TRAIN_IDX_PATH]
g0161:                        [--train-desc-path TRAIN_DESC_PATH]
g0161:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0161:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0161:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0161:                        [--repeated-dataloader] [--adlr-autoresume]
g0161:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0161:                        [--ict-head-size ICT_HEAD_SIZE]
g0161:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0161:                        [--biencoder-shared-query-context-model]
g0161:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0161:                        [--titles-data-path TITLES_DATA_PATH]
g0161:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0161:                        [--use-one-sent-docs]
g0161:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0161:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0161:                        [--retriever-score-scaling]
g0161:                        [--block-data-path BLOCK_DATA_PATH]
g0161:                        [--embedding-path EMBEDDING_PATH]
g0161:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0161:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0161:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0161:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0161:                        [--patch-dim PATCH_DIM]
g0161:                        [--classes-fraction CLASSES_FRACTION]
g0161:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0161:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0161:                        [--vision-pretraining]
g0161:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0161:                        [--vision-backbone-type {vit,mit,swin}]
g0161:                        [--swin-backbone-type {tiny,base,h3}]
g0161:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0161:                        [--iter-per-epoch ITER_PER_EPOCH]
g0161:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0161:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0161:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0161:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0161:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0161:                        [--dino-norm-last-layer]
g0161:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0161:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0161:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0161:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0161:                        [--timing-log-level {0,1,2}]
g0161:                        [--no-barrier-with-level-1-timing]
g0161:                        [--timing-log-option {max,minmax,all}]
g0161:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0161:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0161:                        [--log-timers-to-tensorboard]
g0161:                        [--log-batch-size-to-tensorboard]
g0161:                        [--no-log-learnig-rate-to-tensorboard]
g0161:                        [--no-log-loss-scale-to-tensorboard]
g0161:                        [--log-validation-ppl-to-tensorboard]
g0161:                        [--log-optimizer-states-to-tensorboard]
g0161:                        [--log-memory-to-tensorboard]
g0161:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0161:                        [--wandb-entity WANDB_ENTITY]
g0161:                        [--wandb-project WANDB_PROJECT]
g0161:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0161:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0161:                        [--zero-contigious-gradients]
g0161:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0161:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0161:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0161:                        [--scattered-embeddings] [--split-transformers]
g0161:                        [--memory-centric-tiled-linear]
g0161:                        [--tile-factor TILE_FACTOR]
g0161:                        [--deepspeed-activation-checkpointing]
g0161:                        [--partition-activations] [--contigious-checkpointing]
g0161:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0161:                        [--profile-backward]
g0161:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0161:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0161:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0161:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0161:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0161:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0161:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0161:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0161:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0161:                        [--output-bert-embeddings]
g0161:                        [--bert-embedder-type {megatron,huggingface}]
g0161:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0161:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0161:                        [--transformer-impl {local,transformer_engine}]
g0161:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0161:                        [--fp8-amax-compute-algo {most_recent,max}]
g0161:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0161:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0161:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0161:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0161:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0161:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0161:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0161:                        [--retro-return-doc-ids] [--deepspeed]
g0161:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0161:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0161: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0161: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0161: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0161:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0161:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0161:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0161:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0161:                        [--expert-interval EXPERT_INTERVAL]
g0161:                        [--hidden-size HIDDEN_SIZE]
g0161:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0161:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0161:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0161:                        [--kv-channels KV_CHANNELS]
g0161:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0161:                        [--use-rotary-position-embeddings]
g0161:                        [--rotary-percent ROTARY_PERCENT]
g0161:                        [--no-position-embedding]
g0161:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0161:                        [--normalization {layernorm,rmsnorm}]
g0161:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0161:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0161:                        [--apply-residual-connection-post-layernorm]
g0161:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0161:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0161:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0161:                        [--untie-embeddings-and-output-weights]
g0161:                        [--embedding-weights-in-fp32]
g0161:                        [--attention-dropout ATTENTION_DROPOUT]
g0161:                        [--hidden-dropout HIDDEN_DROPOUT]
g0161:                        [--weight-decay WEIGHT_DECAY]
g0161:                        [--start-weight-decay START_WEIGHT_DECAY]
g0161:                        [--end-weight-decay END_WEIGHT_DECAY]
g0161:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0161:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0161:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0161:                        [--sgd-momentum SGD_MOMENTUM]
g0161:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0161:                        [--batch-size BATCH_SIZE]
g0161:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0161:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0161:                        [--recompute-activations]
g0161:                        [--recompute-granularity {full,selective}]
g0161:                        [--distribute-saved-activations]
g0161:                        [--recompute-method {uniform,block}]
g0161:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0161:                        [--checkpoint-activations]
g0161:                        [--distribute-checkpointed-activations]
g0161:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0161:                        [--train-iters TRAIN_ITERS]
g0161:                        [--train-samples TRAIN_SAMPLES]
g0161:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0161:                        [--log-interval LOG_INTERVAL]
g0161:                        [--exit-interval EXIT_INTERVAL]
g0161:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0161:                        [--exit-signal-handler]
g0161:                        [--tensorboard-dir TENSORBOARD_DIR]
g0161:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0161:                        [--no-bias-dropout-fusion]
g0161:                        [--disable-moe-token-dropping]
g0161:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0161:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0161:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0161:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0161:                        [--create-moe-param-group] [--use-flash-attn]
g0161:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0161:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0161:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0161:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0161:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0161:                        [--no-async-tensor-model-parallel-allreduce]
g0161:                        [--no-persist-layer-norm] [--sequence-parallel]
g0161:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0161:                        [--force-ds-sequence-parallel]
g0161:                        [--no-gradient-accumulation-fusion]
g0161:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0161:                        [--data-parallel-random-init]
g0161:                        [--init-method-std INIT_METHOD_STD]
g0161:                        [--init-method-xavier-uniform] [--lr LR]
g0161:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0161:                        [--lr-decay-iters LR_DECAY_ITERS]
g0161:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0161:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0161:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0161:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0161:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0161:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0161:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0161:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0161:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0161:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0161:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0161:                        [--no-initialization] [--use-checkpoint-args]
g0161:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0161:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0161:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0161:                        [--min-loss-scale MIN_LOSS_SCALE]
g0161:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0161:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0161:                        [--no-query-key-layer-scaling]
g0161:                        [--attention-softmax-in-fp32]
g0161:                        [--accumulate-allreduce-grads-in-fp32]
g0161:                        [--fp16-lm-cross-entropy]
g0161:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0161:                        [--enable-expert-tensor-parallelism]
g0161:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0161:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0161:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0161:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0161:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0161:                        [--overlap-p2p-communication]
g0161:                        [--distributed-backend {nccl,gloo,ccl}]
g0161:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0161:                        [--DDP-impl {local,torch,FSDP}]
g0161:                        [--no-contiguous-buffers-in-local-ddp]
g0161:                        [--no-scatter-gather-tensors-in-pipeline]
g0161:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0161:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0161:                        [--use-cpu-initialization]
g0161:                        [--empty-unused-memory-level {0,1,2}]
g0161:                        [--standalone-embedding-stage]
g0161:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0161:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0161:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0161:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0161:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0161:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0161:                        [--test-data-path [TEST_DATA_PATH ...]]
g0161:                        [--data-cache-path DATA_CACHE_PATH]
g0161:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0161:                        [--merge-file MERGE_FILE]
g0161:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0161:                        [--seq-length SEQ_LENGTH]
g0161:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0161:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0161:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0161:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0161:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0161:                        [--num-workers NUM_WORKERS]
g0161:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0161:                        [--tokenizer-model TOKENIZER_MODEL]
g0161:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0161:                        [--reset-attention-mask] [--eod-mask-loss]
g0161:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0161:                        [--return-data-index]
g0161:                        [--data-efficiency-curriculum-learning]
g0161:                        [--train-idx-path TRAIN_IDX_PATH]
g0161:                        [--train-desc-path TRAIN_DESC_PATH]
g0161:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0161:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0161:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0161:                        [--repeated-dataloader] [--adlr-autoresume]
g0161:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0161:                        [--ict-head-size ICT_HEAD_SIZE]
g0161:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0161:                        [--biencoder-shared-query-context-model]
g0161:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0161:                        [--titles-data-path TITLES_DATA_PATH]
g0161:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0161:                        [--use-one-sent-docs]
g0161:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0161:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0161:                        [--retriever-score-scaling]
g0161:                        [--block-data-path BLOCK_DATA_PATH]
g0161:                        [--embedding-path EMBEDDING_PATH]
g0161:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0161:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0161:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0161:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0161:                        [--patch-dim PATCH_DIM]
g0161:                        [--classes-fraction CLASSES_FRACTION]
g0161:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0161:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0161:                        [--vision-pretraining]
g0161:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0161:                        [--vision-backbone-type {vit,mit,swin}]
g0161:                        [--swin-backbone-type {tiny,base,h3}]
g0161:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0161:                        [--iter-per-epoch ITER_PER_EPOCH]
g0161:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0161:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0161:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0161:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0161:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0161:                        [--dino-norm-last-layer]
g0161:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0161:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0161:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0161:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0161:                        [--timing-log-level {0,1,2}]
g0161:                        [--no-barrier-with-level-1-timing]
g0161:                        [--timing-log-option {max,minmax,all}]
g0161:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0161:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0161:                        [--log-timers-to-tensorboard]
g0161:                        [--log-batch-size-to-tensorboard]
g0161:                        [--no-log-learnig-rate-to-tensorboard]
g0161:                        [--no-log-loss-scale-to-tensorboard]
g0161:                        [--log-validation-ppl-to-tensorboard]
g0161:                        [--log-optimizer-states-to-tensorboard]
g0161:                        [--log-memory-to-tensorboard]
g0161:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0161:                        [--wandb-entity WANDB_ENTITY]
g0161:                        [--wandb-project WANDB_PROJECT]
g0161:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0161:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0161:                        [--zero-contigious-gradients]
g0161:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0161:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0161:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0161:                        [--scattered-embeddings] [--split-transformers]
g0161:                        [--memory-centric-tiled-linear]
g0161:                        [--tile-factor TILE_FACTOR]
g0161:                        [--deepspeed-activation-checkpointing]
g0161:                        [--partition-activations] [--contigious-checkpointing]
g0161:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0161:                        [--profile-backward]
g0161:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0161:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0161:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0161:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0161:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0161:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0161:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0161:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0161:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0161:                        [--output-bert-embeddings]
g0161:                        [--bert-embedder-type {megatron,huggingface}]
g0161:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0161:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0161:                        [--transformer-impl {local,transformer_engine}]
g0161:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0161:                        [--fp8-amax-compute-algo {most_recent,max}]
g0161:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0161:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0161:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0161:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0161:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0161:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0161:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0161:                        [--retro-return-doc-ids] [--deepspeed]
g0161:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0161:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0161: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0163: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0163: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0163: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0163: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0163: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0163:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0163:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0163:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0163:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0163:                        [--expert-interval EXPERT_INTERVAL]
g0163:                        [--hidden-size HIDDEN_SIZE]
g0163:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0163:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0163:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0163:                        [--kv-channels KV_CHANNELS]
g0163:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0163:                        [--use-rotary-position-embeddings]
g0163:                        [--rotary-percent ROTARY_PERCENT]
g0163:                        [--no-position-embedding]
g0163:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0163:                        [--normalization {layernorm,rmsnorm}]
g0163:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0163:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0163:                        [--apply-residual-connection-post-layernorm]
g0163:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0163:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0163:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0163:                        [--untie-embeddings-and-output-weights]
g0163:                        [--embedding-weights-in-fp32]
g0163:                        [--attention-dropout ATTENTION_DROPOUT]
g0163:                        [--hidden-dropout HIDDEN_DROPOUT]
g0163:                        [--weight-decay WEIGHT_DECAY]
g0163:                        [--start-weight-decay START_WEIGHT_DECAY]
g0163:                        [--end-weight-decay END_WEIGHT_DECAY]
g0163:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0163:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0163:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0163:                        [--sgd-momentum SGD_MOMENTUM]
g0163:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0163:                        [--batch-size BATCH_SIZE]
g0163:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0163:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0163:                        [--recompute-activations]
g0163:                        [--recompute-granularity {full,selective}]
g0163:                        [--distribute-saved-activations]
g0163:                        [--recompute-method {uniform,block}]
g0163:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0163:                        [--checkpoint-activations]
g0163:                        [--distribute-checkpointed-activations]
g0163:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0163:                        [--train-iters TRAIN_ITERS]
g0163:                        [--train-samples TRAIN_SAMPLES]
g0163:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0163:                        [--log-interval LOG_INTERVAL]
g0163:                        [--exit-interval EXIT_INTERVAL]
g0163:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0163:                        [--exit-signal-handler]
g0163:                        [--tensorboard-dir TENSORBOARD_DIR]
g0163:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0163:                        [--no-bias-dropout-fusion]
g0163:                        [--disable-moe-token-dropping]
g0163:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0163:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0163:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0163:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0163:                        [--create-moe-param-group] [--use-flash-attn]
g0163:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0163:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0163:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0163:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0163:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0163:                        [--no-async-tensor-model-parallel-allreduce]
g0163:                        [--no-persist-layer-norm] [--sequence-parallel]
g0163:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0163:                        [--force-ds-sequence-parallel]
g0163:                        [--no-gradient-accumulation-fusion]
g0163:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0163:                        [--data-parallel-random-init]
g0163:                        [--init-method-std INIT_METHOD_STD]
g0163:                        [--init-method-xavier-uniform] [--lr LR]
g0163:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0163:                        [--lr-decay-iters LR_DECAY_ITERS]
g0163:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0163:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0163:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0163:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0163:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0163:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0163:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0163:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0163:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0163:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0163:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0163:                        [--no-initialization] [--use-checkpoint-args]
g0163:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0163:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0163:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0163:                        [--min-loss-scale MIN_LOSS_SCALE]
g0163:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0163:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0163:                        [--no-query-key-layer-scaling]
g0163:                        [--attention-softmax-in-fp32]
g0163:                        [--accumulate-allreduce-grads-in-fp32]
g0163:                        [--fp16-lm-cross-entropy]
g0163:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0163:                        [--enable-expert-tensor-parallelism]
g0163:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0163:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0163:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0163:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0163:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0163:                        [--overlap-p2p-communication]
g0163:                        [--distributed-backend {nccl,gloo,ccl}]
g0163:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0163:                        [--DDP-impl {local,torch,FSDP}]
g0163:                        [--no-contiguous-buffers-in-local-ddp]
g0163:                        [--no-scatter-gather-tensors-in-pipeline]
g0163:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0163:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0163:                        [--use-cpu-initialization]
g0163:                        [--empty-unused-memory-level {0,1,2}]
g0163:                        [--standalone-embedding-stage]
g0163:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0163:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0163:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0163:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0163:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0163:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0163:                        [--test-data-path [TEST_DATA_PATH ...]]
g0163:                        [--data-cache-path DATA_CACHE_PATH]
g0163:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0163:                        [--merge-file MERGE_FILE]
g0163:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0163:                        [--seq-length SEQ_LENGTH]
g0163:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0163:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0163:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0163:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0163:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0163:                        [--num-workers NUM_WORKERS]
g0163:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0163:                        [--tokenizer-model TOKENIZER_MODEL]
g0163:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0163:                        [--reset-attention-mask] [--eod-mask-loss]
g0163:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0163:                        [--return-data-index]
g0163:                        [--data-efficiency-curriculum-learning]
g0163:                        [--train-idx-path TRAIN_IDX_PATH]
g0163:                        [--train-desc-path TRAIN_DESC_PATH]
g0163:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0163:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0163:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0163:                        [--repeated-dataloader] [--adlr-autoresume]
g0163:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0163:                        [--ict-head-size ICT_HEAD_SIZE]
g0163:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0163:                        [--biencoder-shared-query-context-model]
g0163:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0163:                        [--titles-data-path TITLES_DATA_PATH]
g0163:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0163:                        [--use-one-sent-docs]
g0163:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0163:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0163:                        [--retriever-score-scaling]
g0163:                        [--block-data-path BLOCK_DATA_PATH]
g0163:                        [--embedding-path EMBEDDING_PATH]
g0163:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0163:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0163:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0163:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0163:                        [--patch-dim PATCH_DIM]
g0163:                        [--classes-fraction CLASSES_FRACTION]
g0163:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0163:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0163:                        [--vision-pretraining]
g0163:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0163:                        [--vision-backbone-type {vit,mit,swin}]
g0163:                        [--swin-backbone-type {tiny,base,h3}]
g0163:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0163:                        [--iter-per-epoch ITER_PER_EPOCH]
g0163:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0163:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0163:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0163:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0163:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0163:                        [--dino-norm-last-layer]
g0163:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0163:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0163:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0163:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0163:                        [--timing-log-level {0,1,2}]
g0163:                        [--no-barrier-with-level-1-timing]
g0163:                        [--timing-log-option {max,minmax,all}]
g0163:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0163:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0163:                        [--log-timers-to-tensorboard]
g0163:                        [--log-batch-size-to-tensorboard]
g0163:                        [--no-log-learnig-rate-to-tensorboard]
g0163:                        [--no-log-loss-scale-to-tensorboard]
g0163:                        [--log-validation-ppl-to-tensorboard]
g0163:                        [--log-optimizer-states-to-tensorboard]
g0163:                        [--log-memory-to-tensorboard]
g0163:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0163:                        [--wandb-entity WANDB_ENTITY]
g0163:                        [--wandb-project WANDB_PROJECT]
g0163:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0163:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0163:                        [--zero-contigious-gradients]
g0163:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0163:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0163:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0163:                        [--scattered-embeddings] [--split-transformers]
g0163:                        [--memory-centric-tiled-linear]
g0163:                        [--tile-factor TILE_FACTOR]
g0163:                        [--deepspeed-activation-checkpointing]
g0163:                        [--partition-activations] [--contigious-checkpointing]
g0163:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0163:                        [--profile-backward]
g0163:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0163:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0163:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0163:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0163:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0163:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0163:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0163:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0163:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0163:                        [--output-bert-embeddings]
g0163:                        [--bert-embedder-type {megatron,huggingface}]
g0163:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0163:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0163:                        [--transformer-impl {local,transformer_engine}]
g0163:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0163:                        [--fp8-amax-compute-algo {most_recent,max}]
g0163:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0163:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0163:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0163:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0163:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0163:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0163:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0163:                        [--retro-return-doc-ids] [--deepspeed]
g0163:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0163:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0163: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0163: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0163:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0163:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0163:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0163:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0163:                        [--expert-interval EXPERT_INTERVAL]
g0163:                        [--hidden-size HIDDEN_SIZE]
g0163:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0163:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0163:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0163:                        [--kv-channels KV_CHANNELS]
g0163:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0163:                        [--use-rotary-position-embeddings]
g0163:                        [--rotary-percent ROTARY_PERCENT]
g0163:                        [--no-position-embedding]
g0163:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0163:                        [--normalization {layernorm,rmsnorm}]
g0163:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0163:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0163:                        [--apply-residual-connection-post-layernorm]
g0163:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0163:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0163:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0163:                        [--untie-embeddings-and-output-weights]
g0163:                        [--embedding-weights-in-fp32]
g0163:                        [--attention-dropout ATTENTION_DROPOUT]
g0163:                        [--hidden-dropout HIDDEN_DROPOUT]
g0163:                        [--weight-decay WEIGHT_DECAY]
g0163:                        [--start-weight-decay START_WEIGHT_DECAY]
g0163:                        [--end-weight-decay END_WEIGHT_DECAY]
g0163:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0163:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0163:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0163:                        [--sgd-momentum SGD_MOMENTUM]
g0163:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0163:                        [--batch-size BATCH_SIZE]
g0163:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0163:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0163:                        [--recompute-activations]
g0163:                        [--recompute-granularity {full,selective}]
g0163:                        [--distribute-saved-activations]
g0163:                        [--recompute-method {uniform,block}]
g0163:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0163:                        [--checkpoint-activations]
g0163:                        [--distribute-checkpointed-activations]
g0163:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0163:                        [--train-iters TRAIN_ITERS]
g0163:                        [--train-samples TRAIN_SAMPLES]
g0163:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0163:                        [--log-interval LOG_INTERVAL]
g0163:                        [--exit-interval EXIT_INTERVAL]
g0163:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0163:                        [--exit-signal-handler]
g0163:                        [--tensorboard-dir TENSORBOARD_DIR]
g0163:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0163:                        [--no-bias-dropout-fusion]
g0163:                        [--disable-moe-token-dropping]
g0163:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0163:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0163:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0163:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0163:                        [--create-moe-param-group] [--use-flash-attn]
g0163:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0163:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0163:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0163:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0163:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0163:                        [--no-async-tensor-model-parallel-allreduce]
g0163:                        [--no-persist-layer-norm] [--sequence-parallel]
g0163:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0163:                        [--force-ds-sequence-parallel]
g0163:                        [--no-gradient-accumulation-fusion]
g0163:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0163:                        [--data-parallel-random-init]
g0163:                        [--init-method-std INIT_METHOD_STD]
g0163:                        [--init-method-xavier-uniform] [--lr LR]
g0163:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0163:                        [--lr-decay-iters LR_DECAY_ITERS]
g0163:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0163:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0163:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0163:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0163:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0163:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0163:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0163:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0163:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0163:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0163:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0163:                        [--no-initialization] [--use-checkpoint-args]
g0163:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0163:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0163:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0163:                        [--min-loss-scale MIN_LOSS_SCALE]
g0163:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0163:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0163:                        [--no-query-key-layer-scaling]
g0163:                        [--attention-softmax-in-fp32]
g0163:                        [--accumulate-allreduce-grads-in-fp32]
g0163:                        [--fp16-lm-cross-entropy]
g0163:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0163:                        [--enable-expert-tensor-parallelism]
g0163:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0163:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0163:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0163:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0163:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0163:                        [--overlap-p2p-communication]
g0163:                        [--distributed-backend {nccl,gloo,ccl}]
g0163:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0163:                        [--DDP-impl {local,torch,FSDP}]
g0163:                        [--no-contiguous-buffers-in-local-ddp]
g0163:                        [--no-scatter-gather-tensors-in-pipeline]
g0163:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0163:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0163:                        [--use-cpu-initialization]
g0163:                        [--empty-unused-memory-level {0,1,2}]
g0163:                        [--standalone-embedding-stage]
g0163:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0163:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0163:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0163:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0163:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0163:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0163:                        [--test-data-path [TEST_DATA_PATH ...]]
g0163:                        [--data-cache-path DATA_CACHE_PATH]
g0163:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0163:                        [--merge-file MERGE_FILE]
g0163:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0163:                        [--seq-length SEQ_LENGTH]
g0163:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0163:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0163:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0163:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0163:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0163:                        [--num-workers NUM_WORKERS]
g0163:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0163:                        [--tokenizer-model TOKENIZER_MODEL]
g0163:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0163:                        [--reset-attention-mask] [--eod-mask-loss]
g0163:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0163:                        [--return-data-index]
g0163:                        [--data-efficiency-curriculum-learning]
g0163:                        [--train-idx-path TRAIN_IDX_PATH]
g0163:                        [--train-desc-path TRAIN_DESC_PATH]
g0163:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0163:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0163:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0163:                        [--repeated-dataloader] [--adlr-autoresume]
g0163:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0163:                        [--ict-head-size ICT_HEAD_SIZE]
g0163:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0163:                        [--biencoder-shared-query-context-model]
g0163:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0163:                        [--titles-data-path TITLES_DATA_PATH]
g0163:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0163:                        [--use-one-sent-docs]
g0163:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0163:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0163:                        [--retriever-score-scaling]
g0163:                        [--block-data-path BLOCK_DATA_PATH]
g0163:                        [--embedding-path EMBEDDING_PATH]
g0163:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0163:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0163:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0163:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0163:                        [--patch-dim PATCH_DIM]
g0163:                        [--classes-fraction CLASSES_FRACTION]
g0163:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0163:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0163:                        [--vision-pretraining]
g0163:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0163:                        [--vision-backbone-type {vit,mit,swin}]
g0163:                        [--swin-backbone-type {tiny,base,h3}]
g0163:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0163:                        [--iter-per-epoch ITER_PER_EPOCH]
g0163:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0163:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0163:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0163:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0163:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0163:                        [--dino-norm-last-layer]
g0163:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0163:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0163:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0163:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0163:                        [--timing-log-level {0,1,2}]
g0163:                        [--no-barrier-with-level-1-timing]
g0163:                        [--timing-log-option {max,minmax,all}]
g0163:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0163:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0163:                        [--log-timers-to-tensorboard]
g0163:                        [--log-batch-size-to-tensorboard]
g0163:                        [--no-log-learnig-rate-to-tensorboard]
g0163:                        [--no-log-loss-scale-to-tensorboard]
g0163:                        [--log-validation-ppl-to-tensorboard]
g0163:                        [--log-optimizer-states-to-tensorboard]
g0163:                        [--log-memory-to-tensorboard]
g0163:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0163:                        [--wandb-entity WANDB_ENTITY]
g0163:                        [--wandb-project WANDB_PROJECT]
g0163:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0163:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0163:                        [--zero-contigious-gradients]
g0163:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0163:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0163:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0163:                        [--scattered-embeddings] [--split-transformers]
g0163:                        [--memory-centric-tiled-linear]
g0163:                        [--tile-factor TILE_FACTOR]
g0163:                        [--deepspeed-activation-checkpointing]
g0163:                        [--partition-activations] [--contigious-checkpointing]
g0163:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0163:                        [--profile-backward]
g0163:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0163:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0163:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0163:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0163:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0163:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0163:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0163:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0163:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0163:                        [--output-bert-embeddings]
g0163:                        [--bert-embedder-type {megatron,huggingface}]
g0163:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0163:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0163:                        [--transformer-impl {local,transformer_engine}]
g0163:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0163:                        [--fp8-amax-compute-algo {most_recent,max}]
g0163:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0163:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0163:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0163:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0163:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0163:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0163:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0163:                        [--retro-return-doc-ids] [--deepspeed]
g0163:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0163:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0163: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0163: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0163:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0163:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0163:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0163:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0163:                        [--expert-interval EXPERT_INTERVAL]
g0163:                        [--hidden-size HIDDEN_SIZE]
g0163:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0163:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0163:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0163:                        [--kv-channels KV_CHANNELS]
g0163:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0163:                        [--use-rotary-position-embeddings]
g0163:                        [--rotary-percent ROTARY_PERCENT]
g0163:                        [--no-position-embedding]
g0163:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0163:                        [--normalization {layernorm,rmsnorm}]
g0163:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0163:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0163:                        [--apply-residual-connection-post-layernorm]
g0163:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0163:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0163:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0163:                        [--untie-embeddings-and-output-weights]
g0163:                        [--embedding-weights-in-fp32]
g0163:                        [--attention-dropout ATTENTION_DROPOUT]
g0163:                        [--hidden-dropout HIDDEN_DROPOUT]
g0163:                        [--weight-decay WEIGHT_DECAY]
g0163:                        [--start-weight-decay START_WEIGHT_DECAY]
g0163:                        [--end-weight-decay END_WEIGHT_DECAY]
g0163:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0163:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0163:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0163:                        [--sgd-momentum SGD_MOMENTUM]
g0163:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0163:                        [--batch-size BATCH_SIZE]
g0163:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0163:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0163:                        [--recompute-activations]
g0163:                        [--recompute-granularity {full,selective}]
g0163:                        [--distribute-saved-activations]
g0163:                        [--recompute-method {uniform,block}]
g0163:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0163:                        [--checkpoint-activations]
g0163:                        [--distribute-checkpointed-activations]
g0163:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0163:                        [--train-iters TRAIN_ITERS]
g0163:                        [--train-samples TRAIN_SAMPLES]
g0163:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0163:                        [--log-interval LOG_INTERVAL]
g0163:                        [--exit-interval EXIT_INTERVAL]
g0163:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0163:                        [--exit-signal-handler]
g0163:                        [--tensorboard-dir TENSORBOARD_DIR]
g0163:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0163:                        [--no-bias-dropout-fusion]
g0163:                        [--disable-moe-token-dropping]
g0163:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0163:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0163:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0163:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0163:                        [--create-moe-param-group] [--use-flash-attn]
g0163:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0163:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0163:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0163:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0163:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0163:                        [--no-async-tensor-model-parallel-allreduce]
g0163:                        [--no-persist-layer-norm] [--sequence-parallel]
g0163:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0163:                        [--force-ds-sequence-parallel]
g0163:                        [--no-gradient-accumulation-fusion]
g0163:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0163:                        [--data-parallel-random-init]
g0163:                        [--init-method-std INIT_METHOD_STD]
g0163:                        [--init-method-xavier-uniform] [--lr LR]
g0163:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0163:                        [--lr-decay-iters LR_DECAY_ITERS]
g0163:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0163:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0163:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0163:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0163:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0163:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0163:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0163:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0163:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0163:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0163:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0163:                        [--no-initialization] [--use-checkpoint-args]
g0163:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0163:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0163:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0163:                        [--min-loss-scale MIN_LOSS_SCALE]
g0163:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0163:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0163:                        [--no-query-key-layer-scaling]
g0163:                        [--attention-softmax-in-fp32]
g0163:                        [--accumulate-allreduce-grads-in-fp32]
g0163:                        [--fp16-lm-cross-entropy]
g0163:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0163:                        [--enable-expert-tensor-parallelism]
g0163:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0163:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0163:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0163:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0163:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0163:                        [--overlap-p2p-communication]
g0163:                        [--distributed-backend {nccl,gloo,ccl}]
g0163:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0163:                        [--DDP-impl {local,torch,FSDP}]
g0163:                        [--no-contiguous-buffers-in-local-ddp]
g0163:                        [--no-scatter-gather-tensors-in-pipeline]
g0163:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0163:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0163:                        [--use-cpu-initialization]
g0163:                        [--empty-unused-memory-level {0,1,2}]
g0163:                        [--standalone-embedding-stage]
g0163:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0163:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0163:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0163:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0163:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0163:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0163:                        [--test-data-path [TEST_DATA_PATH ...]]
g0163:                        [--data-cache-path DATA_CACHE_PATH]
g0163:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0163:                        [--merge-file MERGE_FILE]
g0163:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0163:                        [--seq-length SEQ_LENGTH]
g0163:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0163:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0163:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0163:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0163:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0163:                        [--num-workers NUM_WORKERS]
g0163:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0163:                        [--tokenizer-model TOKENIZER_MODEL]
g0163:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0163:                        [--reset-attention-mask] [--eod-mask-loss]
g0163:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0163:                        [--return-data-index]
g0163:                        [--data-efficiency-curriculum-learning]
g0163:                        [--train-idx-path TRAIN_IDX_PATH]
g0163:                        [--train-desc-path TRAIN_DESC_PATH]
g0163:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0163:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0163:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0163:                        [--repeated-dataloader] [--adlr-autoresume]
g0163:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0163:                        [--ict-head-size ICT_HEAD_SIZE]
g0163:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0163:                        [--biencoder-shared-query-context-model]
g0163:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0163:                        [--titles-data-path TITLES_DATA_PATH]
g0163:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0163:                        [--use-one-sent-docs]
g0163:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0163:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0163:                        [--retriever-score-scaling]
g0163:                        [--block-data-path BLOCK_DATA_PATH]
g0163:                        [--embedding-path EMBEDDING_PATH]
g0163:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0163:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0163:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0163:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0163:                        [--patch-dim PATCH_DIM]
g0163:                        [--classes-fraction CLASSES_FRACTION]
g0163:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0163:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0163:                        [--vision-pretraining]
g0163:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0163:                        [--vision-backbone-type {vit,mit,swin}]
g0163:                        [--swin-backbone-type {tiny,base,h3}]
g0163:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0163:                        [--iter-per-epoch ITER_PER_EPOCH]
g0163:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0163:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0163:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0163:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0163:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0163:                        [--dino-norm-last-layer]
g0163:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0163:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0163:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0163:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0163:                        [--timing-log-level {0,1,2}]
g0163:                        [--no-barrier-with-level-1-timing]
g0163:                        [--timing-log-option {max,minmax,all}]
g0163:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0163:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0163:                        [--log-timers-to-tensorboard]
g0163:                        [--log-batch-size-to-tensorboard]
g0163:                        [--no-log-learnig-rate-to-tensorboard]
g0163:                        [--no-log-loss-scale-to-tensorboard]
g0163:                        [--log-validation-ppl-to-tensorboard]
g0163:                        [--log-optimizer-states-to-tensorboard]
g0163:                        [--log-memory-to-tensorboard]
g0163:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0163:                        [--wandb-entity WANDB_ENTITY]
g0163:                        [--wandb-project WANDB_PROJECT]
g0163:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0163:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0163:                        [--zero-contigious-gradients]
g0163:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0163:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0163:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0163:                        [--scattered-embeddings] [--split-transformers]
g0163:                        [--memory-centric-tiled-linear]
g0163:                        [--tile-factor TILE_FACTOR]
g0163:                        [--deepspeed-activation-checkpointing]
g0163:                        [--partition-activations] [--contigious-checkpointing]
g0163:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0163:                        [--profile-backward]
g0163:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0163:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0163:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0163:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0163:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0163:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0163:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0163:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0163:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0163:                        [--output-bert-embeddings]
g0163:                        [--bert-embedder-type {megatron,huggingface}]
g0163:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0163:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0163:                        [--transformer-impl {local,transformer_engine}]
g0163:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0163:                        [--fp8-amax-compute-algo {most_recent,max}]
g0163:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0163:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0163:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0163:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0163:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0163:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0163:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0163:                        [--retro-return-doc-ids] [--deepspeed]
g0163:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0163:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0163: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0163: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0163:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0163:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0163:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0163:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0163:                        [--expert-interval EXPERT_INTERVAL]
g0163:                        [--hidden-size HIDDEN_SIZE]
g0163:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0163:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0163:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0163:                        [--kv-channels KV_CHANNELS]
g0163:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0163:                        [--use-rotary-position-embeddings]
g0163:                        [--rotary-percent ROTARY_PERCENT]
g0163:                        [--no-position-embedding]
g0163:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0163:                        [--normalization {layernorm,rmsnorm}]
g0163:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0163:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0163:                        [--apply-residual-connection-post-layernorm]
g0163:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0163:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0163:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0163:                        [--untie-embeddings-and-output-weights]
g0163:                        [--embedding-weights-in-fp32]
g0163:                        [--attention-dropout ATTENTION_DROPOUT]
g0163:                        [--hidden-dropout HIDDEN_DROPOUT]
g0163:                        [--weight-decay WEIGHT_DECAY]
g0163:                        [--start-weight-decay START_WEIGHT_DECAY]
g0163:                        [--end-weight-decay END_WEIGHT_DECAY]
g0163:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0163:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0163:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0163:                        [--sgd-momentum SGD_MOMENTUM]
g0163:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0163:                        [--batch-size BATCH_SIZE]
g0163:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0163:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0163:                        [--recompute-activations]
g0163:                        [--recompute-granularity {full,selective}]
g0163:                        [--distribute-saved-activations]
g0163:                        [--recompute-method {uniform,block}]
g0163:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0163:                        [--checkpoint-activations]
g0163:                        [--distribute-checkpointed-activations]
g0163:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0163:                        [--train-iters TRAIN_ITERS]
g0163:                        [--train-samples TRAIN_SAMPLES]
g0163:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0163:                        [--log-interval LOG_INTERVAL]
g0163:                        [--exit-interval EXIT_INTERVAL]
g0163:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0163:                        [--exit-signal-handler]
g0163:                        [--tensorboard-dir TENSORBOARD_DIR]
g0163:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0163:                        [--no-bias-dropout-fusion]
g0163:                        [--disable-moe-token-dropping]
g0163:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0163:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0163:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0163:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0163:                        [--create-moe-param-group] [--use-flash-attn]
g0163:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0163:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0163:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0163:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0163:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0163:                        [--no-async-tensor-model-parallel-allreduce]
g0163:                        [--no-persist-layer-norm] [--sequence-parallel]
g0163:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0163:                        [--force-ds-sequence-parallel]
g0163:                        [--no-gradient-accumulation-fusion]
g0163:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0163:                        [--data-parallel-random-init]
g0163:                        [--init-method-std INIT_METHOD_STD]
g0163:                        [--init-method-xavier-uniform] [--lr LR]
g0163:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0163:                        [--lr-decay-iters LR_DECAY_ITERS]
g0163:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0163:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0163:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0163:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0163:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0163:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0163:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0163:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0163:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0163:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0163:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0163:                        [--no-initialization] [--use-checkpoint-args]
g0163:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0163:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0163:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0163:                        [--min-loss-scale MIN_LOSS_SCALE]
g0163:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0163:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0163:                        [--no-query-key-layer-scaling]
g0163:                        [--attention-softmax-in-fp32]
g0163:                        [--accumulate-allreduce-grads-in-fp32]
g0163:                        [--fp16-lm-cross-entropy]
g0163:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0163:                        [--enable-expert-tensor-parallelism]
g0163:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0163:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0163:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0163:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0163:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0163:                        [--overlap-p2p-communication]
g0163:                        [--distributed-backend {nccl,gloo,ccl}]
g0163:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0163:                        [--DDP-impl {local,torch,FSDP}]
g0163:                        [--no-contiguous-buffers-in-local-ddp]
g0163:                        [--no-scatter-gather-tensors-in-pipeline]
g0163:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0163:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0163:                        [--use-cpu-initialization]
g0163:                        [--empty-unused-memory-level {0,1,2}]
g0163:                        [--standalone-embedding-stage]
g0163:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0163:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0163:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0163:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0163:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0163:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0163:                        [--test-data-path [TEST_DATA_PATH ...]]
g0163:                        [--data-cache-path DATA_CACHE_PATH]
g0163:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0163:                        [--merge-file MERGE_FILE]
g0163:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0163:                        [--seq-length SEQ_LENGTH]
g0163:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0163:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0163:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0163:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0163:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0163:                        [--num-workers NUM_WORKERS]
g0163:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0163:                        [--tokenizer-model TOKENIZER_MODEL]
g0163:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0163:                        [--reset-attention-mask] [--eod-mask-loss]
g0163:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0163:                        [--return-data-index]
g0163:                        [--data-efficiency-curriculum-learning]
g0163:                        [--train-idx-path TRAIN_IDX_PATH]
g0163:                        [--train-desc-path TRAIN_DESC_PATH]
g0163:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0163:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0163:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0163:                        [--repeated-dataloader] [--adlr-autoresume]
g0163:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0163:                        [--ict-head-size ICT_HEAD_SIZE]
g0163:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0163:                        [--biencoder-shared-query-context-model]
g0163:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0163:                        [--titles-data-path TITLES_DATA_PATH]
g0163:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0163:                        [--use-one-sent-docs]
g0163:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0163:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0163:                        [--retriever-score-scaling]
g0163:                        [--block-data-path BLOCK_DATA_PATH]
g0163:                        [--embedding-path EMBEDDING_PATH]
g0163:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0163:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0163:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0163:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0163:                        [--patch-dim PATCH_DIM]
g0163:                        [--classes-fraction CLASSES_FRACTION]
g0163:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0163:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0163:                        [--vision-pretraining]
g0163:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0163:                        [--vision-backbone-type {vit,mit,swin}]
g0163:                        [--swin-backbone-type {tiny,base,h3}]
g0163:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0163:                        [--iter-per-epoch ITER_PER_EPOCH]
g0163:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0163:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0163:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0163:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0163:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0163:                        [--dino-norm-last-layer]
g0163:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0163:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0163:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0163:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0163:                        [--timing-log-level {0,1,2}]
g0163:                        [--no-barrier-with-level-1-timing]
g0163:                        [--timing-log-option {max,minmax,all}]
g0163:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0163:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0163:                        [--log-timers-to-tensorboard]
g0163:                        [--log-batch-size-to-tensorboard]
g0163:                        [--no-log-learnig-rate-to-tensorboard]
g0163:                        [--no-log-loss-scale-to-tensorboard]
g0163:                        [--log-validation-ppl-to-tensorboard]
g0163:                        [--log-optimizer-states-to-tensorboard]
g0163:                        [--log-memory-to-tensorboard]
g0163:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0163:                        [--wandb-entity WANDB_ENTITY]
g0163:                        [--wandb-project WANDB_PROJECT]
g0163:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0163:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0163:                        [--zero-contigious-gradients]
g0163:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0163:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0163:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0163:                        [--scattered-embeddings] [--split-transformers]
g0163:                        [--memory-centric-tiled-linear]
g0163:                        [--tile-factor TILE_FACTOR]
g0163:                        [--deepspeed-activation-checkpointing]
g0163:                        [--partition-activations] [--contigious-checkpointing]
g0163:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0163:                        [--profile-backward]
g0163:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0163:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0163:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0163:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0163:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0163:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0163:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0163:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0163:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0163:                        [--output-bert-embeddings]
g0163:                        [--bert-embedder-type {megatron,huggingface}]
g0163:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0163:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0163:                        [--transformer-impl {local,transformer_engine}]
g0163:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0163:                        [--fp8-amax-compute-algo {most_recent,max}]
g0163:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0163:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0163:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0163:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0163:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0163:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0163:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0163:                        [--retro-return-doc-ids] [--deepspeed]
g0163:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0163:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0163: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0154: --------------------------------------------------
g0154: DeepSpeed C++/CUDA extension op report
g0154: --------------------------------------------------
g0154: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0154:       runtime if needed. Op compatibility means that your system
g0154:       meet the required dependencies to JIT install the op.
g0154: --------------------------------------------------
g0154: JIT compiled ops requires ninja
g0154: --------------------------------------------------
g0154: DeepSpeed C++/CUDA extension op report
g0154: --------------------------------------------------
g0154: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0154:       runtime if needed. Op compatibility means that your system
g0154:       meet the required dependencies to JIT install the op.
g0154: --------------------------------------------------
g0154: JIT compiled ops requires ninja
g0154: --------------------------------------------------
g0154: DeepSpeed C++/CUDA extension op report
g0154: --------------------------------------------------
g0154: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0154:       runtime if needed. Op compatibility means that your system
g0154:       meet the required dependencies to JIT install the op.
g0154: --------------------------------------------------
g0154: JIT compiled ops requires ninja
g0154: --------------------------------------------------
g0154: DeepSpeed C++/CUDA extension op report
g0154: --------------------------------------------------
g0154: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0154:       runtime if needed. Op compatibility means that your system
g0154:       meet the required dependencies to JIT install the op.
g0154: --------------------------------------------------
g0154: JIT compiled ops requires ninja
g0154: ninjaninjaninjaninja   .................. .................................... ..................  [92m[OKAY][0m [92m[OKAY][0m[92m[OKAY][0m
g0154: [92m[OKAY][0m
g0154: 
g0154: --------------------------------------------------
g0154: ----------------------------------------------------------------------------------------------------
g0154: 
g0154: --------------------------------------------------
g0154: op name
g0154: op nameop name   op name................................................    ................installedinstalledinstalled    installed......    compatible..compatible
g0154: compatible 
g0154: 
g0154: --------------------------------------------------compatible----------------------------------------------------------------------------------------------------
g0154: 
g0154: 
g0154: 
g0154: --------------------------------------------------
pdsh@g0153: g0156: ssh exited with exit code 2
g0154: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0154: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0154: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0154: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0154: async_io[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0154: ............... evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0154: ....... [93m[NO][0m
g0154: fused_lambfused_adam  ..........................  [92m[YES][0m [92m[YES][0m......  ......[92m[OKAY][0m 
g0154: [92m[OKAY][0m
g0154: cpu_adam ............... [92m[YES][0m fused_lion......  .............[92m[OKAY][0m 
g0154: [92m[YES][0m ......cpu_adagrad  [92m[OKAY][0m............
g0154:  [92m[YES][0m ...... [92m[OKAY][0m
g0154: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0154: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0154: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0154: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: async_io ............... [92m[YES][0m fused_lion......  .............[92m[OKAY][0m 
g0154: [92m[YES][0m ...... [92m[OKAY][0m
g0154: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0154: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0154: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0154: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0154: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0154: fused_lamb ............. async_io[92m[YES][0m  .....................  [92m[OKAY][0m[92m[YES][0m
g0154:  ...... [92m[OKAY][0m
g0154: fused_lion fused_adam.............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0154: [92m[OKAY][0m
g0154: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0154: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0154: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0154: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0154: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0154: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0154: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0154: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0154: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0154: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0154: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0154: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0154: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0154: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0154: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0154: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0154: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0154: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0154: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0154: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0154: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0154: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0154: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0154: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0154: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0154: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0154: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0154: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0154: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0154: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0154: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0154: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0154: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0154: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0154: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0154: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0154: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0154: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0154: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0154: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0154: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0154: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0154: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0154: --------------------------------------------------
g0154: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0154: --------------------------------------------------
g0154: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0154: --------------------------------------------------
g0154: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0154: --------------------------------------------------
g0154: DeepSpeed general environment info:
g0154: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0154: torch version .................... 2.0.1+cu118
g0154: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0154: deepspeed info ................... 0.12.4, unknown, unknown
g0154: torch cuda version ............... 11.8
g0154: torch hip version ................ None
g0154: nvcc version ..................... 11.8
g0154: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0154: shared memory (/dev/shm) size .... 188.13 GB
g0154: DeepSpeed general environment info:
g0154: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0154: torch version .................... 2.0.1+cu118
g0154: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0154: deepspeed info ................... 0.12.4, unknown, unknown
g0154: torch cuda version ............... 11.8
g0154: torch hip version ................ None
g0154: nvcc version ..................... 11.8
g0154: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0154: shared memory (/dev/shm) size .... 188.13 GB
g0154: DeepSpeed general environment info:
g0154: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0154: torch version .................... 2.0.1+cu118
g0154: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0154: deepspeed info ................... 0.12.4, unknown, unknown
g0154: torch cuda version ............... 11.8
g0154: torch hip version ................ None
g0154: nvcc version ..................... 11.8
g0154: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0154: shared memory (/dev/shm) size .... 188.13 GB
g0154: DeepSpeed general environment info:
g0154: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0154: torch version .................... 2.0.1+cu118
g0154: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0154: deepspeed info ................... 0.12.4, unknown, unknown
g0154: torch cuda version ............... 11.8
g0154: torch hip version ................ None
g0154: nvcc version ..................... 11.8
g0154: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0154: shared memory (/dev/shm) size .... 188.13 GB
g0154: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0154: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0154: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0154: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0154: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0154:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0154:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0154:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0154:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0154:                        [--expert-interval EXPERT_INTERVAL]
g0154:                        [--hidden-size HIDDEN_SIZE]
g0154:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0154:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0154:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0154:                        [--kv-channels KV_CHANNELS]
g0154:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0154:                        [--use-rotary-position-embeddings]
g0154:                        [--rotary-percent ROTARY_PERCENT]
g0154:                        [--no-position-embedding]
g0154:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0154:                        [--normalization {layernorm,rmsnorm}]
g0154:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0154:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0154:                        [--apply-residual-connection-post-layernorm]
g0154:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0154:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0154:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0154:                        [--untie-embeddings-and-output-weights]
g0154:                        [--embedding-weights-in-fp32]
g0154:                        [--attention-dropout ATTENTION_DROPOUT]
g0154:                        [--hidden-dropout HIDDEN_DROPOUT]
g0154:                        [--weight-decay WEIGHT_DECAY]
g0154:                        [--start-weight-decay START_WEIGHT_DECAY]
g0154:                        [--end-weight-decay END_WEIGHT_DECAY]
g0154:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0154:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0154:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0154:                        [--sgd-momentum SGD_MOMENTUM]
g0154:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0154:                        [--batch-size BATCH_SIZE]
g0154:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0154:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0154:                        [--recompute-activations]
g0154:                        [--recompute-granularity {full,selective}]
g0154:                        [--distribute-saved-activations]
g0154:                        [--recompute-method {uniform,block}]
g0154:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0154:                        [--checkpoint-activations]
g0154:                        [--distribute-checkpointed-activations]
g0154:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0154:                        [--train-iters TRAIN_ITERS]
g0154:                        [--train-samples TRAIN_SAMPLES]
g0154:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0154:                        [--log-interval LOG_INTERVAL]
g0154:                        [--exit-interval EXIT_INTERVAL]
g0154:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0154:                        [--exit-signal-handler]
g0154:                        [--tensorboard-dir TENSORBOARD_DIR]
g0154:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0154:                        [--no-bias-dropout-fusion]
g0154:                        [--disable-moe-token-dropping]
g0154:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0154:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0154:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0154:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0154:                        [--create-moe-param-group] [--use-flash-attn]
g0154:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0154:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0154:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0154:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0154:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0154:                        [--no-async-tensor-model-parallel-allreduce]
g0154:                        [--no-persist-layer-norm] [--sequence-parallel]
g0154:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0154:                        [--force-ds-sequence-parallel]
g0154:                        [--no-gradient-accumulation-fusion]
g0154:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0154:                        [--data-parallel-random-init]
g0154:                        [--init-method-std INIT_METHOD_STD]
g0154:                        [--init-method-xavier-uniform] [--lr LR]
g0154:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0154:                        [--lr-decay-iters LR_DECAY_ITERS]
g0154:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0154:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0154:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0154:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0154:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0154:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0154:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0154:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0154:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0154:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0154:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0154:                        [--no-initialization] [--use-checkpoint-args]
g0154:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0154:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0154:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0154:                        [--min-loss-scale MIN_LOSS_SCALE]
g0154:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0154:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0154:                        [--no-query-key-layer-scaling]
g0154:                        [--attention-softmax-in-fp32]
g0154:                        [--accumulate-allreduce-grads-in-fp32]
g0154:                        [--fp16-lm-cross-entropy]
g0154:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0154:                        [--enable-expert-tensor-parallelism]
g0154:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0154:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0154:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0154:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0154:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0154:                        [--overlap-p2p-communication]
g0154:                        [--distributed-backend {nccl,gloo,ccl}]
g0154:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0154:                        [--DDP-impl {local,torch,FSDP}]
g0154:                        [--no-contiguous-buffers-in-local-ddp]
g0154:                        [--no-scatter-gather-tensors-in-pipeline]
g0154:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0154:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0154:                        [--use-cpu-initialization]
g0154:                        [--empty-unused-memory-level {0,1,2}]
g0154:                        [--standalone-embedding-stage]
g0154:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0154:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0154:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0154:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0154:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0154:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0154:                        [--test-data-path [TEST_DATA_PATH ...]]
g0154:                        [--data-cache-path DATA_CACHE_PATH]
g0154:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0154:                        [--merge-file MERGE_FILE]
g0154:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0154:                        [--seq-length SEQ_LENGTH]
g0154:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0154:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0154:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0154:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0154:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0154:                        [--num-workers NUM_WORKERS]
g0154:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0154:                        [--tokenizer-model TOKENIZER_MODEL]
g0154:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0154:                        [--reset-attention-mask] [--eod-mask-loss]
g0154:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0154:                        [--return-data-index]
g0154:                        [--data-efficiency-curriculum-learning]
g0154:                        [--train-idx-path TRAIN_IDX_PATH]
g0154:                        [--train-desc-path TRAIN_DESC_PATH]
g0154:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0154:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0154:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0154:                        [--repeated-dataloader] [--adlr-autoresume]
g0154:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0154:                        [--ict-head-size ICT_HEAD_SIZE]
g0154:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0154:                        [--biencoder-shared-query-context-model]
g0154:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0154:                        [--titles-data-path TITLES_DATA_PATH]
g0154:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0154:                        [--use-one-sent-docs]
g0154:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0154:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0154:                        [--retriever-score-scaling]
g0154:                        [--block-data-path BLOCK_DATA_PATH]
g0154:                        [--embedding-path EMBEDDING_PATH]
g0154:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0154:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0154:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0154:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0154:                        [--patch-dim PATCH_DIM]
g0154:                        [--classes-fraction CLASSES_FRACTION]
g0154:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0154:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0154:                        [--vision-pretraining]
g0154:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0154:                        [--vision-backbone-type {vit,mit,swin}]
g0154:                        [--swin-backbone-type {tiny,base,h3}]
g0154:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0154:                        [--iter-per-epoch ITER_PER_EPOCH]
g0154:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0154:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0154:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0154:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0154:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0154:                        [--dino-norm-last-layer]
g0154:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0154:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0154:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0154:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0154:                        [--timing-log-level {0,1,2}]
g0154:                        [--no-barrier-with-level-1-timing]
g0154:                        [--timing-log-option {max,minmax,all}]
g0154:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0154:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0154:                        [--log-timers-to-tensorboard]
g0154:                        [--log-batch-size-to-tensorboard]
g0154:                        [--no-log-learnig-rate-to-tensorboard]
g0154:                        [--no-log-loss-scale-to-tensorboard]
g0154:                        [--log-validation-ppl-to-tensorboard]
g0154:                        [--log-optimizer-states-to-tensorboard]
g0154:                        [--log-memory-to-tensorboard]
g0154:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0154:                        [--wandb-entity WANDB_ENTITY]
g0154:                        [--wandb-project WANDB_PROJECT]
g0154:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0154:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0154:                        [--zero-contigious-gradients]
g0154:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0154:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0154:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0154:                        [--scattered-embeddings] [--split-transformers]
g0154:                        [--memory-centric-tiled-linear]
g0154:                        [--tile-factor TILE_FACTOR]
g0154:                        [--deepspeed-activation-checkpointing]
g0154:                        [--partition-activations] [--contigious-checkpointing]
g0154:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0154:                        [--profile-backward]
g0154:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0154:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0154:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0154:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0154:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0154:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0154:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0154:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0154:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0154:                        [--output-bert-embeddings]
g0154:                        [--bert-embedder-type {megatron,huggingface}]
g0154:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0154:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0154:                        [--transformer-impl {local,transformer_engine}]
g0154:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0154:                        [--fp8-amax-compute-algo {most_recent,max}]
g0154:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0154:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0154:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0154:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0154:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0154:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0154:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0154:                        [--retro-return-doc-ids] [--deepspeed]
g0154:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0154:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0154: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0154:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0154:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0154:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0154:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0154:                        [--expert-interval EXPERT_INTERVAL]
g0154:                        [--hidden-size HIDDEN_SIZE]
g0154:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0154:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0154:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0154:                        [--kv-channels KV_CHANNELS]
g0154:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0154:                        [--use-rotary-position-embeddings]
g0154:                        [--rotary-percent ROTARY_PERCENT]
g0154:                        [--no-position-embedding]
g0154:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0154:                        [--normalization {layernorm,rmsnorm}]
g0154:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0154:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0154:                        [--apply-residual-connection-post-layernorm]
g0154:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0154:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0154:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0154:                        [--untie-embeddings-and-output-weights]
g0154:                        [--embedding-weights-in-fp32]
g0154:                        [--attention-dropout ATTENTION_DROPOUT]
g0154:                        [--hidden-dropout HIDDEN_DROPOUT]
g0154:                        [--weight-decay WEIGHT_DECAY]
g0154:                        [--start-weight-decay START_WEIGHT_DECAY]
g0154:                        [--end-weight-decay END_WEIGHT_DECAY]
g0154:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0154:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0154:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0154:                        [--sgd-momentum SGD_MOMENTUM]
g0154:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0154:                        [--batch-size BATCH_SIZE]
g0154:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0154:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0154:                        [--recompute-activations]
g0154:                        [--recompute-granularity {full,selective}]
g0154:                        [--distribute-saved-activations]
g0154:                        [--recompute-method {uniform,block}]
g0154:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0154:                        [--checkpoint-activations]
g0154:                        [--distribute-checkpointed-activations]
g0154:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0154:                        [--train-iters TRAIN_ITERS]
g0154:                        [--train-samples TRAIN_SAMPLES]
g0154:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0154:                        [--log-interval LOG_INTERVAL]
g0154:                        [--exit-interval EXIT_INTERVAL]
g0154:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0154:                        [--exit-signal-handler]
g0154:                        [--tensorboard-dir TENSORBOARD_DIR]
g0154:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0154:                        [--no-bias-dropout-fusion]
g0154:                        [--disable-moe-token-dropping]
g0154:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0154:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0154:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0154:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0154:                        [--create-moe-param-group] [--use-flash-attn]
g0154:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0154:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0154:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0154:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0154:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0154:                        [--no-async-tensor-model-parallel-allreduce]
g0154:                        [--no-persist-layer-norm] [--sequence-parallel]
g0154:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0154:                        [--force-ds-sequence-parallel]
g0154:                        [--no-gradient-accumulation-fusion]
g0154:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0154:                        [--data-parallel-random-init]
g0154:                        [--init-method-std INIT_METHOD_STD]
g0154:                        [--init-method-xavier-uniform] [--lr LR]
g0154:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0154:                        [--lr-decay-iters LR_DECAY_ITERS]
g0154:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0154:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0154:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0154:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0154:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0154:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0154:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0154:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0154:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0154:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0154:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0154:                        [--no-initialization] [--use-checkpoint-args]
g0154:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0154:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0154:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0154:                        [--min-loss-scale MIN_LOSS_SCALE]
g0154:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0154:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0154:                        [--no-query-key-layer-scaling]
g0154:                        [--attention-softmax-in-fp32]
g0154:                        [--accumulate-allreduce-grads-in-fp32]
g0154:                        [--fp16-lm-cross-entropy]
g0154:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0154:                        [--enable-expert-tensor-parallelism]
g0154:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0154:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0154:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0154:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0154:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0154:                        [--overlap-p2p-communication]
g0154:                        [--distributed-backend {nccl,gloo,ccl}]
g0154:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0154:                        [--DDP-impl {local,torch,FSDP}]
g0154:                        [--no-contiguous-buffers-in-local-ddp]
g0154:                        [--no-scatter-gather-tensors-in-pipeline]
g0154:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0154:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0154:                        [--use-cpu-initialization]
g0154:                        [--empty-unused-memory-level {0,1,2}]
g0154:                        [--standalone-embedding-stage]
g0154:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0154:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0154:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0154:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0154:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0154:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0154:                        [--test-data-path [TEST_DATA_PATH ...]]
g0154:                        [--data-cache-path DATA_CACHE_PATH]
g0154:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0154:                        [--merge-file MERGE_FILE]
g0154:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0154:                        [--seq-length SEQ_LENGTH]
g0154:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0154:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0154:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0154:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0154:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0154:                        [--num-workers NUM_WORKERS]
g0154:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0154:                        [--tokenizer-model TOKENIZER_MODEL]
g0154:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0154:                        [--reset-attention-mask] [--eod-mask-loss]
g0154:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0154:                        [--return-data-index]
g0154:                        [--data-efficiency-curriculum-learning]
g0154:                        [--train-idx-path TRAIN_IDX_PATH]
g0154:                        [--train-desc-path TRAIN_DESC_PATH]
g0154:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0154:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0154:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0154:                        [--repeated-dataloader] [--adlr-autoresume]
g0154:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0154:                        [--ict-head-size ICT_HEAD_SIZE]
g0154:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0154:                        [--biencoder-shared-query-context-model]
g0154:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0154:                        [--titles-data-path TITLES_DATA_PATH]
g0154:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0154:                        [--use-one-sent-docs]
g0154:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0154:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0154:                        [--retriever-score-scaling]
g0154:                        [--block-data-path BLOCK_DATA_PATH]
g0154:                        [--embedding-path EMBEDDING_PATH]
g0154:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0154:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0154:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0154:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0154:                        [--patch-dim PATCH_DIM]
g0154:                        [--classes-fraction CLASSES_FRACTION]
g0154:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0154:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0154:                        [--vision-pretraining]
g0154:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0154:                        [--vision-backbone-type {vit,mit,swin}]
g0154:                        [--swin-backbone-type {tiny,base,h3}]
g0154:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0154:                        [--iter-per-epoch ITER_PER_EPOCH]
g0154:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0154:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0154:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0154:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0154:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0154:                        [--dino-norm-last-layer]
g0154:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0154:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0154:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0154:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0154:                        [--timing-log-level {0,1,2}]
g0154:                        [--no-barrier-with-level-1-timing]
g0154:                        [--timing-log-option {max,minmax,all}]
g0154:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0154:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0154:                        [--log-timers-to-tensorboard]
g0154:                        [--log-batch-size-to-tensorboard]
g0154:                        [--no-log-learnig-rate-to-tensorboard]
g0154:                        [--no-log-loss-scale-to-tensorboard]
g0154:                        [--log-validation-ppl-to-tensorboard]
g0154:                        [--log-optimizer-states-to-tensorboard]
g0154:                        [--log-memory-to-tensorboard]
g0154:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0154:                        [--wandb-entity WANDB_ENTITY]
g0154:                        [--wandb-project WANDB_PROJECT]
g0154:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0154:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0154:                        [--zero-contigious-gradients]
g0154:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0154:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0154:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0154:                        [--scattered-embeddings] [--split-transformers]
g0154:                        [--memory-centric-tiled-linear]
g0154:                        [--tile-factor TILE_FACTOR]
g0154:                        [--deepspeed-activation-checkpointing]
g0154:                        [--partition-activations] [--contigious-checkpointing]
g0154:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0154:                        [--profile-backward]
g0154:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0154:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0154:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0154:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0154:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0154:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0154:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0154:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0154:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0154:                        [--output-bert-embeddings]
g0154:                        [--bert-embedder-type {megatron,huggingface}]
g0154:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0154:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0154:                        [--transformer-impl {local,transformer_engine}]
g0154:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0154:                        [--fp8-amax-compute-algo {most_recent,max}]
g0154:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0154:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0154:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0154:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0154:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0154:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0154:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0154:                        [--retro-return-doc-ids] [--deepspeed]
g0154:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0154:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0154: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0154: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0154: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0154:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0154:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0154:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0154:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0154:                        [--expert-interval EXPERT_INTERVAL]
g0154:                        [--hidden-size HIDDEN_SIZE]
g0154:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0154:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0154:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0154:                        [--kv-channels KV_CHANNELS]
g0154:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0154:                        [--use-rotary-position-embeddings]
g0154:                        [--rotary-percent ROTARY_PERCENT]
g0154:                        [--no-position-embedding]
g0154:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0154:                        [--normalization {layernorm,rmsnorm}]
g0154:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0154:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0154:                        [--apply-residual-connection-post-layernorm]
g0154:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0154:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0154:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0154:                        [--untie-embeddings-and-output-weights]
g0154:                        [--embedding-weights-in-fp32]
g0154:                        [--attention-dropout ATTENTION_DROPOUT]
g0154:                        [--hidden-dropout HIDDEN_DROPOUT]
g0154:                        [--weight-decay WEIGHT_DECAY]
g0154:                        [--start-weight-decay START_WEIGHT_DECAY]
g0154:                        [--end-weight-decay END_WEIGHT_DECAY]
g0154:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0154:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0154:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0154:                        [--sgd-momentum SGD_MOMENTUM]
g0154:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0154:                        [--batch-size BATCH_SIZE]
g0154:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0154:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0154:                        [--recompute-activations]
g0154:                        [--recompute-granularity {full,selective}]
g0154:                        [--distribute-saved-activations]
g0154:                        [--recompute-method {uniform,block}]
g0154:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0154:                        [--checkpoint-activations]
g0154:                        [--distribute-checkpointed-activations]
g0154:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0154:                        [--train-iters TRAIN_ITERS]
g0154:                        [--train-samples TRAIN_SAMPLES]
g0154:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0154:                        [--log-interval LOG_INTERVAL]
g0154:                        [--exit-interval EXIT_INTERVAL]
g0154:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0154:                        [--exit-signal-handler]
g0154:                        [--tensorboard-dir TENSORBOARD_DIR]
g0154:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0154:                        [--no-bias-dropout-fusion]
g0154:                        [--disable-moe-token-dropping]
g0154:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0154:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0154:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0154:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0154:                        [--create-moe-param-group] [--use-flash-attn]
g0154:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0154:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0154:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0154:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0154:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0154:                        [--no-async-tensor-model-parallel-allreduce]
g0154:                        [--no-persist-layer-norm] [--sequence-parallel]
g0154:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0154:                        [--force-ds-sequence-parallel]
g0154:                        [--no-gradient-accumulation-fusion]
g0154:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0154:                        [--data-parallel-random-init]
g0154:                        [--init-method-std INIT_METHOD_STD]
g0154:                        [--init-method-xavier-uniform] [--lr LR]
g0154:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0154:                        [--lr-decay-iters LR_DECAY_ITERS]
g0154:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0154:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0154:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0154:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0154:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0154:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0154:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0154:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0154:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0154:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0154:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0154:                        [--no-initialization] [--use-checkpoint-args]
g0154:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0154:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0154:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0154:                        [--min-loss-scale MIN_LOSS_SCALE]
g0154:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0154:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0154:                        [--no-query-key-layer-scaling]
g0154:                        [--attention-softmax-in-fp32]
g0154:                        [--accumulate-allreduce-grads-in-fp32]
g0154:                        [--fp16-lm-cross-entropy]
g0154:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0154:                        [--enable-expert-tensor-parallelism]
g0154:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0154:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0154:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0154:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0154:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0154:                        [--overlap-p2p-communication]
g0154:                        [--distributed-backend {nccl,gloo,ccl}]
g0154:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0154:                        [--DDP-impl {local,torch,FSDP}]
g0154:                        [--no-contiguous-buffers-in-local-ddp]
g0154:                        [--no-scatter-gather-tensors-in-pipeline]
g0154:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0154:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0154:                        [--use-cpu-initialization]
g0154:                        [--empty-unused-memory-level {0,1,2}]
g0154:                        [--standalone-embedding-stage]
g0154:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0154:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0154:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0154:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0154:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0154:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0154:                        [--test-data-path [TEST_DATA_PATH ...]]
g0154:                        [--data-cache-path DATA_CACHE_PATH]
g0154:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0154:                        [--merge-file MERGE_FILE]
g0154:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0154:                        [--seq-length SEQ_LENGTH]
g0154:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0154:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0154:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0154:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0154:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0154:                        [--num-workers NUM_WORKERS]
g0154:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0154:                        [--tokenizer-model TOKENIZER_MODEL]
g0154:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0154:                        [--reset-attention-mask] [--eod-mask-loss]
g0154:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0154:                        [--return-data-index]
g0154:                        [--data-efficiency-curriculum-learning]
g0154:                        [--train-idx-path TRAIN_IDX_PATH]
g0154:                        [--train-desc-path TRAIN_DESC_PATH]
g0154:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0154:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0154:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0154:                        [--repeated-dataloader] [--adlr-autoresume]
g0154:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0154:                        [--ict-head-size ICT_HEAD_SIZE]
g0154:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0154:                        [--biencoder-shared-query-context-model]
g0154:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0154:                        [--titles-data-path TITLES_DATA_PATH]
g0154:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0154:                        [--use-one-sent-docs]
g0154:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0154:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0154:                        [--retriever-score-scaling]
g0154:                        [--block-data-path BLOCK_DATA_PATH]
g0154:                        [--embedding-path EMBEDDING_PATH]
g0154:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0154:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0154:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0154:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0154:                        [--patch-dim PATCH_DIM]
g0154:                        [--classes-fraction CLASSES_FRACTION]
g0154:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0154:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0154:                        [--vision-pretraining]
g0154:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0154:                        [--vision-backbone-type {vit,mit,swin}]
g0154:                        [--swin-backbone-type {tiny,base,h3}]
g0154:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0154:                        [--iter-per-epoch ITER_PER_EPOCH]
g0154:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0154:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0154:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0154:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0154:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0154:                        [--dino-norm-last-layer]
g0154:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0154:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0154:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0154:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0154:                        [--timing-log-level {0,1,2}]
g0154:                        [--no-barrier-with-level-1-timing]
g0154:                        [--timing-log-option {max,minmax,all}]
g0154:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0154:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0154:                        [--log-timers-to-tensorboard]
g0154:                        [--log-batch-size-to-tensorboard]
g0154:                        [--no-log-learnig-rate-to-tensorboard]
g0154:                        [--no-log-loss-scale-to-tensorboard]
g0154:                        [--log-validation-ppl-to-tensorboard]
g0154:                        [--log-optimizer-states-to-tensorboard]
g0154:                        [--log-memory-to-tensorboard]
g0154:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0154:                        [--wandb-entity WANDB_ENTITY]
g0154:                        [--wandb-project WANDB_PROJECT]
g0154:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0154:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0154:                        [--zero-contigious-gradients]
g0154:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0154:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0154:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0154:                        [--scattered-embeddings] [--split-transformers]
g0154:                        [--memory-centric-tiled-linear]
g0154:                        [--tile-factor TILE_FACTOR]
g0154:                        [--deepspeed-activation-checkpointing]
g0154:                        [--partition-activations] [--contigious-checkpointing]
g0154:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0154:                        [--profile-backward]
g0154:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0154:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0154:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0154:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0154:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0154:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0154:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0154:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0154:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0154:                        [--output-bert-embeddings]
g0154:                        [--bert-embedder-type {megatron,huggingface}]
g0154:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0154:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0154:                        [--transformer-impl {local,transformer_engine}]
g0154:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0154:                        [--fp8-amax-compute-algo {most_recent,max}]
g0154:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0154:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0154:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0154:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0154:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0154:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0154:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0154:                        [--retro-return-doc-ids] [--deepspeed]
g0154:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0154:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0154: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0154: usage: pretrain_gpt.py [-h] [--num-layers NUM_LAYERS]
g0154:                        [--encoder-num-layers ENCODER_NUM_LAYERS]
g0154:                        [--decoder-num-layers DECODER_NUM_LAYERS]
g0154:                        [--num-experts NUM_EXPERTS [NUM_EXPERTS ...]]
g0154:                        [--mlp-type MLP_TYPE] [--topk TOPK]
g0154:                        [--expert-interval EXPERT_INTERVAL]
g0154:                        [--hidden-size HIDDEN_SIZE]
g0154:                        [--ffn-hidden-size FFN_HIDDEN_SIZE]
g0154:                        [--num-attention-heads NUM_ATTENTION_HEADS]
g0154:                        [--num-key-value-heads NUM_KEY_VALUE_HEADS]
g0154:                        [--kv-channels KV_CHANNELS]
g0154:                        [--max-position-embeddings MAX_POSITION_EMBEDDINGS]
g0154:                        [--use-rotary-position-embeddings]
g0154:                        [--rotary-percent ROTARY_PERCENT]
g0154:                        [--no-position-embedding]
g0154:                        [--make-vocab-size-divisible-by MAKE_VOCAB_SIZE_DIVISIBLE_BY]
g0154:                        [--normalization {layernorm,rmsnorm}]
g0154:                        [--layernorm-epsilon LAYERNORM_EPSILON]
g0154:                        [--apply-layernorm-1p] [--disable-mem-efficient-ln]
g0154:                        [--apply-residual-connection-post-layernorm]
g0154:                        [--openai-gelu] [--squared-relu] [--swiglu]
g0154:                        [--onnx-safe ONNX_SAFE] [--bert-no-binary-head]
g0154:                        [--num-experts-switch NUM_EXPERTS_SWITCH]
g0154:                        [--untie-embeddings-and-output-weights]
g0154:                        [--embedding-weights-in-fp32]
g0154:                        [--attention-dropout ATTENTION_DROPOUT]
g0154:                        [--hidden-dropout HIDDEN_DROPOUT]
g0154:                        [--weight-decay WEIGHT_DECAY]
g0154:                        [--start-weight-decay START_WEIGHT_DECAY]
g0154:                        [--end-weight-decay END_WEIGHT_DECAY]
g0154:                        [--weight-decay-incr-style {constant,linear,cosine}]
g0154:                        [--clip-grad CLIP_GRAD] [--adam-beta1 ADAM_BETA1]
g0154:                        [--adam-beta2 ADAM_BETA2] [--adam-eps ADAM_EPS]
g0154:                        [--sgd-momentum SGD_MOMENTUM]
g0154:                        [--micro-batch-size MICRO_BATCH_SIZE]
g0154:                        [--batch-size BATCH_SIZE]
g0154:                        [--global-batch-size GLOBAL_BATCH_SIZE]
g0154:                        [--rampup-batch-size [RAMPUP_BATCH_SIZE ...]]
g0154:                        [--recompute-activations]
g0154:                        [--recompute-granularity {full,selective}]
g0154:                        [--distribute-saved-activations]
g0154:                        [--recompute-method {uniform,block}]
g0154:                        [--recompute-num-layers RECOMPUTE_NUM_LAYERS]
g0154:                        [--checkpoint-activations]
g0154:                        [--distribute-checkpointed-activations]
g0154:                        [--checkpoint-num-layers CHECKPOINT_NUM_LAYERS]
g0154:                        [--train-iters TRAIN_ITERS]
g0154:                        [--train-samples TRAIN_SAMPLES]
g0154:                        [--train-tokens TRAIN_TOKENS] [--random-ltd]
g0154:                        [--log-interval LOG_INTERVAL]
g0154:                        [--exit-interval EXIT_INTERVAL]
g0154:                        [--exit-duration-in-mins EXIT_DURATION_IN_MINS]
g0154:                        [--exit-signal-handler]
g0154:                        [--tensorboard-dir TENSORBOARD_DIR]
g0154:                        [--no-masked-softmax-fusion] [--no-bias-gelu-fusion]
g0154:                        [--no-bias-dropout-fusion]
g0154:                        [--disable-moe-token-dropping]
g0154:                        [--moe-train-capacity-factor MOE_TRAIN_CAPACITY_FACTOR]
g0154:                        [--moe-eval-capacity-factor MOE_EVAL_CAPACITY_FACTOR]
g0154:                        [--moe-min-capacity MOE_MIN_CAPACITY]
g0154:                        [--moe-loss-coeff MOE_LOSS_COEFF]
g0154:                        [--create-moe-param-group] [--use-flash-attn]
g0154:                        [--use-flash-attn-v2] [--use-flash-attn-triton]
g0154:                        [--disable-bias-linear] [--optimizer {adam,sgd}]
g0154:                        [--dataloader-type {single,cyclic}] [--ds-inference]
g0154:                        [--cpu-optimizer] [--cpu_torch_adam] [--ds_fused_adam]
g0154:                        [--no-pipeline-parallel] [--use-tutel] [--inference]
g0154:                        [--no-async-tensor-model-parallel-allreduce]
g0154:                        [--no-persist-layer-norm] [--sequence-parallel]
g0154:                        [--ds-sequence-parallel-size DS_SEQUENCE_PARALLEL_SIZE]
g0154:                        [--force-ds-sequence-parallel]
g0154:                        [--no-gradient-accumulation-fusion]
g0154:                        [--use-dataset-only USE_DATASET_ONLY] [--seed SEED]
g0154:                        [--data-parallel-random-init]
g0154:                        [--init-method-std INIT_METHOD_STD]
g0154:                        [--init-method-xavier-uniform] [--lr LR]
g0154:                        [--lr-decay-style {constant,linear,cosine,inverse-square-root}]
g0154:                        [--lr-decay-iters LR_DECAY_ITERS]
g0154:                        [--lr-decay-samples LR_DECAY_SAMPLES]
g0154:                        [--lr-decay-tokens LR_DECAY_TOKENS]
g0154:                        [--lr-warmup-fraction LR_WARMUP_FRACTION]
g0154:                        [--lr-warmup-iters LR_WARMUP_ITERS]
g0154:                        [--lr-warmup-samples LR_WARMUP_SAMPLES]
g0154:                        [--lr-warmup-tokens LR_WARMUP_TOKENS] [--warmup WARMUP]
g0154:                        [--min-lr MIN_LR] [--override-opt_param-scheduler]
g0154:                        [--use-checkpoint-opt_param-scheduler] [--save SAVE]
g0154:                        [--save-interval SAVE_INTERVAL] [--no-save-optim]
g0154:                        [--no-save-rng] [--load LOAD] [--no-load-optim]
g0154:                        [--no-load-rng] [--no-load-lr-state] [--finetune]
g0154:                        [--no-initialization] [--use-checkpoint-args]
g0154:                        [--exit-on-missing-checkpoint] [--universal-checkpoint]
g0154:                        [--tf32] [--fp16] [--bf16] [--loss-scale LOSS_SCALE]
g0154:                        [--initial-loss-scale INITIAL_LOSS_SCALE]
g0154:                        [--min-loss-scale MIN_LOSS_SCALE]
g0154:                        [--loss-scale-window LOSS_SCALE_WINDOW]
g0154:                        [--hysteresis HYSTERESIS] [--fp32-residual-connection]
g0154:                        [--no-query-key-layer-scaling]
g0154:                        [--attention-softmax-in-fp32]
g0154:                        [--accumulate-allreduce-grads-in-fp32]
g0154:                        [--fp16-lm-cross-entropy]
g0154:                        [--tensor-model-parallel-size TENSOR_MODEL_PARALLEL_SIZE]
g0154:                        [--enable-expert-tensor-parallelism]
g0154:                        [--pipeline-model-parallel-size PIPELINE_MODEL_PARALLEL_SIZE]
g0154:                        [--pipeline-model-parallel-split-rank PIPELINE_MODEL_PARALLEL_SPLIT_RANK]
g0154:                        [--moe-expert-parallel-size MOE_EXPERT_PARALLEL_SIZE]
g0154:                        [--model-parallel-size MODEL_PARALLEL_SIZE]
g0154:                        [--num-layers-per-virtual-pipeline-stage NUM_LAYERS_PER_VIRTUAL_PIPELINE_STAGE]
g0154:                        [--overlap-p2p-communication]
g0154:                        [--distributed-backend {nccl,gloo,ccl}]
g0154:                        [--distributed-timeout-minutes DISTRIBUTED_TIMEOUT_MINUTES]
g0154:                        [--DDP-impl {local,torch,FSDP}]
g0154:                        [--no-contiguous-buffers-in-local-ddp]
g0154:                        [--no-scatter-gather-tensors-in-pipeline]
g0154:                        [--use-ring-exchange-p2p] [--local-rank LOCAL_RANK]
g0154:                        [--lazy-mpu-init LAZY_MPU_INIT]
g0154:                        [--use-cpu-initialization]
g0154:                        [--empty-unused-memory-level {0,1,2}]
g0154:                        [--standalone-embedding-stage]
g0154:                        [--use-distributed-optimizer] [--eval-iters EVAL_ITERS]
g0154:                        [--eval-interval EVAL_INTERVAL] [--skip-train]
g0154:                        [--aml-data-download-path AML_DATA_DOWNLOAD_PATH]
g0154:                        [--data-path [DATA_PATH ...]] [--split SPLIT]
g0154:                        [--train-data-path [TRAIN_DATA_PATH ...]]
g0154:                        [--valid-data-path [VALID_DATA_PATH ...]]
g0154:                        [--test-data-path [TEST_DATA_PATH ...]]
g0154:                        [--data-cache-path DATA_CACHE_PATH]
g0154:                        [--vocab-size VOCAB_SIZE] [--vocab-file VOCAB_FILE]
g0154:                        [--merge-file MERGE_FILE]
g0154:                        [--vocab-extra-ids VOCAB_EXTRA_IDS]
g0154:                        [--seq-length SEQ_LENGTH]
g0154:                        [--encoder-seq-length ENCODER_SEQ_LENGTH]
g0154:                        [--decoder-seq-length DECODER_SEQ_LENGTH]
g0154:                        [--retriever-seq-length RETRIEVER_SEQ_LENGTH]
g0154:                        [--sample-rate SAMPLE_RATE] [--mask-prob MASK_PROB]
g0154:                        [--short-seq-prob SHORT_SEQ_PROB] [--mmap-warmup]
g0154:                        [--num-workers NUM_WORKERS]
g0154:                        [--tokenizer-type {BertWordPieceLowerCase,BertWordPieceCase,GPT2BPETokenizer,SentencePieceTokenizer,GPTSentencePieceTokenizer,HFTokenizer,NullTokenizer}]
g0154:                        [--tokenizer-model TOKENIZER_MODEL]
g0154:                        [--data-impl {mmap,infer}] [--reset-position-ids]
g0154:                        [--reset-attention-mask] [--eod-mask-loss]
g0154:                        [--train-data-exact-num-epochs TRAIN_DATA_EXACT_NUM_EPOCHS]
g0154:                        [--return-data-index]
g0154:                        [--data-efficiency-curriculum-learning]
g0154:                        [--train-idx-path TRAIN_IDX_PATH]
g0154:                        [--train-desc-path TRAIN_DESC_PATH]
g0154:                        [--train-doc-idx-path TRAIN_DOC_IDX_PATH]
g0154:                        [--train-sample-idx-path TRAIN_SAMPLE_IDX_PATH]
g0154:                        [--train-shuffle-idx-path TRAIN_SHUFFLE_IDX_PATH]
g0154:                        [--repeated-dataloader] [--adlr-autoresume]
g0154:                        [--adlr-autoresume-interval ADLR_AUTORESUME_INTERVAL]
g0154:                        [--ict-head-size ICT_HEAD_SIZE]
g0154:                        [--biencoder-projection-dim BIENCODER_PROJECTION_DIM]
g0154:                        [--biencoder-shared-query-context-model]
g0154:                        [--ict-load ICT_LOAD] [--bert-load BERT_LOAD]
g0154:                        [--titles-data-path TITLES_DATA_PATH]
g0154:                        [--query-in-block-prob QUERY_IN_BLOCK_PROB]
g0154:                        [--use-one-sent-docs]
g0154:                        [--evidence-data-path EVIDENCE_DATA_PATH]
g0154:                        [--retriever-report-topk-accuracies RETRIEVER_REPORT_TOPK_ACCURACIES [RETRIEVER_REPORT_TOPK_ACCURACIES ...]]
g0154:                        [--retriever-score-scaling]
g0154:                        [--block-data-path BLOCK_DATA_PATH]
g0154:                        [--embedding-path EMBEDDING_PATH]
g0154:                        [--indexer-batch-size INDEXER_BATCH_SIZE]
g0154:                        [--indexer-log-interval INDEXER_LOG_INTERVAL]
g0154:                        [--num-classes NUM_CLASSES] [--img-h IMG_H]
g0154:                        [--img-w IMG_W] [--num-channels NUM_CHANNELS]
g0154:                        [--patch-dim PATCH_DIM]
g0154:                        [--classes-fraction CLASSES_FRACTION]
g0154:                        [--data-per-class-fraction DATA_PER_CLASS_FRACTION]
g0154:                        [--no-data-sharding] [--head-lr-mult HEAD_LR_MULT]
g0154:                        [--vision-pretraining]
g0154:                        [--vision-pretraining-type {classify,inpaint,dino}]
g0154:                        [--vision-backbone-type {vit,mit,swin}]
g0154:                        [--swin-backbone-type {tiny,base,h3}]
g0154:                        [--mask-type {random,row}] [--mask-factor MASK_FACTOR]
g0154:                        [--iter-per-epoch ITER_PER_EPOCH]
g0154:                        [--dino-local-img-size DINO_LOCAL_IMG_SIZE]
g0154:                        [--dino-local-crops-number DINO_LOCAL_CROPS_NUMBER]
g0154:                        [--dino-head-hidden-size DINO_HEAD_HIDDEN_SIZE]
g0154:                        [--dino-bottleneck-size DINO_BOTTLENECK_SIZE]
g0154:                        [--dino-freeze-last-layer DINO_FREEZE_LAST_LAYER]
g0154:                        [--dino-norm-last-layer]
g0154:                        [--dino-warmup-teacher-temp DINO_WARMUP_TEACHER_TEMP]
g0154:                        [--dino-teacher-temp DINO_TEACHER_TEMP]
g0154:                        [--dino-warmup-teacher-temp-epochs DINO_WARMUP_TEACHER_TEMP_EPOCHS]
g0154:                        [--log-params-norm] [--log-num-zeros-in-grad]
g0154:                        [--timing-log-level {0,1,2}]
g0154:                        [--no-barrier-with-level-1-timing]
g0154:                        [--timing-log-option {max,minmax,all}]
g0154:                        [--tensorboard-log-interval TENSORBOARD_LOG_INTERVAL]
g0154:                        [--tensorboard-queue-size TENSORBOARD_QUEUE_SIZE]
g0154:                        [--log-timers-to-tensorboard]
g0154:                        [--log-batch-size-to-tensorboard]
g0154:                        [--no-log-learnig-rate-to-tensorboard]
g0154:                        [--no-log-loss-scale-to-tensorboard]
g0154:                        [--log-validation-ppl-to-tensorboard]
g0154:                        [--log-optimizer-states-to-tensorboard]
g0154:                        [--log-memory-to-tensorboard]
g0154:                        [--log-world-size-to-tensorboard] [--use-wandb]
g0154:                        [--wandb-entity WANDB_ENTITY]
g0154:                        [--wandb-project WANDB_PROJECT]
g0154:                        [--wandb-group WANDB_GROUP] [--wandb-tag WANDB_TAG]
g0154:                        [--zero-stage ZERO_STAGE] [--zero-reduce-scatter]
g0154:                        [--zero-contigious-gradients]
g0154:                        [--zero-reduce-bucket-size ZERO_REDUCE_BUCKET_SIZE]
g0154:                        [--zero-allgather-bucket-size ZERO_ALLGATHER_BUCKET_SIZE]
g0154:                        [--remote-device {none,cpu,nvme}] [--use-pin-memory]
g0154:                        [--scattered-embeddings] [--split-transformers]
g0154:                        [--memory-centric-tiled-linear]
g0154:                        [--tile-factor TILE_FACTOR]
g0154:                        [--deepspeed-activation-checkpointing]
g0154:                        [--partition-activations] [--contigious-checkpointing]
g0154:                        [--checkpoint-in-cpu] [--synchronize-each-layer]
g0154:                        [--profile-backward]
g0154:                        [--num-layers-teacher NUM_LAYERS_TEACHER]
g0154:                        [--num-experts-teacher NUM_EXPERTS_TEACHER [NUM_EXPERTS_TEACHER ...]]
g0154:                        [--hidden-size-teacher HIDDEN_SIZE_TEACHER]
g0154:                        [--num-attention-heads-teacher NUM_ATTENTION_HEADS_TEACHER]
g0154:                        [--mos] [--kd] [--kd-alpha-ce KD_ALPHA_CE]
g0154:                        [--kd-beta-ce KD_BETA_CE] [--kd-temp KD_TEMP]
g0154:                        [--reset-iteration] [--load-teacher LOAD_TEACHER]
g0154:                        [--inference-batch-times-seqlen-threshold INFERENCE_BATCH_TIMES_SEQLEN_THRESHOLD]
g0154:                        [--max-tokens-to-oom MAX_TOKENS_TO_OOM]
g0154:                        [--output-bert-embeddings]
g0154:                        [--bert-embedder-type {megatron,huggingface}]
g0154:                        [--fp8-e4m3] [--fp8-hybrid] [--no-fp8-wgrad]
g0154:                        [--fp8-margin FP8_MARGIN] [--fp8-interval FP8_INTERVAL]
g0154:                        [--transformer-impl {local,transformer_engine}]
g0154:                        [--fp8-amax-history-len FP8_AMAX_HISTORY_LEN]
g0154:                        [--fp8-amax-compute-algo {most_recent,max}]
g0154:                        [--retro-workdir RETRO_WORKDIR] [--retro-add-retriever]
g0154:                        [--retro-cyclic-train-iters RETRO_CYCLIC_TRAIN_ITERS]
g0154:                        [--retro-encoder-layers RETRO_ENCODER_LAYERS]
g0154:                        [--retro-encoder-hidden-dropout RETRO_ENCODER_HIDDEN_DROPOUT]
g0154:                        [--retro-encoder-attention-dropout RETRO_ENCODER_ATTENTION_DROPOUT]
g0154:                        [--retro-num-neighbors RETRO_NUM_NEIGHBORS]
g0154:                        [--retro-num-retrieved-chunks RETRO_NUM_RETRIEVED_CHUNKS]
g0154:                        [--retro-return-doc-ids] [--deepspeed]
g0154:                        [--deepspeed_config DEEPSPEED_CONFIG] [--deepscale]
g0154:                        [--deepscale_config DEEPSCALE_CONFIG] [--deepspeed_mpi]
g0154: pretrain_gpt.py: error: argument --train-data-exact-num-epochs: expected one argument
g0165: [2024-08-12 03:40:19,807] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3102999
g0165: [2024-08-12 03:40:19,807] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3103000
g0165: [2024-08-12 03:40:19,825] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3103001
g0165: [2024-08-12 03:40:19,840] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3103002
g0165: [2024-08-12 03:40:19,854] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True', '--log-optimizer-states-to-tensorboard', '--train-data-exact-num-epochs', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 2
g0164: [2024-08-12 03:40:20,195] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3005613
g0164: [2024-08-12 03:40:20,212] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3005614
g0164: [2024-08-12 03:40:20,227] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3005615
g0161: [2024-08-12 03:40:20,230] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1454551
g0161: [2024-08-12 03:40:20,230] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1454552
g0164: [2024-08-12 03:40:20,241] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3005616
g0164: [2024-08-12 03:40:20,241] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True', '--log-optimizer-states-to-tensorboard', '--train-data-exact-num-epochs', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 2
g0161: [2024-08-12 03:40:20,247] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1454553
g0161: [2024-08-12 03:40:20,262] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1454554
g0161: [2024-08-12 03:40:20,276] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True', '--log-optimizer-states-to-tensorboard', '--train-data-exact-num-epochs', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 2
g0159: [2024-08-12 03:40:20,278] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 502828
g0159: [2024-08-12 03:40:20,296] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 502829
g0159: [2024-08-12 03:40:20,296] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 502830
pdsh@g0153: g0165: ssh exited with exit code 2
g0159: [2024-08-12 03:40:20,311] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 502831
g0159: [2024-08-12 03:40:20,326] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True', '--log-optimizer-states-to-tensorboard', '--train-data-exact-num-epochs', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 2
g0163: [2024-08-12 03:40:20,347] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3882162
g0163: [2024-08-12 03:40:20,347] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3882163
g0163: [2024-08-12 03:40:20,365] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3882164
g0163: [2024-08-12 03:40:20,381] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3882165
g0163: [2024-08-12 03:40:20,396] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True', '--log-optimizer-states-to-tensorboard', '--train-data-exact-num-epochs', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 2
g0154: [2024-08-12 03:40:20,401] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1633199
g0154: [2024-08-12 03:40:20,420] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1633200
g0154: [2024-08-12 03:40:20,420] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1633201
g0154: [2024-08-12 03:40:20,435] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1633202
g0154: [2024-08-12 03:40:20,450] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True', '--log-optimizer-states-to-tensorboard', '--train-data-exact-num-epochs', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 2
pdsh@g0153: g0164: ssh exited with exit code 2
pdsh@g0153: g0161: ssh exited with exit code 2
pdsh@g0153: g0159: ssh exited with exit code 2
pdsh@g0153: g0163: ssh exited with exit code 2
pdsh@g0153: g0154: ssh exited with exit code 2
