
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0038
    HostName g0038
    Port 2222
    StrictHostKeyChecking no

Host g0041
    HostName g0041
    Port 2222
    StrictHostKeyChecking no

Host g0042
    HostName g0042
    Port 2222
    StrictHostKeyChecking no

Host g0043
    HostName g0043
    Port 2222
    StrictHostKeyChecking no

Host g0054
    HostName g0054
    Port 2222
    StrictHostKeyChecking no

Host g0056
    HostName g0056
    Port 2222
    StrictHostKeyChecking no

Host g0063
    HostName g0063
    Port 2222
    StrictHostKeyChecking no

Host g0066
    HostName g0066
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42829712
g0038 slots=4
g0041 slots=4
g0042 slots=4
g0043 slots=4
g0054 slots=4
g0056 slots=4
g0063 slots=4
g0066 slots=4

[2024-08-12 03:44:46,563] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-12 03:44:52,259] [INFO] [runner.py:463:main] Using IP address of 10.1.2.4 for node g0038
[2024-08-12 03:44:52,262] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0038,g0041,g0042,g0043,g0054,g0056,g0063,g0066
[2024-08-12 03:44:52,262] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0038,g0041,g0042,g0043,g0054,g0056,g0063,g0066 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDAzOCI6IFswLCAxLCAyLCAzXSwgImcwMDQxIjogWzAsIDEsIDIsIDNdLCAiZzAwNDIiOiBbMCwgMSwgMiwgM10sICJnMDA0MyI6IFswLCAxLCAyLCAzXSwgImcwMDU0IjogWzAsIDEsIDIsIDNdLCAiZzAwNTYiOiBbMCwgMSwgMiwgM10sICJnMDA2MyI6IFswLCAxLCAyLCAzXSwgImcwMDY2IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.2.4 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '13631488000' --train-samples '6656000' --lr '2.0e-4' --min-lr '1.0e-5' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True' --wandb_tag 'other_gpu'
g0038: [2024-08-12 03:44:55,735] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0038: [2024-08-12 03:44:57,937] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0038: [2024-08-12 03:44:57,937] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0038: [2024-08-12 03:44:57,937] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0038: [2024-08-12 03:44:57,937] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0038: [2024-08-12 03:44:57,937] [INFO] [launch.py:163:main] dist_world_size=32
g0038: [2024-08-12 03:44:57,938] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0038: [2024-08-12 03:45:01,125] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0038: [2024-08-12 03:45:01,125] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0038: [2024-08-12 03:45:01,185] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0038: [2024-08-12 03:45:01,238] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0066: [2024-08-12 03:45:03,331] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0054: [2024-08-12 03:45:03,826] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0063: [2024-08-12 03:45:03,909] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0042: [2024-08-12 03:45:03,977] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0056: [2024-08-12 03:45:04,245] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0041: [2024-08-12 03:45:04,255] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0043: [2024-08-12 03:45:04,450] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0038: --------------------------------------------------
g0038: DeepSpeed C++/CUDA extension op report
g0038: --------------------------------------------------
g0038: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0038:       runtime if needed. Op compatibility means that your system
g0038:       meet the required dependencies to JIT install the op.
g0038: --------------------------------------------------
g0038: JIT compiled ops requires ninja
g0038: --------------------------------------------------
g0038: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0038: --------------------------------------------------
g0038: 
g0038: DeepSpeed C++/CUDA extension op reportNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0038:       runtime if needed. Op compatibility means that your system
g0038:       meet the required dependencies to JIT install the op.
g0038: 
g0038: ----------------------------------------------------------------------------------------------------
g0038: 
g0038: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0038:       runtime if needed. Op compatibility means that your system
g0038:       meet the required dependencies to JIT install the op.JIT compiled ops requires ninja
g0038: 
g0038: --------------------------------------------------
g0038: JIT compiled ops requires ninja
g0038: --------------------------------------------------
g0038: DeepSpeed C++/CUDA extension op report
g0038: --------------------------------------------------
g0038: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0038:       runtime if needed. Op compatibility means that your system
g0038:       meet the required dependencies to JIT install the op.
g0038: --------------------------------------------------
g0038: JIT compiled ops requires ninja
g0038: ninjaninja .................. ninja ..................[92m[OKAY][0m  
g0038: [92m[OKAY][0m..................
g0038:  --------------------------------------------------[92m[OKAY][0m
g0038: --------------------------------------------------
g0038: 
g0038: op name-------------------------------------------------- op name
g0038: ................  ................op nameinstalled   installed..................   ..installedcompatible  
g0038: compatible..
g0038: -------------------------------------------------- 
g0038: --------------------------------------------------compatible
g0038: 
g0038: --------------------------------------------------
g0038: ninja .................. [92m[OKAY][0m
g0038: --------------------------------------------------
g0038: op name ................ installed .. compatible
g0038: --------------------------------------------------
g0038: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0038: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0038: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0038: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0038: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0038: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0038: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0038: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0038: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0038: inference_core_ops ..... [92m[YES][0m ......inference_core_ops [92m[OKAY][0m 
g0038: ..... [92m[YES][0m ...... [92m[OKAY][0m
g0038: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0038: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0038: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0038: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0038: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0038: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0038: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0038: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0038: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatiblerandom_ltd
g0038:  ............. sparse_attn[92m[YES][0m  ..................  [93m[NO][0m[92m[OKAY][0m 
g0038: ....... [93m[NO][0m
g0038: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0038: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0038: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0038: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0038: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0038: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0038: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0038: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0038: --------------------------------------------------
g0038: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0038: --------------------------------------------------
g0038: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0038: --------------------------------------------------
g0038: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0038: --------------------------------------------------
g0038: DeepSpeed general environment info:
g0038: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0038: torch version .................... 2.0.1+cu118
g0038: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0038: deepspeed info ................... 0.12.4, unknown, unknown
g0038: torch cuda version ............... 11.8
g0038: torch hip version ................ None
g0038: nvcc version ..................... 11.8
g0038: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0038: shared memory (/dev/shm) size .... 188.13 GB
g0038: DeepSpeed general environment info:
g0038: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0038: torch version .................... 2.0.1+cu118
g0038: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0038: deepspeed info ................... 0.12.4, unknown, unknown
g0038: torch cuda version ............... 11.8
g0038: torch hip version ................ None
g0038: nvcc version ..................... 11.8
g0038: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0038: shared memory (/dev/shm) size .... 188.13 GB
g0038: DeepSpeed general environment info:
g0038: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0038: torch version .................... 2.0.1+cu118
g0038: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0038: deepspeed info ................... 0.12.4, unknown, unknown
g0038: torch cuda version ............... 11.8
g0038: torch hip version ................ None
g0038: nvcc version ..................... 11.8
g0038: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0038: shared memory (/dev/shm) size .... 188.13 GB
g0038: DeepSpeed general environment info:
g0038: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0038: torch version .................... 2.0.1+cu118
g0038: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0038: deepspeed info ................... 0.12.4, unknown, unknown
g0038: torch cuda version ............... 11.8
g0038: torch hip version ................ None
g0038: nvcc version ..................... 11.8
g0038: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0038: shared memory (/dev/shm) size .... 188.13 GB
g0038: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0038: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0038: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0038: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0038: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0038: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0038: using torch.float32 for parameters ...
g0038: ------------------------ arguments ------------------------
g0038:   accumulate_allreduce_grads_in_fp32 .............. False
g0038:   adam_beta1 ...................................... 0.9
g0038:   adam_beta2 ...................................... 0.95
g0038:   adam_eps ........................................ 1e-08
g0038:   add_bias_linear ................................. False
g0038:   add_position_embedding .......................... False
g0038:   adlr_autoresume ................................. False
g0038:   adlr_autoresume_interval ........................ 1000
g0038:   aml_data_download_path .......................... None
g0038:   apply_layernorm_1p .............................. False
g0038:   apply_query_key_layer_scaling ................... False
g0038:   apply_residual_connection_post_layernorm ........ False
g0038:   async_tensor_model_parallel_allreduce ........... False
g0038:   attention_dropout ............................... 0.0
g0038:   attention_softmax_in_fp32 ....................... False
g0038:   barrier_with_L1_time ............................ True
g0038:   bert_binary_head ................................ True
g0038:   bert_embedder_type .............................. megatron
g0038:   bert_load ....................................... None
g0038:   bf16 ............................................ False
g0038:   bias_dropout_fusion ............................. True
g0038:   bias_gelu_fusion ................................ False
g0038:   biencoder_projection_dim ........................ 0
g0038:   biencoder_shared_query_context_model ............ False
g0038:   block_data_path ................................. None
g0038:   checkpoint_activations .......................... False
g0038:   checkpoint_in_cpu ............................... False
g0038:   checkpoint_num_layers ........................... 1
g0038:   classes_fraction ................................ 1.0
g0038:   clip_grad ....................................... 1.0
g0038:   compression_training ............................ False
g0038:   consumed_train_samples .......................... 0
g0038:   consumed_train_tokens ........................... 0
g0038:   consumed_valid_samples .......................... 0
g0038:   contigious_checkpointing ........................ False
g0038:   cpu_optimizer ................................... False
g0038:   cpu_torch_adam .................................. False
g0038:   create_moe_param_group .......................... False
g0038:   curriculum_learning_legacy ...................... False
g0038:   data_cache_path ................................. None
g0038:   data_efficiency_curriculum_learning ............. False
g0038:   data_impl ....................................... mmap
g0038:   data_parallel_random_init ....................... False
g0038:   data_parallel_size .............................. 4
g0038:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document']
g0038:   data_per_class_fraction ......................... 1.0
g0038:   data_sharding ................................... True
g0038:   dataloader_type ................................. single
g0038:   DDP_impl ........................................ local
g0038:   decoder_num_layers .............................. None
g0038:   decoder_seq_length .............................. None
g0038:   deepscale ....................................... False
g0038:   deepscale_config ................................ None
g0038:   deepspeed ....................................... True
g0038:   deepspeed_activation_checkpointing .............. False
g0038:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0038:   deepspeed_mpi ................................... False
g0038:   dino_bottleneck_size ............................ 256
g0038:   dino_freeze_last_layer .......................... 1
g0038:   dino_head_hidden_size ........................... 2048
g0038:   dino_local_crops_number ......................... 10
g0038:   dino_local_img_size ............................. 96
g0038:   dino_norm_last_layer ............................ False
g0038:   dino_teacher_temp ............................... 0.07
g0038:   dino_warmup_teacher_temp ........................ 0.04
g0038:   dino_warmup_teacher_temp_epochs ................. 30
g0038:   distribute_checkpointed_activations ............. False
g0038:   distribute_saved_activations .................... False
g0038:   distributed_backend ............................. nccl
g0038:   distributed_timeout_minutes ..................... 10
g0038:   ds_fused_adam ................................... False
g0038:   ds_inference .................................... False
g0038:   ds_pipeline_enabled ............................. True
g0038:   ds_sequence_parallel_size ....................... 1
g0038:   embedding_path .................................. None
g0038:   embedding_weights_in_fp32 ....................... False
g0038:   empty_unused_memory_level ....................... 0
g0038:   enable_expert_tensor_parallelism ................ False
g0038:   encoder_num_layers .............................. 22
g0038:   encoder_seq_length .............................. 2048
g0038:   end_weight_decay ................................ 0.1
g0038:   eod_mask_loss ................................... False
g0038:   eval_interval ................................... 1000
g0038:   eval_iters ...................................... 100
g0038:   evidence_data_path .............................. None
g0038:   exit_duration_in_mins ........................... 30000000
g0038:   exit_interval ................................... None
g0038:   exit_on_missing_checkpoint ...................... False
g0038:   exit_signal_handler ............................. False
g0038:   expert_interval ................................. 2
g0038:   ffn_hidden_size ................................. 5632
g0038:   finetune ........................................ False
g0038:   force_ds_sequence_parallel ...................... False
g0038:   fp16 ............................................ False
g0038:   fp16_lm_cross_entropy ........................... False
g0038:   fp32_residual_connection ........................ False
g0038:   fp8_amax_compute_algo ........................... most_recent
g0038:   fp8_amax_history_len ............................ 1
g0038:   fp8_e4m3 ........................................ False
g0038:   fp8_hybrid ...................................... False
g0038:   fp8_interval .................................... 1
g0038:   fp8_margin ...................................... 0
g0038:   fp8_wgrad ....................................... True
g0038:   global_batch_size ............................... 128
g0038:   gradient_accumulation_fusion .................... True
g0038:   head_lr_mult .................................... 1.0
g0038:   hidden_dropout .................................. 0.0
g0038:   hidden_size ..................................... 2048
g0038:   hidden_size_teacher ............................. None
g0038:   hysteresis ...................................... 2
g0038:   ict_head_size ................................... None
g0038:   ict_load ........................................ None
g0038:   img_h ........................................... 224
g0038:   img_w ........................................... 224
g0038:   indexer_batch_size .............................. 128
g0038:   indexer_log_interval ............................ 1000
g0038:   inference ....................................... False
g0038:   inference_batch_times_seqlen_threshold .......... 512
g0038:   init_method_std ................................. 0.013
g0038:   init_method_xavier_uniform ...................... False
g0038:   initial_loss_scale .............................. 4294967296
g0038:   iter_per_epoch .................................. 1250
g0038:   kd .............................................. False
g0038:   kd_alpha_ce ..................................... 1
g0038:   kd_beta_ce ...................................... 1
g0038:   kd_temp ......................................... 1.0
g0038:   kv_channels ..................................... 128
g0038:   layernorm_epsilon ............................... 1e-05
g0038:   lazy_mpu_init ................................... None
g0038:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0038:   load_teacher .................................... None
g0038:   local_rank ...................................... 0
g0038:   log_batch_size_to_tensorboard ................... True
g0038:   log_interval .................................... 10
g0038:   log_learning_rate_to_tensorboard ................ True
g0038:   log_loss_scale_to_tensorboard ................... True
g0038:   log_memory_to_tensorboard ....................... False
g0038:   log_num_zeros_in_grad ........................... False
g0038:   log_optimizer_states_to_tensorboard ............. True
g0038:   log_params_norm ................................. False
g0038:   log_timers_to_tensorboard ....................... True
g0038:   log_validation_ppl_to_tensorboard ............... True
g0038:   log_world_size_to_tensorboard ................... False
g0038:   loss_scale ...................................... None
g0038:   loss_scale_window ............................... 1000
g0038:   lr .............................................. 0.0002
g0038:   lr_decay_iters .................................. None
g0038:   lr_decay_samples ................................ None
g0038:   lr_decay_style .................................. cosine
g0038:   lr_decay_tokens ................................. 300000000000
g0038:   lr_warmup_fraction .............................. None
g0038:   lr_warmup_iters ................................. 0
g0038:   lr_warmup_samples ............................... 0
g0038:   lr_warmup_tokens ................................ 3000000000
g0038:   make_vocab_size_divisible_by .................... 128
g0038:   mask_factor ..................................... 1.0
g0038:   mask_prob ....................................... 0.15
g0038:   mask_type ....................................... random
g0038:   masked_softmax_fusion ........................... True
g0038:   max_position_embeddings ......................... 2048
g0038:   max_tokens_to_oom ............................... 12000
g0038:   mem_efficient_ln ................................ True
g0038:   memory_centric_tiled_linear ..................... False
g0038:   merge_file ...................................... None
g0038:   micro_batch_size ................................ 1
g0038:   min_loss_scale .................................. 1.0
g0038:   min_lr .......................................... 1e-05
g0038:   mlp_type ........................................ standard
g0038:   mmap_warmup ..................................... False
g0038:   moe_eval_capacity_factor ........................ 1.0
g0038:   moe_expert_parallel_size ........................ 1
g0038:   moe_loss_coeff .................................. 0.1
g0038:   moe_min_capacity ................................ 4
g0038:   moe_token_dropping .............................. True
g0038:   moe_train_capacity_factor ....................... 1.0
g0038:   mos ............................................. False
g0038:   no_load_lr_state ................................ False
g0038:   no_load_optim ................................... None
g0038:   no_load_rng ..................................... None
g0038:   no_persist_layer_norm ........................... False
g0038:   no_pipeline_parallel ............................ False
g0038:   no_save_optim ................................... None
g0038:   no_save_rng ..................................... None
g0038:   normalization ................................... rmsnorm
g0038:   num_attention_heads ............................. 16
g0038:   num_attention_heads_teacher ..................... None
g0038:   num_channels .................................... 3
g0038:   num_classes ..................................... 1000
g0038:   num_experts ..................................... [1]
g0038:   num_experts_switch .............................. None
g0038:   num_experts_teacher ............................. [1]
g0038:   num_key_value_heads ............................. 4
g0038:   num_layers ...................................... 22
g0038:   num_layers_per_virtual_pipeline_stage ........... None
g0038:   num_layers_teacher .............................. None
g0038:   num_workers ..................................... 0
g0038:   onnx_safe ....................................... None
g0038:   openai_gelu ..................................... False
g0038:   optimizer ....................................... adam
g0038:   output_bert_embeddings .......................... False
g0038:   overlap_p2p_comm ................................ False
g0038:   override_opt_param_scheduler .................... True
g0038:   params_dtype .................................... torch.float32
g0038:   partition_activations ........................... False
g0038:   patch_dim ....................................... 16
g0038:   perform_initialization .......................... True
g0038:   pipeline_model_parallel_size .................... 8
g0038:   pipeline_model_parallel_split_rank .............. None
g0038:   profile_backward ................................ False
g0038:   query_in_block_prob ............................. 0.1
g0038:   rampup_batch_size ............................... None
g0038:   random_ltd ...................................... False
g0038:   rank ............................................ 0
g0038:   recompute_granularity ........................... None
g0038:   recompute_method ................................ None
g0038:   recompute_num_layers ............................ 1
g0038:   remote_device ................................... none
g0038:   repeated_dataloader ............................. False
g0038:   reset_attention_mask ............................ False
g0038:   reset_iteration ................................. False
g0038:   reset_position_ids .............................. False
g0038:   retriever_report_topk_accuracies ................ []
g0038:   retriever_score_scaling ......................... False
g0038:   retriever_seq_length ............................ 256
g0038:   retro_add_retriever ............................. False
g0038:   retro_cyclic_train_iters ........................ None
g0038:   retro_encoder_attention_dropout ................. 0.1
g0038:   retro_encoder_hidden_dropout .................... 0.1
g0038:   retro_encoder_layers ............................ 2
g0038:   retro_num_neighbors ............................. 2
g0038:   retro_num_retrieved_chunks ...................... 2
g0038:   retro_return_doc_ids ............................ False
g0038:   retro_workdir ................................... None
g0038:   return_data_index ............................... False
g0038:   rotary_percent .................................. 1.0
g0038:   sample_rate ..................................... 1.0
g0038:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0038:   save_interval ................................... 1000
g0038:   scatter_gather_tensors_in_pipeline .............. True
g0038:   scattered_embeddings ............................ False
g0038:   seed ............................................ 1234
g0038:   seq_length ...................................... 2048
g0038:   sequence_parallel ............................... False
g0038:   sgd_momentum .................................... 0.9
g0038:   short_seq_prob .................................. 0.1
g0038:   skip_train ...................................... False
g0038:   split ........................................... 949,50,1
g0038:   split_transformers .............................. False
g0038:   squared_relu .................................... False
g0038:   standalone_embedding_stage ...................... False
g0038:   start_weight_decay .............................. 0.1
g0038:   swiglu .......................................... True
g0038:   swin_backbone_type .............................. tiny
g0038:   synchronize_each_layer .......................... False
g0038:   tensor_model_parallel_size ...................... 1
g0038:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True
g0038:   tensorboard_log_interval ........................ 1
g0038:   tensorboard_queue_size .......................... 1
g0038:   test_data_path .................................. None
g0038:   tf32 ............................................ False
g0038:   tile_factor ..................................... 1
g0038:   timing_log_level ................................ 0
g0038:   timing_log_option ............................... minmax
g0038:   titles_data_path ................................ None
g0038:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
g0038:   tokenizer_type .................................. SentencePieceTokenizer
g0038:   topk ............................................ 1
g0038:   train_data_exact_num_epochs ..................... None
g0038:   train_data_path ................................. None
g0038:   train_desc_path ................................. None
g0038:   train_doc_idx_path .............................. None
g0038:   train_idx_path .................................. None
g0038:   train_iters ..................................... None
g0038:   train_sample_idx_path ........................... None
g0038:   train_samples ................................... 6656000
g0038:   train_shuffle_idx_path .......................... None
g0038:   train_tokens .................................... 13631488000
g0038:   transformer_impl ................................ local
g0038:   transformer_pipeline_model_parallel_size ........ 8
g0038:   universal_checkpoint ............................ False
g0038:   untie_embeddings_and_output_weights ............. True
g0038:   use_checkpoint_args ............................. False
g0038:   use_checkpoint_opt_param_scheduler .............. False
g0038:   use_contiguous_buffers_in_local_ddp ............. True
g0038:   use_cpu_initialization .......................... None
g0038:   use_dataset_only ................................ False
g0038:   use_distributed_optimizer ....................... False
g0038:   use_flash_attn .................................. False
g0038:   use_flash_attn_triton ........................... False
g0038:   use_flash_attn_v1 ............................... False
g0038:   use_flash_attn_v2 ............................... False
g0038:   use_one_sent_docs ............................... False
g0038:   use_pin_memory .................................. False
g0038:   use_ring_exchange_p2p ........................... False
g0038:   use_rotary_position_embeddings .................. True
g0038:   use_tutel ....................................... False
g0038:   use_wandb ....................................... True
g0038:   valid_data_path ................................. None
g0038:   variable_seq_lengths ............................ False
g0038:   virtual_pipeline_model_parallel_size ............ None
g0038:   vision_backbone_type ............................ vit
g0038:   vision_pretraining .............................. False
g0038:   vision_pretraining_type ......................... classify
g0038:   vocab_extra_ids ................................. 0
g0038:   vocab_file ...................................... None
g0038:   vocab_size ...................................... None
g0038:   wandb_entity .................................... yohei-kobashi
g0038:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True
g0038:   wandb_project ................................... encrypted_data_LLM
g0038:   wandb_tag ....................................... other_gpu
g0038:   weight_decay .................................... 0.1
g0038:   weight_decay_incr_style ......................... constant
g0038:   world_size ...................................... 32
g0038:   zero_allgather_bucket_size ...................... 0.0
g0038:   zero_contigious_gradients ....................... False
g0038:   zero_reduce_bucket_size ......................... 0.0
g0038:   zero_reduce_scatter ............................. False
g0038:   zero_stage ...................................... 0
g0038: -------------------- end of arguments ---------------------
g0038: setting number of micro-batches to constant 32
g0038: > building SentencePieceTokenizer tokenizer ...
g0038: [2024-08-12 03:45:06,123] [INFO] [comm.py:637:init_distributed] cdb=None
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0038: > initializing torch distributed ...
g0038: [2024-08-12 03:45:06,131] [INFO] [comm.py:637:init_distributed] cdb=None
g0038: [2024-08-12 03:45:06,131] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0038: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [2024-08-12 03:45:06,173] [INFO] [comm.py:637:init_distributed] cdb=None
g0038: [2024-08-12 03:45:06,174] [INFO] [comm.py:637:init_distributed] cdb=None
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [2024-08-12 03:45:07,424] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0066: [2024-08-12 03:45:07,424] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0066: [2024-08-12 03:45:07,424] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0066: [2024-08-12 03:45:07,424] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0066: [2024-08-12 03:45:07,424] [INFO] [launch.py:163:main] dist_world_size=32
g0066: [2024-08-12 03:45:07,424] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0054: [2024-08-12 03:45:07,877] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0054: [2024-08-12 03:45:07,877] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0054: [2024-08-12 03:45:07,877] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0054: [2024-08-12 03:45:07,877] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0054: [2024-08-12 03:45:07,877] [INFO] [launch.py:163:main] dist_world_size=32
g0054: [2024-08-12 03:45:07,877] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0063: [2024-08-12 03:45:08,018] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0063: [2024-08-12 03:45:08,018] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0063: [2024-08-12 03:45:08,018] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0063: [2024-08-12 03:45:08,018] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0063: [2024-08-12 03:45:08,019] [INFO] [launch.py:163:main] dist_world_size=32
g0063: [2024-08-12 03:45:08,019] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0042: [2024-08-12 03:45:08,195] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0042: [2024-08-12 03:45:08,195] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0042: [2024-08-12 03:45:08,196] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0042: [2024-08-12 03:45:08,196] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0042: [2024-08-12 03:45:08,196] [INFO] [launch.py:163:main] dist_world_size=32
g0042: [2024-08-12 03:45:08,196] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0041: [2024-08-12 03:45:08,391] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0041: [2024-08-12 03:45:08,391] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0041: [2024-08-12 03:45:08,391] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0041: [2024-08-12 03:45:08,391] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0041: [2024-08-12 03:45:08,391] [INFO] [launch.py:163:main] dist_world_size=32
g0041: [2024-08-12 03:45:08,391] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0056: [2024-08-12 03:45:08,397] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0056: [2024-08-12 03:45:08,397] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0056: [2024-08-12 03:45:08,397] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0056: [2024-08-12 03:45:08,397] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0056: [2024-08-12 03:45:08,397] [INFO] [launch.py:163:main] dist_world_size=32
g0056: [2024-08-12 03:45:08,397] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0043: [2024-08-12 03:45:08,508] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0043: [2024-08-12 03:45:08,508] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0043: [2024-08-12 03:45:08,508] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0043: [2024-08-12 03:45:08,508] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0043: [2024-08-12 03:45:08,508] [INFO] [launch.py:163:main] dist_world_size=32
g0043: [2024-08-12 03:45:08,508] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0066: [2024-08-12 03:45:10,615] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0066: [2024-08-12 03:45:10,615] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0066: [2024-08-12 03:45:10,616] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0066: [2024-08-12 03:45:10,882] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0054: [2024-08-12 03:45:11,105] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0054: [2024-08-12 03:45:11,118] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0054: [2024-08-12 03:45:11,120] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0063: [2024-08-12 03:45:11,165] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0063: [2024-08-12 03:45:11,165] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0063: [2024-08-12 03:45:11,204] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0063: [2024-08-12 03:45:11,213] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0054: [2024-08-12 03:45:11,301] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0042: [2024-08-12 03:45:11,325] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0042: [2024-08-12 03:45:11,326] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0042: [2024-08-12 03:45:11,430] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0042: [2024-08-12 03:45:11,430] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0041: [2024-08-12 03:45:11,527] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0041: [2024-08-12 03:45:11,527] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0041: [2024-08-12 03:45:11,550] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0056: [2024-08-12 03:45:11,558] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0056: [2024-08-12 03:45:11,577] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0056: [2024-08-12 03:45:11,613] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0041: [2024-08-12 03:45:11,629] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0043: [2024-08-12 03:45:11,667] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0043: [2024-08-12 03:45:11,667] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0043: [2024-08-12 03:45:11,667] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0056: [2024-08-12 03:45:11,668] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0043: [2024-08-12 03:45:11,888] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0066: --------------------------------------------------
g0066: DeepSpeed C++/CUDA extension op report
g0066: --------------------------------------------------
g0066: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0066:       runtime if needed. Op compatibility means that your system
g0066:       meet the required dependencies to JIT install the op.
g0066: --------------------------------------------------
g0066: JIT compiled ops requires ninja
g0066: --------------------------------------------------
g0066: DeepSpeed C++/CUDA extension op report
g0066: --------------------------------------------------
g0066: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0066:       runtime if needed. Op compatibility means that your system
g0066:       meet the required dependencies to JIT install the op.
g0066: --------------------------------------------------
g0066: JIT compiled ops requires ninja
g0066: --------------------------------------------------
g0066: DeepSpeed C++/CUDA extension op report
g0066: --------------------------------------------------
g0066: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0066:       runtime if needed. Op compatibility means that your system
g0066:       meet the required dependencies to JIT install the op.
g0066: --------------------------------------------------
g0066: JIT compiled ops requires ninja
g0066: --------------------------------------------------
g0066: DeepSpeed C++/CUDA extension op report
g0066: --------------------------------------------------
g0066: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0066:       runtime if needed. Op compatibility means that your system
g0066:       meet the required dependencies to JIT install the op.
g0066: --------------------------------------------------
g0066: JIT compiled ops requires ninja
g0066: ninjaninja ninja..................   ..................[92m[OKAY][0m.................. 
g0066:  [92m[OKAY][0m[92m[OKAY][0m
g0066: --------------------------------------------------
g0066: 
g0066: ----------------------------------------------------------------------------------------------------op name
g0066: 
g0066:  op name................op name   ................installed................   installed..installed   ..compatible..ninja
g0066:   compatible --------------------------------------------------compatible
g0066: ..................
g0066: 
g0066: -------------------------------------------------- --------------------------------------------------
g0066: [92m[OKAY][0m
g0066: 
g0066: --------------------------------------------------
g0066: op name ................ installed .. compatible
g0066: --------------------------------------------------
g0066: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: async_iocpu_adagrad  ...........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0066: 
g0066: cpu_lion ............... [92m[YES][0m fused_adam......  .............[92m[OKAY][0m 
g0066: [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_adam ............... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0066:  ...... evoformer_attn[92m[OKAY][0m 
g0066: ......... [93m[NO][0mcpu_adagrad  ...................  [93m[NO][0m[92m[YES][0m
g0066:  ...... fused_lamb[92m[OKAY][0m 
g0066: ............. [92m[YES][0mcpu_lion  .....................  [92m[OKAY][0m[92m[YES][0m
g0066:  ...... [92m[OKAY][0m
g0066: fused_lion ............. [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0066:  ......evoformer_attn  [92m[OKAY][0m.........
g0066:  [93m[NO][0m ....... [93m[NO][0m
g0066: async_iofused_lamb  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0066: 
g0066: fused_adam fused_lion.............  ............. [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0066: 
g0066: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0066: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0066: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0066: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0066: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0066: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0066: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatibleragged_ops
g0066:  ............. sparse_attn[92m[YES][0m  ..................  [93m[NO][0m[92m[OKAY][0m 
g0066: ....... [93m[NO][0m
g0066: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0066: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0066: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0066: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0066: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0066: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0066: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0066: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0066: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0066: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0066: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0066: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0066: --------------------------------------------------
g0066: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0066: --------------------------------------------------
g0066: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0066: --------------------------------------------------
g0066: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0066: --------------------------------------------------
g0066: DeepSpeed general environment info:
g0066: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0066: torch version .................... 2.0.1+cu118
g0066: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0066: deepspeed info ................... 0.12.4, unknown, unknown
g0066: torch cuda version ............... 11.8
g0066: torch hip version ................ None
g0066: nvcc version ..................... 11.8
g0066: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0066: shared memory (/dev/shm) size .... 188.13 GB
g0066: DeepSpeed general environment info:
g0066: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0066: torch version .................... 2.0.1+cu118
g0066: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0066: deepspeed info ................... 0.12.4, unknown, unknown
g0066: torch cuda version ............... 11.8
g0066: torch hip version ................ None
g0066: nvcc version DeepSpeed general environment info:.....................
g0066:  11.8
g0066: torch install path ............... deepspeed wheel compiled w. ...... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']torch 2.0, cuda 11.8
g0066: 
g0066: shared memory (/dev/shm) sizetorch version  ........................  188.13 GB
g0066: 2.0.1+cu118
g0066: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0066: deepspeed info ................... 0.12.4, unknown, unknown
g0066: torch cuda version ............... 11.8
g0066: torch hip version ................ None
g0066: nvcc version ..................... 11.8
g0066: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0066: shared memory (/dev/shm) size .... 188.13 GB
g0066: DeepSpeed general environment info:
g0066: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0066: torch version .................... 2.0.1+cu118
g0066: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0066: deepspeed info ................... 0.12.4, unknown, unknown
g0066: torch cuda version ............... 11.8
g0066: torch hip version ................ None
g0066: nvcc version ..................... 11.8
g0066: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0066: shared memory (/dev/shm) size .... 188.13 GB
g0066: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0066: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0066: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0066: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0066: [2024-08-12 03:45:15,208] [INFO] [comm.py:637:init_distributed] cdb=None
g0066: [2024-08-12 03:45:15,208] [INFO] [comm.py:637:init_distributed] cdb=None
g0066: [2024-08-12 03:45:15,209] [INFO] [comm.py:637:init_distributed] cdb=None
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: --------------------------------------------------
g0054: DeepSpeed C++/CUDA extension op report
g0054: --------------------------------------------------
g0054: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0054:       runtime if needed. Op compatibility means that your system
g0054:       meet the required dependencies to JIT install the op.
g0054: --------------------------------------------------
g0054: JIT compiled ops requires ninja
g0054: --------------------------------------------------
g0054: DeepSpeed C++/CUDA extension op report
g0054: --------------------------------------------------
g0054: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0054:       runtime if needed. Op compatibility means that your system
g0054:       meet the required dependencies to JIT install the op.
g0054: --------------------------------------------------
g0054: JIT compiled ops requires ninja
g0054: --------------------------------------------------
g0054: DeepSpeed C++/CUDA extension op report
g0054: --------------------------------------------------
g0054: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0054:       runtime if needed. Op compatibility means that your system
g0054:       meet the required dependencies to JIT install the op.
g0054: --------------------------------------------------
g0054: JIT compiled ops requires ninja
g0054: --------------------------------------------------
g0054: DeepSpeed C++/CUDA extension op report
g0054: --------------------------------------------------
g0054: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0054:       runtime if needed. Op compatibility means that your system
g0054:       meet the required dependencies to JIT install the op.
g0054: --------------------------------------------------
g0054: JIT compiled ops requires ninja
g0054: ninjaninjaninja ninja  ......................................................    ..................[92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m 
g0054: 
g0054: 
g0054: [92m[OKAY][0m
g0054: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0054: 
g0054: 
g0054: --------------------------------------------------op name
g0054: op nameop name   ................op name................................    installed................installedinstalled    ..installed....    compatible..compatiblecompatible
g0054:  
g0054: 
g0054: compatible------------------------------------------------------------------------------------------------------------------------------------------------------
g0054: 
g0054: 
g0054: 
g0054: --------------------------------------------------
g0063: ----------------------------------------------------------------------------------------------------
g0063: 
g0063: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report--------------------------------------------------
g0063: 
g0063: ----------------------------------------------------------------------------------------------------
g0063: 
g0063: 
g0063: DeepSpeed C++/CUDA extension op reportNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0063:       runtime if needed. Op compatibility means that your system
g0063:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0063:       runtime if needed. Op compatibility means that your system
g0063:       meet the required dependencies to JIT install the op.
g0063: 
g0063: 
g0063: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0063: 
g0063: 
g0063: JIT compiled ops requires ninjaNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0063:       runtime if needed. Op compatibility means that your system
g0063:       meet the required dependencies to JIT install the op.JIT compiled ops requires ninja
g0063: 
g0063: 
g0063: --------------------------------------------------
g0063: JIT compiled ops requires ninja
g0063: --------------------------------------------------
g0063: DeepSpeed C++/CUDA extension op report
g0063: --------------------------------------------------
g0063: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0063:       runtime if needed. Op compatibility means that your system
g0063:       meet the required dependencies to JIT install the op.
g0063: --------------------------------------------------
g0063: JIT compiled ops requires ninja
g0063: ninjaninja ninja ..................ninja..................    [92m[OKAY][0m..................[92m[OKAY][0m..................
g0063:  
g0063:  [92m[OKAY][0m[92m[OKAY][0m--------------------------------------------------
g0063: --------------------------------------------------
g0063: 
g0063: 
g0063: --------------------------------------------------op name
g0063: op name--------------------------------------------------  op name
g0063: ................................   ................op nameinstalled installed  installed .................. ..  .. installedcompatible compatible 
g0063: compatible
g0063: ..
g0063: -------------------------------------------------- --------------------------------------------------
g0063: compatible--------------------------------------------------
g0063: 
g0063: 
g0063: --------------------------------------------------
g0054: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0054: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0054: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0054: evoformer_attn ......... [93m[NO][0m async_io.......  [93m[NO][0m...............
g0054:  [92m[YES][0mfused_lamb  ...................  [92m[OKAY][0m[92m[YES][0m
g0054:  ...... [92m[OKAY][0m
g0054: fused_adam ............. [92m[YES][0m fused_lion......  .............[92m[OKAY][0m 
g0054: [92m[YES][0m ...... [92m[OKAY][0mcpu_adam
g0054:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: async_iocpu_adagrad  ...........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0054: 
g0054: cpu_lion ............... fused_adam[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0054: ...... [92m[OKAY][0m
g0054: cpu_adam[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0054: ............... [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0054:  ....... cpu_adagrad[93m[NO][0m 
g0054: ............ [92m[YES][0mfused_lamb  ...................  [92m[OKAY][0m[92m[YES][0m
g0054:  ...... cpu_lion[92m[OKAY][0m 
g0054: ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_lion ............. [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0054: [92m[OKAY][0m
g0054: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0054: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: async_io cpu_adagrad...............  ............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m [92m[OKAY][0m
g0063: 
g0063: cpu_lion ............... fused_adam[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0063: ...... [92m[OKAY][0m
g0063: cpu_adam [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH...............
g0063:  [92m[YES][0m evoformer_attn......  .........[92m[OKAY][0m 
g0063: [93m[NO][0m ....... cpu_adagrad[93m[NO][0m 
g0063: ............ [92m[YES][0m fused_lamb......  .............[92m[OKAY][0m 
g0063: [92m[YES][0m ......cpu_lion  [92m[OKAY][0m...............
g0063:  [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_lion ............. [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0063: [92m[OKAY][0m
g0063: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0063: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0054: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0054: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0054: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0063: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0063: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0063: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0063: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0063: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: ragged_device_ops ...... [92m[YES][0m ragged_device_ops......  [92m[OKAY][0m......
g0054:  [92m[YES][0m ...... [92m[OKAY][0m
g0054: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0054: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0054: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0054: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0054: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0054: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0054: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0054: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0054: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0054: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0054: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0054: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0063: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0063: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0063: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0063: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0054: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0054: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0054: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0054: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0042: ----------------------------------------------------------------------------------------------------
g0042: 
g0042: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0042: 
g0042: ----------------------------------------------------------------------------------------------------
g0042: 
g0042: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0042:       runtime if needed. Op compatibility means that your system
g0042:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0042:       runtime if needed. Op compatibility means that your system
g0042:       meet the required dependencies to JIT install the op.
g0042: 
g0042: ----------------------------------------------------------------------------------------------------
g0042: 
g0042: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0042: 
g0042: --------------------------------------------------
g0042: DeepSpeed C++/CUDA extension op report
g0042: --------------------------------------------------
g0042: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0042:       runtime if needed. Op compatibility means that your system
g0042:       meet the required dependencies to JIT install the op.
g0042: --------------------------------------------------
g0042: JIT compiled ops requires ninja
g0042: --------------------------------------------------
g0042: DeepSpeed C++/CUDA extension op report
g0042: --------------------------------------------------
g0042: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0042:       runtime if needed. Op compatibility means that your system
g0042:       meet the required dependencies to JIT install the op.
g0042: --------------------------------------------------
g0042: JIT compiled ops requires ninja
g0063: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: cutlass_ops ............quantizer  [92m[YES][0m..............  ......[92m[YES][0m  [92m[OKAY][0m......
g0063:  [92m[OKAY][0m
g0063: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0054: --------------------------------------------------
g0054: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0054: --------------------------------------------------
g0054: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0054: --------------------------------------------------
g0054: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0054: --------------------------------------------------
g0063: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: ninjaninjaninja  ninja ......................................................    [92m[OKAY][0m..................[92m[OKAY][0m[92m[OKAY][0m
g0042:  
g0063: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: 
g0042: [92m[OKAY][0m--------------------------------------------------
g0042: 
g0063: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0042: op name
g0042:  
g0042: op name................ op name op name................installed    ................installed..................    installed..installedcompatible  
g0042: compatible ..
g0042: --------------------------------------------------.. --------------------------------------------------
g0042:  compatible
g0042: 
g0042: compatible--------------------------------------------------
g0042: 
g0042: --------------------------------------------------
g0063: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: DeepSpeed general environment info:
g0054: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0054: torch version .................... 2.0.1+cu118
g0054: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0054: deepspeed info ................... 0.12.4, unknown, unknown
g0054: torch cuda version ............... 11.8
g0054: torch hip version ................ None
g0054: nvcc version ..................... 11.8
g0054: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0054: shared memory (/dev/shm) size .... 188.13 GB
g0054: DeepSpeed general environment info:
g0054: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0054: torch version .................... 2.0.1+cu118
g0054: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0054: deepspeed info ................... 0.12.4, unknown, unknown
g0054: torch cuda version ............... 11.8
g0054: torch hip version ................ None
g0054: nvcc version ..................... 11.8
g0054: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0054: shared memory (/dev/shm) size .... 188.13 GB
g0054: DeepSpeed general environment info:
g0054: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0054: torch version .................... 2.0.1+cu118
g0054: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0054: deepspeed info ................... 0.12.4, unknown, unknown
g0054: torch cuda version ............... 11.8
g0054: torch hip version ................ None
g0054: nvcc version ..................... 11.8
g0054: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0054: shared memory (/dev/shm) size .... 188.13 GB
g0054: DeepSpeed general environment info:
g0054: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0054: torch version .................... 2.0.1+cu118
g0054: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0054: deepspeed info ................... 0.12.4, unknown, unknown
g0054: torch cuda version ............... 11.8
g0054: torch hip version ................ None
g0054: nvcc version ..................... 11.8
g0054: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0054: shared memory (/dev/shm) size .... 188.13 GB
g0063: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0063: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0063: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0063: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0063: ragged_ops ............. [92m[YES][0m [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible......
g0063:  [92m[OKAY][0m
g0063: sparse_attn ............ random_ltd[93m[NO][0m  ....................  [92m[YES][0m[93m[NO][0m 
g0063: ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0063: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0063: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0063: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0063: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0063: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0063: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0063: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0063: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0063: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0063: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0063: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0063: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0063: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0063: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0063: --------------------------------------------------
g0063: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0063: --------------------------------------------------
g0063: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0063: --------------------------------------------------
g0063: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0063: --------------------------------------------------
g0063: DeepSpeed general environment info:
g0063: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0063: torch version .................... 2.0.1+cu118
g0063: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0063: deepspeed info ................... 0.12.4, unknown, unknown
g0063: torch cuda version ............... 11.8
g0063: torch hip version ................ None
g0063: nvcc version ..................... 11.8
g0063: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0063: shared memory (/dev/shm) size .... 188.13 GB
g0063: DeepSpeed general environment info:
g0063: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0063: torch version .................... 2.0.1+cu118
g0063: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0063: deepspeed info ................... 0.12.4, unknown, unknown
g0063: torch cuda version ............... 11.8
g0063: torch hip version ................ None
g0063: nvcc version ..................... 11.8
g0063: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0063: shared memory (/dev/shm) size .... 188.13 GB
g0063: DeepSpeed general environment info:
g0063: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0063: torch version .................... 2.0.1+cu118
g0063: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0063: deepspeed info ................... 0.12.4, unknown, unknown
g0063: torch cuda version ............... 11.8
g0063: torch hip version ................ None
g0063: nvcc version ..................... 11.8
g0063: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0063: shared memory (/dev/shm) size .... 188.13 GB
g0063: DeepSpeed general environment info:
g0063: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0063: torch version .................... 2.0.1+cu118
g0063: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0063: deepspeed info ................... 0.12.4, unknown, unknown
g0063: torch cuda version ............... 11.8
g0063: torch hip version ................ None
g0063: nvcc version ..................... 11.8
g0063: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0063: shared memory (/dev/shm) size .... 188.13 GB
g0054: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0054: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0054: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0054: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0063: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0063: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0063: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0063: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0066: > setting tensorboard ...
g0066: [2024-08-12 03:45:15,633] [INFO] [comm.py:637:init_distributed] cdb=None
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: async_iofused_adam  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0042: 
g0042: cpu_adam ...............fused_adam  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0042:  [92m[OKAY][0m
g0042: cpu_adagrad ............cpu_adam  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m......
g0042:  [92m[OKAY][0m
g0042: cpu_lion ............... cpu_adagrad[92m[YES][0m  ..................  [92m[YES][0m[92m[OKAY][0m 
g0042: ...... [92m[OKAY][0m
g0042: cpu_lion async_io...............  [92m[YES][0m ...............[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH...... 
g0042:  [92m[YES][0m[92m[OKAY][0m evoformer_attn
g0042: ......  .........[92m[OKAY][0m [93m[NO][0m
g0042:  ....... [93m[NO][0m
g0042: fused_adam[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHfused_lamb 
g0042:  .......................... evoformer_attn [92m[YES][0m [92m[YES][0m ......... ...... ...... [93m[NO][0m [92m[OKAY][0m [92m[OKAY][0m
g0042: .......
g0042:  [93m[NO][0m
g0042: cpu_adam ...............fused_lamb  [92m[YES][0m.............  ......fused_lion[92m[YES][0m   [92m[OKAY][0m...................
g0042:   [92m[YES][0m[92m[OKAY][0m 
g0042: cpu_adagrad......  ............[92m[OKAY][0m 
g0042: [92m[YES][0m ...... fused_lion[92m[OKAY][0m 
g0042: ............. [92m[YES][0m cpu_lion......  ...............[92m[OKAY][0m 
g0042: [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0042: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0042: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0042: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0042: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0042: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0042: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0042: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0042: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0042: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0042: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0042: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0042: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0042: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0042: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0042: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0042: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0042: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0042: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0042: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0042: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0042: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0042: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0042: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0042: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0042: --------------------------------------------------
g0042: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0042: --------------------------------------------------
g0042: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0042: --------------------------------------------------
g0042: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0042: --------------------------------------------------
g0042: DeepSpeed general environment info:
g0042: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0042: torch version .................... 2.0.1+cu118
g0042: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0042: deepspeed info ................... 0.12.4, unknown, unknown
g0042: torch cuda version ............... 11.8
g0042: torch hip version ................ None
g0042: nvcc version ..................... 11.8
g0042: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0042: shared memory (/dev/shm) size .... 188.13 GB
g0042: DeepSpeed general environment info:
g0042: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0042: torch version .................... 2.0.1+cu118
g0042: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0042: deepspeed info ................... 0.12.4, unknown, unknown
g0042: torch cuda version ............... 11.8
g0042: torch hip version ................ None
g0042: nvcc version ..................... 11.8
g0042: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0042: shared memory (/dev/shm) size .... 188.13 GB
g0042: DeepSpeed general environment info:
g0042: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0042: torch version .................... 2.0.1+cu118
g0042: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0042: deepspeed info ................... 0.12.4, unknown, unknown
g0042: torch cuda version ............... 11.8
g0042: torch hip version ................ None
g0042: nvcc version ..................... 11.8
g0042: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0042: shared memory (/dev/shm) size .... 188.13 GB
g0042: DeepSpeed general environment info:
g0042: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0042: torch version .................... 2.0.1+cu118
g0042: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0042: deepspeed info ................... 0.12.4, unknown, unknown
g0042: torch cuda version ............... 11.8
g0042: torch hip version ................ None
g0042: nvcc version ..................... 11.8
g0042: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0042: shared memory (/dev/shm) size .... 188.13 GB
g0054: [2024-08-12 03:45:15,695] [INFO] [comm.py:637:init_distributed] cdb=None
g0054: [2024-08-12 03:45:15,695] [INFO] [comm.py:637:init_distributed] cdb=None
g0054: [2024-08-12 03:45:15,696] [INFO] [comm.py:637:init_distributed] cdb=None
g0054: [2024-08-12 03:45:15,696] [INFO] [comm.py:637:init_distributed] cdb=None
g0041: --------------------------------------------------
g0041: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0041: --------------------------------------------------
g0041: 
g0041: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0041:       runtime if needed. Op compatibility means that your system
g0041:       meet the required dependencies to JIT install the op.DeepSpeed C++/CUDA extension op report
g0041: 
g0041: ----------------------------------------------------------------------------------------------------
g0041: 
g0041: JIT compiled ops requires ninjaNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0041:       runtime if needed. Op compatibility means that your system
g0041:       meet the required dependencies to JIT install the op.
g0041: 
g0041: --------------------------------------------------
g0041: JIT compiled ops requires ninja
g0041: --------------------------------------------------
g0041: DeepSpeed C++/CUDA extension op report
g0041: --------------------------------------------------
g0041: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0041:       runtime if needed. Op compatibility means that your system
g0041:       meet the required dependencies to JIT install the op.
g0041: --------------------------------------------------
g0041: JIT compiled ops requires ninja
g0041: --------------------------------------------------
g0041: DeepSpeed C++/CUDA extension op report
g0041: --------------------------------------------------
g0041: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0041:       runtime if needed. Op compatibility means that your system
g0041:       meet the required dependencies to JIT install the op.
g0041: --------------------------------------------------
g0041: JIT compiled ops requires ninja
g0041: ninjaninjaninja  ninja ......................................................    [92m[OKAY][0m..................[92m[OKAY][0m[92m[OKAY][0m
g0041:  
g0041: 
g0041: [92m[OKAY][0m--------------------------------------------------
g0041: --------------------------------------------------
g0041: --------------------------------------------------
g0041: 
g0041: --------------------------------------------------op nameop name
g0041:  op name ................................op name    ................installedinstalled................    installed....installed    ..compatiblecompatible.. 
g0041: 
g0041:  compatiblecompatible----------------------------------------------------------------------------------------------------
g0041: 
g0041: 
g0041: 
g0041: ----------------------------------------------------------------------------------------------------
g0041: 
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0063: [2024-08-12 03:45:15,715] [INFO] [comm.py:637:init_distributed] cdb=None
g0063: [2024-08-12 03:45:15,715] [INFO] [comm.py:637:init_distributed] cdb=None
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0042: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0042: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0042: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0063: [2024-08-12 03:45:15,757] [INFO] [comm.py:637:init_distributed] cdb=None
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0063: [2024-08-12 03:45:15,763] [INFO] [comm.py:637:init_distributed] cdb=None
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: async_iocpu_adam  ..............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0041: 
g0041: cpu_adagrad ............ [92m[YES][0mfused_adam  ...................  [92m[OKAY][0m[92m[YES][0m
g0041:  ...... [92m[OKAY][0mcpu_lionasync_io
g0041:  ...............  ...............[92m[YES][0mcpu_adam   [92m[YES][0m.....................   ......[92m[OKAY][0m[92m[YES][0m 
g0041:  [92m[OKAY][0m......
g0041:  [92m[OKAY][0m
g0041: cpu_adagrad [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHfused_adam............ 
g0041:  .............[92m[YES][0m evoformer_attn [92m[YES][0m  .....................   [92m[OKAY][0m[93m[NO][0m[92m[OKAY][0m
g0041:  
g0041: ....... cpu_lion[93m[NO][0m 
g0041: cpu_adam...............  ...............[92m[YES][0m async_iofused_lamb [92m[YES][0m ......  ............. .....................[92m[OKAY][0m   
g0041: [92m[YES][0m[92m[YES][0m[92m[OKAY][0m  
g0041: ............  [92m[OKAY][0mcpu_adagrad
g0041: [92m[OKAY][0m ............
g0041:  [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0041:  ...... evoformer_attn[92m[OKAY][0m 
g0041: fused_lionfused_adam.........   cpu_lion..........................[93m[NO][0m    ...............[92m[YES][0m[92m[YES][0m.......    [92m[YES][0m............ [93m[NO][0m  ......
g0041: [92m[OKAY][0m[92m[OKAY][0m 
g0041: 
g0041: [92m[OKAY][0m
g0041: fused_lamb .............cpu_adam  [92m[YES][0m...............  ......[92m[YES][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m 
g0041: 
g0041: ...... evoformer_attn[92m[OKAY][0m 
g0041: ......... [93m[NO][0m cpu_adagrad....... fused_lion ............ [93m[NO][0m .............
g0041: [92m[YES][0m  [92m[YES][0m...... fused_lamb ...... [92m[OKAY][0m .............
g0041: [92m[OKAY][0m 
g0041: [92m[YES][0m ......cpu_lion  [92m[OKAY][0m...............
g0041:  [92m[YES][0m ...... [92m[OKAY][0m
g0041: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0041: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0041: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: --------------------------------------------------
g0056: DeepSpeed C++/CUDA extension op report
g0056: --------------------------------------------------
g0056: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0056:       runtime if needed. Op compatibility means that your system
g0056:       meet the required dependencies to JIT install the op.
g0056: --------------------------------------------------
g0056: JIT compiled ops requires ninja
g0056: --------------------------------------------------
g0056: DeepSpeed C++/CUDA extension op report
g0056: --------------------------------------------------
g0056: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0056:       runtime if needed. Op compatibility means that your system
g0056:       meet the required dependencies to JIT install the op.
g0056: --------------------------------------------------
g0056: JIT compiled ops requires ninja
g0056: --------------------------------------------------
g0056: DeepSpeed C++/CUDA extension op report
g0056: --------------------------------------------------
g0056: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0056:       runtime if needed. Op compatibility means that your system
g0056:       meet the required dependencies to JIT install the op.
g0056: --------------------------------------------------
g0056: JIT compiled ops requires ninja
g0056: --------------------------------------------------
g0056: DeepSpeed C++/CUDA extension op report
g0056: --------------------------------------------------
g0056: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0056:       runtime if needed. Op compatibility means that your system
g0056:       meet the required dependencies to JIT install the op.
g0056: --------------------------------------------------
g0056: JIT compiled ops requires ninja
g0056: ninjaninjaninja   ......................................................   ninja[92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0056: 
g0056:  
g0056: ..................------------------------------------------------------------------------------------------------------------------------------------------------------ 
g0056: 
g0056: 
g0056: [92m[OKAY][0mop nameop name
g0056:  op name ................................  --------------------------------------------------................installed 
g0056: installed   op nameinstalled....    ..................compatiblecompatible 
g0056:  
g0056: installedcompatible---------------------------------------------------------------------------------------------------- 
g0056: 
g0056: 
g0056: .. --------------------------------------------------compatible
g0056: 
g0056: --------------------------------------------------
g0041: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0041: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0041: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0041: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0041: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0041: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0041: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0041: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0041: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0041: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0041: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0041: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0041: random_ltd .............sparse_attn  [92m[YES][0m............  ......[93m[NO][0m  [92m[OKAY][0m.......
g0041:  [93m[NO][0m
g0041: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0041: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0041: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0041: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0041: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0041: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0041: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0042: [2024-08-12 03:45:15,828] [INFO] [comm.py:637:init_distributed] cdb=None
g0041: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0041: --------------------------------------------------
g0042: [2024-08-12 03:45:15,830] [INFO] [comm.py:637:init_distributed] cdb=None
g0042: [2024-08-12 03:45:15,830] [INFO] [comm.py:637:init_distributed] cdb=None
g0041: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0041: --------------------------------------------------
g0041: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0041: --------------------------------------------------
g0041: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0041: --------------------------------------------------
g0041: DeepSpeed general environment info:
g0041: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0041: torch version .................... 2.0.1+cu118
g0041: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0041: deepspeed info ................... 0.12.4, unknown, unknown
g0041: torch cuda version ............... 11.8
g0041: torch hip version ................ None
g0041: nvcc version ..................... 11.8
g0041: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0041: shared memory (/dev/shm) size .... 188.13 GB
g0042: [2024-08-12 03:45:15,834] [INFO] [comm.py:637:init_distributed] cdb=None
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: DeepSpeed general environment info:
g0041: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0041: torch version .................... 2.0.1+cu118
g0041: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0041: deepspeed info ................... 0.12.4, unknown, unknown
g0041: torch cuda version ............... 11.8
g0041: torch hip version ................ None
g0041: nvcc version ..................... 11.8
g0041: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0041: shared memory (/dev/shm) size .... 188.13 GB
g0041: DeepSpeed general environment info:
g0041: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0041: torch version .................... 2.0.1+cu118
g0041: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0041: deepspeed info ................... 0.12.4, unknown, unknown
g0041: torch cuda version ............... 11.8
g0041: torch hip version ................ None
g0041: nvcc version ..................... 11.8
g0041: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0041: shared memory (/dev/shm) size .... 188.13 GB
g0041: DeepSpeed general environment info:
g0041: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0041: torch version .................... 2.0.1+cu118
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0041: deepspeed info ................... 0.12.4, unknown, unknown
g0041: torch cuda version ............... 11.8
g0041: torch hip version ................ None
g0041: nvcc version ..................... 11.8
g0041: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0041: shared memory (/dev/shm) size .... 188.13 GB
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0041: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0041: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0041: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0056: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0056: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0056: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_adam async_io...............  [92m[YES][0m .....................  [92m[YES][0m[92m[OKAY][0m 
g0056: ...... [92m[OKAY][0mcpu_adagrad
g0056: async_io ............  ...............[92m[YES][0m  fused_adam[92m[YES][0m......   ...................[92m[OKAY][0m  
g0056: [92m[YES][0m[92m[OKAY][0m 
g0056: ......cpu_lion  [92m[OKAY][0m...............
g0056:  [92m[YES][0m fused_adam......cpu_adam   .............[92m[OKAY][0m............... 
g0056:  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0056: 
g0056: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_adam
g0056: cpu_adagrad  ...............evoformer_attn............  [92m[YES][0m ......... [92m[YES][0m ...... [93m[NO][0m ...... [92m[OKAY][0m .......
g0056: [92m[OKAY][0m 
g0056: [93m[NO][0m
g0056: cpu_adagradcpu_lion  ............fused_lamb...............   [92m[YES][0m.............[92m[YES][0m   ......[92m[YES][0m......   [92m[OKAY][0m......[92m[OKAY][0m
g0056:  
g0056: [92m[OKAY][0m
g0056: cpu_lion ............... [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0056: [92m[OKAY][0m
g0056: fused_lionevoformer_attn  ......................  [92m[YES][0m[93m[NO][0m  .............[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH  
g0056: [92m[OKAY][0m[93m[NO][0m
g0056: 
g0056: evoformer_attn .........fused_lamb  [93m[NO][0m.............  .......[92m[YES][0m  [93m[NO][0m......
g0056:  [92m[OKAY][0m
g0056: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0mfused_lion
g0056:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0056: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0056: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0056: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0056: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0056: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0056: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0056: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0056: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0056: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0056: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0056: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0056: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0056: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0056: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0056: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0056: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0056: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0056: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0056: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0056: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0056: --------------------------------------------------
g0056: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0056: --------------------------------------------------
g0056: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0056: --------------------------------------------------
g0056: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0056: --------------------------------------------------
g0056: DeepSpeed general environment info:
g0056: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0056: torch version .................... 2.0.1+cu118
g0056: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0056: deepspeed info ................... 0.12.4, unknown, unknown
g0056: torch cuda version ............... 11.8
g0056: torch hip version ................ None
g0056: nvcc version ..................... 11.8
g0056: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0056: shared memory (/dev/shm) size .... 188.13 GB
g0056: DeepSpeed general environment info:
g0056: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0056: torch version .................... 2.0.1+cu118
g0056: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0056: deepspeed info ................... 0.12.4, unknown, unknown
g0056: torch cuda version ............... 11.8
g0056: torch hip version ................ None
g0056: nvcc version ..................... 11.8
g0056: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0056: shared memory (/dev/shm) size .... 188.13 GB
g0056: DeepSpeed general environment info:
g0056: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0056: torch version .................... 2.0.1+cu118
g0056: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0056: deepspeed info ................... 0.12.4, unknown, unknown
g0056: torch cuda version ............... 11.8
g0056: torch hip version ................ None
g0056: nvcc version ..................... 11.8
g0056: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0056: shared memory (/dev/shm) size .... 188.13 GB
g0056: DeepSpeed general environment info:
g0056: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0056: torch version .................... 2.0.1+cu118
g0056: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0056: deepspeed info ................... 0.12.4, unknown, unknown
g0056: torch cuda version ............... 11.8
g0056: torch hip version ................ None
g0056: nvcc version ..................... 11.8
g0056: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0056: shared memory (/dev/shm) size .... 188.13 GB
g0041: [2024-08-12 03:45:15,966] [INFO] [comm.py:637:init_distributed] cdb=None
g0041: [2024-08-12 03:45:15,966] [INFO] [comm.py:637:init_distributed] cdb=None
g0041: [2024-08-12 03:45:15,967] [INFO] [comm.py:637:init_distributed] cdb=None
g0041: [2024-08-12 03:45:15,967] [INFO] [comm.py:637:init_distributed] cdb=None
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0056: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0056: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0056: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: --------------------------------------------------
g0043: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0043: 
g0043: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0043: 
g0043: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0043:       runtime if needed. Op compatibility means that your system
g0043:       meet the required dependencies to JIT install the op.--------------------------------------------------
g0043: 
g0043: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0043:       runtime if needed. Op compatibility means that your system
g0043:       meet the required dependencies to JIT install the op.
g0043: 
g0043: JIT compiled ops requires ninja--------------------------------------------------
g0043: 
g0043: --------------------------------------------------JIT compiled ops requires ninja
g0043: 
g0043: DeepSpeed C++/CUDA extension op report
g0043: --------------------------------------------------
g0043: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0043:       runtime if needed. Op compatibility means that your system
g0043:       meet the required dependencies to JIT install the op.
g0043: --------------------------------------------------
g0043: JIT compiled ops requires ninja
g0043: --------------------------------------------------
g0043: DeepSpeed C++/CUDA extension op report
g0043: --------------------------------------------------
g0043: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0043:       runtime if needed. Op compatibility means that your system
g0043:       meet the required dependencies to JIT install the op.
g0043: --------------------------------------------------
g0043: JIT compiled ops requires ninja
g0043: ninjaninjaninja   ......................................................   [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0043: 
g0043: 
g0043: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0043: 
g0043: 
g0043: ninjaop nameop nameop name    ..................................................................    installed[92m[OKAY][0minstalledinstalled 
g0043:   ......--------------------------------------------------   
g0043: compatiblecompatiblecompatible
g0043: 
g0043: op name
g0043:  ------------------------------------------------------------------------------------------------------------------------------------------------------................
g0043: 
g0043: 
g0043:  installed .. compatible
g0043: --------------------------------------------------
g0056: [2024-08-12 03:45:16,074] [INFO] [comm.py:637:init_distributed] cdb=None
g0056: [2024-08-12 03:45:16,079] [INFO] [comm.py:637:init_distributed] cdb=None
g0056: [2024-08-12 03:45:16,081] [INFO] [comm.py:637:init_distributed] cdb=None
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [2024-08-12 03:45:16,089] [INFO] [comm.py:637:init_distributed] cdb=None
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0043: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0043: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0043: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0043: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adam ............... [92m[YES][0masync_io ......  ...............[92m[OKAY][0m 
g0043: [92m[YES][0m ......cpu_adagrad  [92m[OKAY][0m............
g0043:  [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_adam ............. cpu_lion[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0043: ...... [92m[OKAY][0m
g0043: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0043: 
g0043: evoformer_attncpu_adagrad  .....................  [93m[NO][0m[92m[YES][0m  .............  [93m[NO][0m[92m[OKAY][0m
g0043: 
g0043: fused_lambcpu_lion  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0043: 
g0043: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHfused_lion
g0043:  .............evoformer_attn  [92m[YES][0m.........  ...... [93m[NO][0m[92m[OKAY][0m 
g0043: ....... [93m[NO][0m
g0043: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0043: inference_core_ops inference_core_ops.....  [92m[YES][0m.....  ......[92m[YES][0m  [92m[OKAY][0m......
g0043:  [92m[OKAY][0m
g0043: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0043: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0043: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0043: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0043: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0043: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: random_ltd ............. [92m[YES][0m ...... [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible[92m[OKAY][0m
g0043: 
g0043: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0043: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0043: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: random_ltd ............. [92m[YES][0m [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible......
g0043:  [92m[OKAY][0m
g0043: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0043: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0043: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0043: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0043: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0043: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0043: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0043: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0043: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: DeepSpeed general environment info:
g0043: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0043: torch version .................... 2.0.1+cu118
g0043: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0043: deepspeed info ................... 0.12.4, unknown, unknown
g0043: torch cuda version ............... 11.8
g0043: torch hip version ................ None
g0043: nvcc version ..................... 11.8
g0043: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0043: shared memory (/dev/shm) size .... 188.13 GB
g0043: DeepSpeed general environment info:
g0043: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0043: torch version .................... 2.0.1+cu118DeepSpeed general environment info:
g0043: 
g0043: deepspeed install path ...........torch install path  ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']...............
g0043:  deepspeed info ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']...................
g0043:  0.12.4, unknown, unknown
g0043: torch versiontorch cuda version  ...................................  2.0.1+cu11811.8
g0043: 
g0043: deepspeed install pathtorch hip version  ...........................  None['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0043: 
g0043: nvcc versiondeepspeed info  ........................................  11.80.12.4, unknown, unknown
g0043: 
g0043: torch cuda version ............... deepspeed wheel compiled w.11.8 
g0043: ......torch hip version  torch 2.0, cuda 11.8................
g0043:  shared memory (/dev/shm) sizeNone 
g0043: ....nvcc version  188.13 GB.....................
g0043:  11.8
g0043: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0043: shared memory (/dev/shm) size .... 188.13 GB
g0043: DeepSpeed general environment info:
g0043: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0043: torch version .................... 2.0.1+cu118
g0043: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0043: deepspeed info ................... 0.12.4, unknown, unknown
g0043: torch cuda version ............... 11.8
g0043: torch hip version ................ None
g0043: nvcc version ..................... 11.8
g0043: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0043: shared memory (/dev/shm) size .... 188.13 GB
g0043: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: [2024-08-12 03:45:16,269] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: [2024-08-12 03:45:16,269] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: [2024-08-12 03:45:16,270] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: [2024-08-12 03:45:16,272] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: > initialized tensor model parallel with size 1
g0038: > initialized pipeline model parallel with size 8
g0038: > setting random seeds to 1234 ...
g0038: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0038: > compiling dataset index builder ...
g0038: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0038: make: Nothing to be done for 'default'.
g0038: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0038: >>> done with dataset index builder. Compilation time: 0.080 seconds
g0038: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0038: > compiling and loading fused kernels ...
g0038: Detected CUDA files, patching ldflags
g0038: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0038: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0038: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0038: ninja: no work to do.
g0038: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0038: Detected CUDA files, patching ldflags
g0038: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0038: Building extension module scaled_masked_softmax_cuda...
g0038: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0038: ninja: no work to do.
g0038: Loading extension module scaled_masked_softmax_cuda...
g0038: Detected CUDA files, patching ldflags
g0038: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0038: Building extension module scaled_softmax_cuda...
g0038: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0038: ninja: no work to do.
g0038: Loading extension module scaled_softmax_cuda...
g0038: >>> done with compiling and loading fused kernels. Compilation time: 8.136 seconds
g0038: time to initialize megatron (seconds): 23.059
g0038: [after megatron is initialized] datetime: 2024-08-12 03:45:27 
g0038: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0056: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0066: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0042: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0054: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0041: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0043: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0063: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0038: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0038: wandb:  $ pip install wandb --upgrade
g0038: wandb: Tracking run with wandb version 0.17.5
g0038: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_034528-ig0bki62
g0038: wandb: Run `wandb offline` to turn off syncing.
g0038: wandb: Syncing run g0038.abci.local
g0038: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0038: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/ig0bki62
g0054: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0054: wandb:  $ pip install wandb --upgrade
g0054: wandb: Tracking run with wandb version 0.17.5
g0054: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_034528-trmcflbg
g0054: wandb: Run `wandb offline` to turn off syncing.
g0054: wandb: Syncing run g0054.abci.local
g0054: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0054: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/trmcflbg
g0063: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0063: wandb:  $ pip install wandb --upgrade
g0063: wandb: Tracking run with wandb version 0.17.5
g0063: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_034528-gl39m2rf
g0063: wandb: Run `wandb offline` to turn off syncing.
g0066: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0066: wandb:  $ pip install wandb --upgrade
g0066: wandb: Tracking run with wandb version 0.17.5
g0066: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_034528-k8m6vft1
g0066: wandb: Run `wandb offline` to turn off syncing.
g0063: wandb: Syncing run g0063.abci.local
g0063: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0063: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/gl39m2rf
g0066: wandb: Syncing run g0066.abci.local
g0066: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0066: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/k8m6vft1
g0043: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0043: wandb:  $ pip install wandb --upgrade
g0043: wandb: Tracking run with wandb version 0.17.5
g0043: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_034528-px933buq
g0043: wandb: Run `wandb offline` to turn off syncing.
g0043: wandb: Syncing run g0043.abci.local
g0043: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0043: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/px933buq
g0042: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0042: wandb:  $ pip install wandb --upgrade
g0042: wandb: Tracking run with wandb version 0.17.5
g0042: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_034528-0dxgzvng
g0042: wandb: Run `wandb offline` to turn off syncing.
g0056: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0056: wandb:  $ pip install wandb --upgrade
g0056: wandb: Tracking run with wandb version 0.17.5
g0056: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_034528-r8fhrpnx
g0056: wandb: Run `wandb offline` to turn off syncing.
g0042: wandb: Syncing run g0042.abci.local
g0042: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0042: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/0dxgzvng
g0056: wandb: Syncing run g0056.abci.local
g0056: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0056: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/r8fhrpnx
g0041: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0041: wandb:  $ pip install wandb --upgrade
g0041: wandb: Tracking run with wandb version 0.17.5
g0041: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_034528-0g62l89z
g0041: wandb: Run `wandb offline` to turn off syncing.
g0041: wandb: Syncing run g0041.abci.local
g0041: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0041: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/0g62l89z
g0038: building GPT model ...
g0038: [2024-08-12 03:45:29,733] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0038: [2024-08-12 03:45:29,734] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0038: [2024-08-12 03:45:29,735] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 55.18 GB, percent = 14.7%
g0038: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0038: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0038: [2024-08-12 03:45:30,248] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0038: stage=0 layers=5
g0038:      0: _to_float16
g0038:      1: EmbeddingPipe
g0038:      2: ParallelTransformerLayerPipe
g0038:      3: ParallelTransformerLayerPipe
g0038:      4: ParallelTransformerLayerPipe
g0038: stage=1 layers=3
g0038:      5: ParallelTransformerLayerPipe
g0038:      6: ParallelTransformerLayerPipe
g0038:      7: ParallelTransformerLayerPipe
g0038: stage=2 layers=3
g0038:      8: ParallelTransformerLayerPipe
g0038:      9: ParallelTransformerLayerPipe
g0038:     10: ParallelTransformerLayerPipe
g0038: stage=3 layers=3
g0038:     11: ParallelTransformerLayerPipe
g0038:     12: ParallelTransformerLayerPipe
g0038:     13: ParallelTransformerLayerPipe
g0038: stage=4 layers=3
g0038:     14: ParallelTransformerLayerPipe
g0038:     15: ParallelTransformerLayerPipe
g0038:     16: ParallelTransformerLayerPipe
g0038: stage=5 layers=3
g0038:     17: ParallelTransformerLayerPipe
g0038:     18: ParallelTransformerLayerPipe
g0038:     19: ParallelTransformerLayerPipe
g0038: stage=6 layers=3
g0038:     20: ParallelTransformerLayerPipe
g0038:     21: ParallelTransformerLayerPipe
g0038:     22: ParallelTransformerLayerPipe
g0038: stage=7 layers=3
g0038:     23: ParallelTransformerLayerPipe
g0038:     24: MixedFusedRMSNorm
g0038:     25: LMHeadPipe
g0038:   loss: CrossEntropy
g0066:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0063:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0043:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0054:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0041:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0056:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0042:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0038: [2024-08-12 03:45:30,856] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0038: [2024-08-12 03:45:30,858] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0038: [2024-08-12 03:45:30,858] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 55.24 GB, percent = 14.7%
g0038:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0038: setting training iterations to 52000
g0038: > learning rate decay style: cosine
g0038: DeepSpeed is enabled.
g0038: [2024-08-12 03:45:30,861] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0066: [2024-08-12 03:45:30,863] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0066: [2024-08-12 03:45:30,863] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0066: [2024-08-12 03:45:30,863] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0066: [2024-08-12 03:45:30,863] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0063: [2024-08-12 03:45:30,864] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0063: [2024-08-12 03:45:30,864] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0063: [2024-08-12 03:45:30,864] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0063: [2024-08-12 03:45:30,866] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0056: [2024-08-12 03:45:30,886] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0056: [2024-08-12 03:45:30,886] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0056: [2024-08-12 03:45:30,886] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0056: [2024-08-12 03:45:30,886] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0054: [2024-08-12 03:45:30,890] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0054: [2024-08-12 03:45:30,890] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0054: [2024-08-12 03:45:30,890] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0054: [2024-08-12 03:45:30,890] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0043: [2024-08-12 03:45:30,897] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0043: [2024-08-12 03:45:30,897] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0043: [2024-08-12 03:45:30,897] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0043: [2024-08-12 03:45:30,897] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0041: [2024-08-12 03:45:30,902] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0041: [2024-08-12 03:45:30,902] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0041: [2024-08-12 03:45:30,902] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0041: [2024-08-12 03:45:30,903] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0042: [2024-08-12 03:45:30,927] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0042: [2024-08-12 03:45:30,927] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0042: [2024-08-12 03:45:30,927] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0042: [2024-08-12 03:45:30,928] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0038: [2024-08-12 03:45:31,053] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0038: [2024-08-12 03:45:31,054] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0038: [2024-08-12 03:45:31,054] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0038: [2024-08-12 03:45:31,055] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0038: [2024-08-12 03:45:31,055] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0038: [2024-08-12 03:45:31,087] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0038: [2024-08-12 03:45:31,087] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0038: [2024-08-12 03:45:31,087] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0038: [2024-08-12 03:45:31,087] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f154d5f1f50>
g0038: [2024-08-12 03:45:31,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: [2024-08-12 03:45:31,087] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0038: [2024-08-12 03:45:31,088] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0038: [2024-08-12 03:45:31,088] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0038: [2024-08-12 03:45:31,089] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0038:     "partition_activations": false, 
g0038:     "contiguous_memory_optimization": false, 
g0038:     "cpu_checkpointing": false, 
g0038:     "number_checkpoints": null, 
g0038:     "synchronize_checkpoint_boundary": false, 
g0038:     "profile": false
g0038: }
g0038: [2024-08-12 03:45:31,089] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0038: [2024-08-12 03:45:31,089] [INFO] [config.py:983:print]   amp_enabled .................. False
g0038: [2024-08-12 03:45:31,089] [INFO] [config.py:983:print]   amp_params ................... False
g0038: [2024-08-12 03:45:31,090] [INFO] [config.py:983:print]   autotuning_config ............ {
g0038:     "enabled": false, 
g0038:     "start_step": null, 
g0038:     "end_step": null, 
g0038:     "metric_path": null, 
g0038:     "arg_mappings": null, 
g0038:     "metric": "throughput", 
g0038:     "model_info": null, 
g0038:     "results_dir": "autotuning_results", 
g0038:     "exps_dir": "autotuning_exps", 
g0038:     "overwrite": true, 
g0038:     "fast": true, 
g0038:     "start_profile_step": 3, 
g0038:     "end_profile_step": 5, 
g0038:     "tuner_type": "gridsearch", 
g0038:     "tuner_early_stopping": 5, 
g0038:     "tuner_num_trials": 50, 
g0038:     "model_info_path": null, 
g0038:     "mp_size": 1, 
g0038:     "max_train_batch_size": null, 
g0038:     "min_train_batch_size": 1, 
g0038:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0038:     "min_train_micro_batch_size_per_gpu": 1, 
g0038:     "num_tuning_micro_batch_sizes": 3
g0038: }
g0038: [2024-08-12 03:45:31,090] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0038: [2024-08-12 03:45:31,090] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0038: [2024-08-12 03:45:31,090] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0038: [2024-08-12 03:45:31,090] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0038: [2024-08-12 03:45:31,090] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f151a1778d0>
g0038: [2024-08-12 03:45:31,090] [INFO] [config.py:983:print]   communication_data_type ...... None
g0038: [2024-08-12 03:45:31,091] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0038: [2024-08-12 03:45:31,091] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0038: [2024-08-12 03:45:31,091] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0038: [2024-08-12 03:45:31,091] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0038: [2024-08-12 03:45:31,091] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0038: [2024-08-12 03:45:31,091] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0038: [2024-08-12 03:45:31,091] [INFO] [config.py:983:print]   disable_allgather ............ False
g0038: [2024-08-12 03:45:31,091] [INFO] [config.py:983:print]   dump_state ................... False
g0038: [2024-08-12 03:45:31,091] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0038: [2024-08-12 03:45:31,092] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0038: [2024-08-12 03:45:31,092] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0038: [2024-08-12 03:45:31,092] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0038: [2024-08-12 03:45:31,092] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0038: [2024-08-12 03:45:31,092] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0038: [2024-08-12 03:45:31,092] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0038: [2024-08-12 03:45:31,092] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0038: [2024-08-12 03:45:31,092] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0038: [2024-08-12 03:45:31,092] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0038: [2024-08-12 03:45:31,093] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0038:     "enabled": false, 
g0038:     "recompute_fwd_factor": 0.0, 
g0038:     "profile_step": 1, 
g0038:     "module_depth": -1, 
g0038:     "top_modules": 1, 
g0038:     "detailed": true, 
g0038:     "output_file": null
g0038: }
g0038: [2024-08-12 03:45:31,093] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0038: [2024-08-12 03:45:31,093] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0038: [2024-08-12 03:45:31,093] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0038: [2024-08-12 03:45:31,093] [INFO] [config.py:983:print]   global_rank .................. 0
g0038: [2024-08-12 03:45:31,093] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0038: [2024-08-12 03:45:31,093] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0038: [2024-08-12 03:45:31,093] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0038: [2024-08-12 03:45:31,094] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0038: [2024-08-12 03:45:31,094] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0038: [2024-08-12 03:45:31,094] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0038: [2024-08-12 03:45:31,094] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0038: [2024-08-12 03:45:31,094] [INFO] [config.py:983:print]   loss_scale ................... 0
g0038: [2024-08-12 03:45:31,094] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0038: [2024-08-12 03:45:31,094] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0038: [2024-08-12 03:45:31,094] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0038: [2024-08-12 03:45:31,095] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0038: [2024-08-12 03:45:31,095] [INFO] [config.py:983:print]   nebula_config ................ {
g0038:     "enabled": false, 
g0038:     "persistent_storage_path": null, 
g0038:     "persistent_time_interval": 100, 
g0038:     "num_of_version_in_retention": 2, 
g0038:     "enable_nebula_load": true, 
g0038:     "load_path": null
g0038: }
g0038: [2024-08-12 03:45:31,095] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0038: [2024-08-12 03:45:31,095] [INFO] [config.py:983:print]   optimizer_name ............... None
g0038: [2024-08-12 03:45:31,095] [INFO] [config.py:983:print]   optimizer_params ............. None
g0038: [2024-08-12 03:45:31,095] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0038: [2024-08-12 03:45:31,095] [INFO] [config.py:983:print]   pld_enabled .................. False
g0038: [2024-08-12 03:45:31,096] [INFO] [config.py:983:print]   pld_params ................... False
g0038: [2024-08-12 03:45:31,096] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0038: [2024-08-12 03:45:31,096] [INFO] [config.py:983:print]   scheduler_name ............... None
g0038: [2024-08-12 03:45:31,096] [INFO] [config.py:983:print]   scheduler_params ............. None
g0038: [2024-08-12 03:45:31,096] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0038: [2024-08-12 03:45:31,096] [INFO] [config.py:983:print]   sparse_attention ............. None
g0038: [2024-08-12 03:45:31,096] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0038: [2024-08-12 03:45:31,096] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0038: [2024-08-12 03:45:31,097] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0038: [2024-08-12 03:45:31,097] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0038: [2024-08-12 03:45:31,097] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0038: [2024-08-12 03:45:31,097] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0038: [2024-08-12 03:45:31,097] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0038: [2024-08-12 03:45:31,097] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0038: [2024-08-12 03:45:31,097] [INFO] [config.py:983:print]   world_size ................... 4
g0038: [2024-08-12 03:45:31,097] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0038: [2024-08-12 03:45:31,097] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0038: [2024-08-12 03:45:31,098] [INFO] [config.py:983:print]   zero_enabled ................. False
g0038: [2024-08-12 03:45:31,098] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0038: [2024-08-12 03:45:31,098] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0038: [2024-08-12 03:45:31,098] [INFO] [config.py:969:print_user_config]   json = {
g0038:     "train_batch_size": 128, 
g0038:     "train_micro_batch_size_per_gpu": 1, 
g0038:     "steps_per_print": 10, 
g0038:     "zero_optimization": {
g0038:         "stage": 0
g0038:     }, 
g0038:     "gradient_clipping": 1.0, 
g0038:     "prescale_gradients": true, 
g0038:     "fp16": {
g0038:         "enabled": true, 
g0038:         "loss_scale": 0, 
g0038:         "loss_scale_window": 500, 
g0038:         "hysteresis": 2, 
g0038:         "min_loss_scale": 1, 
g0038:         "initial_scale_power": 11
g0038:     }, 
g0038:     "wall_clock_breakdown": false
g0038: }
g0038: [2024-08-12 03:45:31,098] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0038: [2024-08-12 03:45:31,098] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0038: [2024-08-12 03:45:31,799] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0054: [2024-08-12 03:45:31,800] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0063: [2024-08-12 03:45:31,800] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0041: [2024-08-12 03:45:31,800] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0056: [2024-08-12 03:45:31,801] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0066: [2024-08-12 03:45:31,801] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0042: [2024-08-12 03:45:31,800] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0043: [2024-08-12 03:45:31,801] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0066: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0066: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0038: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0038: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0038: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0054: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0043: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0038: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0042: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0043: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0054: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0042: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0054: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0056: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0041: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0042: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0056: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0041: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0056: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0041: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0063: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0063: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0063: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0042: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0063: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0066: [2024-08-12 03:45:32,507] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0043: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0043: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0041: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0066: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0054: [2024-08-12 03:45:32,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0056: [2024-08-12 03:45:32,516] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0041: [2024-08-12 03:45:35,404] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0041: [2024-08-12 03:45:35,404] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0041: [2024-08-12 03:45:35,404] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0041: [2024-08-12 03:45:35,404] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0056: [2024-08-12 03:45:35,406] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0056: [2024-08-12 03:45:35,406] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0056: [2024-08-12 03:45:35,406] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0041: [2024-08-12 03:45:35,412] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0041: [2024-08-12 03:45:35,413] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0041: [2024-08-12 03:45:35,413] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0041: [2024-08-12 03:45:35,414] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0056: [2024-08-12 03:45:35,414] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0056: [2024-08-12 03:45:35,415] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0056: [2024-08-12 03:45:35,415] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0056: [2024-08-12 03:45:35,415] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0056: [2024-08-12 03:45:35,419] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0042: [2024-08-12 03:45:35,445] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0042: [2024-08-12 03:45:35,445] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0042: [2024-08-12 03:45:35,445] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0042: [2024-08-12 03:45:35,445] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0042: [2024-08-12 03:45:35,454] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0042: [2024-08-12 03:45:35,454] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0042: [2024-08-12 03:45:35,454] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0042: [2024-08-12 03:45:35,454] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0063: [2024-08-12 03:45:35,472] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0063: [2024-08-12 03:45:35,472] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0063: [2024-08-12 03:45:35,472] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0063: [2024-08-12 03:45:35,472] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0063: [2024-08-12 03:45:35,480] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0063: [2024-08-12 03:45:35,480] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0063: [2024-08-12 03:45:35,480] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0063: [2024-08-12 03:45:35,480] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0043: [2024-08-12 03:45:35,501] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0043: [2024-08-12 03:45:35,502] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0043: [2024-08-12 03:45:35,502] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0043: [2024-08-12 03:45:35,505] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0043: [2024-08-12 03:45:35,510] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0043: [2024-08-12 03:45:35,511] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0043: [2024-08-12 03:45:35,511] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0043: [2024-08-12 03:45:35,512] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0054: [2024-08-12 03:45:35,512] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0054: [2024-08-12 03:45:35,512] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0054: [2024-08-12 03:45:35,512] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0054: [2024-08-12 03:45:35,513] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 03:45:35,513] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 03:45:35,513] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 03:45:35,513] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 03:45:35,513] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 03:45:35,520] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0054: [2024-08-12 03:45:35,520] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0054: [2024-08-12 03:45:35,520] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0038: [2024-08-12 03:45:35,521] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0054: [2024-08-12 03:45:35,522] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0054: [2024-08-12 03:45:35,522] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0038: [2024-08-12 03:45:35,522] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0038: [2024-08-12 03:45:35,522] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0066: [2024-08-12 03:45:35,543] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0066: [2024-08-12 03:45:35,543] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0066: [2024-08-12 03:45:35,543] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0066: [2024-08-12 03:45:35,543] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0066: [2024-08-12 03:45:35,552] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0066: [2024-08-12 03:45:35,552] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0066: [2024-08-12 03:45:35,552] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0066: [2024-08-12 03:45:35,552] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0056: [2024-08-12 03:45:37,108] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0056: [2024-08-12 03:45:37,109] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,110] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0056: [2024-08-12 03:45:37,110] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0056: [2024-08-12 03:45:37,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,111] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0056: [2024-08-12 03:45:37,113] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0043: [2024-08-12 03:45:37,174] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0043: [2024-08-12 03:45:37,174] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0043: [2024-08-12 03:45:37,174] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0043: [2024-08-12 03:45:37,175] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 03:45:37,175] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 03:45:37,175] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0043: [2024-08-12 03:45:37,175] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 03:45:37,177] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,210] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0054: [2024-08-12 03:45:37,210] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0054: [2024-08-12 03:45:37,210] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0054: [2024-08-12 03:45:37,211] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,211] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,211] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,212] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0054: [2024-08-12 03:45:37,213] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,287] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0042: [2024-08-12 03:45:37,287] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0042: [2024-08-12 03:45:37,288] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,288] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,289] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0042: [2024-08-12 03:45:37,290] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0042: [2024-08-12 03:45:37,290] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,290] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,408] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,408] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,408] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,408] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,409] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,409] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,409] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,410] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,415] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0063: [2024-08-12 03:45:37,416] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0063: [2024-08-12 03:45:37,416] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,417] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,419] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0063: [2024-08-12 03:45:37,419] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0063: [2024-08-12 03:45:37,419] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,420] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0041: [2024-08-12 03:45:37,435] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0041: [2024-08-12 03:45:37,435] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0041: [2024-08-12 03:45:37,435] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0041: [2024-08-12 03:45:37,436] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0041: [2024-08-12 03:45:37,436] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 03:45:37,436] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 03:45:37,436] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 03:45:37,437] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,441] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,441] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,445] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,445] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,460] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,463] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,463] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,466] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,471] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,471] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,471] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,471] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,471] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,472] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,472] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,472] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,506] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,506] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,507] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,508] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,522] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,523] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,529] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,529] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,635] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 03:45:37,636] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,636] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 03:45:37,636] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 03:45:37,636] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 03:45:37,636] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,637] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,637] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,669] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 03:45:37,669] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 03:45:37,672] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 03:45:37,672] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,677] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,677] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,677] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,677] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,677] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,678] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,678] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,678] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,685] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,687] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,692] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 03:45:37,694] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,706] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 03:45:37,706] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 03:45:37,706] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 03:45:37,707] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,707] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 03:45:37,707] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,707] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,708] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,709] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,709] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,711] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,711] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,724] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,731] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,732] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,733] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,741] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 03:45:37,741] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 03:45:37,743] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 03:45:37,744] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 03:45:37,756] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,756] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,766] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 03:45:37,766] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,783] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,784] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,784] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,784] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,784] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,784] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,784] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,784] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,816] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,816] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,817] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,818] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 03:45:37,830] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,830] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,839] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 03:45:37,839] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,936] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,936] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,936] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,936] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,936] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,937] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,937] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,937] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 03:45:37,968] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,968] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,970] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 03:45:37,970] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0041: [2024-08-12 03:45:37,991] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 03:45:37,991] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 03:45:37,991] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 03:45:37,992] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 03:45:37,992] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 03:45:37,992] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 03:45:37,992] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 03:45:37,993] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,026] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,026] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,027] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,027] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,027] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,027] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,027] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 03:45:38,027] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 03:45:38,028] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 03:45:38,028] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,028] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,028] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,032] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 03:45:38,032] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 03:45:38,032] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 03:45:38,032] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 03:45:38,033] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,033] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,033] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,033] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0054: [2024-08-12 03:45:38,040] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 03:45:38,040] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 03:45:38,040] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 03:45:38,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 03:45:38,041] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 03:45:38,041] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 03:45:38,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 03:45:38,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,042] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,042] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,049] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,049] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0063: [2024-08-12 03:45:38,058] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,058] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,060] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,060] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0054: [2024-08-12 03:45:38,072] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,072] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0054: [2024-08-12 03:45:38,073] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,074] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0054: [2024-08-12 03:45:38,075] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 03:45:38,075] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,079] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0063: [2024-08-12 03:45:38,081] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,094] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,095] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,095] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,096] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,096] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,126] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,126] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,128] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,129] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,131] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,131] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,131] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,132] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,132] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,132] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,132] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,132] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,142] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,143] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,148] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,148] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,165] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,165] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,168] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,169] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,185] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,186] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,187] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,187] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0063: [2024-08-12 03:45:38,265] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,265] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,265] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,266] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,266] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0063: [2024-08-12 03:45:38,266] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0063: [2024-08-12 03:45:38,266] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0063: [2024-08-12 03:45:38,266] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,287] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,287] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,287] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,287] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,287] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,288] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,288] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,288] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0063: [2024-08-12 03:45:38,296] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,296] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,297] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 03:45:38,298] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,335] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,335] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,339] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,339] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,356] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,357] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,357] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,357] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,357] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,357] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,357] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,357] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,358] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,359] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,365] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,366] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,379] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,380] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,380] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,380] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,380] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0042: [2024-08-12 03:45:38,390] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,390] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,391] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 03:45:38,392] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,410] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,410] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,413] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,413] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,430] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,430] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,431] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,431] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,567] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,568] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,568] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,568] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,568] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,568] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,568] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,568] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,601] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,601] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,603] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,604] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,615] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,615] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,625] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,626] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,631] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,631] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,632] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,632] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,632] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,632] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,632] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,632] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,667] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,667] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,669] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,669] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 03:45:38,684] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,685] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,689] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0038: [2024-08-12 03:45:38,689] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,713] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,713] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,713] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,713] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,713] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,714] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,714] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,714] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 03:45:38,745] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,745] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,747] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 03:45:38,747] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,800] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,800] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,800] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,801] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,801] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,801] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,801] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,801] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 03:45:38,835] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,835] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,836] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 03:45:38,837] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0038: [2024-08-12 03:45:39,038] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 03:45:39,038] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 03:45:39,039] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 03:45:39,039] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0038: [2024-08-12 03:45:39,039] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 03:45:39,039] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0038: [2024-08-12 03:45:39,039] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0038: [2024-08-12 03:45:39,040] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0038: [2024-08-12 03:45:39,071] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 03:45:39,071] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 03:45:39,072] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 03:45:39,073] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 03:45:39,085] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 03:45:39,085] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 03:45:39,094] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 03:45:39,094] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,308] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0066: [2024-08-12 03:45:40,308] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0066: [2024-08-12 03:45:40,308] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0066: [2024-08-12 03:45:40,309] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,309] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,309] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,310] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0066: [2024-08-12 03:45:40,311] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0038: [2024-08-12 03:45:40,682] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 03:45:40,682] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 03:45:40,682] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 03:45:40,682] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 03:45:40,683] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 03:45:40,683] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 03:45:40,683] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 03:45:40,683] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,724] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,725] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,725] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,725] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,725] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,726] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,726] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,726] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,761] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,761] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,762] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,762] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,775] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,779] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,783] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,783] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,785] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,785] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,785] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,785] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,785] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,785] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,786] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,786] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,786] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,786] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,786] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,786] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,787] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,787] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,787] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,787] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,972] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,973] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,977] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,978] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,978] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,978] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 03:45:40,979] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 03:45:40,979] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0038: [2024-08-12 03:45:41,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 03:45:41,010] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 03:45:41,010] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 03:45:41,010] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0066: [2024-08-12 03:45:41,019] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 03:45:41,022] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 03:45:41,029] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 03:45:41,030] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0038:  > overriding learning rate value to 0.0002
g0038:  > overriding minimum learning rate value to 1e-05
g0038:  > overriding warmup iterations value to 0
g0038:  > overriding warmup tokens value to 3000000000
g0038:  > overriding total number of iterations value to 6656000
g0038:  > overriding decay tokens value to 300000000000
g0038:  > overriding learning rate decay style value to cosine
g0038:  > overriding start weight decay value to 0.1
g0038:  > overriding end weight decay value to 0.1
g0038:  > overriding total number of weight decay iterations value to 6656000
g0038:  > overriding weight decay incr style value to constant
g0038:  checkpoint version 3.0
g0038:   successfully loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase at iteration 42000
g0066: (min, max) time across ranks (ms):
g0066:     load-checkpoint ................................: (10028.27, 10029.75)
g0038: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-12 03:45:42 
g0038: > building train, validation, and test datasets ...
g0038:  > datasets target sizes (minimum size):
g0038:     train:      6656000
g0038:     validation: 678400
g0038:     test:       12800
g0038: > building train, validation, and test datasets for GPT ...
g0038: Single data path provided for train, valid & test
g0038:  > building dataset index ...
g0038:     reading sizes...
g0038:     reading pointers...
g0038:     reading document index...
g0038:     creating numpy buffer of mmap...
g0038:     creating memory view of numpy buffer...
g0038:  > finished creating indexed dataset in 0.063799 seconds
g0038:     number of documents: 250886
g0038:  > dataset split:
g0038:     train:
g0038:      document indices in [0, 238091) total of 238091 documents
g0038:     validation:
g0038:      document indices in [238091, 250635) total of 12544 documents
g0038:     test:
g0038:      document indices in [250635, 250886) total of 251 documents
g0038:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0038:  > last epoch number of samples (457326) is larger than 80% of number of samples per epoch (476821), setting separate_last_epoch to False
g0038:  > elasped time to build and save doc-idx mapping (seconds): 0.135530
g0038:     using:
g0038:      number of documents:       238091
g0038:      number of epochs:          14
g0038:      sequence length:           2048
g0038:      total number of samples:   6675495
g0038:  > elasped time to build and save sample-idx mapping (seconds): 0.220537
g0038:  > building shuffle index with split [0, 6675495) and [6675495, 6675495) ...
g0038:  > elasped time to build and save shuffle-idx mapping (seconds): 0.186920
g0038:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/1b748295524dd12ee996beae300eb264_doc_idx.npy
g0038:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/1b748295524dd12ee996beae300eb264_sample_idx.npy
g0038:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/1b748295524dd12ee996beae300eb264_shuffle_idx.npy
g0038:     loaded indexed file in 0.014 seconds
g0038:     total number of samples: 6675496
g0038:     total number of epochs: 14
g0038:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0038:  > last epoch number of samples (11709) is smaller than 80% of number of samples per epoch (25641), setting separate_last_epoch to True
g0038:  > elasped time to build and save doc-idx mapping (seconds): 0.014060
g0038:     using:
g0038:      number of documents:       12544
g0038:      number of epochs:          27
g0038:      sequence length:           2048
g0038:      total number of samples:   692333
g0038:  > elasped time to build and save sample-idx mapping (seconds): 0.014068
g0038:  > building shuffle index with split [0, 666691) and [666691, 692333) ...
g0038:  > elasped time to build and save shuffle-idx mapping (seconds): 0.020968
g0038:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3fb31b5e2ebe09b3ddb60248542435ee_doc_idx.npy
g0038:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3fb31b5e2ebe09b3ddb60248542435ee_sample_idx.npy
g0038:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3fb31b5e2ebe09b3ddb60248542435ee_shuffle_idx.npy
g0038:     loaded indexed file in 0.010 seconds
g0038:     total number of samples: 692334
g0038:     total number of epochs: 27
g0038:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0038:  > last epoch number of samples (369) is larger than 80% of number of samples per epoch (376), setting separate_last_epoch to False
g0038:  > elasped time to build and save doc-idx mapping (seconds): 0.001598
g0038:     using:
g0038:      number of documents:       251
g0038:      number of epochs:          34
g0038:      sequence length:           2048
g0038:      total number of samples:   12808
g0038:  > elasped time to build and save sample-idx mapping (seconds): 0.001470
g0038:  > building shuffle index with split [0, 12808) and [12808, 12808) ...
g0038:  > elasped time to build and save shuffle-idx mapping (seconds): 0.001627
g0038:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/b4dcabb9403349ef9dc0eded6edcbcbf_doc_idx.npy
g0038:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/b4dcabb9403349ef9dc0eded6edcbcbf_sample_idx.npy
g0038:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/b4dcabb9403349ef9dc0eded6edcbcbf_shuffle_idx.npy
g0038:     loaded indexed file in 0.003 seconds
g0038:     total number of samples: 12809
g0038:     total number of epochs: 34
g0038: > finished creating GPT datasets ...
g0038: [after dataloaders are built] datetime: 2024-08-12 03:45:45 
g0038: done with setup ...
g0038: training ...
g0066: (min, max) time across ranks (ms):
g0066:     model-and-optimizer-setup ......................: (13011.84, 13022.24)
g0066:     train/valid/test-data-iterators-setup ..........: (2533.79, 2624.25)
g0038: [before the start of training step] datetime: 2024-08-12 03:45:45 
g0038: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42000
g0038: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0038: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42000
g0038: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0054: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42000
g0054: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0054: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42000
g0054: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0054: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42000
g0054: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42000
g0041: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42000
g0041: Grad overflow on iteration 42000
g0056: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42000
g0038: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42000
g0063: Grad overflow on iteration 42000
g0041: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0056: Grad overflow on iteration 42000
g0056: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42000
g0038: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0056: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0042: Grad overflow on iteration 42000
g0041: Grad overflow on iteration 42000
g0056: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0043: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0063: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42000
g0043: Grad overflow on iteration 42000
g0041: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0063: Grad overflow on iteration 42000
g0041: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0042: Grad overflow on iteration 42000
g0066: Grad overflow on iteration 42000
g0056: Grad overflow on iteration 42000
g0066: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0042: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0042: Grad overflow on iteration 42000
g0063: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0042: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0043: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0063: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0042: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0042: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42000
g0042: Grad overflow on iteration 42000
g0066: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0041: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0063: Grad overflow on iteration 42000
g0043: Grad overflow on iteration 42000
g0041: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42000
g0041: Grad overflow on iteration 42000
g0043: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0063: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42000
g0043: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42000
g0066: Grad overflow on iteration 42000
g0043: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0066: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0063: Grad overflow on iteration 42000
g0063: [2024-08-12 03:46:01,529] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0056: [2024-08-12 03:46:01,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0038: [2024-08-12 03:46:01,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0041: [2024-08-12 03:46:01,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0066: [2024-08-12 03:46:01,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0063: [2024-08-12 03:46:01,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0054: [2024-08-12 03:46:01,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0042: [2024-08-12 03:46:01,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0043: [2024-08-12 03:46:01,530] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0038: [2024-08-12 03:46:01,530] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0063: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42001
g0063: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42001
g0063: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42001
g0063: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0063: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0063: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0042: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42001
g0042: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0063: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42001
g0042: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42001
g0042: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42001
g0042: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42001
g0041: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42001
g0041: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42001
g0041: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0041: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42001
g0041: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0042: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0042: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0038: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42001
g0038: Grad overflow on iteration 42001
g0043: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0043: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42001
g0038: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42001
g0038: Grad overflow on iteration 42001
g0054: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42001
g0043: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0038: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0056: Grad overflow on iteration 42001
g0056: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0056: Grad overflow on iteration 42001
g0038: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0056: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42001
g0056: Grad overflow on iteration 42001
g0056: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0043: Grad overflow on iteration 42001
g0056: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0054: Grad overflow on iteration 42001
g0041: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0041: Grad overflow on iteration 42001
g0054: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42001
g0066: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0066: Grad overflow on iteration 42001
g0066: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42001
g0041: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0066: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0054: Grad overflow on iteration 42001
g0066: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0054: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0041: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0054: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0043: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42001
g0056: Grad overflow on iteration 42001
g0043: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0043: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0056: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0066: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42001
g0066: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42001
g0066: [2024-08-12 03:46:06,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0054: [2024-08-12 03:46:06,829] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0063: [2024-08-12 03:46:06,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0056: [2024-08-12 03:46:06,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0038: [2024-08-12 03:46:06,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0066: [2024-08-12 03:46:06,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0038: [2024-08-12 03:46:06,830] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
g0038: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42002
g0063: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42002
g0063: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0043: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42002
g0043: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42002
g0056: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42002
g0042: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0063: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42002
g0041: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0043: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0056: Grad overflow on iteration 42002
g0054: Grad overflow on iteration 42002
g0054: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0056: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0041: Grad overflow on iteration 42002
g0056: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42002
g0043: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42002
g0041: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0038: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42002
g0056: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0063: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0043: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0056: Grad overflow on iteration 42002
g0042: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42002
g0041: Grad overflow on iteration 42002
g0066: Grad overflow on iteration 42002
g0066: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0042: Grad overflow on iteration 42002
g0041: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42002
g0042: Grad overflow on iteration 42002
g0041: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0063: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0054: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42002
g0066: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42002
g0038: Grad overflow on iteration 42002
g0041: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0038: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0056: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0056: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42002
g0042: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0066: Grad overflow on iteration 42002
g0063: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0041: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0063: Grad overflow on iteration 42002
g0038: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0054: Grad overflow on iteration 42002
g0066: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0038: Grad overflow on iteration 42002
g0066: Grad overflow on iteration 42002
g0038: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0054: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0042: Grad overflow on iteration 42002
g0066: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42002
g0054: Grad overflow on iteration 42002
g0066: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0054: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0066: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0038: [2024-08-12 03:46:11,296] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
g0063: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0043: [2024-08-12 03:46:11,295] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42002
g0043: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0056: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0063: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0066: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0042: [2024-08-12 03:46:11,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0041: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42004
g0041: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0041: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42004
g0043: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42004
g0041: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0041: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42004
g0042: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42004
g0041: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0042: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42004
g0043: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0043: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42004
g0042: Grad overflow on iteration 42004
g0043: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42004
g0043: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0043: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42004
g0043: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0042: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42004
g0042: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0042: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0042: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42004
g0042: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0063: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0038: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42004
g0063: Grad overflow on iteration 42004
g0063: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42004
g0063: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42004
g0038: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42004
g0056: Grad overflow on iteration 42004
g0063: Grad overflow on iteration 42004
g0054: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0054: Grad overflow on iteration 42004
g0056: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42004
g0054: Grad overflow on iteration 42004
g0066: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0054: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0066: Grad overflow on iteration 42004
g0054: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0066: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0038: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42004
g0066: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0066: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42004
g0041: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0054: Grad overflow on iteration 42004
g0054: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0066: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0054: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0038: Grad overflow on iteration 42004
g0043: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0038: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0056: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0056: Grad overflow on iteration 42004
g0056: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42004
g0038: Grad overflow on iteration 42004
g0063: Grad overflow on iteration 42004
g0063: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0056: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0038: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0038: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0063: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0056: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0038: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0056: [2024-08-12 03:46:21,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0038: [2024-08-12 03:46:21,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
g0066: [2024-08-12 03:46:21,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42004
g0066: [2024-08-12 03:46:21,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0038: [2024-08-12 03:46:42,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=42010, skipped=65, lr=[0.00019965900617800823, 0.00019965900617800823], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42010 loss: 8.8245 iter time (s): 5.687 samples/sec: 22.509
g0066:  iteration    42010/   52000 | consumed samples:      5377280 | consumed tokens:  11012669440 | elapsed time per iteration (ms): 5726.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.025291E+01 | loss scale: 4096.0 | grad norm: 12.350 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.353 | tokens per gpu per second (tgs): 1430.616 | TFLOPs: 11.51 |
g0063: [Rank 24] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 5054.0 | max reserved: 5054.0
g0041: [Rank 4] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 8990.0 | max reserved: 8990.0
g0054: [Rank 16] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6308.0 | max reserved: 6308.0
g0056: [Rank 20] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5414.0 | max reserved: 5414.0
g0066: [Rank 28] (after 42010 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2984.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0043: [Rank 12] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7202.0 | max reserved: 7202.0
g0042: [Rank 8] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8096.0 | max reserved: 8096.0
g0038: [Rank 0] (after 42010 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 11010.0 | max reserved: 11010.0
g0038: [2024-08-12 03:47:26,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=42020, skipped=65, lr=[0.00019965878314753572, 0.00019965878314753572], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42020 loss: 6.0671 iter time (s): 4.198 samples/sec: 30.492
g0066:  iteration    42020/   52000 | consumed samples:      5378560 | consumed tokens:  11015290880 | elapsed time per iteration (ms): 4365.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 6.890160E+00 | loss scale: 4096.0 | grad norm: 2.580 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.323 | tokens per gpu per second (tgs): 1876.672 | TFLOPs: 15.10 |
g0038: [2024-08-12 03:48:07,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=42030, skipped=65, lr=[0.00019965856004428058, 0.00019965856004428058], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42030 loss: 5.5661 iter time (s): 4.119 samples/sec: 31.078
g0066:  iteration    42030/   52000 | consumed samples:      5379840 | consumed tokens:  11017912320 | elapsed time per iteration (ms): 4151.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 5.742688E+00 | loss scale: 4096.0 | grad norm: 3.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.829 | tokens per gpu per second (tgs): 1973.064 | TFLOPs: 15.88 |
g0038: [2024-08-12 03:48:49,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=42040, skipped=65, lr=[0.000199658336868243, 0.000199658336868243], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42040 loss: 4.9697 iter time (s): 4.115 samples/sec: 31.104
g0066:  iteration    42040/   52000 | consumed samples:      5381120 | consumed tokens:  11020533760 | elapsed time per iteration (ms): 4148.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 5.118137E+00 | loss scale: 4096.0 | grad norm: 2.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.855 | tokens per gpu per second (tgs): 1974.694 | TFLOPs: 15.89 |
g0038: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42046
g0038: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0042: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42046
g0054: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0054: Grad overflow on iteration 42046
g0041: Grad overflow on iteration 42046
g0054: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0043: Grad overflow on iteration 42046
g0041: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0043: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0054: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42046
g0056: Grad overflow on iteration 42046
g0043: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42046
g0043: Grad overflow on iteration 42046
g0063: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42046
g0042: Grad overflow on iteration 42046
g0041: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42046
g0043: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42046
g0041: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42046
g0056: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0041: Grad overflow on iteration 42046
g0056: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0041: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0056: Grad overflow on iteration 42046
g0043: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42046
g0054: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0066: Grad overflow on iteration 42046
g0054: Grad overflow on iteration 42046
g0041: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0054: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42046
g0063: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42046
g0056: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0063: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42046
g0042: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42046
g0042: Grad overflow on iteration 42046
g0038: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42046
g0066: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42046
g0063: Grad overflow on iteration 42046
g0066: Grad overflow on iteration 42046
g0063: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0066: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0043: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0063: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0066: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42046
g0063: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0054: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0066: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0054: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0066: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0054: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0056: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0066: Grad overflow on iteration 42046
g0056: Grad overflow on iteration 42046
g0066: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0038: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0056: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0038: [2024-08-12 03:49:17,993] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42046
g0042: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0056: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0038: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0041: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0038: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0043: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0066: [2024-08-12 03:49:17,994] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0038: [2024-08-12 03:49:17,994] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0
g0042: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42047
g0042: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42047
g0042: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42047
g0054: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42047
g0054: Grad overflow on iteration 42047
g0056: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42047
g0054: Grad overflow on iteration 42047
g0042: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0038: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42047
g0038: Grad overflow on iteration 42047
g0038: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42047
g0063: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0056: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42047
g0056: Grad overflow on iteration 42047
g0054: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42047
g0042: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0041: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42047
g0041: Grad overflow on iteration 42047
g0063: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42047
g0063: Grad overflow on iteration 42047
g0041: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42047
g0066: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0041: Grad overflow on iteration 42047
g0043: Grad overflow on iteration 42047
g0038: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0038: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0042: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0066: Grad overflow on iteration 42047
g0056: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0066: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42047
g0066: Grad overflow on iteration 42047
g0042: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0066: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0066: Grad overflow on iteration 42047
g0056: Grad overflow on iteration 42047
g0054: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42047
g0063: Grad overflow on iteration 42047
g0054: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0066: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42047
g0056: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0066: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0066: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0038: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0066: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0066: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0043: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0043: Grad overflow on iteration 42047
g0063: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0043: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0043: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42047
g0063: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0043: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42047
g0043: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0056: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0043: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0043: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0038: Grad overflow on iteration 42047
g0056: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0038: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0038: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0054: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0041: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0041: [2024-08-12 03:49:22,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0041: [2024-08-12 03:49:22,030] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0038: [2024-08-12 03:49:22,030] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
g0041: [2024-08-12 03:49:30,652] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42049
g0063: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42049
g0063: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42049
g0043: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42049
g0043: Grad overflow on iteration 42049
g0043: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42049
g0041: Grad overflow on iteration 42049
g0042: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42049
g0043: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0042: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0041: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0063: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0042: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42049
g0041: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42049
g0043: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42049
g0043: Grad overflow on iteration 42049
g0063: Grad overflow on iteration 42049
g0054: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42049
g0041: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42049
g0063: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0041: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0054: Grad overflow on iteration 42049
g0041: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0043: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42049
g0054: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0043: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0042: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0063: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0042: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0063: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0041: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0042: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0038: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42049
g0038: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42049
g0038: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0054: Grad overflow on iteration 42049
g0066: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42049
g0043: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0038: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42049
g0038: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42049
g0056: Grad overflow on iteration 42049
g0038: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0066: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0066: Grad overflow on iteration 42049
g0066: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0066: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42049
g0056: Grad overflow on iteration 42049
g0056: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0066: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0056: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0056: Grad overflow on iteration 42049
g0054: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42049
g0054: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42049
g0054: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0056: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0066: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0066: Grad overflow on iteration 42049
g0038: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0054: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0056: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0056: Grad overflow on iteration 42049
g0056: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0056: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0066: [2024-08-12 03:49:30,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0038: [2024-08-12 03:49:30,653] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1024.0, reducing to 512.0
g0038: [2024-08-12 03:49:30,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=42050, skipped=68, lr=[0.00019965813594758036, 0.00019965813594758036], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42050 loss: nan iter time (s): 4.111 samples/sec: 31.136
g0066:  iteration    42050/   52000 | consumed samples:      5382400 | consumed tokens:  11023155200 | elapsed time per iteration (ms): 4144.0 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 512.0 | grad norm: 2.065 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.888 | tokens per gpu per second (tgs): 1976.835 | TFLOPs: 15.91 |
g0042: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42052
g0056: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42052
g0042: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0056: Grad overflow on iteration 42052
g0056: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42052
g0056: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0056: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42052
g0056: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0038: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42052
g0038: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42052
g0042: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42052
g0042: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0042: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0056: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42052
g0042: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0043: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42052
g0038: Grad overflow on iteration 42052
g0043: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42052
g0041: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42052
g0056: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0054: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42052
g0056: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0041: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42052
g0041: Grad overflow on iteration 42052
g0063: Grad overflow on iteration 42052
g0041: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0038: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42052
g0041: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0043: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0041: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0041: Grad overflow on iteration 42052
g0038: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0038: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0041: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42052
g0041: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0054: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42052
g0054: Grad overflow on iteration 42052
g0043: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42052
g0063: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42052
g0043: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0063: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42052
g0063: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0063: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0054: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42052
g0054: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42052
g0063: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0054: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0063: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0054: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0054: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0054: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0041: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0066: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42052
g0066: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42052
g0066: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42052
g0066: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42052
g0066: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0066: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0066: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0043: [2024-08-12 03:49:43,036] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0038: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0038: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42052
g0038: [2024-08-12 03:49:43,035] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0038: [2024-08-12 03:49:43,036] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 512.0, reducing to 256.0
g0066: [2024-08-12 03:49:43,036] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0038: [2024-08-12 03:50:11,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=42060, skipped=69, lr=[0.00019965789029782125, 0.00019965789029782125], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42060 loss: 4.3563 iter time (s): 4.011 samples/sec: 31.909
g0066:  iteration    42060/   52000 | consumed samples:      5383680 | consumed tokens:  11025776640 | elapsed time per iteration (ms): 4044.5 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 256.0 | grad norm: 1.475 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.648 | tokens per gpu per second (tgs): 2025.477 | TFLOPs: 16.30 |
g0038: [2024-08-12 03:50:52,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=42070, skipped=69, lr=[0.00019965766690343742, 0.00019965766690343742], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42070 loss: 4.0502 iter time (s): 4.157 samples/sec: 30.788
g0066:  iteration    42070/   52000 | consumed samples:      5384960 | consumed tokens:  11028398080 | elapsed time per iteration (ms): 4190.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 4.209018E+00 | loss scale: 256.0 | grad norm: 1.555 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.544 | tokens per gpu per second (tgs): 1954.829 | TFLOPs: 15.73 |
g0038: [2024-08-12 03:51:34,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=42080, skipped=69, lr=[0.00019965744343627184, 0.00019965744343627184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42080 loss: 3.8596 iter time (s): 4.076 samples/sec: 31.400
g0066:  iteration    42080/   52000 | consumed samples:      5386240 | consumed tokens:  11031019520 | elapsed time per iteration (ms): 4109.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 3.942877E+00 | loss scale: 256.0 | grad norm: 1.799 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.147 | tokens per gpu per second (tgs): 1993.380 | TFLOPs: 16.04 |
g0038: [2024-08-12 03:52:17,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=42090, skipped=69, lr=[0.00019965721989632465, 0.00019965721989632465], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42090 loss: 3.6133 iter time (s): 4.286 samples/sec: 29.868
g0066:  iteration    42090/   52000 | consumed samples:      5387520 | consumed tokens:  11033640960 | elapsed time per iteration (ms): 4318.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 3.655609E+00 | loss scale: 256.0 | grad norm: 2.908 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.640 | tokens per gpu per second (tgs): 1896.959 | TFLOPs: 15.27 |
g0038: [2024-08-12 03:53:03,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=42100, skipped=69, lr=[0.00019965699628359606, 0.00019965699628359606], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42100 loss: 3.3597 iter time (s): 4.634 samples/sec: 27.620
g0066:  iteration    42100/   52000 | consumed samples:      5388800 | consumed tokens:  11036262400 | elapsed time per iteration (ms): 4667.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 3.499904E+00 | loss scale: 256.0 | grad norm: 0.999 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.424 | tokens per gpu per second (tgs): 1755.151 | TFLOPs: 14.12 |
g0038: [2024-08-12 03:53:46,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=42110, skipped=69, lr=[0.00019965677259808624, 0.00019965677259808624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42110 loss: 3.2479 iter time (s): 4.271 samples/sec: 29.967
g0066:  iteration    42110/   52000 | consumed samples:      5390080 | consumed tokens:  11038883840 | elapsed time per iteration (ms): 4304.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 3.341807E+00 | loss scale: 256.0 | grad norm: 1.426 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.738 | tokens per gpu per second (tgs): 1903.246 | TFLOPs: 15.32 |
