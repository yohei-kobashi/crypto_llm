
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0038
    HostName g0038
    Port 2222
    StrictHostKeyChecking no

Host g0041
    HostName g0041
    Port 2222
    StrictHostKeyChecking no

Host g0042
    HostName g0042
    Port 2222
    StrictHostKeyChecking no

Host g0043
    HostName g0043
    Port 2222
    StrictHostKeyChecking no

Host g0054
    HostName g0054
    Port 2222
    StrictHostKeyChecking no

Host g0056
    HostName g0056
    Port 2222
    StrictHostKeyChecking no

Host g0063
    HostName g0063
    Port 2222
    StrictHostKeyChecking no

Host g0066
    HostName g0066
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42829713
g0038 slots=4
g0041 slots=4
g0042 slots=4
g0043 slots=4
g0054 slots=4
g0056 slots=4
g0063 slots=4
g0066 slots=4

[2024-08-12 04:00:07,971] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-12 04:00:11,688] [INFO] [runner.py:463:main] Using IP address of 10.1.2.4 for node g0038
[2024-08-12 04:00:11,689] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0038,g0041,g0042,g0043,g0054,g0056,g0063,g0066
[2024-08-12 04:00:11,689] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0038,g0041,g0042,g0043,g0054,g0056,g0063,g0066 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDAzOCI6IFswLCAxLCAyLCAzXSwgImcwMDQxIjogWzAsIDEsIDIsIDNdLCAiZzAwNDIiOiBbMCwgMSwgMiwgM10sICJnMDA0MyI6IFswLCAxLCAyLCAzXSwgImcwMDU0IjogWzAsIDEsIDIsIDNdLCAiZzAwNTYiOiBbMCwgMSwgMiwgM10sICJnMDA2MyI6IFswLCAxLCAyLCAzXSwgImcwMDY2IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.2.4 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '13631488000' --train-samples '6656000' --lr '2.0e-4' --min-lr '1.0e-6' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True' --wandb_tag 'other_gpu'
g0054: [2024-08-12 04:00:15,063] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0042: [2024-08-12 04:00:15,138] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0038: [2024-08-12 04:00:15,141] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0063: [2024-08-12 04:00:15,173] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0041: [2024-08-12 04:00:15,178] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0043: [2024-08-12 04:00:15,195] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0066: [2024-08-12 04:00:15,197] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0056: [2024-08-12 04:00:15,207] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0054: [2024-08-12 04:00:17,202] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0054: [2024-08-12 04:00:17,202] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0054: [2024-08-12 04:00:17,203] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0054: [2024-08-12 04:00:17,203] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0054: [2024-08-12 04:00:17,203] [INFO] [launch.py:163:main] dist_world_size=32
g0054: [2024-08-12 04:00:17,203] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0042: [2024-08-12 04:00:17,318] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0042: [2024-08-12 04:00:17,318] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0042: [2024-08-12 04:00:17,318] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0042: [2024-08-12 04:00:17,318] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0042: [2024-08-12 04:00:17,318] [INFO] [launch.py:163:main] dist_world_size=32
g0042: [2024-08-12 04:00:17,318] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0038: [2024-08-12 04:00:17,334] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0038: [2024-08-12 04:00:17,334] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0038: [2024-08-12 04:00:17,334] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0038: [2024-08-12 04:00:17,335] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0038: [2024-08-12 04:00:17,335] [INFO] [launch.py:163:main] dist_world_size=32
g0038: [2024-08-12 04:00:17,335] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0066: [2024-08-12 04:00:17,365] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0066: [2024-08-12 04:00:17,365] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0066: [2024-08-12 04:00:17,365] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0066: [2024-08-12 04:00:17,365] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0066: [2024-08-12 04:00:17,365] [INFO] [launch.py:163:main] dist_world_size=32
g0066: [2024-08-12 04:00:17,365] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0041: [2024-08-12 04:00:17,407] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0041: [2024-08-12 04:00:17,407] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0041: [2024-08-12 04:00:17,407] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0041: [2024-08-12 04:00:17,407] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0041: [2024-08-12 04:00:17,407] [INFO] [launch.py:163:main] dist_world_size=32
g0041: [2024-08-12 04:00:17,407] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0056: [2024-08-12 04:00:17,412] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0056: [2024-08-12 04:00:17,412] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0056: [2024-08-12 04:00:17,412] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0056: [2024-08-12 04:00:17,412] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0056: [2024-08-12 04:00:17,412] [INFO] [launch.py:163:main] dist_world_size=32
g0056: [2024-08-12 04:00:17,413] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0043: [2024-08-12 04:00:17,417] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0043: [2024-08-12 04:00:17,417] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0043: [2024-08-12 04:00:17,417] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0043: [2024-08-12 04:00:17,417] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0043: [2024-08-12 04:00:17,417] [INFO] [launch.py:163:main] dist_world_size=32
g0043: [2024-08-12 04:00:17,417] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0063: [2024-08-12 04:00:17,420] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0063: [2024-08-12 04:00:17,421] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0038': [0, 1, 2, 3], 'g0041': [0, 1, 2, 3], 'g0042': [0, 1, 2, 3], 'g0043': [0, 1, 2, 3], 'g0054': [0, 1, 2, 3], 'g0056': [0, 1, 2, 3], 'g0063': [0, 1, 2, 3], 'g0066': [0, 1, 2, 3]}
g0063: [2024-08-12 04:00:17,421] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0063: [2024-08-12 04:00:17,421] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0038': [0, 1, 2, 3], 'g0041': [4, 5, 6, 7], 'g0042': [8, 9, 10, 11], 'g0043': [12, 13, 14, 15], 'g0054': [16, 17, 18, 19], 'g0056': [20, 21, 22, 23], 'g0063': [24, 25, 26, 27], 'g0066': [28, 29, 30, 31]})
g0063: [2024-08-12 04:00:17,421] [INFO] [launch.py:163:main] dist_world_size=32
g0063: [2024-08-12 04:00:17,421] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0054: [2024-08-12 04:00:20,443] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0038: [2024-08-12 04:00:20,497] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0041: [2024-08-12 04:00:20,505] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0041: [2024-08-12 04:00:20,505] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0038: [2024-08-12 04:00:20,512] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0042: [2024-08-12 04:00:20,514] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0042: [2024-08-12 04:00:20,514] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0066: [2024-08-12 04:00:20,525] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0066: [2024-08-12 04:00:20,525] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0063: [2024-08-12 04:00:20,539] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0063: [2024-08-12 04:00:20,539] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0056: [2024-08-12 04:00:20,542] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0043: [2024-08-12 04:00:20,546] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0063: [2024-08-12 04:00:20,548] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0041: [2024-08-12 04:00:20,552] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0038: [2024-08-12 04:00:20,557] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0056: [2024-08-12 04:00:20,563] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0043: [2024-08-12 04:00:20,574] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0054: [2024-08-12 04:00:20,578] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0054: [2024-08-12 04:00:20,579] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0056: [2024-08-12 04:00:20,583] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0042: [2024-08-12 04:00:20,591] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0054: [2024-08-12 04:00:20,604] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0041: [2024-08-12 04:00:20,625] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0043: [2024-08-12 04:00:20,638] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0043: [2024-08-12 04:00:20,639] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0038: [2024-08-12 04:00:20,647] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0066: [2024-08-12 04:00:20,655] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0066: [2024-08-12 04:00:20,655] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0042: [2024-08-12 04:00:20,673] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0063: [2024-08-12 04:00:20,740] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0056: [2024-08-12 04:00:20,798] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0041: --------------------------------------------------
g0041: DeepSpeed C++/CUDA extension op report
g0041: --------------------------------------------------
g0041: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0041:       runtime if needed. Op compatibility means that your system
g0041:       meet the required dependencies to JIT install the op.
g0041: --------------------------------------------------
g0041: JIT compiled ops requires ninja
g0066: --------------------------------------------------
g0066: DeepSpeed C++/CUDA extension op report
g0066: --------------------------------------------------
g0066: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0066:       runtime if needed. Op compatibility means that your system
g0066:       meet the required dependencies to JIT install the op.
g0066: --------------------------------------------------
g0066: JIT compiled ops requires ninja
g0041: ninja .................. [92m[OKAY][0m
g0041: --------------------------------------------------
g0041: op name ................ installed .. compatible
g0041: --------------------------------------------------
g0066: ninja .................. [92m[OKAY][0m
g0066: --------------------------------------------------
g0066: op name ................ installed .. compatible
g0066: --------------------------------------------------
g0041: --------------------------------------------------
g0041: DeepSpeed C++/CUDA extension op report
g0041: --------------------------------------------------
g0041: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0041:       runtime if needed. Op compatibility means that your system
g0041:       meet the required dependencies to JIT install the op.
g0041: --------------------------------------------------
g0041: JIT compiled ops requires ninja
g0041: ninja .................. [92m[OKAY][0m
g0041: --------------------------------------------------
g0041: op name ................ installed .. compatible
g0041: --------------------------------------------------
g0066: --------------------------------------------------
g0066: DeepSpeed C++/CUDA extension op report
g0066: --------------------------------------------------
g0066: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0066:       runtime if needed. Op compatibility means that your system
g0066:       meet the required dependencies to JIT install the op.
g0066: --------------------------------------------------
g0066: JIT compiled ops requires ninja
g0066: ninja .................. [92m[OKAY][0m
g0066: --------------------------------------------------
g0066: op name ................ installed .. compatible
g0066: --------------------------------------------------
g0054: --------------------------------------------------
g0054: DeepSpeed C++/CUDA extension op report
g0054: --------------------------------------------------
g0054: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0054:       runtime if needed. Op compatibility means that your system
g0054:       meet the required dependencies to JIT install the op.
g0054: --------------------------------------------------
g0054: JIT compiled ops requires ninja
g0054: ninja .................. [92m[OKAY][0m
g0054: --------------------------------------------------
g0054: op name ................ installed .. compatible
g0054: --------------------------------------------------
g0038: --------------------------------------------------
g0038: DeepSpeed C++/CUDA extension op report
g0038: --------------------------------------------------
g0038: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0038:       runtime if needed. Op compatibility means that your system
g0038:       meet the required dependencies to JIT install the op.
g0038: --------------------------------------------------
g0038: JIT compiled ops requires ninja
g0038: --------------------------------------------------
g0038: DeepSpeed C++/CUDA extension op report
g0038: --------------------------------------------------
g0038: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0038:       runtime if needed. Op compatibility means that your system
g0038:       meet the required dependencies to JIT install the op.
g0038: --------------------------------------------------
g0038: JIT compiled ops requires ninja
g0038: ninja .................. [92m[OKAY][0m
g0038: --------------------------------------------------
g0038: op name ................ installed .. compatible
g0038: --------------------------------------------------
g0038: ninja .................. [92m[OKAY][0m
g0038: --------------------------------------------------
g0038: op name ................ installed .. compatible
g0038: --------------------------------------------------
g0056: --------------------------------------------------
g0056: DeepSpeed C++/CUDA extension op report
g0056: --------------------------------------------------
g0056: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0056:       runtime if needed. Op compatibility means that your system
g0056:       meet the required dependencies to JIT install the op.
g0056: --------------------------------------------------
g0056: JIT compiled ops requires ninja
g0056: ninja .................. [92m[OKAY][0m
g0056: --------------------------------------------------
g0056: op name ................ installed .. compatible
g0056: --------------------------------------------------
g0041: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0041: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0041: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0066: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0066: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0041: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0041: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0041: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0066: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0066: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0041: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0041: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0041: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0066: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0066: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0066: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0041: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0066: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0041: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0041: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0041: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0041: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0041: --------------------------------------------------
g0066: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0066: --------------------------------------------------
g0041: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0041: DeepSpeed general environment info:
g0041: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0041: torch version .................... 2.0.1+cu118
g0041: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0041: deepspeed info ................... 0.12.4, unknown, unknown
g0041: torch cuda version ............... 11.8
g0041: torch hip version ................ None
g0041: nvcc version ..................... 11.8
g0041: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0041: shared memory (/dev/shm) size .... 188.13 GB
g0066: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0066: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0066: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0066: DeepSpeed general environment info:
g0066: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0066: torch version .................... 2.0.1+cu118
g0066: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0066: deepspeed info ................... 0.12.4, unknown, unknown
g0066: torch cuda version ............... 11.8
g0066: torch hip version ................ None
g0066: nvcc version ..................... 11.8
g0066: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0066: shared memory (/dev/shm) size .... 188.13 GB
g0041: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0041: --------------------------------------------------
g0054: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0054: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0054: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0041: DeepSpeed general environment info:
g0041: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0041: torch version .................... 2.0.1+cu118
g0041: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0041: deepspeed info ................... 0.12.4, unknown, unknown
g0041: torch cuda version ............... 11.8
g0041: torch hip version ................ None
g0041: nvcc version ..................... 11.8
g0041: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0041: shared memory (/dev/shm) size .... 188.13 GB
g0038: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0038: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0038: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0038: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0038: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0066: --------------------------------------------------
g0054: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0038: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0038: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0054: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: DeepSpeed general environment info:
g0066: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0066: torch version .................... 2.0.1+cu118
g0066: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0066: deepspeed info ................... 0.12.4, unknown, unknown
g0066: torch cuda version ............... 11.8
g0066: torch hip version ................ None
g0066: nvcc version ..................... 11.8
g0066: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0066: shared memory (/dev/shm) size .... 188.13 GB
g0054: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: --------------------------------------------------
g0056: DeepSpeed C++/CUDA extension op report
g0056: --------------------------------------------------
g0056: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0056:       runtime if needed. Op compatibility means that your system
g0056:       meet the required dependencies to JIT install the op.
g0056: --------------------------------------------------
g0056: JIT compiled ops requires ninja
g0042: --------------------------------------------------
g0042: DeepSpeed C++/CUDA extension op report
g0042: --------------------------------------------------
g0042: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0042:       runtime if needed. Op compatibility means that your system
g0042:       meet the required dependencies to JIT install the op.
g0042: --------------------------------------------------
g0042: JIT compiled ops requires ninja
g0056: ninja .................. [92m[OKAY][0m
g0056: --------------------------------------------------
g0056: op name ................ installed .. compatible
g0056: --------------------------------------------------
g0054: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0054: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0054: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0038: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: ninja .................. [92m[OKAY][0m
g0042: --------------------------------------------------
g0042: op name ................ installed .. compatible
g0042: --------------------------------------------------
g0054: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0038: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0038: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0038: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0038: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0041: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0038: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0038: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0066: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0054: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0054: --------------------------------------------------
g0038: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0041: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0038: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0056: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0056: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0056: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: DeepSpeed general environment info:
g0054: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0054: torch version .................... 2.0.1+cu118
g0054: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0054: deepspeed info ................... 0.12.4, unknown, unknown
g0054: torch cuda version ............... 11.8
g0054: torch hip version ................ None
g0054: nvcc version ..................... 11.8
g0054: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0054: shared memory (/dev/shm) size .... 188.13 GB
g0038: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0038: --------------------------------------------------
g0038: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0038: --------------------------------------------------
g0056: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0038: DeepSpeed general environment info:
g0038: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0038: torch version .................... 2.0.1+cu118
g0038: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0038: deepspeed info ................... 0.12.4, unknown, unknown
g0038: torch cuda version ............... 11.8
g0038: torch hip version ................ None
g0038: nvcc version ..................... 11.8
g0038: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0038: shared memory (/dev/shm) size .... 188.13 GB
g0056: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: DeepSpeed general environment info:
g0038: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0038: torch version .................... 2.0.1+cu118
g0038: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0038: deepspeed info ................... 0.12.4, unknown, unknown
g0038: torch cuda version ............... 11.8
g0038: torch hip version ................ None
g0038: nvcc version ..................... 11.8
g0038: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0038: shared memory (/dev/shm) size .... 188.13 GB
g0066: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0056: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0056: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0056: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0056: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0056: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0056: --------------------------------------------------
g0038: --------------------------------------------------
g0038: DeepSpeed C++/CUDA extension op report
g0038: --------------------------------------------------
g0038: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0038:       runtime if needed. Op compatibility means that your system
g0038:       meet the required dependencies to JIT install the op.
g0038: --------------------------------------------------
g0038: JIT compiled ops requires ninja
g0063: --------------------------------------------------
g0063: DeepSpeed C++/CUDA extension op report
g0063: --------------------------------------------------
g0063: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0063:       runtime if needed. Op compatibility means that your system
g0063:       meet the required dependencies to JIT install the op.
g0063: --------------------------------------------------
g0063: JIT compiled ops requires ninja
g0038: ninja .................. [92m[OKAY][0m
g0038: --------------------------------------------------
g0038: op name ................ installed .. compatible
g0038: --------------------------------------------------
g0063: ninja .................. [92m[OKAY][0m
g0063: --------------------------------------------------
g0063: op name ................ installed .. compatible
g0063: --------------------------------------------------
g0054: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0056: DeepSpeed general environment info:
g0056: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0056: torch version .................... 2.0.1+cu118
g0056: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0056: deepspeed info ................... 0.12.4, unknown, unknown
g0056: torch cuda version ............... 11.8
g0056: torch hip version ................ None
g0056: nvcc version ..................... 11.8
g0056: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0056: shared memory (/dev/shm) size .... 188.13 GB
g0038: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0038: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0042: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0042: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0042: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0056: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0056: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0056: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0042: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0042: --------------------------------------------------
g0042: DeepSpeed C++/CUDA extension op report
g0042: --------------------------------------------------
g0042: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0042:       runtime if needed. Op compatibility means that your system
g0042:       meet the required dependencies to JIT install the op.
g0042: --------------------------------------------------
g0042: JIT compiled ops requires ninja
g0056: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: ninja .................. [92m[OKAY][0m
g0042: --------------------------------------------------
g0042: op name ................ installed .. compatible
g0042: --------------------------------------------------
g0042: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0042: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0042: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0056: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0056: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0056: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0042: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0038: --------------------------------------------------
g0038: DeepSpeed C++/CUDA extension op report
g0038: --------------------------------------------------
g0038: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0038:       runtime if needed. Op compatibility means that your system
g0038:       meet the required dependencies to JIT install the op.
g0038: --------------------------------------------------
g0038: JIT compiled ops requires ninja
g0038: ninja .................. [92m[OKAY][0m
g0038: --------------------------------------------------
g0038: op name ................ installed .. compatible
g0038: --------------------------------------------------
g0056: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0054: --------------------------------------------------
g0054: DeepSpeed C++/CUDA extension op report
g0054: --------------------------------------------------
g0054: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0054:       runtime if needed. Op compatibility means that your system
g0042: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0054:       meet the required dependencies to JIT install the op.
g0042: --------------------------------------------------
g0054: --------------------------------------------------
g0054: JIT compiled ops requires ninja
g0054: ninja .................. [92m[OKAY][0m
g0054: --------------------------------------------------
g0054: op name ................ installed .. compatible
g0054: --------------------------------------------------
g0041: --------------------------------------------------
g0041: DeepSpeed C++/CUDA extension op report
g0041: --------------------------------------------------
g0041: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0041:       runtime if needed. Op compatibility means that your system
g0041:       meet the required dependencies to JIT install the op.
g0041: --------------------------------------------------
g0041: JIT compiled ops requires ninja
g0042: DeepSpeed general environment info:
g0042: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0042: torch version .................... 2.0.1+cu118
g0042: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0042: deepspeed info ................... 0.12.4, unknown, unknown
g0042: torch cuda version ............... 11.8
g0042: torch hip version ................ None
g0042: nvcc version ..................... 11.8
g0042: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0042: shared memory (/dev/shm) size .... 188.13 GB
g0056: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0056: --------------------------------------------------
g0041: ninja .................. [92m[OKAY][0m
g0041: --------------------------------------------------
g0041: op name ................ installed .. compatible
g0041: --------------------------------------------------
g0056: DeepSpeed general environment info:
g0056: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0056: torch version .................... 2.0.1+cu118
g0056: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0056: deepspeed info ................... 0.12.4, unknown, unknown
g0056: torch cuda version ............... 11.8
g0056: torch hip version ................ None
g0056: nvcc version ..................... 11.8
g0056: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0056: shared memory (/dev/shm) size .... 188.13 GB
g0038: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0038: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0038: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0063: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0063: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0063: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0063: --------------------------------------------------
g0063: DeepSpeed C++/CUDA extension op report
g0063: --------------------------------------------------
g0063: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0063:       runtime if needed. Op compatibility means that your system
g0063:       meet the required dependencies to JIT install the op.
g0063: --------------------------------------------------
g0063: JIT compiled ops requires ninja
g0038: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: ninja .................. [92m[OKAY][0m
g0063: --------------------------------------------------
g0063: op name ................ installed .. compatible
g0063: --------------------------------------------------
g0063: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0063: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0038: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0038: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0056: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0063: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0063: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0063: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0038: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0063: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0063: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0038: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0038: --------------------------------------------------
g0042: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0042: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0042: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0063: --------------------------------------------------
g0038: DeepSpeed general environment info:
g0038: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0038: torch version .................... 2.0.1+cu118
g0038: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0038: deepspeed info ................... 0.12.4, unknown, unknown
g0038: torch cuda version ............... 11.8
g0038: torch hip version ................ None
g0038: nvcc version ..................... 11.8
g0038: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0038: shared memory (/dev/shm) size .... 188.13 GB
g0041: [2024-08-12 04:00:24,096] [INFO] [comm.py:637:init_distributed] cdb=None
g0041: [2024-08-12 04:00:24,096] [INFO] [comm.py:637:init_distributed] cdb=None
g0042: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: [2024-08-12 04:00:24,096] [INFO] [comm.py:637:init_distributed] cdb=None
g0066: [2024-08-12 04:00:24,096] [INFO] [comm.py:637:init_distributed] cdb=None
g0063: DeepSpeed general environment info:
g0063: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0063: torch version .................... 2.0.1+cu118
g0063: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0063: deepspeed info ................... 0.12.4, unknown, unknown
g0063: torch cuda version ............... 11.8
g0063: torch hip version ................ None
g0063: nvcc version ..................... 11.8
g0063: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0063: shared memory (/dev/shm) size .... 188.13 GB
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0038: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0038: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0054: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0054: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0042: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0042: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0038: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0054: [2024-08-12 04:00:24,109] [INFO] [comm.py:637:init_distributed] cdb=None
g0054: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0041: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0041: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0041: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0038: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0042: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0042: --------------------------------------------------
g0038: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: [2024-08-12 04:00:24,116] [INFO] [comm.py:637:init_distributed] cdb=None
g0038: [2024-08-12 04:00:24,116] [INFO] [comm.py:637:init_distributed] cdb=None
g0054: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0041: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: DeepSpeed general environment info:
g0042: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0042: torch version .................... 2.0.1+cu118
g0042: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0042: deepspeed info ................... 0.12.4, unknown, unknown
g0042: torch cuda version ............... 11.8
g0042: torch hip version ................ None
g0042: nvcc version ..................... 11.8
g0042: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0042: shared memory (/dev/shm) size .... 188.13 GB
g0038: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0038: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0038: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0038: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0054: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0054: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0054: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0041: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0063: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0054: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0041: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0041: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0041: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0038: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0038: --------------------------------------------------
g0054: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0054: --------------------------------------------------
g0041: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0038: DeepSpeed general environment info:
g0038: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0038: torch version .................... 2.0.1+cu118
g0038: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0038: deepspeed info ................... 0.12.4, unknown, unknown
g0038: torch cuda version ............... 11.8
g0038: torch hip version ................ None
g0038: nvcc version ..................... 11.8
g0038: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0038: shared memory (/dev/shm) size .... 188.13 GB
g0054: DeepSpeed general environment info:
g0054: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0054: torch version .................... 2.0.1+cu118
g0054: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0054: deepspeed info ................... 0.12.4, unknown, unknown
g0054: torch cuda version ............... 11.8
g0054: torch hip version ................ None
g0054: nvcc version ..................... 11.8
g0054: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0054: shared memory (/dev/shm) size .... 188.13 GB
g0041: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0041: --------------------------------------------------
g0063: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0063: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0063: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: --------------------------------------------------
g0066: DeepSpeed C++/CUDA extension op report
g0066: --------------------------------------------------
g0066: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0066:       runtime if needed. Op compatibility means that your system
g0066:       meet the required dependencies to JIT install the op.
g0066: --------------------------------------------------
g0066: JIT compiled ops requires ninja
g0066: --------------------------------------------------
g0066: DeepSpeed C++/CUDA extension op report
g0066: --------------------------------------------------
g0066: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0066:       runtime if needed. Op compatibility means that your system
g0066:       meet the required dependencies to JIT install the op.
g0066: --------------------------------------------------
g0066: JIT compiled ops requires ninja
g0066: ninja .................. [92m[OKAY][0m
g0066: --------------------------------------------------
g0066: op name ................ installed .. compatible
g0066: --------------------------------------------------
g0066: ninja .................. [92m[OKAY][0m
g0066: --------------------------------------------------
g0066: op name ................ installed .. compatible
g0066: --------------------------------------------------
g0041: DeepSpeed general environment info:
g0041: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0041: torch version .................... 2.0.1+cu118
g0041: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0041: deepspeed info ................... 0.12.4, unknown, unknown
g0041: torch cuda version ............... 11.8
g0041: torch hip version ................ None
g0041: nvcc version ..................... 11.8
g0041: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0041: shared memory (/dev/shm) size .... 188.13 GB
g0063: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0056: [2024-08-12 04:00:24,138] [INFO] [comm.py:637:init_distributed] cdb=None
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0063: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: --------------------------------------------------
g0063: DeepSpeed C++/CUDA extension op report
g0063: --------------------------------------------------
g0063: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0063:       runtime if needed. Op compatibility means that your system
g0063:       meet the required dependencies to JIT install the op.
g0063: --------------------------------------------------
g0063: JIT compiled ops requires ninja
g0063: --------------------------------------------------
g0063: DeepSpeed C++/CUDA extension op report
g0063: --------------------------------------------------
g0063: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0063:       runtime if needed. Op compatibility means that your system
g0063:       meet the required dependencies to JIT install the op.
g0063: --------------------------------------------------
g0063: JIT compiled ops requires ninja
g0042: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0063: ninja .................. [92m[OKAY][0m
g0063: --------------------------------------------------
g0063: op name ................ installed .. compatible
g0063: --------------------------------------------------
g0063: ninja .................. [92m[OKAY][0m
g0063: --------------------------------------------------
g0063: op name ................ installed .. compatible
g0063: --------------------------------------------------
g0063: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: --------------------------------------------------
g0056: DeepSpeed C++/CUDA extension op report
g0056: --------------------------------------------------
g0056: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0056:       runtime if needed. Op compatibility means that your system
g0056:       meet the required dependencies to JIT install the op.
g0056: --------------------------------------------------
g0056: JIT compiled ops requires ninja
g0056: ninja .................. [92m[OKAY][0m
g0056: --------------------------------------------------
g0056: op name ................ installed .. compatible
g0056: --------------------------------------------------
g0063: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0063: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0063: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0038: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0054: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0038: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0038: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0038: using torch.float32 for parameters ...
g0038: ------------------------ arguments ------------------------
g0038:   accumulate_allreduce_grads_in_fp32 .............. False
g0038:   adam_beta1 ...................................... 0.9
g0038:   adam_beta2 ...................................... 0.95
g0038:   adam_eps ........................................ 1e-08
g0038:   add_bias_linear ................................. False
g0038:   add_position_embedding .......................... False
g0038:   adlr_autoresume ................................. False
g0038:   adlr_autoresume_interval ........................ 1000
g0038:   aml_data_download_path .......................... None
g0038:   apply_layernorm_1p .............................. False
g0038:   apply_query_key_layer_scaling ................... False
g0038:   apply_residual_connection_post_layernorm ........ False
g0038:   async_tensor_model_parallel_allreduce ........... False
g0038:   attention_dropout ............................... 0.0
g0038:   attention_softmax_in_fp32 ....................... False
g0038:   barrier_with_L1_time ............................ True
g0038:   bert_binary_head ................................ True
g0038:   bert_embedder_type .............................. megatron
g0038:   bert_load ....................................... None
g0038:   bf16 ............................................ False
g0038:   bias_dropout_fusion ............................. True
g0038:   bias_gelu_fusion ................................ False
g0038:   biencoder_projection_dim ........................ 0
g0038:   biencoder_shared_query_context_model ............ False
g0038:   block_data_path ................................. None
g0038:   checkpoint_activations .......................... False
g0038:   checkpoint_in_cpu ............................... False
g0038:   checkpoint_num_layers ........................... 1
g0038:   classes_fraction ................................ 1.0
g0038:   clip_grad ....................................... 1.0
g0038:   compression_training ............................ False
g0038:   consumed_train_samples .......................... 0
g0038:   consumed_train_tokens ........................... 0
g0038:   consumed_valid_samples .......................... 0
g0038:   contigious_checkpointing ........................ False
g0038:   cpu_optimizer ................................... False
g0038:   cpu_torch_adam .................................. False
g0038:   create_moe_param_group .......................... False
g0038:   curriculum_learning_legacy ...................... False
g0038:   data_cache_path ................................. None
g0038:   data_efficiency_curriculum_learning ............. False
g0038:   data_impl ....................................... mmap
g0038:   data_parallel_random_init ....................... False
g0038:   data_parallel_size .............................. 4
g0038:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document']
g0038:   data_per_class_fraction ......................... 1.0
g0038:   data_sharding ................................... True
g0038:   dataloader_type ................................. single
g0038:   DDP_impl ........................................ local
g0038:   decoder_num_layers .............................. None
g0038:   decoder_seq_length .............................. None
g0038:   deepscale ....................................... False
g0038:   deepscale_config ................................ None
g0038:   deepspeed ....................................... True
g0038:   deepspeed_activation_checkpointing .............. False
g0038:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0038:   deepspeed_mpi ................................... False
g0038:   dino_bottleneck_size ............................ 256
g0038:   dino_freeze_last_layer .......................... 1
g0038:   dino_head_hidden_size ........................... 2048
g0038:   dino_local_crops_number ......................... 10
g0038:   dino_local_img_size ............................. 96
g0038:   dino_norm_last_layer ............................ False
g0038:   dino_teacher_temp ............................... 0.07
g0038:   dino_warmup_teacher_temp ........................ 0.04
g0038:   dino_warmup_teacher_temp_epochs ................. 30
g0038:   distribute_checkpointed_activations ............. False
g0038:   distribute_saved_activations .................... False
g0038:   distributed_backend ............................. nccl
g0038:   distributed_timeout_minutes ..................... 10
g0038:   ds_fused_adam ................................... False
g0038:   ds_inference .................................... False
g0038:   ds_pipeline_enabled ............................. True
g0038:   ds_sequence_parallel_size ....................... 1
g0038:   embedding_path .................................. None
g0038:   embedding_weights_in_fp32 ....................... False
g0038:   empty_unused_memory_level ....................... 0
g0038:   enable_expert_tensor_parallelism ................ False
g0038:   encoder_num_layers .............................. 22
g0038:   encoder_seq_length .............................. 2048
g0038:   end_weight_decay ................................ 0.1
g0038:   eod_mask_loss ................................... False
g0038:   eval_interval ................................... 1000
g0038:   eval_iters ...................................... 100
g0038:   evidence_data_path .............................. None
g0038:   exit_duration_in_mins ........................... 30000000
g0038:   exit_interval ................................... None
g0038:   exit_on_missing_checkpoint ...................... False
g0038:   exit_signal_handler ............................. False
g0038:   expert_interval ................................. 2
g0038:   ffn_hidden_size ................................. 5632
g0038:   finetune ........................................ False
g0038:   force_ds_sequence_parallel ...................... False
g0038:   fp16 ............................................ False
g0038:   fp16_lm_cross_entropy ........................... False
g0038:   fp32_residual_connection ........................ False
g0038:   fp8_amax_compute_algo ........................... most_recent
g0038:   fp8_amax_history_len ............................ 1
g0038:   fp8_e4m3 ........................................ False
g0038:   fp8_hybrid ...................................... False
g0038:   fp8_interval .................................... 1
g0038:   fp8_margin ...................................... 0
g0038:   fp8_wgrad ....................................... True
g0038:   global_batch_size ............................... 128
g0038:   gradient_accumulation_fusion .................... True
g0038:   head_lr_mult .................................... 1.0
g0038:   hidden_dropout .................................. 0.0
g0038:   hidden_size ..................................... 2048
g0038:   hidden_size_teacher ............................. None
g0038:   hysteresis ...................................... 2
g0038:   ict_head_size ................................... None
g0038:   ict_load ........................................ None
g0038:   img_h ........................................... 224
g0038:   img_w ........................................... 224
g0038:   indexer_batch_size .............................. 128
g0038:   indexer_log_interval ............................ 1000
g0038:   inference ....................................... False
g0038:   inference_batch_times_seqlen_threshold .......... 512
g0038:   init_method_std ................................. 0.013
g0038:   init_method_xavier_uniform ...................... False
g0038:   initial_loss_scale .............................. 4294967296
g0038:   iter_per_epoch .................................. 1250
g0038:   kd .............................................. False
g0038:   kd_alpha_ce ..................................... 1
g0038:   kd_beta_ce ...................................... 1
g0038:   kd_temp ......................................... 1.0
g0038:   kv_channels ..................................... 128
g0038:   layernorm_epsilon ............................... 1e-05
g0038:   lazy_mpu_init ................................... None
g0038:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0038:   load_teacher .................................... None
g0038:   local_rank ...................................... 0
g0038:   log_batch_size_to_tensorboard ................... True
g0038:   log_interval .................................... 10
g0038:   log_learning_rate_to_tensorboard ................ True
g0038:   log_loss_scale_to_tensorboard ................... True
g0038:   log_memory_to_tensorboard ....................... False
g0038:   log_num_zeros_in_grad ........................... False
g0038:   log_optimizer_states_to_tensorboard ............. True
g0038:   log_params_norm ................................. False
g0038:   log_timers_to_tensorboard ....................... True
g0038:   log_validation_ppl_to_tensorboard ............... True
g0038:   log_world_size_to_tensorboard ................... False
g0038:   loss_scale ...................................... None
g0038:   loss_scale_window ............................... 1000
g0038:   lr .............................................. 0.0002
g0038:   lr_decay_iters .................................. None
g0038:   lr_decay_samples ................................ None
g0038:   lr_decay_style .................................. cosine
g0038:   lr_decay_tokens ................................. 300000000000
g0038:   lr_warmup_fraction .............................. None
g0038:   lr_warmup_iters ................................. 0
g0038:   lr_warmup_samples ............................... 0
g0038:   lr_warmup_tokens ................................ 3000000000
g0038:   make_vocab_size_divisible_by .................... 128
g0038:   mask_factor ..................................... 1.0
g0038:   mask_prob ....................................... 0.15
g0038:   mask_type ....................................... random
g0038:   masked_softmax_fusion ........................... True
g0038:   max_position_embeddings ......................... 2048
g0038:   max_tokens_to_oom ............................... 12000
g0038:   mem_efficient_ln ................................ True
g0038:   memory_centric_tiled_linear ..................... False
g0063: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0038:   merge_file ...................................... None
g0063: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0038:   micro_batch_size ................................ 1
g0038:   min_loss_scale .................................. 1.0
g0063: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0038:   min_lr .......................................... 1e-06
g0038:   mlp_type ........................................ standard
g0038:   mmap_warmup ..................................... False
g0038:   moe_eval_capacity_factor ........................ 1.0
g0038:   moe_expert_parallel_size ........................ 1
g0038:   moe_loss_coeff .................................. 0.1
g0038:   moe_min_capacity ................................ 4
g0038:   moe_token_dropping .............................. True
g0038:   moe_train_capacity_factor ....................... 1.0
g0038:   mos ............................................. False
g0038:   no_load_lr_state ................................ False
g0038:   no_load_optim ................................... None
g0038:   no_load_rng ..................................... None
g0038:   no_persist_layer_norm ........................... False
g0038:   no_pipeline_parallel ............................ False
g0038:   no_save_optim ................................... None
g0038:   no_save_rng ..................................... None
g0038:   normalization ................................... rmsnorm
g0038:   num_attention_heads ............................. 16
g0038:   num_attention_heads_teacher ..................... None
g0038:   num_channels .................................... 3
g0038:   num_classes ..................................... 1000
g0038:   num_experts ..................................... [1]
g0038:   num_experts_switch .............................. None
g0038:   num_experts_teacher ............................. [1]
g0038:   num_key_value_heads ............................. 4
g0038:   num_layers ...................................... 22
g0038:   num_layers_per_virtual_pipeline_stage ........... None
g0038:   num_layers_teacher .............................. None
g0038:   num_workers ..................................... 0
g0038:   onnx_safe ....................................... None
g0038:   openai_gelu ..................................... False
g0038:   optimizer ....................................... adam
g0038:   output_bert_embeddings .......................... False
g0038:   overlap_p2p_comm ................................ False
g0038:   override_opt_param_scheduler .................... True
g0038:   params_dtype .................................... torch.float32
g0038:   partition_activations ........................... False
g0038:   patch_dim ....................................... 16
g0038:   perform_initialization .......................... True
g0038:   pipeline_model_parallel_size .................... 8
g0038:   pipeline_model_parallel_split_rank .............. None
g0038:   profile_backward ................................ False
g0038:   query_in_block_prob ............................. 0.1
g0038:   rampup_batch_size ............................... None
g0038:   random_ltd ...................................... False
g0038:   rank ............................................ 0
g0038:   recompute_granularity ........................... None
g0038:   recompute_method ................................ None
g0038:   recompute_num_layers ............................ 1
g0038:   remote_device ................................... none
g0038:   repeated_dataloader ............................. False
g0038:   reset_attention_mask ............................ False
g0038:   reset_iteration ................................. False
g0038:   reset_position_ids .............................. False
g0038:   retriever_report_topk_accuracies ................ []
g0038:   retriever_score_scaling ......................... False
g0038:   retriever_seq_length ............................ 256
g0038:   retro_add_retriever ............................. False
g0038:   retro_cyclic_train_iters ........................ None
g0038:   retro_encoder_attention_dropout ................. 0.1
g0038:   retro_encoder_hidden_dropout .................... 0.1
g0038:   retro_encoder_layers ............................ 2
g0038:   retro_num_neighbors ............................. 2
g0038:   retro_num_retrieved_chunks ...................... 2
g0038:   retro_return_doc_ids ............................ False
g0038:   retro_workdir ................................... None
g0038:   return_data_index ............................... False
g0038:   rotary_percent .................................. 1.0
g0038:   sample_rate ..................................... 1.0
g0038:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0038:   save_interval ................................... 1000
g0038:   scatter_gather_tensors_in_pipeline .............. True
g0038:   scattered_embeddings ............................ False
g0038:   seed ............................................ 1234
g0038:   seq_length ...................................... 2048
g0038:   sequence_parallel ............................... False
g0038:   sgd_momentum .................................... 0.9
g0038:   short_seq_prob .................................. 0.1
g0038:   skip_train ...................................... False
g0038:   split ........................................... 949,50,1
g0038:   split_transformers .............................. False
g0038:   squared_relu .................................... False
g0038:   standalone_embedding_stage ...................... False
g0038:   start_weight_decay .............................. 0.1
g0038:   swiglu .......................................... True
g0038:   swin_backbone_type .............................. tiny
g0038:   synchronize_each_layer .......................... False
g0038:   tensor_model_parallel_size ...................... 1
g0038:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True
g0038:   tensorboard_log_interval ........................ 1
g0038:   tensorboard_queue_size .......................... 1
g0038:   test_data_path .................................. None
g0038:   tf32 ............................................ False
g0038:   tile_factor ..................................... 1
g0038:   timing_log_level ................................ 0
g0038:   timing_log_option ............................... minmax
g0038:   titles_data_path ................................ None
g0038:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
g0038:   tokenizer_type .................................. SentencePieceTokenizer
g0038:   topk ............................................ 1
g0038:   train_data_exact_num_epochs ..................... None
g0038:   train_data_path ................................. None
g0038:   train_desc_path ................................. None
g0038:   train_doc_idx_path .............................. None
g0038:   train_idx_path .................................. None
g0038:   train_iters ..................................... None
g0038:   train_sample_idx_path ........................... None
g0038:   train_samples ................................... 6656000
g0038:   train_shuffle_idx_path .......................... None
g0038:   train_tokens .................................... 13631488000
g0038:   transformer_impl ................................ local
g0038:   transformer_pipeline_model_parallel_size ........ 8
g0038:   universal_checkpoint ............................ False
g0038:   untie_embeddings_and_output_weights ............. True
g0038:   use_checkpoint_args ............................. False
g0038:   use_checkpoint_opt_param_scheduler .............. False
g0038:   use_contiguous_buffers_in_local_ddp ............. True
g0038:   use_cpu_initialization .......................... None
g0038:   use_dataset_only ................................ False
g0038:   use_distributed_optimizer ....................... False
g0038:   use_flash_attn .................................. False
g0038:   use_flash_attn_triton ........................... False
g0038:   use_flash_attn_v1 ............................... False
g0038:   use_flash_attn_v2 ............................... False
g0038:   use_one_sent_docs ............................... False
g0038:   use_pin_memory .................................. False
g0038:   use_ring_exchange_p2p ........................... False
g0038:   use_rotary_position_embeddings .................. True
g0038:   use_tutel ....................................... False
g0038:   use_wandb ....................................... True
g0038:   valid_data_path ................................. None
g0038:   variable_seq_lengths ............................ False
g0038:   virtual_pipeline_model_parallel_size ............ None
g0038:   vision_backbone_type ............................ vit
g0038:   vision_pretraining .............................. False
g0038:   vision_pretraining_type ......................... classify
g0038:   vocab_extra_ids ................................. 0
g0038:   vocab_file ...................................... None
g0038:   vocab_size ...................................... None
g0038:   wandb_entity .................................... yohei-kobashi
g0038:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True
g0038:   wandb_project ................................... encrypted_data_LLM
g0038:   wandb_tag ....................................... other_gpu
g0038:   weight_decay .................................... 0.1
g0038:   weight_decay_incr_style ......................... constant
g0038:   world_size ...................................... 32
g0038:   zero_allgather_bucket_size ...................... 0.0
g0038:   zero_contigious_gradients ....................... False
g0038:   zero_reduce_bucket_size ......................... 0.0
g0038:   zero_reduce_scatter ............................. False
g0038:   zero_stage ...................................... 0
g0038: -------------------- end of arguments ---------------------
g0038: setting number of micro-batches to constant 32
g0038: > building SentencePieceTokenizer tokenizer ...
g0041: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0063: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0063: --------------------------------------------------
g0041: --------------------------------------------------
g0041: DeepSpeed C++/CUDA extension op report
g0041: --------------------------------------------------
g0041: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0041:       runtime if needed. Op compatibility means that your system
g0041:       meet the required dependencies to JIT install the op.
g0041: --------------------------------------------------
g0041: JIT compiled ops requires ninja
g0063: DeepSpeed general environment info:
g0063: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0063: torch version .................... 2.0.1+cu118
g0063: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0063: deepspeed info ................... 0.12.4, unknown, unknown
g0063: torch cuda version ............... 11.8
g0063: torch hip version ................ None
g0063: nvcc version ..................... 11.8
g0063: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0063: shared memory (/dev/shm) size .... 188.13 GB
g0041: ninja .................. [92m[OKAY][0m
g0041: --------------------------------------------------
g0041: op name ................ installed .. compatible
g0041: --------------------------------------------------
g0042: [2024-08-12 04:00:24,175] [INFO] [comm.py:637:init_distributed] cdb=None
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [2024-08-12 04:00:24,177] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: --------------------------------------------------
g0043: DeepSpeed C++/CUDA extension op report
g0043: --------------------------------------------------
g0043: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0043:       runtime if needed. Op compatibility means that your system
g0043:       meet the required dependencies to JIT install the op.
g0043: --------------------------------------------------
g0043: JIT compiled ops requires ninja
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: ninja .................. [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: op name ................ installed .. compatible
g0043: --------------------------------------------------
g0066: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: async_io ............... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0066:  ...... evoformer_attn[92m[OKAY][0m 
g0066: ......... [93m[NO][0m ....... fused_adam[93m[NO][0m 
g0066: ............. [92m[YES][0mfused_lamb  ...................  [92m[OKAY][0m[92m[YES][0m
g0066:  ...... [92m[OKAY][0mcpu_adam
g0066:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: fused_lion cpu_adagrad.............  ............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0066: [92m[OKAY][0m
g0066: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0066: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0066: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0066: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0056: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0056: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0063: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: cpu_lionasync_io ...............  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m......
g0063:  [92m[OKAY][0m
g0063: fused_adam[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0063: ............. [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0063:  ....... [93m[NO][0mcpu_adam
g0038: [2024-08-12 04:00:24,208] [INFO] [comm.py:637:init_distributed] cdb=None
g0063:  ............... [92m[YES][0mfused_lamb  ...................  [92m[OKAY][0m[92m[YES][0m
g0063:  ...... [92m[OKAY][0mcpu_adagrad
g0063:  ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_lioncpu_lion  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0063: 
g0063: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0063: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0063: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0066: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0066: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0066: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0066: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0042: --------------------------------------------------
g0042: DeepSpeed C++/CUDA extension op report
g0042: --------------------------------------------------
g0042: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0042:       runtime if needed. Op compatibility means that your system
g0042:       meet the required dependencies to JIT install the op.
g0042: --------------------------------------------------
g0042: JIT compiled ops requires ninja
g0056: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: ninja .................. [92m[OKAY][0m
g0042: --------------------------------------------------
g0042: op name ................ installed .. compatible
g0042: --------------------------------------------------
g0063: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0063: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0066: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0066: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0056: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0063: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: DeepSpeed C++/CUDA extension op report
g0043: --------------------------------------------------
g0043: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0043:       runtime if needed. Op compatibility means that your system
g0043:       meet the required dependencies to JIT install the op.
g0043: --------------------------------------------------
g0043: JIT compiled ops requires ninja
g0041: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0041: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0041: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0066: --------------------------------------------------
g0066: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0066: --------------------------------------------------
g0056: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0056: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0056: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0043: ninja .................. [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: op name ................ installed .. compatible
g0043: --------------------------------------------------
g0063: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0063: ragged_device_ops ...... [92m[YES][0m ...... [2024-08-12 04:00:24,222] [INFO] [comm.py:637:init_distributed] cdb=None
g0063: [92m[OKAY][0m
g0041: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: DeepSpeed general environment info:
g0066: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0066: torch version .................... 2.0.1+cu118
g0066: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0066: deepspeed info ................... 0.12.4, unknown, unknown
g0066: torch cuda version ............... 11.8
g0066: torch hip version ................ None
g0066: nvcc version ..................... 11.8
g0066: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0066: shared memory (/dev/shm) size .... 188.13 GB
g0066: DeepSpeed general environment info:
g0066: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0066: torch version .................... 2.0.1+cu118
g0066: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0066: deepspeed info ................... 0.12.4, unknown, unknown
g0066: torch cuda version ............... 11.8
g0066: torch hip version ................ None
g0066: nvcc version ..................... 11.8
g0066: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0066: shared memory (/dev/shm) size .... 188.13 GB
g0056: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0042: --------------------------------------------------
g0042: DeepSpeed C++/CUDA extension op report
g0042: --------------------------------------------------
g0042: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0042:       runtime if needed. Op compatibility means that your system
g0042:       meet the required dependencies to JIT install the op.
g0042: --------------------------------------------------
g0042: JIT compiled ops requires ninja
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: ninja .................. [92m[OKAY][0m
g0042: --------------------------------------------------
g0042: op name ................ installed .. compatible
g0042: --------------------------------------------------
g0041: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0063: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0063: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0063: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0063: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0063: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0056: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0056: --------------------------------------------------
g0041: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0063: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0063: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0056: DeepSpeed general environment info:
g0056: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0056: torch version .................... 2.0.1+cu118
g0056: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0056: deepspeed info ................... 0.12.4, unknown, unknown
g0056: torch cuda version ............... 11.8
g0056: torch hip version ................ None
g0056: nvcc version ..................... 11.8
g0056: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0056: shared memory (/dev/shm) size .... 188.13 GB
g0063: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0063: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0063: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: DeepSpeed C++/CUDA extension op report
g0043: --------------------------------------------------
g0043: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0043:       runtime if needed. Op compatibility means that your system
g0043:       meet the required dependencies to JIT install the op.
g0043: --------------------------------------------------
g0043: JIT compiled ops requires ninja
g0041: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0041: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0041: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0041: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0043: ninja .................. [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: op name ................ installed .. compatible
g0043: --------------------------------------------------
g0063: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0063: --------------------------------------------------
g0063: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0063: --------------------------------------------------
g0041: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0041: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0041: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0042: [2024-08-12 04:00:24,237] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0043: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0043: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: DeepSpeed general environment info:
g0063: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0063: torch version .................... 2.0.1+cu118
g0063: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0063: deepspeed info ................... 0.12.4, unknown, unknown
g0063: torch cuda version ............... 11.8
g0063: torch hip version ................ None
g0063: nvcc version ..................... 11.8
g0063: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0063: shared memory (/dev/shm) size .... 188.13 GB
g0063: DeepSpeed general environment info:
g0063: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0063: torch version .................... 2.0.1+cu118
g0063: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0063: deepspeed info ................... 0.12.4, unknown, unknown
g0063: torch cuda version ............... 11.8
g0063: torch hip version ................ None
g0063: nvcc version ..................... 11.8
g0063: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0063: shared memory (/dev/shm) size .... 188.13 GB
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0041: --------------------------------------------------
g0038:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0038: > initializing torch distributed ...
g0038: [2024-08-12 04:00:24,241] [INFO] [comm.py:637:init_distributed] cdb=None
g0038: [2024-08-12 04:00:24,241] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0043: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0038: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: DeepSpeed general environment info:
g0041: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0041: torch version .................... 2.0.1+cu118
g0041: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0041: deepspeed info ................... 0.12.4, unknown, unknown
g0041: torch cuda version ............... 11.8
g0041: torch hip version ................ None
g0041: nvcc version ..................... 11.8
g0041: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0041: shared memory (/dev/shm) size .... 188.13 GB
g0054: [2024-08-12 04:00:24,246] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0066: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0066: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0041: [2024-08-12 04:00:24,248] [INFO] [comm.py:637:init_distributed] cdb=None
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0043: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0043: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0056: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0043: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: --------------------------------------------------
g0043: DeepSpeed C++/CUDA extension op report
g0043: --------------------------------------------------
g0043: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0043:       runtime if needed. Op compatibility means that your system
g0043:       meet the required dependencies to JIT install the op.
g0043: --------------------------------------------------
g0043: JIT compiled ops requires ninja
g0063: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0063: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: ninja .................. [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: op name ................ installed .. compatible
g0043: --------------------------------------------------
g0042: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0042: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0042: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: DeepSpeed general environment info:
g0043: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0043: torch version .................... 2.0.1+cu118
g0043: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0043: deepspeed info ................... 0.12.4, unknown, unknown
g0043: torch cuda version ............... 11.8
g0043: torch hip version ................ None
g0043: nvcc version ..................... 11.8
g0043: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0043: shared memory (/dev/shm) size .... 188.13 GB
g0041: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0042: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0042: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0043: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0043: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0042: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0042: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0042: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0042: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0042: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0042: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0063: [2024-08-12 04:00:24,282] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: --------------------------------------------------
g0054: DeepSpeed C++/CUDA extension op report
g0054: --------------------------------------------------
g0054: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0054:       runtime if needed. Op compatibility means that your system
g0054:       meet the required dependencies to JIT install the op.
g0054: --------------------------------------------------
g0054: JIT compiled ops requires ninja
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0054: ninja .................. [92m[OKAY][0m
g0054: --------------------------------------------------
g0054: op name ................ installed .. compatible
g0054: --------------------------------------------------
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0043: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0043: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0042: --------------------------------------------------
g0043: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0043: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0043: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0042: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0042: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0042: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0042: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0042: DeepSpeed general environment info:
g0042: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0042: torch version .................... 2.0.1+cu118
g0042: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0042: deepspeed info ................... 0.12.4, unknown, unknown
g0042: torch cuda version ............... 11.8
g0042: torch hip version ................ None
g0042: nvcc version ..................... 11.8
g0042: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0042: shared memory (/dev/shm) size .... 188.13 GB
g0043: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0043: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0054: --------------------------------------------------
g0054: DeepSpeed C++/CUDA extension op report
g0054: --------------------------------------------------
g0054: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0054:       runtime if needed. Op compatibility means that your system
g0054:       meet the required dependencies to JIT install the op.
g0054: --------------------------------------------------
g0054: JIT compiled ops requires ninja
g0042: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0042: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0042: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0043: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: ninja .................. [92m[OKAY][0m
g0054: --------------------------------------------------
g0054: op name ................ installed .. compatible
g0054: --------------------------------------------------
g0043: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0043: --------------------------------------------------
g0042: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0042: --------------------------------------------------
g0043: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: DeepSpeed general environment info:
g0043: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0043: torch version .................... 2.0.1+cu118
g0043: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0043: deepspeed info ................... 0.12.4, unknown, unknown
g0043: torch cuda version ............... 11.8
g0043: torch hip version ................ None
g0043: nvcc version ..................... 11.8
g0043: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0043: shared memory (/dev/shm) size .... 188.13 GB
g0042: DeepSpeed general environment info:
g0042: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0042: torch version .................... 2.0.1+cu118
g0042: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0042: deepspeed info ................... 0.12.4, unknown, unknown
g0042: torch cuda version ............... 11.8
g0042: torch hip version ................ None
g0042: nvcc version ..................... 11.8
g0042: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0042: shared memory (/dev/shm) size .... 188.13 GB
g0043: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0043: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0043: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0043: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0043: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0043: --------------------------------------------------
g0042: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: DeepSpeed general environment info:
g0043: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0043: torch version .................... 2.0.1+cu118
g0043: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0043: deepspeed info ................... 0.12.4, unknown, unknown
g0043: torch cuda version ............... 11.8
g0043: torch hip version ................ None
g0043: nvcc version ..................... 11.8
g0043: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0043: shared memory (/dev/shm) size .... 188.13 GB
g0056: --------------------------------------------------
g0056: DeepSpeed C++/CUDA extension op report
g0056: --------------------------------------------------
g0056: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0056:       runtime if needed. Op compatibility means that your system
g0056:       meet the required dependencies to JIT install the op.
g0056: --------------------------------------------------
g0056: JIT compiled ops requires ninja
g0042: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0056: ninja .................. [92m[OKAY][0m
g0056: --------------------------------------------------
g0056: op name ................ installed .. compatible
g0056: --------------------------------------------------
g0043: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0043: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0043: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0066: [2024-08-12 04:00:24,334] [INFO] [comm.py:637:init_distributed] cdb=None
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0054: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0054: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: [2024-08-12 04:00:24,343] [INFO] [comm.py:637:init_distributed] cdb=None
g0054: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0043: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0043: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0043: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0043: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0063: [2024-08-12 04:00:24,350] [INFO] [comm.py:637:init_distributed] cdb=None
g0063: [2024-08-12 04:00:24,350] [INFO] [comm.py:637:init_distributed] cdb=None
g0054: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0054: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0054: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [2024-08-12 04:00:24,353] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0043: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0054: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0054: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0054: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0043: --------------------------------------------------
g0043: DeepSpeed general environment info:
g0043: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0043: torch version .................... 2.0.1+cu118
g0043: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0043: deepspeed info ................... 0.12.4, unknown, unknown
g0043: torch cuda version ............... 11.8
g0043: torch hip version ................ None
g0043: nvcc version ..................... 11.8
g0043: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0043: shared memory (/dev/shm) size .... 188.13 GB
g0054: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0054: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0054: --------------------------------------------------
g0054: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: DeepSpeed general environment info:
g0054: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0054: torch version .................... 2.0.1+cu118
g0054: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0054: deepspeed info ................... 0.12.4, unknown, unknown
g0054: torch cuda version ............... 11.8
g0054: torch hip version ................ None
g0054: nvcc version ..................... 11.8
g0054: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0054: shared memory (/dev/shm) size .... 188.13 GB
g0054: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0054: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0054: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0054: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0054: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0054: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0054: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0054: --------------------------------------------------
g0043: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0056: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0056: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0056: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0054: DeepSpeed general environment info:
g0054: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0054: torch version .................... 2.0.1+cu118
g0054: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0054: deepspeed info ................... 0.12.4, unknown, unknown
g0054: torch cuda version ............... 11.8
g0054: torch hip version ................ None
g0054: nvcc version ..................... 11.8
g0054: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0054: shared memory (/dev/shm) size .... 188.13 GB
g0056: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0056: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0043: [2024-08-12 04:00:24,395] [INFO] [comm.py:637:init_distributed] cdb=None
g0054: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0056: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0056: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0056: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0056: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0056: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0056: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0056: --------------------------------------------------
g0042: [2024-08-12 04:00:24,406] [INFO] [comm.py:637:init_distributed] cdb=None
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: DeepSpeed general environment info:
g0056: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0056: torch version .................... 2.0.1+cu118
g0056: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0056: deepspeed info ................... 0.12.4, unknown, unknown
g0056: torch cuda version ............... 11.8
g0056: torch hip version ................ None
g0056: nvcc version ..................... 11.8
g0056: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0056: shared memory (/dev/shm) size .... 188.13 GB
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0042: [2024-08-12 04:00:24,414] [INFO] [comm.py:637:init_distributed] cdb=None
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [2024-08-12 04:00:24,421] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0043: [2024-08-12 04:00:24,436] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [2024-08-12 04:00:24,468] [INFO] [comm.py:637:init_distributed] cdb=None
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0043: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [2024-08-12 04:00:24,479] [INFO] [comm.py:637:init_distributed] cdb=None
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [2024-08-12 04:00:24,493] [INFO] [comm.py:637:init_distributed] cdb=None
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [2024-08-12 04:00:24,519] [INFO] [comm.py:637:init_distributed] cdb=None
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: > setting tensorboard ...
g0066: [2024-08-12 04:00:24,530] [INFO] [comm.py:637:init_distributed] cdb=None
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0041: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0066: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0054: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0056: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0063: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0042: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0038-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0038: > initialized tensor model parallel with size 1
g0038: > initialized pipeline model parallel with size 8
g0038: > setting random seeds to 1234 ...
g0038: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0038: > compiling dataset index builder ...
g0038: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0038: make: Nothing to be done for 'default'.
g0038: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0038: >>> done with dataset index builder. Compilation time: 0.071 seconds
g0038: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0038: > compiling and loading fused kernels ...
g0038: Detected CUDA files, patching ldflags
g0038: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0038: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0038: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0038: ninja: no work to do.
g0038: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0038: Detected CUDA files, patching ldflags
g0038: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0038: Building extension module scaled_masked_softmax_cuda...
g0038: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0038: ninja: no work to do.
g0038: Loading extension module scaled_masked_softmax_cuda...
g0038: Detected CUDA files, patching ldflags
g0038: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0038: Building extension module scaled_softmax_cuda...
g0038: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0038: ninja: no work to do.
g0038: Loading extension module scaled_softmax_cuda...
g0038: >>> done with compiling and loading fused kernels. Compilation time: 4.184 seconds
g0038: time to initialize megatron (seconds): 8.478
g0038: [after megatron is initialized] datetime: 2024-08-12 04:00:31 
g0041: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0042: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0056: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0038: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0066: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0043: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0054: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0063: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0038: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0038: wandb:  $ pip install wandb --upgrade
g0038: wandb: Tracking run with wandb version 0.17.5
g0038: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_040033-w4u22f8h
g0038: wandb: Run `wandb offline` to turn off syncing.
g0042: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0042: wandb:  $ pip install wandb --upgrade
g0042: wandb: Tracking run with wandb version 0.17.5
g0042: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_040033-6de05f2s
g0042: wandb: Run `wandb offline` to turn off syncing.
g0038: wandb: Syncing run g0038.abci.local
g0038: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0038: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/w4u22f8h
g0042: wandb: Syncing run g0042.abci.local
g0042: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0042: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/6de05f2s
g0054: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0054: wandb:  $ pip install wandb --upgrade
g0054: wandb: Tracking run with wandb version 0.17.5
g0054: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_040033-b6bqwyok
g0054: wandb: Run `wandb offline` to turn off syncing.
g0054: wandb: Syncing run g0054.abci.local
g0054: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0054: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/b6bqwyok
g0041: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0041: wandb:  $ pip install wandb --upgrade
g0041: wandb: Tracking run with wandb version 0.17.5
g0041: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_040033-ku37jkzz
g0041: wandb: Run `wandb offline` to turn off syncing.
g0066: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0066: wandb:  $ pip install wandb --upgrade
g0066: wandb: Tracking run with wandb version 0.17.5
g0066: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_040033-6op7bgmo
g0066: wandb: Run `wandb offline` to turn off syncing.
g0041: wandb: Syncing run g0041.abci.local
g0041: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0041: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/ku37jkzz
g0066: wandb: Syncing run g0066.abci.local
g0066: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0066: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/6op7bgmo
g0056: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0056: wandb:  $ pip install wandb --upgrade
g0056: wandb: Tracking run with wandb version 0.17.5
g0056: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_040033-df42d7yt
g0056: wandb: Run `wandb offline` to turn off syncing.
g0056: wandb: Syncing run g0056.abci.local
g0056: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0056: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/df42d7yt
g0043: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0043: wandb:  $ pip install wandb --upgrade
g0043: wandb: Tracking run with wandb version 0.17.5
g0043: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_040033-tgwl9386
g0043: wandb: Run `wandb offline` to turn off syncing.
g0043: wandb: Syncing run g0043.abci.local
g0043: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0043: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/tgwl9386
g0063: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0063: wandb:  $ pip install wandb --upgrade
g0063: wandb: Tracking run with wandb version 0.17.5
g0063: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_040033-xd1ckxw5
g0063: wandb: Run `wandb offline` to turn off syncing.
g0063: wandb: Syncing run g0063.abci.local
g0063: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0063: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/xd1ckxw5
g0038: building GPT model ...
g0038: [2024-08-12 04:00:33,964] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0038: [2024-08-12 04:00:33,964] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0038: [2024-08-12 04:00:33,965] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 55.26 GB, percent = 14.7%
g0038: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0038: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0038: [2024-08-12 04:00:34,479] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0038: stage=0 layers=5
g0038:      0: _to_float16
g0038:      1: EmbeddingPipe
g0038:      2: ParallelTransformerLayerPipe
g0038:      3: ParallelTransformerLayerPipe
g0038:      4: ParallelTransformerLayerPipe
g0038: stage=1 layers=3
g0038:      5: ParallelTransformerLayerPipe
g0038:      6: ParallelTransformerLayerPipe
g0038:      7: ParallelTransformerLayerPipe
g0038: stage=2 layers=3
g0038:      8: ParallelTransformerLayerPipe
g0038:      9: ParallelTransformerLayerPipe
g0038:     10: ParallelTransformerLayerPipe
g0038: stage=3 layers=3
g0038:     11: ParallelTransformerLayerPipe
g0038:     12: ParallelTransformerLayerPipe
g0038:     13: ParallelTransformerLayerPipe
g0038: stage=4 layers=3
g0038:     14: ParallelTransformerLayerPipe
g0038:     15: ParallelTransformerLayerPipe
g0038:     16: ParallelTransformerLayerPipe
g0038: stage=5 layers=3
g0038:     17: ParallelTransformerLayerPipe
g0038:     18: ParallelTransformerLayerPipe
g0038:     19: ParallelTransformerLayerPipe
g0038: stage=6 layers=3
g0038:     20: ParallelTransformerLayerPipe
g0038:     21: ParallelTransformerLayerPipe
g0038:     22: ParallelTransformerLayerPipe
g0038: stage=7 layers=3
g0038:     23: ParallelTransformerLayerPipe
g0038:     24: MixedFusedRMSNorm
g0038:     25: LMHeadPipe
g0038:   loss: CrossEntropy
g0043:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0054:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0042:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0041:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0063:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0066:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0056:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0043: [2024-08-12 04:00:34,916] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0043: [2024-08-12 04:00:34,916] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0043: [2024-08-12 04:00:34,916] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0043: [2024-08-12 04:00:34,917] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0038: [2024-08-12 04:00:34,943] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0038: [2024-08-12 04:00:34,945] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0038: [2024-08-12 04:00:34,945] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 55.32 GB, percent = 14.7%
g0038:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0038: setting training iterations to 52000
g0038: > learning rate decay style: cosine
g0038: DeepSpeed is enabled.
g0038: [2024-08-12 04:00:34,949] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0042: [2024-08-12 04:00:34,951] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0042: [2024-08-12 04:00:34,953] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0042: [2024-08-12 04:00:34,953] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0042: [2024-08-12 04:00:34,953] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0054: [2024-08-12 04:00:34,959] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0054: [2024-08-12 04:00:34,959] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0054: [2024-08-12 04:00:34,959] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0054: [2024-08-12 04:00:34,959] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0063: [2024-08-12 04:00:34,966] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0063: [2024-08-12 04:00:34,967] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0063: [2024-08-12 04:00:34,967] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0063: [2024-08-12 04:00:34,967] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0041: [2024-08-12 04:00:34,969] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0041: [2024-08-12 04:00:34,971] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0041: [2024-08-12 04:00:34,971] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0041: [2024-08-12 04:00:34,971] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0066: [2024-08-12 04:00:34,993] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0066: [2024-08-12 04:00:34,993] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0066: [2024-08-12 04:00:34,993] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0066: [2024-08-12 04:00:34,993] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0056: [2024-08-12 04:00:35,034] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0056: [2024-08-12 04:00:35,037] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0056: [2024-08-12 04:00:35,037] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0056: [2024-08-12 04:00:35,038] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0038: [2024-08-12 04:00:35,165] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0038: [2024-08-12 04:00:35,166] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0038: [2024-08-12 04:00:35,167] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0038: [2024-08-12 04:00:35,167] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0038: [2024-08-12 04:00:35,167] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0038: [2024-08-12 04:00:35,182] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0038: [2024-08-12 04:00:35,182] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0038: [2024-08-12 04:00:35,182] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0038: [2024-08-12 04:00:35,182] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0038: [2024-08-12 04:00:35,183] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0038: [2024-08-12 04:00:35,183] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f025c077e50>
g0038: [2024-08-12 04:00:35,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: [2024-08-12 04:00:35,183] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0038: [2024-08-12 04:00:35,184] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0038:     "partition_activations": false, 
g0038:     "contiguous_memory_optimization": false, 
g0038:     "cpu_checkpointing": false, 
g0038:     "number_checkpoints": null, 
g0038:     "synchronize_checkpoint_boundary": false, 
g0038:     "profile": false
g0038: }
g0038: [2024-08-12 04:00:35,184] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0038: [2024-08-12 04:00:35,184] [INFO] [config.py:983:print]   amp_enabled .................. False
g0038: [2024-08-12 04:00:35,184] [INFO] [config.py:983:print]   amp_params ................... False
g0038: [2024-08-12 04:00:35,184] [INFO] [config.py:983:print]   autotuning_config ............ {
g0038:     "enabled": false, 
g0038:     "start_step": null, 
g0038:     "end_step": null, 
g0038:     "metric_path": null, 
g0038:     "arg_mappings": null, 
g0038:     "metric": "throughput", 
g0038:     "model_info": null, 
g0038:     "results_dir": "autotuning_results", 
g0038:     "exps_dir": "autotuning_exps", 
g0038:     "overwrite": true, 
g0038:     "fast": true, 
g0038:     "start_profile_step": 3, 
g0038:     "end_profile_step": 5, 
g0038:     "tuner_type": "gridsearch", 
g0038:     "tuner_early_stopping": 5, 
g0038:     "tuner_num_trials": 50, 
g0038:     "model_info_path": null, 
g0038:     "mp_size": 1, 
g0038:     "max_train_batch_size": null, 
g0038:     "min_train_batch_size": 1, 
g0038:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0038:     "min_train_micro_batch_size_per_gpu": 1, 
g0038:     "num_tuning_micro_batch_sizes": 3
g0038: }
g0038: [2024-08-12 04:00:35,185] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0038: [2024-08-12 04:00:35,185] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0038: [2024-08-12 04:00:35,185] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0038: [2024-08-12 04:00:35,185] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0038: [2024-08-12 04:00:35,185] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f01e80e2a90>
g0038: [2024-08-12 04:00:35,186] [INFO] [config.py:983:print]   communication_data_type ...... None
g0038: [2024-08-12 04:00:35,186] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0038: [2024-08-12 04:00:35,186] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0038: [2024-08-12 04:00:35,186] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0038: [2024-08-12 04:00:35,186] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0038: [2024-08-12 04:00:35,186] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0038: [2024-08-12 04:00:35,186] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0038: [2024-08-12 04:00:35,186] [INFO] [config.py:983:print]   disable_allgather ............ False
g0038: [2024-08-12 04:00:35,187] [INFO] [config.py:983:print]   dump_state ................... False
g0038: [2024-08-12 04:00:35,187] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0038: [2024-08-12 04:00:35,187] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0038: [2024-08-12 04:00:35,187] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0038: [2024-08-12 04:00:35,187] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0038: [2024-08-12 04:00:35,187] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0038: [2024-08-12 04:00:35,187] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0038: [2024-08-12 04:00:35,187] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0038: [2024-08-12 04:00:35,188] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0038: [2024-08-12 04:00:35,188] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0038: [2024-08-12 04:00:35,188] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0038: [2024-08-12 04:00:35,188] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0038:     "enabled": false, 
g0038:     "recompute_fwd_factor": 0.0, 
g0038:     "profile_step": 1, 
g0038:     "module_depth": -1, 
g0038:     "top_modules": 1, 
g0038:     "detailed": true, 
g0038:     "output_file": null
g0038: }
g0038: [2024-08-12 04:00:35,188] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0038: [2024-08-12 04:00:35,188] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0038: [2024-08-12 04:00:35,188] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0038: [2024-08-12 04:00:35,189] [INFO] [config.py:983:print]   global_rank .................. 0
g0038: [2024-08-12 04:00:35,189] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0038: [2024-08-12 04:00:35,189] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0038: [2024-08-12 04:00:35,189] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0038: [2024-08-12 04:00:35,189] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0038: [2024-08-12 04:00:35,189] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0038: [2024-08-12 04:00:35,189] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0038: [2024-08-12 04:00:35,189] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0038: [2024-08-12 04:00:35,190] [INFO] [config.py:983:print]   loss_scale ................... 0
g0038: [2024-08-12 04:00:35,190] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0038: [2024-08-12 04:00:35,190] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0038: [2024-08-12 04:00:35,190] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0038: [2024-08-12 04:00:35,190] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0038: [2024-08-12 04:00:35,190] [INFO] [config.py:983:print]   nebula_config ................ {
g0038:     "enabled": false, 
g0038:     "persistent_storage_path": null, 
g0038:     "persistent_time_interval": 100, 
g0038:     "num_of_version_in_retention": 2, 
g0038:     "enable_nebula_load": true, 
g0038:     "load_path": null
g0038: }
g0038: [2024-08-12 04:00:35,190] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0038: [2024-08-12 04:00:35,191] [INFO] [config.py:983:print]   optimizer_name ............... None
g0038: [2024-08-12 04:00:35,191] [INFO] [config.py:983:print]   optimizer_params ............. None
g0038: [2024-08-12 04:00:35,191] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0038: [2024-08-12 04:00:35,191] [INFO] [config.py:983:print]   pld_enabled .................. False
g0038: [2024-08-12 04:00:35,191] [INFO] [config.py:983:print]   pld_params ................... False
g0038: [2024-08-12 04:00:35,191] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0038: [2024-08-12 04:00:35,191] [INFO] [config.py:983:print]   scheduler_name ............... None
g0038: [2024-08-12 04:00:35,191] [INFO] [config.py:983:print]   scheduler_params ............. None
g0038: [2024-08-12 04:00:35,192] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0038: [2024-08-12 04:00:35,192] [INFO] [config.py:983:print]   sparse_attention ............. None
g0038: [2024-08-12 04:00:35,192] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0038: [2024-08-12 04:00:35,192] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0038: [2024-08-12 04:00:35,192] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0038: [2024-08-12 04:00:35,192] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0038: [2024-08-12 04:00:35,192] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0038: [2024-08-12 04:00:35,192] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0038: [2024-08-12 04:00:35,193] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0038: [2024-08-12 04:00:35,193] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0038: [2024-08-12 04:00:35,193] [INFO] [config.py:983:print]   world_size ................... 4
g0038: [2024-08-12 04:00:35,193] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0038: [2024-08-12 04:00:35,193] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0038: [2024-08-12 04:00:35,193] [INFO] [config.py:983:print]   zero_enabled ................. False
g0038: [2024-08-12 04:00:35,193] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0038: [2024-08-12 04:00:35,193] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0038: [2024-08-12 04:00:35,194] [INFO] [config.py:969:print_user_config]   json = {
g0038:     "train_batch_size": 128, 
g0038:     "train_micro_batch_size_per_gpu": 1, 
g0038:     "steps_per_print": 10, 
g0038:     "zero_optimization": {
g0038:         "stage": 0
g0038:     }, 
g0038:     "gradient_clipping": 1.0, 
g0038:     "prescale_gradients": true, 
g0038:     "fp16": {
g0038:         "enabled": true, 
g0038:         "loss_scale": 0, 
g0038:         "loss_scale_window": 500, 
g0038:         "hysteresis": 2, 
g0038:         "min_loss_scale": 1, 
g0038:         "initial_scale_power": 11
g0038:     }, 
g0038:     "wall_clock_breakdown": false
g0038: }
g0038: [2024-08-12 04:00:35,194] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0038: [2024-08-12 04:00:35,194] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0054: [2024-08-12 04:00:35,904] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0038: [2024-08-12 04:00:35,904] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0063: [2024-08-12 04:00:35,905] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0042: [2024-08-12 04:00:35,905] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0041: [2024-08-12 04:00:35,905] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0066: [2024-08-12 04:00:35,905] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0056: [2024-08-12 04:00:35,905] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0043: [2024-08-12 04:00:35,905] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0038: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0038: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0042: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0042: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0038: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0042: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0042: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0063: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0063: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0063: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0063: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0038: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0054: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0066: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0066: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0066: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0066: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0043: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0043: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0054: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0054: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0054: [2024-08-12 04:00:36,602] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0043: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0043: [2024-08-12 04:00:36,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0041: [2024-08-12 04:00:36,602] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0041: [2024-08-12 04:00:36,602] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0041: [2024-08-12 04:00:36,602] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0041: [2024-08-12 04:00:36,617] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0056: [2024-08-12 04:00:36,663] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0056: [2024-08-12 04:00:36,663] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0056: [2024-08-12 04:00:36,663] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0056: [2024-08-12 04:00:36,663] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0054: [2024-08-12 04:00:39,207] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0054: [2024-08-12 04:00:39,207] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0054: [2024-08-12 04:00:39,208] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0054: [2024-08-12 04:00:39,208] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0054: [2024-08-12 04:00:39,215] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0054: [2024-08-12 04:00:39,216] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0054: [2024-08-12 04:00:39,216] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0054: [2024-08-12 04:00:39,216] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0038: [2024-08-12 04:00:39,421] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 04:00:39,421] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 04:00:39,421] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 04:00:39,421] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0042: [2024-08-12 04:00:39,425] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0042: [2024-08-12 04:00:39,425] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0042: [2024-08-12 04:00:39,425] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0042: [2024-08-12 04:00:39,426] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 04:00:39,428] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0038: [2024-08-12 04:00:39,430] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0038: [2024-08-12 04:00:39,430] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0038: [2024-08-12 04:00:39,431] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0042: [2024-08-12 04:00:39,434] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0042: [2024-08-12 04:00:39,434] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0042: [2024-08-12 04:00:39,434] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0042: [2024-08-12 04:00:39,434] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0043: [2024-08-12 04:00:39,711] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0043: [2024-08-12 04:00:39,711] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0043: [2024-08-12 04:00:39,711] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0043: [2024-08-12 04:00:39,711] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0043: [2024-08-12 04:00:39,718] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0043: [2024-08-12 04:00:39,719] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0043: [2024-08-12 04:00:39,719] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0043: [2024-08-12 04:00:39,720] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0063: [2024-08-12 04:00:39,747] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0063: [2024-08-12 04:00:39,747] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0063: [2024-08-12 04:00:39,747] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0063: [2024-08-12 04:00:39,747] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0063: [2024-08-12 04:00:39,756] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0063: [2024-08-12 04:00:39,756] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0063: [2024-08-12 04:00:39,756] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0063: [2024-08-12 04:00:39,757] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0041: [2024-08-12 04:00:39,812] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0041: [2024-08-12 04:00:39,812] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0041: [2024-08-12 04:00:39,812] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0041: [2024-08-12 04:00:39,812] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0041: [2024-08-12 04:00:39,820] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0041: [2024-08-12 04:00:39,820] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0041: [2024-08-12 04:00:39,820] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0041: [2024-08-12 04:00:39,820] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0056: [2024-08-12 04:00:40,118] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0056: [2024-08-12 04:00:40,120] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0056: [2024-08-12 04:00:40,120] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0056: [2024-08-12 04:00:40,121] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0056: [2024-08-12 04:00:40,125] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0056: [2024-08-12 04:00:40,130] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0056: [2024-08-12 04:00:40,130] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0056: [2024-08-12 04:00:40,130] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0066: [2024-08-12 04:00:40,143] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0066: [2024-08-12 04:00:40,143] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0066: [2024-08-12 04:00:40,143] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0066: [2024-08-12 04:00:40,143] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0066: [2024-08-12 04:00:40,151] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0066: [2024-08-12 04:00:40,151] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0066: [2024-08-12 04:00:40,151] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0066: [2024-08-12 04:00:40,151] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0042: [2024-08-12 04:00:41,040] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0042: [2024-08-12 04:00:41,040] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0042: [2024-08-12 04:00:41,040] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0042: [2024-08-12 04:00:41,041] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0042: [2024-08-12 04:00:41,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,355] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0054: [2024-08-12 04:00:41,355] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0054: [2024-08-12 04:00:41,356] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,356] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0054: [2024-08-12 04:00:41,356] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0054: [2024-08-12 04:00:41,356] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,357] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,357] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,365] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,366] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,366] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,366] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,366] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,366] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,367] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,367] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,400] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,400] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,401] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,402] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,420] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,421] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,423] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,423] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,583] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0063: [2024-08-12 04:00:41,583] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0063: [2024-08-12 04:00:41,583] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0063: [2024-08-12 04:00:41,584] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0063: [2024-08-12 04:00:41,584] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,584] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,584] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,585] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0041: [2024-08-12 04:00:41,593] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0041: [2024-08-12 04:00:41,593] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0041: [2024-08-12 04:00:41,593] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 04:00:41,594] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0041: [2024-08-12 04:00:41,594] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0041: [2024-08-12 04:00:41,594] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 04:00:41,595] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 04:00:41,595] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0043: [2024-08-12 04:00:41,636] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0043: [2024-08-12 04:00:41,636] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0043: [2024-08-12 04:00:41,637] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0043: [2024-08-12 04:00:41,637] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 04:00:41,637] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 04:00:41,638] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0043: [2024-08-12 04:00:41,638] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 04:00:41,639] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,654] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 04:00:41,654] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 04:00:41,654] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,654] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 04:00:41,655] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0038: [2024-08-12 04:00:41,655] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,655] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,655] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,704] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 04:00:41,704] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 04:00:41,704] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 04:00:41,704] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,705] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 04:00:41,705] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,705] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,705] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,739] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 04:00:41,739] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 04:00:41,741] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 04:00:41,741] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0054: [2024-08-12 04:00:41,755] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,760] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,761] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 04:00:41,762] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,818] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 04:00:41,819] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,821] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 04:00:41,821] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,822] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 04:00:41,822] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 04:00:41,823] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,823] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,825] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 04:00:41,825] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 04:00:41,825] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 04:00:41,825] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 04:00:41,825] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,825] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,825] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,826] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,833] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,833] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,833] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,834] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,834] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,834] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,834] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,834] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,859] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 04:00:41,859] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 04:00:41,859] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0063: [2024-08-12 04:00:41,860] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,865] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,865] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,866] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0042: [2024-08-12 04:00:41,867] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0038: [2024-08-12 04:00:41,869] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 04:00:41,872] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 04:00:41,874] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0038: [2024-08-12 04:00:41,874] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0066: [2024-08-12 04:00:41,877] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0066: [2024-08-12 04:00:41,878] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 04:00:41,878] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0066: [2024-08-12 04:00:41,878] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0066: [2024-08-12 04:00:41,878] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0066: [2024-08-12 04:00:41,878] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 04:00:41,879] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 04:00:41,879] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,881] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,881] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,881] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 04:00:41,881] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,888] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 04:00:41,888] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,898] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,902] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,903] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 04:00:41,903] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,018] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,019] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,019] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,019] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,019] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,020] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,020] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,020] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,053] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,053] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,055] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,055] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,074] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,075] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,076] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,076] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,123] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0056: [2024-08-12 04:00:42,123] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0056: [2024-08-12 04:00:42,124] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,124] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0056: [2024-08-12 04:00:42,124] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,125] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,126] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0056: [2024-08-12 04:00:42,127] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,189] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,189] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,189] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,189] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,189] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,189] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,190] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,190] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,196] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,196] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,196] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,197] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,197] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,197] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,197] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,198] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0042: [2024-08-12 04:00:42,214] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 04:00:42,215] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 04:00:42,215] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 04:00:42,215] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 04:00:42,215] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 04:00:42,216] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 04:00:42,216] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0042: [2024-08-12 04:00:42,216] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,224] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,224] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,226] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,226] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,230] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,230] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,233] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,233] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,246] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,246] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0042: [2024-08-12 04:00:42,246] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 04:00:42,246] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,248] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,248] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0042: [2024-08-12 04:00:42,249] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0042: [2024-08-12 04:00:42,249] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,250] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,251] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,252] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,254] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,255] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,255] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,255] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,255] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,256] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,256] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,256] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,256] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,286] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,286] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,286] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,299] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,307] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,308] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,308] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,313] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,358] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,358] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,359] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,359] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,359] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,359] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,359] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,359] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,389] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,390] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,392] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,392] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,411] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,411] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,413] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,413] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,463] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,463] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,463] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,463] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,463] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,463] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,463] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,464] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,494] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,494] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,498] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,498] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,509] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,516] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,519] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,519] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,520] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,520] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,520] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,520] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,520] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,520] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,520] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,521] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,554] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,554] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,557] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,557] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,569] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,573] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,576] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,580] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,582] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,582] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,583] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,583] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,583] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,583] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,583] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,583] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,600] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,600] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,601] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,601] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0063: [2024-08-12 04:00:42,601] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,615] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,615] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,618] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,618] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,631] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,631] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,634] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0063: [2024-08-12 04:00:42,634] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,637] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,638] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,639] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,641] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,687] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,688] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,688] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,688] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,688] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,688] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,689] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,689] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,719] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,719] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,722] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,722] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0043: [2024-08-12 04:00:42,722] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0043: [2024-08-12 04:00:42,722] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,724] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,724] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,724] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,724] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,724] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,724] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0041: [2024-08-12 04:00:42,756] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,756] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,756] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0041: [2024-08-12 04:00:42,756] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,834] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,834] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,834] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,834] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,834] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,834] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,835] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,835] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,866] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,866] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,868] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,868] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0056: [2024-08-12 04:00:42,881] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,884] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,889] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 04:00:42,889] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,921] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,921] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,922] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,922] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,922] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,922] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,922] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,922] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0038: [2024-08-12 04:00:42,954] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,954] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,957] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0038: [2024-08-12 04:00:42,957] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0056: [2024-08-12 04:00:43,100] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 04:00:43,100] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 04:00:43,100] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 04:00:43,101] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 04:00:43,101] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 04:00:43,101] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 04:00:43,101] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 04:00:43,101] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0056: [2024-08-12 04:00:43,132] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 04:00:43,132] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 04:00:43,135] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0056: [2024-08-12 04:00:43,135] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,503] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,503] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,503] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,503] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,503] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,504] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,535] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,535] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,537] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,537] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,550] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,557] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,558] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,559] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,755] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,755] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,755] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,755] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,755] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,755] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,756] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,756] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0054: [2024-08-12 04:00:43,787] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,787] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,789] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0054: [2024-08-12 04:00:43,789] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0038:  > overriding learning rate value to 0.0002
g0038:  > overriding minimum learning rate value to 1e-06
g0038:  > overriding warmup iterations value to 0
g0038:  > overriding warmup tokens value to 3000000000
g0038:  > overriding total number of iterations value to 6656000
g0038:  > overriding decay tokens value to 300000000000
g0038:  > overriding learning rate decay style value to cosine
g0038:  > overriding start weight decay value to 0.1
g0038:  > overriding end weight decay value to 0.1
g0038:  > overriding total number of weight decay iterations value to 6656000
g0038:  > overriding weight decay incr style value to constant
g0038:  checkpoint version 3.0
g0066: [2024-08-12 04:00:44,395] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,395] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,396] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,396] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,396] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,396] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,396] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,397] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,820] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,820] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,822] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,822] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,842] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,842] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,843] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,843] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,853] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,853] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,853] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,853] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,853] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,853] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,853] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,853] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,854] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,854] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,854] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,854] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0066: [2024-08-12 04:00:44,854] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,854] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,855] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 04:00:44,855] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 04:00:45,109] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 04:00:45,110] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 04:00:45,110] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 04:00:45,110] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 04:00:45,110] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 04:00:45,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 04:00:45,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 04:00:45,111] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0066: [2024-08-12 04:00:45,153] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 04:00:45,153] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 04:00:45,160] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0066: [2024-08-12 04:00:45,160] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0038:   successfully loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase at iteration 42000
g0066: (min, max) time across ranks (ms):
g0066:     load-checkpoint ................................: (9424.87, 9425.24)
g0038: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-12 04:00:46 
g0038: > building train, validation, and test datasets ...
g0038:  > datasets target sizes (minimum size):
g0038:     train:      6656000
g0038:     validation: 678400
g0038:     test:       12800
g0038: > building train, validation, and test datasets for GPT ...
g0038: Single data path provided for train, valid & test
g0038:  > building dataset index ...
g0038:     reading sizes...
g0038:     reading pointers...
g0038:     reading document index...
g0038:     creating numpy buffer of mmap...
g0038:     creating memory view of numpy buffer...
g0038:  > finished creating indexed dataset in 0.032601 seconds
g0038:     number of documents: 250886
g0038:  > dataset split:
g0038:     train:
g0038:      document indices in [0, 238091) total of 238091 documents
g0038:     validation:
g0038:      document indices in [238091, 250635) total of 12544 documents
g0038:     test:
g0038:      document indices in [250635, 250886) total of 251 documents
g0038:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/08ead6f9dd193a5300c72fd3e8930ec7_doc_idx.npy
g0038:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/08ead6f9dd193a5300c72fd3e8930ec7_sample_idx.npy
g0038:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/08ead6f9dd193a5300c72fd3e8930ec7_shuffle_idx.npy
g0038:     loaded indexed file in 0.073 seconds
g0038:     total number of samples: 6675496
g0038:     total number of epochs: 14
g0038:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/d5dc985126ccf4420fa9c4046a4e1bc0_doc_idx.npy
g0038:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/d5dc985126ccf4420fa9c4046a4e1bc0_sample_idx.npy
g0038:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/d5dc985126ccf4420fa9c4046a4e1bc0_shuffle_idx.npy
g0038:     loaded indexed file in 0.105 seconds
g0038:     total number of samples: 692334
g0038:     total number of epochs: 27
g0038:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/7a71560402d99012f7c883a368e28919_doc_idx.npy
g0038:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/7a71560402d99012f7c883a368e28919_sample_idx.npy
g0038:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/7a71560402d99012f7c883a368e28919_shuffle_idx.npy
g0038:     loaded indexed file in 0.054 seconds
g0038:     total number of samples: 12809
g0038:     total number of epochs: 34
g0038: > finished creating GPT datasets ...
g0038: [after dataloaders are built] datetime: 2024-08-12 04:00:48 
g0038: done with setup ...
g0066: (min, max) time across ranks (ms):
g0066:     model-and-optimizer-setup ......................: (12220.61, 12221.41)
g0066:     train/valid/test-data-iterators-setup ..........: (2477.87, 2491.18)
g0038: training ...
g0038: [before the start of training step] datetime: 2024-08-12 04:00:48 
g0038: [2024-08-12 04:01:37,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=42010, skipped=71, lr=[0.00019964285383907178, 0.00019964285383907178], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42010 loss: 6.8196 iter time (s): 4.886 samples/sec: 26.200
g0066:  iteration    42010/   52000 | consumed samples:      5377280 | consumed tokens:  11012669440 | elapsed time per iteration (ms): 4920.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 8.090987E+00 | loss scale: 32.0 | grad norm: 6.650 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.011 | tokens per gpu per second (tgs): 1664.725 | TFLOPs: 13.40 |
g0066: [Rank 28] (after 42010 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0056: [Rank 20] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5542.0 | max reserved: 5542.0
g0054: [Rank 16] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6308.0 | max reserved: 6308.0
g0042: [Rank 8] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8116.0 | max reserved: 8116.0
g0043: [Rank 12] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7202.0 | max reserved: 7202.0
g0041: [Rank 4] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 8990.0 | max reserved: 8990.0
g0063: [Rank 24] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 5054.0 | max reserved: 5054.0
g0038: [Rank 0] (after 42010 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 11670.0 | max reserved: 11670.0
g0038: [2024-08-12 04:02:20,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=42020, skipped=71, lr=[0.00019964262024399794, 0.00019964262024399794], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42020 loss: 6.4137 iter time (s): 4.173 samples/sec: 30.673
g0066:  iteration    42020/   52000 | consumed samples:      5378560 | consumed tokens:  11015290880 | elapsed time per iteration (ms): 4245.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.508395E+00 | loss scale: 32.0 | grad norm: 2.768 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.149 | tokens per gpu per second (tgs): 1929.507 | TFLOPs: 15.53 |
g0038: [2024-08-12 04:03:01,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=42030, skipped=71, lr=[0.00019964238657269387, 0.00019964238657269387], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42030 loss: 6.5717 iter time (s): 4.132 samples/sec: 30.976
g0066:  iteration    42030/   52000 | consumed samples:      5379840 | consumed tokens:  11017912320 | elapsed time per iteration (ms): 4164.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.471465E+00 | loss scale: 32.0 | grad norm: 2.884 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.734 | tokens per gpu per second (tgs): 1966.958 | TFLOPs: 15.83 |
g0038: [2024-08-12 04:03:44,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=42040, skipped=71, lr=[0.0001996421528251598, 0.0001996421528251598], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42040 loss: 6.4798 iter time (s): 4.191 samples/sec: 30.542
g0066:  iteration    42040/   52000 | consumed samples:      5381120 | consumed tokens:  11020533760 | elapsed time per iteration (ms): 4223.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.502630E+00 | loss scale: 32.0 | grad norm: 3.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.306 | tokens per gpu per second (tgs): 1939.584 | TFLOPs: 15.61 |
g0038: [2024-08-12 04:04:26,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=42050, skipped=71, lr=[0.00019964191900139587, 0.00019964191900139587], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42050 loss: 6.5774 iter time (s): 4.183 samples/sec: 30.600
g0066:  iteration    42050/   52000 | consumed samples:      5382400 | consumed tokens:  11023155200 | elapsed time per iteration (ms): 4215.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.524469E+00 | loss scale: 32.0 | grad norm: 4.432 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.364 | tokens per gpu per second (tgs): 1943.295 | TFLOPs: 15.64 |
g0038: [2024-08-12 04:05:08,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=42060, skipped=71, lr=[0.00019964168510140228, 0.00019964168510140228], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42060 loss: 6.6929 iter time (s): 4.148 samples/sec: 30.859
g0066:  iteration    42060/   52000 | consumed samples:      5383680 | consumed tokens:  11025776640 | elapsed time per iteration (ms): 4181.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.643484E+00 | loss scale: 32.0 | grad norm: 4.006 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.612 | tokens per gpu per second (tgs): 1959.164 | TFLOPs: 15.77 |
g0038: [2024-08-12 04:05:50,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=42070, skipped=71, lr=[0.0001996414511251792, 0.0001996414511251792], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42070 loss: 6.5290 iter time (s): 4.165 samples/sec: 30.735
g0066:  iteration    42070/   52000 | consumed samples:      5384960 | consumed tokens:  11028398080 | elapsed time per iteration (ms): 4197.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.616885E+00 | loss scale: 32.0 | grad norm: 3.259 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.496 | tokens per gpu per second (tgs): 1951.732 | TFLOPs: 15.71 |
g0038: [2024-08-12 04:06:32,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=42080, skipped=71, lr=[0.0001996412170727268, 0.0001996412170727268], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42080 loss: 6.4407 iter time (s): 4.173 samples/sec: 30.677
g0066:  iteration    42080/   52000 | consumed samples:      5386240 | consumed tokens:  11031019520 | elapsed time per iteration (ms): 4205.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.478317E+00 | loss scale: 32.0 | grad norm: 2.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.438 | tokens per gpu per second (tgs): 1948.034 | TFLOPs: 15.68 |
g0038: [2024-08-12 04:07:14,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=42090, skipped=71, lr=[0.00019964098294404528, 0.00019964098294404528], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42090 loss: 6.3873 iter time (s): 4.164 samples/sec: 30.742
g0066:  iteration    42090/   52000 | consumed samples:      5387520 | consumed tokens:  11033640960 | elapsed time per iteration (ms): 4196.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.397546E+00 | loss scale: 32.0 | grad norm: 1.484 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.503 | tokens per gpu per second (tgs): 1952.210 | TFLOPs: 15.71 |
g0038: [2024-08-12 04:07:56,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=42100, skipped=71, lr=[0.0001996407487391348, 0.0001996407487391348], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42100 loss: 6.6548 iter time (s): 4.239 samples/sec: 30.193
g0066:  iteration    42100/   52000 | consumed samples:      5388800 | consumed tokens:  11036262400 | elapsed time per iteration (ms): 4271.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.419079E+00 | loss scale: 32.0 | grad norm: 12.045 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.963 | tokens per gpu per second (tgs): 1917.639 | TFLOPs: 15.43 |
g0038: [2024-08-12 04:08:39,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=42110, skipped=71, lr=[0.00019964051445799558, 0.00019964051445799558], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42110 loss: 6.4078 iter time (s): 4.266 samples/sec: 30.001
g0066:  iteration    42110/   52000 | consumed samples:      5390080 | consumed tokens:  11038883840 | elapsed time per iteration (ms): 4299.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.959726E+00 | loss scale: 32.0 | grad norm: 2.516 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.772 | tokens per gpu per second (tgs): 1905.422 | TFLOPs: 15.33 |
g0038: [2024-08-12 04:09:20,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=42120, skipped=71, lr=[0.00019964028010062778, 0.00019964028010062778], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42120 loss: 6.4002 iter time (s): 4.056 samples/sec: 31.559
g0066:  iteration    42120/   52000 | consumed samples:      5391360 | consumed tokens:  11041505280 | elapsed time per iteration (ms): 4088.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.417476E+00 | loss scale: 32.0 | grad norm: 1.898 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.308 | tokens per gpu per second (tgs): 2003.717 | TFLOPs: 16.12 |
g0038: [2024-08-12 04:10:03,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=42130, skipped=71, lr=[0.00019964004566703158, 0.00019964004566703158], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42130 loss: 6.3992 iter time (s): 4.204 samples/sec: 30.448
g0066:  iteration    42130/   52000 | consumed samples:      5392640 | consumed tokens:  11044126720 | elapsed time per iteration (ms): 4236.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.405817E+00 | loss scale: 32.0 | grad norm: 1.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.213 | tokens per gpu per second (tgs): 1933.654 | TFLOPs: 15.56 |
g0038: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42132
g0041: Grad overflow on iteration 42132
g0041: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42132
g0041: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0054: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42132
g0054: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0038: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42132
g0038: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0042: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0066: Grad overflow on iteration 42132
g0042: Grad overflow on iteration 42132
g0056: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0038: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42132
g0054: Grad overflow on iteration 42132
g0054: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0054: Grad overflow on iteration 42132
g0043: Grad overflow on iteration 42132
g0054: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0042: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0041: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42132
g0043: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42132
g0063: Grad overflow on iteration 42132
g0056: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42132
g0063: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0056: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0056: Grad overflow on iteration 42132
g0066: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42132
g0056: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0041: Grad overflow on iteration 42132
g0056: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0063: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0043: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0042: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0042: Grad overflow on iteration 42132
g0042: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0066: Grad overflow on iteration 42132
g0054: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42132
g0043: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0043: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0043: Grad overflow on iteration 42132
g0041: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0063: Grad overflow on iteration 42132
g0041: Grad overflow on iteration 42132
g0043: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0066: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0063: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0063: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42132
g0063: Grad overflow on iteration 42132
g0066: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0063: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0042: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42132
g0043: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42132
g0063: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42132
g0066: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42132
g0042: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0054: [2024-08-12 04:10:15,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0056: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42132
g0041: [2024-08-12 04:10:15,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0043: [2024-08-12 04:10:15,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0038: [2024-08-12 04:10:15,085] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42132
g0063: [2024-08-12 04:10:15,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0066: [2024-08-12 04:10:15,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0038: [2024-08-12 04:10:15,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0056: [2024-08-12 04:10:15,086] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0038: [2024-08-12 04:10:15,086] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32.0, reducing to 16.0
g0043: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42134
g0043: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42134
g0043: Grad overflow on iteration 42134
g0041: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42134
g0054: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42134
g0041: Grad overflow on iteration 42134
g0063: Grad overflow on iteration 42134
g0063: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42134
g0042: Grad overflow on iteration 42134
g0038: Grad overflow on iteration 42134
g0056: Grad overflow on iteration 42134
g0056: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42134
g0066: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42134
g0063: Grad overflow on iteration 42134
g0041: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0043: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0056: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0038: Grad overflow on iteration 42134
g0063: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0038: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0042: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42134
g0066: Grad overflow on iteration 42134
g0042: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42134
g0043: Grad overflow on iteration 42134
g0042: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0043: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0063: Grad overflow on iteration 42134
g0042: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0043: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0042: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0042: Grad overflow on iteration 42134
g0043: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0042: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0063: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0038: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:10:23,595] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0038: Grad overflow on iteration 42134
g0038: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42134
g0056: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0054: Grad overflow on iteration 42134
g0056: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0054: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0054: Grad overflow on iteration 42134
g0063: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0056: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42134
g0054: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:10:23,595] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0054: Grad overflow on iteration 42134
g0066: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0054: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0063: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42134
g0038: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0066: Grad overflow on iteration 42134
g0063: [2024-08-12 04:10:23,595] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0054: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0038: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0054: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42134
g0054: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0054: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0066: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42134
g0066: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0066: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0066: [2024-08-12 04:10:23,594] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42134
g0066: [2024-08-12 04:10:23,595] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0066: [2024-08-12 04:10:23,595] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0038: [2024-08-12 04:10:23,595] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0038: [2024-08-12 04:10:23,595] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16.0, reducing to 8.0
g0038: [2024-08-12 04:10:44,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=42140, skipped=73, lr=[0.00019963981115720715, 0.00019963981115720715], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42140 loss: 7.0657 iter time (s): 4.099 samples/sec: 31.227
g0066:  iteration    42140/   52000 | consumed samples:      5393920 | consumed tokens:  11046748160 | elapsed time per iteration (ms): 4131.5 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 8.0 | grad norm: 9.068 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.981 | tokens per gpu per second (tgs): 1982.798 | TFLOPs: 15.96 |
g0038: [2024-08-12 04:11:26,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=42150, skipped=73, lr=[0.00019963957657115466, 0.00019963957657115466], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42150 loss: 6.5505 iter time (s): 4.139 samples/sec: 30.923
g0066:  iteration    42150/   52000 | consumed samples:      5395200 | consumed tokens:  11049369600 | elapsed time per iteration (ms): 4171.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.779195E+00 | loss scale: 8.0 | grad norm: 1.602 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.682 | tokens per gpu per second (tgs): 1963.631 | TFLOPs: 15.80 |
g0038: [2024-08-12 04:12:08,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=42160, skipped=73, lr=[0.0001996393419088743, 0.0001996393419088743], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42160 loss: 6.5349 iter time (s): 4.220 samples/sec: 30.328
g0066:  iteration    42160/   52000 | consumed samples:      5396480 | consumed tokens:  11051991040 | elapsed time per iteration (ms): 4253.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 6.539017E+00 | loss scale: 8.0 | grad norm: 1.622 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.096 | tokens per gpu per second (tgs): 1926.167 | TFLOPs: 15.50 |
g0038: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42169
g0038: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0038: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42169
g0041: Grad overflow on iteration 42169
g0041: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42169
g0038: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42169
g0038: Grad overflow on iteration 42169
g0041: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0063: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42169
g0038: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0054: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0041: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0056: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42169
g0041: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42169
g0042: Grad overflow on iteration 42169
g0038: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42169
g0038: Grad overflow on iteration 42169
g0063: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42169
g0063: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0054: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42169
g0043: Grad overflow on iteration 42169
g0056: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0056: Grad overflow on iteration 42169
g0038: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0056: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0054: Grad overflow on iteration 42169
g0054: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0041: Grad overflow on iteration 42169
g0043: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42169
g0041: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0043: Grad overflow on iteration 42169
g0042: Grad overflow on iteration 42169
g0054: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0043: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0043: Grad overflow on iteration 42169
g0042: Grad overflow on iteration 42169
g0041: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0042: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0054: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0043: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42169
g0043: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0054: Grad overflow on iteration 42169
g0054: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0066: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0043: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0054: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42169
g0042: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0042: [2024-08-12 04:12:51,381] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42169
g0042: Grad overflow on iteration 42169
g0056: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42169
g0042: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0056: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0066: Grad overflow on iteration 42169
g0042: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0066: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0056: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0066: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42169
g0056: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0066: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42169
g0066: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0063: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0038: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0038: [2024-08-12 04:12:51,382] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8.0, reducing to 4.0
g0066: [2024-08-12 04:12:51,382] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0054: [2024-08-12 04:12:51,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0038: [2024-08-12 04:12:51,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=42170, skipped=74, lr=[0.0001996391306476473, 0.0001996391306476473], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42170 loss: nan iter time (s): 4.241 samples/sec: 30.178
g0066:  iteration    42170/   52000 | consumed samples:      5397760 | consumed tokens:  11054612480 | elapsed time per iteration (ms): 4274.7 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 4.0 | grad norm: 1.519 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.944 | tokens per gpu per second (tgs): 1916.391 | TFLOPs: 15.42 |
g0041: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42172
g0041: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0042: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42172
g0056: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42172
g0056: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42172
g0042: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42172
g0041: Grad overflow on iteration 42172
g0041: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42172
g0043: Grad overflow on iteration 42172
g0042: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0054: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0056: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0056: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42172
g0056: Grad overflow on iteration 42172
g0043: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0063: Grad overflow on iteration 42172
g0066: Grad overflow on iteration 42172
g0063: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42172
g0063: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0038: Grad overflow on iteration 42172
g0056: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0038: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42172
g0042: Grad overflow on iteration 42172
g0038: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0038: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0041: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42172
g0038: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0056: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42172
g0056: Grad overflow on iteration 42172
g0041: Grad overflow on iteration 42172
g0042: Grad overflow on iteration 42172
g0042: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0063: Grad overflow on iteration 42172
g0042: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0041: [2024-08-12 04:13:04,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0063: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0041: [2024-08-12 04:13:04,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0054: Grad overflow on iteration 42172
g0056: [2024-08-12 04:13:04,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0066: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0043: Grad overflow on iteration 42172
g0063: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0043: Grad overflow on iteration 42172
g0066: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0066: Grad overflow on iteration 42172
g0054: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42172
g0066: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0054: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0043: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0066: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42172
g0043: Grad overflow on iteration 42172
g0054: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0063: Grad overflow on iteration 42172
g0063: [2024-08-12 04:13:04,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0066: Grad overflow on iteration 42172
g0043: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0066: [2024-08-12 04:13:04,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0054: [2024-08-12 04:13:04,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0063: [2024-08-12 04:13:04,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0043: [2024-08-12 04:13:04,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0066: [2024-08-12 04:13:04,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0038: [2024-08-12 04:13:04,090] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42172
g0038: [2024-08-12 04:13:04,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0038: [2024-08-12 04:13:04,091] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0038: [2024-08-12 04:13:04,091] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4.0, reducing to 2.0
g0041: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42174
g0041: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42174
g0056: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42174
g0056: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42174
g0056: Grad overflow on iteration 42174
g0056: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0056: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0056: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42174
g0056: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0063: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42174
g0063: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42174
g0042: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42174
g0063: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42174
g0042: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42174
g0063: Grad overflow on iteration 42174
g0038: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0038: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42174
g0054: Grad overflow on iteration 42174
g0063: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0043: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0041: Grad overflow on iteration 42174
g0042: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0041: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0063: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0042: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0041: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0063: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0043: Grad overflow on iteration 42174
g0042: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42174
g0063: Grad overflow on iteration 42174
g0041: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0041: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42174
g0041: Grad overflow on iteration 42174
g0042: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0038: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0054: Grad overflow on iteration 42174
g0054: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0054: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0066: Grad overflow on iteration 42174
g0054: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42174
g0054: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0054: Grad overflow on iteration 42174
g0043: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42174
g0066: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42174
g0043: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0043: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42174
g0054: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0043: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0043: Grad overflow on iteration 42174
g0043: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0066: Grad overflow on iteration 42174
g0043: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0038: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0066: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:12,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0038: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42174
g0038: Grad overflow on iteration 42174
g0041: [2024-08-12 04:13:12,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0066: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0066: [2024-08-12 04:13:12,785] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0066: [2024-08-12 04:13:12,786] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42174
g0054: [2024-08-12 04:13:12,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0063: [2024-08-12 04:13:12,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0038: [2024-08-12 04:13:12,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0038: [2024-08-12 04:13:12,786] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2.0, reducing to 1.0
g0066: [2024-08-12 04:13:12,786] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0041: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42177
g0041: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0041: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42177
g0041: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0041: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42177
g0041: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42177
g0043: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42177
g0041: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0063: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0042: Grad overflow on iteration 42177
g0063: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0042: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42177
g0042: Grad overflow on iteration 42177
g0063: Grad overflow on iteration 42177
g0056: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0066: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42177
g0063: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0056: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42177
g0042: Grad overflow on iteration 42177
g0066: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0043: Grad overflow on iteration 42177
g0056: Grad overflow on iteration 42177
g0042: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0063: Grad overflow on iteration 42177
g0056: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0042: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42177
g0063: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42177
g0038: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0054: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0038: Grad overflow on iteration 42177
g0056: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42177
g0038: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42177
g0038: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42177
g0066: Grad overflow on iteration 42177
g0056: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0043: Grad overflow on iteration 42177
g0043: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0066: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0056: Grad overflow on iteration 42177
g0066: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0041: [2024-08-12 04:13:25,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0054: Grad overflow on iteration 42177
g0042: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0043: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42177
g0038: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0054: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42177
g0056: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0054: Grad overflow on iteration 42177
g0063: [2024-08-12 04:13:25,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0054: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0043: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0056: [2024-08-12 04:13:25,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0066: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0054: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0066: Grad overflow on iteration 42177
g0054: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0054: Grad overflow on iteration 42177
g0054: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42177
g0054: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0054: [2024-08-12 04:13:25,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0038: [2024-08-12 04:13:25,776] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42177
g0038: [2024-08-12 04:13:25,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0038: [2024-08-12 04:13:25,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0066: [2024-08-12 04:13:25,777] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0038: [2024-08-12 04:13:25,777] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1.0, reducing to 1
g0056: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42178
g0041: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42178
g0038: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42178
g0038: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42178
g0038: Grad overflow on iteration 42178
g0038: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42178
g0054: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42178
g0056: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42178
g0054: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42178
g0054: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42178
g0063: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42178
g0041: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42178
g0066: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42178
g0042: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42178
g0056: Grad overflow on iteration 42178
g0042: Grad overflow on iteration 42178
g0056: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42178
g0066: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42178
g0043: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42178
g0066: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42178
g0041: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42178
g0056: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42178
g0042: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42178
g0056: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42178
g0054: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42178
g0063: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42178
g0066: Grad overflow on iteration 42178
g0063: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42178
g0042: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42178
g0063: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42178
g0038: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42178
g0066: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42178
g0066: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:29,837] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:29,836] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:29,837] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0043: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42179
g0043: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42179
g0056: Grad overflow on iteration 42179
g0054: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42179
g0043: Grad overflow on iteration 42179
g0041: Grad overflow on iteration 42179
g0054: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42179
g0043: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42179
g0038: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42179
g0063: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42179
g0054: Grad overflow on iteration 42179
g0042: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42179
g0056: Grad overflow on iteration 42179
g0042: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42179
g0042: Grad overflow on iteration 42179
g0056: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42179
g0041: Grad overflow on iteration 42179
g0038: Grad overflow on iteration 42179
g0043: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42179
g0066: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42179
g0042: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42179
g0042: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42179
g0042: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42179
g0063: Grad overflow on iteration 42179
g0066: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42179
g0038: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42179
g0066: Grad overflow on iteration 42179
g0063: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42179
g0066: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42179
g0066: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42179
g0038: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42179
g0038: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:33,968] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42179
g0041: [2024-08-12 04:13:33,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:33,969] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0038: [2024-08-12 04:13:33,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=42180, skipped=79, lr=[0.0001996389428080553, 0.0001996389428080553], mom=[(0.9, 0.95), (0.9, 0.95)]
g0066: [2024-08-12 04:13:33,969] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: steps: 42180 loss: nan iter time (s): 4.226 samples/sec: 30.289
g0066:  iteration    42180/   52000 | consumed samples:      5399040 | consumed tokens:  11057233920 | elapsed time per iteration (ms): 4258.5 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 1.993 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.057 | tokens per gpu per second (tgs): 1923.679 | TFLOPs: 15.48 |
g0066: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42182
g0066: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42182
g0054: Grad overflow on iteration 42182
g0066: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42182
g0054: Grad overflow on iteration 42182
g0041: Grad overflow on iteration 42182
g0063: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42182
g0038: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42182
g0054: Grad overflow on iteration 42182
g0038: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42182
g0038: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42182
g0041: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42182
g0041: Grad overflow on iteration 42182
g0043: Grad overflow on iteration 42182
g0066: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42182
g0041: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42182
g0054: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42182
g0056: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42182
g0056: Grad overflow on iteration 42182
g0042: Grad overflow on iteration 42182
g0041: Grad overflow on iteration 42182
g0043: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42182
g0042: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42182
g0042: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42182
g0056: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42182
g0042: Grad overflow on iteration 42182
g0063: Grad overflow on iteration 42182
g0043: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42182
g0063: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42182
g0056: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42182
g0042: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42182
g0038: [2024-08-12 04:13:46,317] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:46,317] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42182
g0063: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:46,316] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:46,317] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:46,317] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0038: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42184
g0054: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42184
g0054: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42184
g0038: Grad overflow on iteration 42184
g0056: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42184
g0056: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42184
g0056: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42184
g0056: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42184
g0042: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42184
g0042: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42184
g0054: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42184
g0041: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42184
g0041: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42184
g0041: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42184
g0042: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42184
g0063: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42184
g0063: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42184
g0063: Grad overflow on iteration 42184
g0043: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42184
g0063: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42184
g0041: Grad overflow on iteration 42184
g0041: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42184
g0043: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42184
g0043: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42184
g0043: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42184
g0038: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42184
g0038: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42184
g0066: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42184
g0063: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42184
g0066: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42184
g0063: [2024-08-12 04:13:54,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:54,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42184
g0056: [2024-08-12 04:13:54,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:54,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:54,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:54,029] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0038: [2024-08-12 04:13:54,028] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42184
g0038: [2024-08-12 04:13:54,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:13:54,029] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42185
g0041: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42185
g0056: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42185
g0054: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42185
g0041: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42185
g0038: Grad overflow on iteration 42185
g0056: Grad overflow on iteration 42185
g0041: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42185
g0054: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42185
g0066: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42185
g0066: Grad overflow on iteration 42185
g0043: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42185
g0042: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42185
g0042: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42185
g0041: Grad overflow on iteration 42185
g0038: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42185
g0063: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42185
g0038: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42185
g0054: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42185
g0063: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42185
g0042: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42185
g0043: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42185
g0056: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42185
g0043: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42185
g0066: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42185
g0066: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42185
g0063: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42185
g0066: Grad overflow on iteration 42185
g0038: Grad overflow on iteration 42185
g0038: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42185
g0038: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:58,215] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:13:58,215] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0063: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42185
g0066: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42185
g0063: [2024-08-12 04:13:58,215] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:13:58,214] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42186
g0041: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42186
g0041: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42186
g0038: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42186
g0041: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42186
g0054: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42186
g0042: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42186
g0054: Grad overflow on iteration 42186
g0066: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42186
g0056: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42186
g0043: Grad overflow on iteration 42186
g0056: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42186
g0043: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42186
g0041: Grad overflow on iteration 42186
g0063: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42186
g0054: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42186
g0054: Grad overflow on iteration 42186
g0038: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42186
g0066: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42186
g0063: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42186
g0042: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42186
g0063: Grad overflow on iteration 42186
g0042: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42186
g0043: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:14:02,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42186
g0054: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42186
g0056: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42186
g0038: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42186
g0042: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42186
g0054: Grad overflow on iteration 42186
g0043: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42186
g0041: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42186
g0063: [2024-08-12 04:14:02,442] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:14:02,442] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42186
g0041: [2024-08-12 04:14:02,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:02,442] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:02,442] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:02,442] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0066: [2024-08-12 04:14:02,442] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42187
g0038: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42187
g0038: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42187
g0041: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42187
g0056: Grad overflow on iteration 42187
g0042: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42187
g0041: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42187
g0054: Grad overflow on iteration 42187
g0054: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42187
g0042: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42187
g0063: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42187
g0043: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42187
g0054: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42187
g0056: Grad overflow on iteration 42187
g0041: Grad overflow on iteration 42187
g0041: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42187
g0056: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42187
g0054: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42187
g0041: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42187
g0066: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42187
g0054: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42187
g0043: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42187
g0042: Grad overflow on iteration 42187
g0041: Grad overflow on iteration 42187
g0066: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42187
g0043: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42187
g0066: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42187
g0041: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42187
g0043: Grad overflow on iteration 42187
g0063: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42187
g0043: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42187
g0042: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42187
g0042: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:14:06,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:06,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:06,748] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0041: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42189
g0041: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42189
g0041: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42189
g0056: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42189
g0056: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42189
g0056: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42189
g0042: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42189
g0041: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42189
g0041: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42189
g0042: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42189
g0042: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42189
g0042: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42189
g0066: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42189
g0066: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42189
g0056: Grad overflow on iteration 42189
g0066: Grad overflow on iteration 42189
g0063: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42189
g0063: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42189
g0043: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42189
g0042: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42189
g0063: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42189
g0066: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42189
g0063: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42189
g0043: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42189
g0038: Grad overflow on iteration 42189
g0054: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42189
g0054: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42189
g0043: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42189
g0054: Grad overflow on iteration 42189
g0038: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42189
g0054: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42189
g0038: [2024-08-12 04:14:15,184] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42189
g0038: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:15,185] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0066: [2024-08-12 04:14:15,185] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:15,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=42190, skipped=85, lr=[0.00019963866095719441, 0.00019963866095719441], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42190 loss: nan iter time (s): 4.089 samples/sec: 31.304
g0066:  iteration    42190/   52000 | consumed samples:      5400320 | consumed tokens:  11059855360 | elapsed time per iteration (ms): 4121.6 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 2.412 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.056 | tokens per gpu per second (tgs): 1987.554 | TFLOPs: 15.99 |
g0041: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42195
g0041: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42195
g0041: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42195
g0041: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42195
g0054: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42195
g0054: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42195
g0054: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42195
g0054: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42195
g0066: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42195
g0042: Grad overflow on iteration 42195
g0042: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42195
g0042: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42195
g0066: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42195
g0056: Grad overflow on iteration 42195
g0042: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42195
g0038: Grad overflow on iteration 42195
g0041: [2024-08-12 04:14:40,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42195
g0038: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42195
g0056: Grad overflow on iteration 42195
g0063: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42195
g0038: Grad overflow on iteration 42195
g0056: Grad overflow on iteration 42195
g0056: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42195
g0056: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42195
g0042: Grad overflow on iteration 42195
g0043: Grad overflow on iteration 42195
g0042: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42195
g0043: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42195
g0038: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42195
g0063: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42195
g0063: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42195
g0063: [2024-08-12 04:14:40,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:14:40,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42195
g0043: [2024-08-12 04:14:40,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:14:40,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:40,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:14:40,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0038: [2024-08-12 04:14:56,593] [INFO] [logging.py:96:log_dist] [Rank 0] step=42200, skipped=86, lr=[0.0001996384024974779, 0.0001996384024974779], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42200 loss: 6.6940 iter time (s): 4.109 samples/sec: 31.150
g0066:  iteration    42200/   52000 | consumed samples:      5401600 | consumed tokens:  11062476800 | elapsed time per iteration (ms): 4141.7 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 2.378 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.905 | tokens per gpu per second (tgs): 1977.915 | TFLOPs: 15.92 |
g0042: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42201
g0042: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42201
g0042: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42201
g0042: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42201
g0042: Grad overflow on iteration 42201
g0043: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42201
g0054: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42201
g0043: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42201
g0042: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42201
g0041: Grad overflow on iteration 42201
g0054: Grad overflow on iteration 42201
g0054: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42201
g0043: Grad overflow on iteration 42201
g0066: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42201
g0054: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42201
g0043: Grad overflow on iteration 42201
g0056: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42201
g0066: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42201
g0066: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42201
g0066: Grad overflow on iteration 42201
g0041: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42201
g0038: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42201
g0038: Grad overflow on iteration 42201
g0041: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42201
g0043: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42201
g0063: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42201
g0066: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42201
g0056: Grad overflow on iteration 42201
g0043: Grad overflow on iteration 42201
g0056: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42201
g0038: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:04,661] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:15:04,661] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42201
g0038: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42201
g0038: [2024-08-12 04:15:04,660] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:04,661] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:04,661] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:04,661] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:04,661] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:04,661] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0038: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42202
g0038: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42202
g0038: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42202
g0041: Grad overflow on iteration 42202
g0054: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42202
g0054: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42202
g0038: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42202
g0054: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42202
g0042: Grad overflow on iteration 42202
g0038: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42202
g0066: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42202
g0054: Grad overflow on iteration 42202
g0063: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42202
g0054: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42202
g0056: Grad overflow on iteration 42202
g0041: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42202
g0063: Grad overflow on iteration 42202
g0041: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42202
g0063: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42202
g0054: Grad overflow on iteration 42202
g0063: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42202
g0056: Grad overflow on iteration 42202
g0043: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42202
g0063: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42202
g0041: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42202
g0041: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42202
g0041: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42202
g0043: Grad overflow on iteration 42202
g0066: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42202
g0066: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42202
g0038: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42202
g0066: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42202
g0042: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:08,756] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:08,757] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0054: [2024-08-12 04:15:08,757] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:08,757] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42206
g0043: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42206
g0056: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42206
g0041: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42206
g0042: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42206
g0043: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42206
g0054: Grad overflow on iteration 42206
g0056: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42206
g0054: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42206
g0054: Grad overflow on iteration 42206
g0066: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42206
g0043: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42206
g0043: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42206
g0043: Grad overflow on iteration 42206
g0066: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42206
g0043: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42206
g0043: Grad overflow on iteration 42206
g0038: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42206
g0056: Grad overflow on iteration 42206
g0038: Grad overflow on iteration 42206
g0042: Grad overflow on iteration 42206
g0038: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42206
g0063: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42206
g0054: Grad overflow on iteration 42206
g0038: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42206
g0054: Grad overflow on iteration 42206
g0038: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42206
g0042: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42206
g0056: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42206
g0056: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42206
g0056: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42206
g0041: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:25,335] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:25,334] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42206
g0038: [2024-08-12 04:15:25,335] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0066: [2024-08-12 04:15:25,335] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42207
g0054: Grad overflow on iteration 42207
g0054: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42207
g0054: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42207
g0054: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42207
g0054: Grad overflow on iteration 42207
g0041: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42207
g0041: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42207
g0041: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:29,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:29,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42207
g0038: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42207
g0038: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42207
g0042: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42207
g0042: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42207
g0066: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42207
g0042: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42207
g0042: Grad overflow on iteration 42207
g0042: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42207
g0063: Grad overflow on iteration 42207
g0056: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42207
g0043: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42207
g0043: Grad overflow on iteration 42207
g0056: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42207
g0066: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42207
g0063: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42207
g0056: Grad overflow on iteration 42207
g0043: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42207
g0043: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42207
g0063: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42207
g0066: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42207
g0038: Grad overflow on iteration 42207
g0043: Grad overflow on iteration 42207
g0038: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42207
g0043: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:29,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:29,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:29,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:29,396] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0056: [2024-08-12 04:15:29,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:29,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:29,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:37,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=42210, skipped=90, lr=[0.00019963816745406093, 0.00019963816745406093], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42210 loss: 6.7210 iter time (s): 4.072 samples/sec: 31.434
g0066:  iteration    42210/   52000 | consumed samples:      5402880 | consumed tokens:  11065098240 | elapsed time per iteration (ms): 4104.6 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 2.278 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.185 | tokens per gpu per second (tgs): 1995.810 | TFLOPs: 16.06 |
g0063: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42212
g0063: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42212
g0042: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42212
g0041: Grad overflow on iteration 42212
g0054: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42212
g0043: Grad overflow on iteration 42212
g0054: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42212
g0063: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42212
g0054: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42212
g0063: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42212
g0043: Grad overflow on iteration 42212
g0042: Grad overflow on iteration 42212
g0056: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42212
g0042: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42212
g0043: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42212
g0041: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42212
g0041: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42212
g0038: Grad overflow on iteration 42212
g0041: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42212
g0054: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42212
g0063: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42212
g0063: Grad overflow on iteration 42212
g0066: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42212
g0038: Grad overflow on iteration 42212
g0043: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42212
g0056: Grad overflow on iteration 42212
g0038: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42212
g0066: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42212
g0056: Grad overflow on iteration 42212
g0066: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42212
g0066: Grad overflow on iteration 42212
g0066: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42212
g0038: [2024-08-12 04:15:50,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:50,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:50,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:50,327] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0066: [2024-08-12 04:15:50,327] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42213
g0056: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42213
g0042: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42213
g0042: Grad overflow on iteration 42213
g0041: Grad overflow on iteration 42213
g0041: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42213
g0043: Grad overflow on iteration 42213
g0056: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42213
g0063: Grad overflow on iteration 42213
g0038: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42213
g0063: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42213
g0038: Grad overflow on iteration 42213
g0056: Grad overflow on iteration 42213
g0043: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42213
g0043: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42213
g0054: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42213
g0054: Grad overflow on iteration 42213
g0043: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42213
g0041: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42213
g0063: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42213
g0063: Grad overflow on iteration 42213
g0054: Grad overflow on iteration 42213
g0066: Grad overflow on iteration 42213
g0042: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42213
g0066: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42213
g0066: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42213
g0038: Grad overflow on iteration 42213
g0054: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42213
g0054: Grad overflow on iteration 42213
g0042: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42213
g0056: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42213
g0054: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42213
g0066: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:54,513] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:15:54,513] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0041: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42215
g0043: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42215
g0054: Grad overflow on iteration 42215
g0063: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42215
g0056: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42215
g0054: Grad overflow on iteration 42215
g0041: Grad overflow on iteration 42215
g0063: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42215
g0056: Grad overflow on iteration 42215
g0043: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42215
g0056: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42215
g0054: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42215
g0042: Grad overflow on iteration 42215
g0042: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42215
g0042: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42215
g0054: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42215
g0056: Grad overflow on iteration 42215
g0056: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42215
g0038: Grad overflow on iteration 42215
g0056: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42215
g0054: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42215
g0063: Grad overflow on iteration 42215
g0066: Grad overflow on iteration 42215
g0063: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42215
g0054: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42215
g0042: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42215
g0066: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42215
g0066: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42215
g0042: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42215
g0041: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42215
g0038: Grad overflow on iteration 42215
g0043: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42215
g0066: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:03,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:16:03,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:03,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0041: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42217
g0041: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42217
g0041: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42217
g0041: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42217
g0054: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42217
g0042: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42217
g0054: Grad overflow on iteration 42217
g0043: Grad overflow on iteration 42217
g0054: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42217
g0056: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42217
g0063: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42217
g0038: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42217
g0066: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42217
g0038: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42217
g0038: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42217
g0038: Grad overflow on iteration 42217
g0038: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:16:10,942] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42217
g0054: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42217
g0066: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42217
g0043: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42217
g0043: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42217
g0056: Grad overflow on iteration 42217
g0056: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42217
g0056: Grad overflow on iteration 42217
g0063: Grad overflow on iteration 42217
g0066: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42217
g0054: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42217
g0066: Grad overflow on iteration 42217
g0042: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42217
g0063: Grad overflow on iteration 42217
g0042: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:16:10,941] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42217
g0054: [2024-08-12 04:16:10,942] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42217
g0038: [2024-08-12 04:16:10,942] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0063: [2024-08-12 04:16:10,942] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:16:10,942] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:16:10,942] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:10,942] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42218
g0041: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42218
g0041: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42218
g0041: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42218
g0041: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42218
g0054: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42218
g0054: Grad overflow on iteration 42218
g0063: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42218
g0066: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42218
g0063: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42218
g0054: Grad overflow on iteration 42218
g0063: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42218
g0042: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42218
g0056: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42218
g0056: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42218
g0056: Grad overflow on iteration 42218
g0038: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42218
g0054: Grad overflow on iteration 42218
g0066: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42218
g0043: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42218
g0056: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42218
g0043: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42218
g0054: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42218
g0056: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42218
g0043: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42218
g0042: Grad overflow on iteration 42218
g0063: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42218
g0063: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42218
g0042: Grad overflow on iteration 42218
g0038: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:16:15,115] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42218
g0038: Grad overflow on iteration 42218
g0038: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42218
g0038: [2024-08-12 04:16:15,115] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0042: [2024-08-12 04:16:15,115] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:16:15,115] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:15,115] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:19,230] [INFO] [logging.py:96:log_dist] [Rank 0] step=42220, skipped=95, lr=[0.0001996379323344172, 0.0001996379323344172], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42220 loss: 6.9149 iter time (s): 4.126 samples/sec: 31.020
g0066:  iteration    42220/   52000 | consumed samples:      5404160 | consumed tokens:  11067719680 | elapsed time per iteration (ms): 4159.1 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 2.113 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.776 | tokens per gpu per second (tgs): 1969.670 | TFLOPs: 15.85 |
g0063: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42226
g0063: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42226
g0043: Grad overflow on iteration 42226
g0043: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42226
g0043: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42226
g0043: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42226
g0043: Grad overflow on iteration 42226
g0041: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42226
g0042: Grad overflow on iteration 42226
g0041: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42226
g0041: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42226
g0038: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42226
g0042: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42226
g0054: Grad overflow on iteration 42226
g0042: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42226
g0038: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42226
g0054: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42226
g0066: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42226
g0063: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42226
g0063: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42226
g0054: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42226
g0054: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42226
g0043: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42226
g0038: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42226
g0042: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42226
g0041: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:16:49,009] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42226
g0056: Grad overflow on iteration 42226
g0041: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42226
g0041: Grad overflow on iteration 42226
g0056: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42226
g0056: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42226
g0038: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42226
g0066: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:16:49,010] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0041: [2024-08-12 04:16:49,010] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42229
g0043: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42229
g0043: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42229
g0054: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42229
g0054: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42229
g0041: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42229
g0041: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42229
g0066: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42229
g0054: Grad overflow on iteration 42229
g0066: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42229
g0042: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42229
g0041: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: Grad overflow on iteration 42229
g0038: Grad overflow on iteration 42229
g0043: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42229
g0038: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42229
g0063: Grad overflow on iteration 42229
g0056: [2024-08-12 04:17:01,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42229
g0056: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42229
g0041: Grad overflow on iteration 42229
g0056: Grad overflow on iteration 42229
g0042: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42229
g0056: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42229
g0056: Grad overflow on iteration 42229
g0042: Grad overflow on iteration 42229
g0042: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42229
g0063: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42229
g0038: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42229
g0038: Grad overflow on iteration 42229
g0038: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42229
g0066: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42229
g0066: Grad overflow on iteration 42229
g0066: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42229
g0043: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:01,566] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:01,567] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0063: [2024-08-12 04:17:01,567] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:01,567] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:01,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=42230, skipped=97, lr=[0.0001996377206615641, 0.0001996377206615641], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42230 loss: nan iter time (s): 4.200 samples/sec: 30.476
g0066:  iteration    42230/   52000 | consumed samples:      5405440 | consumed tokens:  11070341120 | elapsed time per iteration (ms): 4232.7 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 2.353 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.241 | tokens per gpu per second (tgs): 1935.403 | TFLOPs: 15.57 |
g0041: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42232
g0041: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42232
g0041: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42232
g0041: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42232
g0054: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42232
g0041: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42232
g0054: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42232
g0054: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42232
g0041: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42232
g0054: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42232
g0043: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42232
g0043: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42232
g0038: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42232
g0043: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42232
g0043: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42232
g0056: Grad overflow on iteration 42232
g0063: Grad overflow on iteration 42232
g0054: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42232
g0066: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42232
g0056: Grad overflow on iteration 42232
g0042: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42232
g0042: Grad overflow on iteration 42232
g0038: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42232
g0042: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42232
g0056: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42232
g0066: Grad overflow on iteration 42232
g0056: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42232
g0042: Grad overflow on iteration 42232
g0056: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42232
g0056: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42232
g0042: Grad overflow on iteration 42232
g0042: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:13,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42232
g0038: [2024-08-12 04:17:13,974] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0066: [2024-08-12 04:17:13,974] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42238
g0041: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42238
g0041: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42238
g0041: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42238
g0054: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42238
g0041: Grad overflow on iteration 42238
g0042: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42238
g0054: Grad overflow on iteration 42238
g0054: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42238
g0054: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: Grad overflow on iteration 42238
g0043: Grad overflow on iteration 42238
g0042: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42238
g0043: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42238
g0042: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42238
g0056: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42238
g0038: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42238
g0056: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42238
g0056: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42238
g0038: Grad overflow on iteration 42238
g0063: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42238
g0038: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42238
g0056: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42238
g0038: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42238
g0038: Grad overflow on iteration 42238
g0043: Grad overflow on iteration 42238
g0063: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42238
g0043: Grad overflow on iteration 42238
g0063: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42238
g0038: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42238
g0066: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:39,600] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42238
g0066: Grad overflow on iteration 42238
g0063: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42238
g0038: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:39,601] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0056: [2024-08-12 04:17:39,601] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:17:39,602] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:17:43,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=42240, skipped=99, lr=[0.00019963746186645013, 0.00019963746186645013], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42240 loss: 7.2189 iter time (s): 4.181 samples/sec: 30.611
g0066:  iteration    42240/   52000 | consumed samples:      5406720 | consumed tokens:  11072962560 | elapsed time per iteration (ms): 4214.2 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 2.094 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.374 | tokens per gpu per second (tgs): 1943.924 | TFLOPs: 15.64 |
g0054: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42249
g0054: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42249
g0043: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42249
g0043: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42249
g0041: Grad overflow on iteration 42249
g0054: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: Grad overflow on iteration 42249
g0054: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: Grad overflow on iteration 42249
g0043: Grad overflow on iteration 42249
g0043: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0043: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: Grad overflow on iteration 42249
g0042: Grad overflow on iteration 42249
g0041: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42249
g0054: Grad overflow on iteration 42249
g0041: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0054: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42249
g0041: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: Grad overflow on iteration 42249
g0042: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0041: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42249
g0063: Grad overflow on iteration 42249
g0042: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: Grad overflow on iteration 42249
g0038: Grad overflow on iteration 42249
g0056: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0054: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: Grad overflow on iteration 42249
g0042: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: Grad overflow on iteration 42249
g0066: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0042: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42249
g0042: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0042: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42249
g0063: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0063: Grad overflow on iteration 42249
g0063: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: Grad overflow on iteration 42249
g0066: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0063: Grad overflow on iteration 42249
g0038: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: Grad overflow on iteration 42249
g0063: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0041: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0043: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42249
g0038: Grad overflow on iteration 42249
g0038: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0038: Grad overflow on iteration 42249
g0066: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42249
g0066: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0066: Grad overflow on iteration 42249
g0056: [2024-08-12 04:18:25,141] [INFO] [fused_optimizer.py:344:_update_scale] 
g0056: Grad overflow on iteration 42249
g0063: [2024-08-12 04:18:25,142] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0056: [2024-08-12 04:18:25,142] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0066: [2024-08-12 04:18:25,142] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:18:25,142] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0038: [2024-08-12 04:18:25,142] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0038: [2024-08-12 04:18:25,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=42250, skipped=100, lr=[0.00019963725005638956, 0.00019963725005638956], mom=[(0.9, 0.95), (0.9, 0.95)]
g0038: steps: 42250 loss: nan iter time (s): 4.111 samples/sec: 31.137
g0066:  iteration    42250/   52000 | consumed samples:      5408000 | consumed tokens:  11075584000 | elapsed time per iteration (ms): 4143.4 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 2.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.893 | tokens per gpu per second (tgs): 1977.139 | TFLOPs: 15.91 |
