
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0214
    HostName g0214
    Port 2222
    StrictHostKeyChecking no

Host g0220
    HostName g0220
    Port 2222
    StrictHostKeyChecking no

Host g0225
    HostName g0225
    Port 2222
    StrictHostKeyChecking no

Host g0232
    HostName g0232
    Port 2222
    StrictHostKeyChecking no

Host g0233
    HostName g0233
    Port 2222
    StrictHostKeyChecking no

Host g0234
    HostName g0234
    Port 2222
    StrictHostKeyChecking no

Host g0235
    HostName g0235
    Port 2222
    StrictHostKeyChecking no

Host g0236
    HostName g0236
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42825012
g0214 slots=4
g0220 slots=4
g0225 slots=4
g0232 slots=4
g0233 slots=4
g0234 slots=4
g0235 slots=4
g0236 slots=4

[2024-08-10 08:08:44,954] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-10 08:08:50,591] [INFO] [runner.py:463:main] Using IP address of 10.1.7.10 for node g0214
[2024-08-10 08:08:50,593] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0214,g0220,g0225,g0232,g0233,g0234,g0235,g0236
[2024-08-10 08:08:50,593] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0214,g0220,g0225,g0232,g0233,g0234,g0235,g0236 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDIxNCI6IFswLCAxLCAyLCAzXSwgImcwMjIwIjogWzAsIDEsIDIsIDNdLCAiZzAyMjUiOiBbMCwgMSwgMiwgM10sICJnMDIzMiI6IFswLCAxLCAyLCAzXSwgImcwMjMzIjogWzAsIDEsIDIsIDNdLCAiZzAyMzQiOiBbMCwgMSwgMiwgM10sICJnMDIzNSI6IFswLCAxLCAyLCAzXSwgImcwMjM2IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.7.10 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '2621440000000' --train-samples '1280000000' --train-data-exact-num-epochs '1' --lr '2.0e-4' --min-lr '2.0e-6' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True' --wandb_tag 'other_gpu'
g0214: [2024-08-10 08:08:54,059] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0214: [2024-08-10 08:08:56,256] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0214: [2024-08-10 08:08:56,257] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0214': [0, 1, 2, 3], 'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0232': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3]}
g0214: [2024-08-10 08:08:56,257] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0214: [2024-08-10 08:08:56,257] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0214': [0, 1, 2, 3], 'g0220': [4, 5, 6, 7], 'g0225': [8, 9, 10, 11], 'g0232': [12, 13, 14, 15], 'g0233': [16, 17, 18, 19], 'g0234': [20, 21, 22, 23], 'g0235': [24, 25, 26, 27], 'g0236': [28, 29, 30, 31]})
g0214: [2024-08-10 08:08:56,257] [INFO] [launch.py:163:main] dist_world_size=32
g0214: [2024-08-10 08:08:56,257] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0214: [2024-08-10 08:08:59,355] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0214: [2024-08-10 08:08:59,355] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0214: [2024-08-10 08:08:59,419] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0214: [2024-08-10 08:08:59,475] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0233: [2024-08-10 08:09:01,590] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0225: [2024-08-10 08:09:01,592] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0236: [2024-08-10 08:09:01,611] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0235: [2024-08-10 08:09:01,987] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0234: [2024-08-10 08:09:02,060] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0220: [2024-08-10 08:09:02,162] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0232: [2024-08-10 08:09:02,355] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0214: --------------------------------------------------
g0214: DeepSpeed C++/CUDA extension op report
g0214: --------------------------------------------------
g0214: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0214:       runtime if needed. Op compatibility means that your system
g0214:       meet the required dependencies to JIT install the op.
g0214: --------------------------------------------------
g0214: JIT compiled ops requires ninja
g0214: --------------------------------------------------
g0214: DeepSpeed C++/CUDA extension op report
g0214: --------------------------------------------------
g0214: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0214:       runtime if needed. Op compatibility means that your system
g0214:       meet the required dependencies to JIT install the op.
g0214: --------------------------------------------------
g0214: JIT compiled ops requires ninja
g0214: --------------------------------------------------
g0214: DeepSpeed C++/CUDA extension op report
g0214: --------------------------------------------------
g0214: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0214:       runtime if needed. Op compatibility means that your system
g0214:       meet the required dependencies to JIT install the op.
g0214: --------------------------------------------------
g0214: JIT compiled ops requires ninja
g0214: --------------------------------------------------
g0214: DeepSpeed C++/CUDA extension op report
g0214: --------------------------------------------------
g0214: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0214:       runtime if needed. Op compatibility means that your system
g0214:       meet the required dependencies to JIT install the op.
g0214: --------------------------------------------------
g0214: JIT compiled ops requires ninja
g0214: ninjaninja  ....................................  [92m[OKAY][0m[92m[OKAY][0m
g0214: 
g0214: ----------------------------------------------------------------------------------------------------
g0214: 
g0214: op nameop name  ................................  installedinstalled  ....  compatiblecompatible
g0214: 
g0214: ----------------------------------------------------------------------------------------------------
g0214: 
g0214: ninja .................. [92m[OKAY][0m
g0214: --------------------------------------------------
g0214: op name ................ installed .. compatible
g0214: --------------------------------------------------
g0214: ninja .................. [92m[OKAY][0m
g0214: --------------------------------------------------
g0214: op name ................ installed .. compatible
g0214: --------------------------------------------------
g0214: async_io ............... [92m[YES][0m ......async_io [92m[OKAY][0m 
g0214: ............... [92m[YES][0m ...... [92m[OKAY][0mfused_adam
g0214:  ............. [92m[YES][0m ...... fused_adam[92m[OKAY][0m 
g0214: ............. [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0214: [92m[YES][0m ...... cpu_adam[92m[OKAY][0m 
g0214: ............... [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0m[92m[YES][0m
g0214:  ...... cpu_adagrad[92m[OKAY][0m 
g0214: ............ [92m[YES][0mcpu_lion  .....................  [92m[OKAY][0m[92m[YES][0m
g0214:  ...... [92m[OKAY][0mcpu_lion
g0214:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0214: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0214: evoformer_attn .........[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH [93m[NO][0m
g0214:  .......evoformer_attn  [93m[NO][0m.........
g0214:  [93m[NO][0m fused_lamb.......  [93m[NO][0m.............
g0214:  [92m[YES][0m fused_lamb......  .............[92m[OKAY][0m 
g0214: [92m[YES][0m ...... [92m[OKAY][0m
g0214: fused_lion ............. [92m[YES][0m fused_lion......  .............[92m[OKAY][0m 
g0214: [92m[YES][0m ...... [92m[OKAY][0m
g0214: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0214: async_iofused_adam ............. ...............  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0214: 
g0214: cpu_adam ............... fused_adam[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0214: ...... [92m[OKAY][0mcpu_adagrad
g0214:  ............ [92m[YES][0mcpu_adam  .....................  [92m[OKAY][0m[92m[YES][0m
g0214:  ...... cpu_lion[92m[OKAY][0m 
g0214: ............... [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0m[92m[YES][0m
g0214:  ...... [92m[OKAY][0m
g0214: cpu_lion [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH...............
g0214:  [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0214:  ....... [93m[NO][0m
g0214: fused_lamb [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH.............
g0214:  [92m[YES][0m evoformer_attn......  .........[92m[OKAY][0m 
g0214: [93m[NO][0m ....... [93m[NO][0m
g0214: fused_lamb fused_lion.............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0214: [92m[OKAY][0m
g0214: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m
g0214: [92m[OKAY][0m
g0214: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0214: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0214: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0214: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0214: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0214: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0214: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0214: ragged_device_opsragged_device_ops  ............  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0214: 
g0214: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0214: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0214: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0214: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0214: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0214: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0214: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0214: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0214: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0214: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0214: sparse_attn [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0............
g0214:  [93m[NO][0m ....... [93m[NO][0m
g0214: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0214: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0214: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0214: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0214: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0214: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0214: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0214: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0214: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0214: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0214: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0214: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0214: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0214: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0214: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0214: --------------------------------------------------
g0214: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0214: --------------------------------------------------
g0214: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0214: --------------------------------------------------
g0214: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0214: --------------------------------------------------
g0214: DeepSpeed general environment info:
g0214: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0214: torch version .................... 2.0.1+cu118
g0214: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0214: deepspeed info ................... 0.12.4, unknown, unknown
g0214: torch cuda version ............... 11.8
g0214: torch hip version ................ None
g0214: nvcc version ..................... 11.8
g0214: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0214: shared memory (/dev/shm) size .... 188.13 GB
g0214: DeepSpeed general environment info:
g0214: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0214: torch version .................... 2.0.1+cu118
g0214: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0214: deepspeed info ................... 0.12.4, unknown, unknown
g0214: torch cuda version ............... 11.8
g0214: torch hip version ................ None
g0214: nvcc version ..................... 11.8
g0214: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0214: shared memory (/dev/shm) size .... 188.13 GB
g0214: DeepSpeed general environment info:
g0214: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0214: torch version .................... 2.0.1+cu118
g0214: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0214: deepspeed info ................... 0.12.4, unknown, unknown
g0214: torch cuda version ............... 11.8
g0214: torch hip version ................ None
g0214: nvcc version ..................... 11.8
g0214: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0214: shared memory (/dev/shm) size .... 188.13 GB
g0214: DeepSpeed general environment info:
g0214: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0214: torch version .................... 2.0.1+cu118
g0214: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0214: deepspeed info ................... 0.12.4, unknown, unknown
g0214: torch cuda version ............... 11.8
g0214: torch hip version ................ None
g0214: nvcc version ..................... 11.8
g0214: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0214: shared memory (/dev/shm) size .... 188.13 GB
g0214: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0214: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0214: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0214: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0214: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0214: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0214: using torch.float32 for parameters ...
g0214: ------------------------ arguments ------------------------
g0214:   accumulate_allreduce_grads_in_fp32 .............. False
g0214:   adam_beta1 ...................................... 0.9
g0214:   adam_beta2 ...................................... 0.95
g0214:   adam_eps ........................................ 1e-08
g0214:   add_bias_linear ................................. False
g0214:   add_position_embedding .......................... False
g0214:   adlr_autoresume ................................. False
g0214:   adlr_autoresume_interval ........................ 1000
g0214:   aml_data_download_path .......................... None
g0214:   apply_layernorm_1p .............................. False
g0214:   apply_query_key_layer_scaling ................... False
g0214:   apply_residual_connection_post_layernorm ........ False
g0214:   async_tensor_model_parallel_allreduce ........... False
g0214:   attention_dropout ............................... 0.0
g0214:   attention_softmax_in_fp32 ....................... False
g0214:   barrier_with_L1_time ............................ True
g0214:   bert_binary_head ................................ True
g0214:   bert_embedder_type .............................. megatron
g0214:   bert_load ....................................... None
g0214:   bf16 ............................................ False
g0214:   bias_dropout_fusion ............................. True
g0214:   bias_gelu_fusion ................................ False
g0214:   biencoder_projection_dim ........................ 0
g0214:   biencoder_shared_query_context_model ............ False
g0214:   block_data_path ................................. None
g0214:   checkpoint_activations .......................... False
g0214:   checkpoint_in_cpu ............................... False
g0214:   checkpoint_num_layers ........................... 1
g0214:   classes_fraction ................................ 1.0
g0214:   clip_grad ....................................... 1.0
g0214:   compression_training ............................ False
g0214:   consumed_train_samples .......................... 0
g0214:   consumed_train_tokens ........................... 0
g0214:   consumed_valid_samples .......................... 0
g0214:   contigious_checkpointing ........................ False
g0214:   cpu_optimizer ................................... False
g0214:   cpu_torch_adam .................................. False
g0214:   create_moe_param_group .......................... False
g0214:   curriculum_learning_legacy ...................... False
g0214:   data_cache_path ................................. None
g0214:   data_efficiency_curriculum_learning ............. False
g0214:   data_impl ....................................... mmap
g0214:   data_parallel_random_init ....................... False
g0214:   data_parallel_size .............................. 4
g0214:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_text_document']
g0214:   data_per_class_fraction ......................... 1.0
g0214:   data_sharding ................................... True
g0214:   dataloader_type ................................. single
g0214:   DDP_impl ........................................ local
g0214:   decoder_num_layers .............................. None
g0214:   decoder_seq_length .............................. None
g0214:   deepscale ....................................... False
g0214:   deepscale_config ................................ None
g0214:   deepspeed ....................................... True
g0214:   deepspeed_activation_checkpointing .............. False
g0214:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0214:   deepspeed_mpi ................................... False
g0214:   dino_bottleneck_size ............................ 256
g0214:   dino_freeze_last_layer .......................... 1
g0214:   dino_head_hidden_size ........................... 2048
g0214:   dino_local_crops_number ......................... 10
g0214:   dino_local_img_size ............................. 96
g0214:   dino_norm_last_layer ............................ False
g0214:   dino_teacher_temp ............................... 0.07
g0214:   dino_warmup_teacher_temp ........................ 0.04
g0214:   dino_warmup_teacher_temp_epochs ................. 30
g0214:   distribute_checkpointed_activations ............. False
g0214:   distribute_saved_activations .................... False
g0214:   distributed_backend ............................. nccl
g0214:   distributed_timeout_minutes ..................... 10
g0214:   ds_fused_adam ................................... False
g0214:   ds_inference .................................... False
g0214:   ds_pipeline_enabled ............................. True
g0214:   ds_sequence_parallel_size ....................... 1
g0214:   embedding_path .................................. None
g0214:   embedding_weights_in_fp32 ....................... False
g0214:   empty_unused_memory_level ....................... 0
g0214:   enable_expert_tensor_parallelism ................ False
g0214:   encoder_num_layers .............................. 22
g0214:   encoder_seq_length .............................. 2048
g0214:   end_weight_decay ................................ 0.1
g0214:   eod_mask_loss ................................... False
g0214:   eval_interval ................................... 1000
g0214:   eval_iters ...................................... 100
g0214:   evidence_data_path .............................. None
g0214:   exit_duration_in_mins ........................... 30000000
g0214:   exit_interval ................................... None
g0214:   exit_on_missing_checkpoint ...................... False
g0214:   exit_signal_handler ............................. False
g0214:   expert_interval ................................. 2
g0214:   ffn_hidden_size ................................. 5632
g0214:   finetune ........................................ False
g0214:   force_ds_sequence_parallel ...................... False
g0214:   fp16 ............................................ False
g0214:   fp16_lm_cross_entropy ........................... False
g0214:   fp32_residual_connection ........................ False
g0214:   fp8_amax_compute_algo ........................... most_recent
g0214:   fp8_amax_history_len ............................ 1
g0214:   fp8_e4m3 ........................................ False
g0214:   fp8_hybrid ...................................... False
g0214:   fp8_interval .................................... 1
g0214:   fp8_margin ...................................... 0
g0214:   fp8_wgrad ....................................... True
g0214:   global_batch_size ............................... 128
g0214:   gradient_accumulation_fusion .................... True
g0214:   head_lr_mult .................................... 1.0
g0214:   hidden_dropout .................................. 0.0
g0214:   hidden_size ..................................... 2048
g0214:   hidden_size_teacher ............................. None
g0214:   hysteresis ...................................... 2
g0214:   ict_head_size ................................... None
g0214:   ict_load ........................................ None
g0214:   img_h ........................................... 224
g0214:   img_w ........................................... 224
g0214:   indexer_batch_size .............................. 128
g0214:   indexer_log_interval ............................ 1000
g0214:   inference ....................................... False
g0214:   inference_batch_times_seqlen_threshold .......... 512
g0214:   init_method_std ................................. 0.013
g0214:   init_method_xavier_uniform ...................... False
g0214:   initial_loss_scale .............................. 4294967296
g0214:   iter_per_epoch .................................. 1250
g0214:   kd .............................................. False
g0214:   kd_alpha_ce ..................................... 1
g0214:   kd_beta_ce ...................................... 1
g0214:   kd_temp ......................................... 1.0
g0214:   kv_channels ..................................... 128
g0214:   layernorm_epsilon ............................... 1e-05
g0214:   lazy_mpu_init ................................... None
g0214:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214:   load_teacher .................................... None
g0214:   local_rank ...................................... 0
g0214:   log_batch_size_to_tensorboard ................... True
g0214:   log_interval .................................... 10
g0214:   log_learning_rate_to_tensorboard ................ True
g0214:   log_loss_scale_to_tensorboard ................... True
g0214:   log_memory_to_tensorboard ....................... False
g0214:   log_num_zeros_in_grad ........................... False
g0214:   log_optimizer_states_to_tensorboard ............. True
g0214:   log_params_norm ................................. False
g0214:   log_timers_to_tensorboard ....................... True
g0214:   log_validation_ppl_to_tensorboard ............... True
g0214:   log_world_size_to_tensorboard ................... False
g0214:   loss_scale ...................................... None
g0214:   loss_scale_window ............................... 1000
g0214:   lr .............................................. 0.0002
g0214:   lr_decay_iters .................................. None
g0214:   lr_decay_samples ................................ None
g0214:   lr_decay_style .................................. cosine
g0214:   lr_decay_tokens ................................. 300000000000
g0214:   lr_warmup_fraction .............................. None
g0214:   lr_warmup_iters ................................. 0
g0214:   lr_warmup_samples ............................... 0
g0214:   lr_warmup_tokens ................................ 3000000000
g0214:   make_vocab_size_divisible_by .................... 128
g0214:   mask_factor ..................................... 1.0
g0214:   mask_prob ....................................... 0.15
g0214:   mask_type ....................................... random
g0214:   masked_softmax_fusion ........................... True
g0214:   max_position_embeddings ......................... 2048
g0214:   max_tokens_to_oom ............................... 12000
g0214:   mem_efficient_ln ................................ True
g0214:   memory_centric_tiled_linear ..................... False
g0214:   merge_file ...................................... None
g0214:   micro_batch_size ................................ 1
g0214:   min_loss_scale .................................. 1.0
g0214:   min_lr .......................................... 2e-06
g0214:   mlp_type ........................................ standard
g0214:   mmap_warmup ..................................... False
g0214:   moe_eval_capacity_factor ........................ 1.0
g0214:   moe_expert_parallel_size ........................ 1
g0214:   moe_loss_coeff .................................. 0.1
g0214:   moe_min_capacity ................................ 4
g0214:   moe_token_dropping .............................. True
g0214:   moe_train_capacity_factor ....................... 1.0
g0214:   mos ............................................. False
g0214:   no_load_lr_state ................................ False
g0214:   no_load_optim ................................... None
g0214:   no_load_rng ..................................... None
g0214:   no_persist_layer_norm ........................... False
g0214:   no_pipeline_parallel ............................ False
g0214:   no_save_optim ................................... None
g0214:   no_save_rng ..................................... None
g0214:   normalization ................................... rmsnorm
g0214:   num_attention_heads ............................. 16
g0214:   num_attention_heads_teacher ..................... None
g0214:   num_channels .................................... 3
g0214:   num_classes ..................................... 1000
g0214:   num_experts ..................................... [1]
g0214:   num_experts_switch .............................. None
g0214:   num_experts_teacher ............................. [1]
g0214:   num_key_value_heads ............................. 4
g0214:   num_layers ...................................... 22
g0214:   num_layers_per_virtual_pipeline_stage ........... None
g0214:   num_layers_teacher .............................. None
g0214:   num_workers ..................................... 0
g0214:   onnx_safe ....................................... None
g0214:   openai_gelu ..................................... False
g0214:   optimizer ....................................... adam
g0214:   output_bert_embeddings .......................... False
g0214:   overlap_p2p_comm ................................ False
g0214:   override_opt_param_scheduler .................... True
g0214:   params_dtype .................................... torch.float32
g0214:   partition_activations ........................... False
g0214:   patch_dim ....................................... 16
g0214:   perform_initialization .......................... True
g0214:   pipeline_model_parallel_size .................... 8
g0214:   pipeline_model_parallel_split_rank .............. None
g0214:   profile_backward ................................ False
g0214:   query_in_block_prob ............................. 0.1
g0214:   rampup_batch_size ............................... None
g0214:   random_ltd ...................................... False
g0214:   rank ............................................ 0
g0214:   recompute_granularity ........................... None
g0214:   recompute_method ................................ None
g0214:   recompute_num_layers ............................ 1
g0214:   remote_device ................................... none
g0214:   repeated_dataloader ............................. False
g0214:   reset_attention_mask ............................ False
g0214:   reset_iteration ................................. False
g0214:   reset_position_ids .............................. False
g0214:   retriever_report_topk_accuracies ................ []
g0214:   retriever_score_scaling ......................... False
g0214:   retriever_seq_length ............................ 256
g0214:   retro_add_retriever ............................. False
g0214:   retro_cyclic_train_iters ........................ None
g0214:   retro_encoder_attention_dropout ................. 0.1
g0214:   retro_encoder_hidden_dropout .................... 0.1
g0214:   retro_encoder_layers ............................ 2
g0214:   retro_num_neighbors ............................. 2
g0214:   retro_num_retrieved_chunks ...................... 2
g0214:   retro_return_doc_ids ............................ False
g0214:   retro_workdir ................................... None
g0214:   return_data_index ............................... False
g0214:   rotary_percent .................................. 1.0
g0214:   sample_rate ..................................... 1.0
g0214:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214:   save_interval ................................... 1000
g0214:   scatter_gather_tensors_in_pipeline .............. True
g0214:   scattered_embeddings ............................ False
g0214:   seed ............................................ 1234
g0214:   seq_length ...................................... 2048
g0214:   sequence_parallel ............................... False
g0214:   sgd_momentum .................................... 0.9
g0214:   short_seq_prob .................................. 0.1
g0214:   skip_train ...................................... False
g0214:   split ........................................... 949,50,1
g0214:   split_transformers .............................. False
g0214:   squared_relu .................................... False
g0214:   standalone_embedding_stage ...................... False
g0214:   start_weight_decay .............................. 0.1
g0214:   swiglu .......................................... True
g0214:   swin_backbone_type .............................. tiny
g0214:   synchronize_each_layer .......................... False
g0214:   tensor_model_parallel_size ...................... 1
g0214:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True
g0214:   tensorboard_log_interval ........................ 1
g0214:   tensorboard_queue_size .......................... 1
g0214:   test_data_path .................................. None
g0214:   tf32 ............................................ False
g0214:   tile_factor ..................................... 1
g0214:   timing_log_level ................................ 0
g0214:   timing_log_option ............................... minmax
g0214:   titles_data_path ................................ None
g0214:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
g0214:   tokenizer_type .................................. SentencePieceTokenizer
g0214:   topk ............................................ 1
g0214:   train_data_exact_num_epochs ..................... 1
g0214:   train_data_path ................................. None
g0214:   train_desc_path ................................. None
g0214:   train_doc_idx_path .............................. None
g0214:   train_idx_path .................................. None
g0214:   train_iters ..................................... None
g0214:   train_sample_idx_path ........................... None
g0214:   train_samples ................................... 1280000000
g0214:   train_shuffle_idx_path .......................... None
g0214:   train_tokens .................................... 2621440000000
g0214:   transformer_impl ................................ local
g0214:   transformer_pipeline_model_parallel_size ........ 8
g0214:   universal_checkpoint ............................ False
g0214:   untie_embeddings_and_output_weights ............. True
g0214:   use_checkpoint_args ............................. False
g0214:   use_checkpoint_opt_param_scheduler .............. False
g0214:   use_contiguous_buffers_in_local_ddp ............. True
g0214:   use_cpu_initialization .......................... None
g0214:   use_dataset_only ................................ False
g0214:   use_distributed_optimizer ....................... False
g0214:   use_flash_attn .................................. False
g0214:   use_flash_attn_triton ........................... False
g0214:   use_flash_attn_v1 ............................... False
g0214:   use_flash_attn_v2 ............................... False
g0214:   use_one_sent_docs ............................... False
g0214:   use_pin_memory .................................. False
g0214:   use_ring_exchange_p2p ........................... False
g0214:   use_rotary_position_embeddings .................. True
g0214:   use_tutel ....................................... False
g0214:   use_wandb ....................................... True
g0214:   valid_data_path ................................. None
g0214:   variable_seq_lengths ............................ False
g0214:   virtual_pipeline_model_parallel_size ............ None
g0214:   vision_backbone_type ............................ vit
g0214:   vision_pretraining .............................. False
g0214:   vision_pretraining_type ......................... classify
g0214:   vocab_extra_ids ................................. 0
g0214:   vocab_file ...................................... None
g0214:   vocab_size ...................................... None
g0214:   wandb_entity .................................... yohei-kobashi
g0214:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True
g0214:   wandb_project ................................... encrypted_data_LLM
g0214:   wandb_tag ....................................... other_gpu
g0214:   weight_decay .................................... 0.1
g0214:   weight_decay_incr_style ......................... constant
g0214:   world_size ...................................... 32
g0214:   zero_allgather_bucket_size ...................... 0.0
g0214:   zero_contigious_gradients ....................... False
g0214:   zero_reduce_bucket_size ......................... 0.0
g0214:   zero_reduce_scatter ............................. False
g0214:   zero_stage ...................................... 0
g0214: -------------------- end of arguments ---------------------
g0214: setting number of micro-batches to constant 32
g0214: > building SentencePieceTokenizer tokenizer ...
g0214: [2024-08-10 08:09:04,040] [INFO] [comm.py:637:init_distributed] cdb=None
g0214: [2024-08-10 08:09:04,041] [INFO] [comm.py:637:init_distributed] cdb=None
g0214: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0214: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0214: [2024-08-10 08:09:04,068] [INFO] [comm.py:637:init_distributed] cdb=None
g0214:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0214: > initializing torch distributed ...
g0214: [2024-08-10 08:09:04,068] [INFO] [comm.py:637:init_distributed] cdb=None
g0214: [2024-08-10 08:09:04,069] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0214: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0214: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0214: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0214: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0214: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0214: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [2024-08-10 08:09:05,540] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0236: [2024-08-10 08:09:05,540] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0214': [0, 1, 2, 3], 'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0232': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3]}
g0236: [2024-08-10 08:09:05,540] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0236: [2024-08-10 08:09:05,540] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0214': [0, 1, 2, 3], 'g0220': [4, 5, 6, 7], 'g0225': [8, 9, 10, 11], 'g0232': [12, 13, 14, 15], 'g0233': [16, 17, 18, 19], 'g0234': [20, 21, 22, 23], 'g0235': [24, 25, 26, 27], 'g0236': [28, 29, 30, 31]})
g0236: [2024-08-10 08:09:05,540] [INFO] [launch.py:163:main] dist_world_size=32
g0236: [2024-08-10 08:09:05,540] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0233: [2024-08-10 08:09:05,591] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0233: [2024-08-10 08:09:05,591] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0214': [0, 1, 2, 3], 'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0232': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3]}
g0233: [2024-08-10 08:09:05,591] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0233: [2024-08-10 08:09:05,591] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0214': [0, 1, 2, 3], 'g0220': [4, 5, 6, 7], 'g0225': [8, 9, 10, 11], 'g0232': [12, 13, 14, 15], 'g0233': [16, 17, 18, 19], 'g0234': [20, 21, 22, 23], 'g0235': [24, 25, 26, 27], 'g0236': [28, 29, 30, 31]})
g0233: [2024-08-10 08:09:05,591] [INFO] [launch.py:163:main] dist_world_size=32
g0233: [2024-08-10 08:09:05,591] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0225: [2024-08-10 08:09:05,596] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0225: [2024-08-10 08:09:05,596] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0214': [0, 1, 2, 3], 'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0232': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3]}
g0225: [2024-08-10 08:09:05,596] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0225: [2024-08-10 08:09:05,596] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0214': [0, 1, 2, 3], 'g0220': [4, 5, 6, 7], 'g0225': [8, 9, 10, 11], 'g0232': [12, 13, 14, 15], 'g0233': [16, 17, 18, 19], 'g0234': [20, 21, 22, 23], 'g0235': [24, 25, 26, 27], 'g0236': [28, 29, 30, 31]})
g0225: [2024-08-10 08:09:05,596] [INFO] [launch.py:163:main] dist_world_size=32
g0225: [2024-08-10 08:09:05,596] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0220: [2024-08-10 08:09:05,953] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0220: [2024-08-10 08:09:05,953] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0214': [0, 1, 2, 3], 'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0232': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3]}
g0220: [2024-08-10 08:09:05,953] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0220: [2024-08-10 08:09:05,953] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0214': [0, 1, 2, 3], 'g0220': [4, 5, 6, 7], 'g0225': [8, 9, 10, 11], 'g0232': [12, 13, 14, 15], 'g0233': [16, 17, 18, 19], 'g0234': [20, 21, 22, 23], 'g0235': [24, 25, 26, 27], 'g0236': [28, 29, 30, 31]})
g0220: [2024-08-10 08:09:05,953] [INFO] [launch.py:163:main] dist_world_size=32
g0220: [2024-08-10 08:09:05,953] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0235: [2024-08-10 08:09:06,076] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0235: [2024-08-10 08:09:06,076] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0214': [0, 1, 2, 3], 'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0232': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3]}
g0235: [2024-08-10 08:09:06,076] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0235: [2024-08-10 08:09:06,076] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0214': [0, 1, 2, 3], 'g0220': [4, 5, 6, 7], 'g0225': [8, 9, 10, 11], 'g0232': [12, 13, 14, 15], 'g0233': [16, 17, 18, 19], 'g0234': [20, 21, 22, 23], 'g0235': [24, 25, 26, 27], 'g0236': [28, 29, 30, 31]})
g0235: [2024-08-10 08:09:06,076] [INFO] [launch.py:163:main] dist_world_size=32
g0235: [2024-08-10 08:09:06,076] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0234: [2024-08-10 08:09:06,269] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0234: [2024-08-10 08:09:06,269] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0214': [0, 1, 2, 3], 'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0232': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3]}
g0234: [2024-08-10 08:09:06,269] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0234: [2024-08-10 08:09:06,269] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0214': [0, 1, 2, 3], 'g0220': [4, 5, 6, 7], 'g0225': [8, 9, 10, 11], 'g0232': [12, 13, 14, 15], 'g0233': [16, 17, 18, 19], 'g0234': [20, 21, 22, 23], 'g0235': [24, 25, 26, 27], 'g0236': [28, 29, 30, 31]})
g0234: [2024-08-10 08:09:06,269] [INFO] [launch.py:163:main] dist_world_size=32
g0234: [2024-08-10 08:09:06,269] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0232: [2024-08-10 08:09:06,425] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0232: [2024-08-10 08:09:06,425] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0214': [0, 1, 2, 3], 'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0232': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3]}
g0232: [2024-08-10 08:09:06,425] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0232: [2024-08-10 08:09:06,425] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0214': [0, 1, 2, 3], 'g0220': [4, 5, 6, 7], 'g0225': [8, 9, 10, 11], 'g0232': [12, 13, 14, 15], 'g0233': [16, 17, 18, 19], 'g0234': [20, 21, 22, 23], 'g0235': [24, 25, 26, 27], 'g0236': [28, 29, 30, 31]})
g0232: [2024-08-10 08:09:06,425] [INFO] [launch.py:163:main] dist_world_size=32
g0232: [2024-08-10 08:09:06,425] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0236: [2024-08-10 08:09:08,623] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0236: [2024-08-10 08:09:08,636] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0225: [2024-08-10 08:09:08,638] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0225: [2024-08-10 08:09:08,650] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0236: [2024-08-10 08:09:08,662] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0236: [2024-08-10 08:09:08,663] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0225: [2024-08-10 08:09:08,680] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0233: [2024-08-10 08:09:08,726] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0233: [2024-08-10 08:09:08,727] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0233: [2024-08-10 08:09:08,750] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0233: [2024-08-10 08:09:08,774] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0225: [2024-08-10 08:09:08,795] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0220: [2024-08-10 08:09:09,079] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0220: [2024-08-10 08:09:09,079] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0220: [2024-08-10 08:09:09,080] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0220: [2024-08-10 08:09:09,239] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0235: [2024-08-10 08:09:09,242] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0235: [2024-08-10 08:09:09,242] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0235: [2024-08-10 08:09:09,276] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0235: [2024-08-10 08:09:09,399] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0234: [2024-08-10 08:09:09,458] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0234: [2024-08-10 08:09:09,458] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0234: [2024-08-10 08:09:09,493] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0234: [2024-08-10 08:09:09,541] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0232: [2024-08-10 08:09:09,626] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0232: [2024-08-10 08:09:09,626] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0232: [2024-08-10 08:09:09,662] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0232: [2024-08-10 08:09:09,727] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0225: --------------------------------------------------
g0225: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0225: --------------------------------------------------
g0225: 
g0225: DeepSpeed C++/CUDA extension op reportNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0225:       runtime if needed. Op compatibility means that your system
g0225:       meet the required dependencies to JIT install the op.
g0225: 
g0225: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0225: 
g0225: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0225:       runtime if needed. Op compatibility means that your system
g0225:       meet the required dependencies to JIT install the op.JIT compiled ops requires ninja
g0225: 
g0225: 
g0225: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0225: 
g0225: JIT compiled ops requires ninja--------------------------------------------------
g0225: 
g0225: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0225:       runtime if needed. Op compatibility means that your system
g0225:       meet the required dependencies to JIT install the op.
g0225: --------------------------------------------------
g0225: JIT compiled ops requires ninja
g0225: --------------------------------------------------
g0225: DeepSpeed C++/CUDA extension op report
g0225: --------------------------------------------------
g0225: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0225:       runtime if needed. Op compatibility means that your system
g0225:       meet the required dependencies to JIT install the op.
g0225: --------------------------------------------------
g0225: JIT compiled ops requires ninja
g0225: ninjaninjaninja  ....................................   [92m[OKAY][0m..................[92m[OKAY][0m
g0225:  
g0225: [92m[OKAY][0m--------------------------------------------------
g0225: --------------------------------------------------
g0225: 
g0225: --------------------------------------------------op name
g0225: op name  ................op name................   installed................installed   ..installed..   compatible..compatible
g0225:  
g0225: compatible----------------------------------------------------------------------------------------------------
g0225: 
g0225: 
g0225: --------------------------------------------------
g0236: --------------------------------------------------
g0236: DeepSpeed C++/CUDA extension op report
g0236: --------------------------------------------------
g0236: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0236:       runtime if needed. Op compatibility means that your system
g0236:       meet the required dependencies to JIT install the op.
g0236: --------------------------------------------------
g0236: JIT compiled ops requires ninja
g0236: --------------------------------------------------
g0236: DeepSpeed C++/CUDA extension op report
g0236: --------------------------------------------------
g0236: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0236:       runtime if needed. Op compatibility means that your system
g0236:       meet the required dependencies to JIT install the op.
g0236: --------------------------------------------------
g0236: JIT compiled ops requires ninja
g0236: --------------------------------------------------
g0236: DeepSpeed C++/CUDA extension op report
g0236: --------------------------------------------------
g0236: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0236:       runtime if needed. Op compatibility means that your system
g0236:       meet the required dependencies to JIT install the op.
g0236: --------------------------------------------------
g0236: JIT compiled ops requires ninja
g0236: --------------------------------------------------
g0236: DeepSpeed C++/CUDA extension op report
g0236: --------------------------------------------------
g0236: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0236:       runtime if needed. Op compatibility means that your system
g0236:       meet the required dependencies to JIT install the op.
g0236: --------------------------------------------------
g0236: JIT compiled ops requires ninja
g0225: ninja .................. [92m[OKAY][0m
g0225: --------------------------------------------------
g0225: op name ................ installed .. compatible
g0225: --------------------------------------------------
g0236: ninjaninjaninja   ninja......................................................    [92m[OKAY][0m[92m[OKAY][0m..................
g0236: [92m[OKAY][0m 
g0236: [92m[OKAY][0m
g0236: --------------------------------------------------
g0236: --------------------------------------------------
g0236: 
g0236: ----------------------------------------------------------------------------------------------------op nameop name
g0236:  
g0236:  ................op name  ................installedop name ................ installed ..   ................installed..compatible   
g0236: ..compatible installed
g0236: --------------------------------------------------compatible 
g0236: 
g0236: --------------------------------------------------..
g0236: -------------------------------------------------- 
g0236: compatible
g0236: --------------------------------------------------
g0233: --------------------------------------------------
g0233: DeepSpeed C++/CUDA extension op report
g0233: --------------------------------------------------
g0233: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0233:       runtime if needed. Op compatibility means that your system
g0233:       meet the required dependencies to JIT install the op.
g0233: --------------------------------------------------
g0233: JIT compiled ops requires ninja
g0233: --------------------------------------------------
g0233: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0233: 
g0233: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0233: 
g0233: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0233:       runtime if needed. Op compatibility means that your system
g0233:       meet the required dependencies to JIT install the op.--------------------------------------------------
g0233: 
g0233: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0233:       runtime if needed. Op compatibility means that your system
g0233:       meet the required dependencies to JIT install the op.
g0233: 
g0233: JIT compiled ops requires ninja
g0233: --------------------------------------------------
g0233: JIT compiled ops requires ninja
g0233: --------------------------------------------------
g0233: DeepSpeed C++/CUDA extension op report
g0233: --------------------------------------------------
g0233: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0233:       runtime if needed. Op compatibility means that your system
g0233:       meet the required dependencies to JIT install the op.
g0233: --------------------------------------------------
g0233: JIT compiled ops requires ninja
g0233: ninjaninjaninja ninja .................. ..................   ..................[92m[OKAY][0m..................[92m[OKAY][0m 
g0233:  
g0233: [92m[OKAY][0m[92m[OKAY][0m----------------------------------------------------------------------------------------------------
g0233: 
g0233: 
g0233: 
g0233: ----------------------------------------------------------------------------------------------------op nameop name
g0233: 
g0233:   op name................................op name    ................installedinstalled................    ..installedinstalled ..  compatible ....
g0233: compatible  
g0233: compatiblecompatible--------------------------------------------------
g0233: 
g0233: --------------------------------------------------
g0233: --------------------------------------------------
g0233: --------------------------------------------------
g0233: 
g0225: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0225: evoformer_attn ......... [93m[NO][0m ....... async_io[93m[NO][0m
g0225:  ............... fused_lamb[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0225: ...... [92m[OKAY][0m
g0225: fused_adam fused_lion.............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0225: [92m[OKAY][0m
g0225: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0225: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0225: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_lion ............... [92m[YES][0masync_io ......  ...............[92m[OKAY][0m 
g0225: [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0225: fused_adamevoformer_attn  ......................  [92m[YES][0m[93m[NO][0m  .............  [92m[OKAY][0m[93m[NO][0m
g0225: 
g0225: fused_lambcpu_adam  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0225: 
g0225: cpu_adagrad ............ [92m[YES][0m fused_lion......  .............[92m[OKAY][0m 
g0225: [92m[YES][0m ......cpu_lion  [92m[OKAY][0m...............
g0225:  [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0225: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0225: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0236: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_lion ............... [92m[YES][0masync_io  .....................  [92m[OKAY][0m[92m[YES][0m
g0236:  ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0236: fused_adam evoformer_attn.............  .........[92m[YES][0m  [93m[NO][0m......  .......[92m[OKAY][0m 
g0236: [93m[NO][0m
g0236: cpu_adam fused_lamb...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0236: [92m[OKAY][0m
g0236: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: fused_lion ............. cpu_lion[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0236: ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0236: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0236: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0236: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0236: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0236: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: fused_lion ............. [92m[YES][0m async_io...... [92m[OKAY][0m 
g0236: ............... [92m[YES][0m ...... [92m[OKAY][0m
g0236: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0236: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0236: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0225: 
g0225: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0225: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0225: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: cutlass_ops cutlass_ops............  [92m[YES][0m............  ......[92m[YES][0m  [92m[OKAY][0m......
g0225:  [92m[OKAY][0m
g0225: quantizer ..............quantizer  [92m[YES][0m..............  ......[92m[YES][0m  [92m[OKAY][0m......
g0225:  [92m[OKAY][0m
g0225: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0236: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0236: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0236: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0225: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0225: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0225: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0225: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0225: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0225: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0225: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0225: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0225: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0236: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0225: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0225: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0236: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: cutlass_ops ............quantizer [92m[YES][0m  ....................  [92m[YES][0m[92m[OKAY][0m 
g0236: ...... [92m[OKAY][0m
g0236: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0225: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0225: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0236: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0236: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0225: --------------------------------------------------
g0236: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0236: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0236: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0225: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0225: --------------------------------------------------
g0225: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0225: --------------------------------------------------
g0236: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0236: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0236: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0225: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0225: --------------------------------------------------
g0236: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0236: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0236: sparse_attn ............ [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0[93m[NO][0m
g0236:  ....... [93m[NO][0m
g0236: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0236: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0225: DeepSpeed general environment info:
g0225: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0225: torch version .................... 2.0.1+cu118
g0225: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0225: deepspeed info ................... 0.12.4, unknown, unknown
g0225: torch cuda version ............... 11.8
g0225: torch hip version ................ None
g0225: nvcc version ..................... 11.8
g0225: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0225: shared memory (/dev/shm) size .... 188.13 GB
g0236: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0225: DeepSpeed general environment info:
g0225: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0225: torch version .................... 2.0.1+cu118
g0225: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0225: deepspeed info ................... 0.12.4, unknown, unknown
g0225: torch cuda version ............... 11.8
g0225: torch hip version ................ None
g0225: nvcc version ..................... 11.8
g0225: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0225: shared memory (/dev/shm) size .... 188.13 GB
g0225: DeepSpeed general environment info:
g0225: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0225: torch version .................... 2.0.1+cu118
g0225: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0225: deepspeed info ................... 0.12.4, unknown, unknown
g0225: torch cuda version ............... 11.8
g0225: torch hip version ................ None
g0225: nvcc version ..................... 11.8
g0225: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0225: shared memory (/dev/shm) size .... 188.13 GB
g0225: DeepSpeed general environment info:
g0236: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0225: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0225: torch version .................... 2.0.1+cu118
g0225: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0225: deepspeed info ................... 0.12.4, unknown, unknown
g0225: torch cuda version ............... 11.8
g0225: torch hip version ................ None
g0225: nvcc version ..................... 11.8
g0225: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0225: shared memory (/dev/shm) size .... 188.13 GB
g0236: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0236: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0233: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0233: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0233: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_lion ............. [92m[YES][0m ......async_io [92m[OKAY][0m 
g0233: ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0233: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0233: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0233: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0233: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0233: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0233: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0236: --------------------------------------------------
g0236: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0236: --------------------------------------------------
g0236: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0236: --------------------------------------------------
g0236: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0236: --------------------------------------------------
g0236: DeepSpeed general environment info:
g0236: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0236: torch version .................... 2.0.1+cu118
g0236: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0236: deepspeed info ................... 0.12.4, unknown, unknown
g0236: torch cuda version ............... 11.8
g0236: torch hip version ................ None
g0236: nvcc version ..................... 11.8
g0236: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0236: shared memory (/dev/shm) size .... 188.13 GB
g0236: DeepSpeed general environment info:
g0236: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0236: torch version .................... 2.0.1+cu118
g0236: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0236: deepspeed info ................... 0.12.4, unknown, unknown
g0236: torch cuda version ............... 11.8
g0236: torch hip version ................ None
g0236: nvcc version ..................... 11.8
g0236: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0236: shared memory (/dev/shm) size .... 188.13 GB
g0236: DeepSpeed general environment info:
g0236: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0236: torch version .................... 2.0.1+cu118
g0236: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0236: deepspeed info ................... 0.12.4, unknown, unknown
g0236: torch cuda version ............... 11.8
g0236: torch hip version ................ None
g0236: nvcc version ..................... 11.8
g0236: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0236: shared memory (/dev/shm) size .... 188.13 GB
g0236: DeepSpeed general environment info:
g0236: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0236: torch version .................... 2.0.1+cu118
g0236: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0236: deepspeed info ................... 0.12.4, unknown, unknown
g0236: torch cuda version ............... 11.8
g0236: torch hip version ................ None
g0236: nvcc version ..................... 11.8
g0236: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0236: shared memory (/dev/shm) size .... 188.13 GB
g0233: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0233: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0233: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0233: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0233: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: cutlass_opscutlass_ops ............  ............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0233: [92m[OKAY][0m
g0233: quantizer quantizer..............  ..............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0233: [92m[OKAY][0m
g0233: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0225: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0225: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0225: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0233: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0233: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0233: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0233: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0233: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0233: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0233: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0233: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0233: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0233: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0233: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0233: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0233: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0233: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0233: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0mspatial_inference
g0233:  ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0236: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0233: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0233: --------------------------------------------------
g0236: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0236: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0236: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0233: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0233: --------------------------------------------------
g0233: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0233: --------------------------------------------------
g0233: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0233: --------------------------------------------------
g0233: DeepSpeed general environment info:
g0233: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0233: torch version .................... 2.0.1+cu118
g0233: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0233: deepspeed info ................... 0.12.4, unknown, unknown
g0233: torch cuda version ............... 11.8
g0233: torch hip version ................ None
g0233: nvcc version ..................... 11.8
g0233: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0233: shared memory (/dev/shm) size .... 188.13 GB
g0233: DeepSpeed general environment info:
g0233: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0233: torch version .................... 2.0.1+cu118
g0233: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0233: deepspeed info ................... 0.12.4, unknown, unknown
g0233: torch cuda version ............... 11.8
g0233: torch hip version ................ None
g0233: nvcc version ..................... 11.8
g0233: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0233: shared memory (/dev/shm) size .... 188.13 GB
g0233: DeepSpeed general environment info:
g0233: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0233: torch version .................... 2.0.1+cu118
g0233: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0233: deepspeed info ................... 0.12.4, unknown, unknown
g0233: torch cuda version ............... 11.8
g0233: torch hip version ................ None
g0233: nvcc version ..................... 11.8
g0233: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0233: shared memory (/dev/shm) size .... 188.13 GB
g0233: DeepSpeed general environment info:
g0233: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0233: torch version .................... 2.0.1+cu118
g0233: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0233: deepspeed info ................... 0.12.4, unknown, unknown
g0233: torch cuda version ............... 11.8
g0233: torch hip version ................ None
g0233: nvcc version ..................... 11.8
g0233: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0233: shared memory (/dev/shm) size .... 188.13 GB
g0233: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0233: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0233: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0233: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0225: [2024-08-10 08:09:13,296] [INFO] [comm.py:637:init_distributed] cdb=None
g0225: [2024-08-10 08:09:13,296] [INFO] [comm.py:637:init_distributed] cdb=None
g0225: [2024-08-10 08:09:13,296] [INFO] [comm.py:637:init_distributed] cdb=None
g0236: [2024-08-10 08:09:13,299] [INFO] [comm.py:637:init_distributed] cdb=None
g0236: [2024-08-10 08:09:13,299] [INFO] [comm.py:637:init_distributed] cdb=None
g0236: [2024-08-10 08:09:13,299] [INFO] [comm.py:637:init_distributed] cdb=None
g0225: [2024-08-10 08:09:13,300] [INFO] [comm.py:637:init_distributed] cdb=None
g0233: [2024-08-10 08:09:13,299] [INFO] [comm.py:637:init_distributed] cdb=None
g0233: [2024-08-10 08:09:13,299] [INFO] [comm.py:637:init_distributed] cdb=None
g0233: [2024-08-10 08:09:13,300] [INFO] [comm.py:637:init_distributed] cdb=None
g0233: [2024-08-10 08:09:13,300] [INFO] [comm.py:637:init_distributed] cdb=None
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: ----------------------------------------------------------------------------------------------------
g0220: DeepSpeed C++/CUDA extension op report
g0220: 
g0220: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0220: 
g0220: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0220:       runtime if needed. Op compatibility means that your system
g0220:       meet the required dependencies to JIT install the op.
g0220: 
g0220: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0220:       runtime if needed. Op compatibility means that your system
g0220:       meet the required dependencies to JIT install the op.--------------------------------------------------
g0220: 
g0220: --------------------------------------------------JIT compiled ops requires ninja
g0220: 
g0220: JIT compiled ops requires ninja
g0220: --------------------------------------------------
g0220: DeepSpeed C++/CUDA extension op report
g0220: --------------------------------------------------
g0220: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0220:       runtime if needed. Op compatibility means that your system
g0220:       meet the required dependencies to JIT install the op.
g0220: --------------------------------------------------
g0220: JIT compiled ops requires ninja
g0220: --------------------------------------------------
g0220: DeepSpeed C++/CUDA extension op report
g0220: --------------------------------------------------
g0220: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0220:       runtime if needed. Op compatibility means that your system
g0220:       meet the required dependencies to JIT install the op.
g0220: --------------------------------------------------
g0220: JIT compiled ops requires ninja
g0220: ninjaninjaninja   ......................................................   [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0220: 
g0220: 
g0220: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0220: 
g0220: 
g0220: op nameop nameop name   ................................................   installedinstalledinstalled   ......   compatiblecompatiblecompatible
g0220: 
g0220: 
g0220: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0220: 
g0220: 
g0220: ninja .................. [92m[OKAY][0m
g0220: --------------------------------------------------
g0220: op name ................ installed .. compatible
g0220: --------------------------------------------------
g0220: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0220: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0220: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: async_io fused_adam...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0220: [92m[OKAY][0m
g0220: cpu_adam fused_adam...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0220: [92m[OKAY][0m
g0220: cpu_adagrad ............cpu_adam  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m......
g0220:  [92m[OKAY][0m
g0220: cpu_lion ............... cpu_adagrad[92m[YES][0m  ..................  [92m[YES][0m[92m[OKAY][0m 
g0220: ...... [92m[OKAY][0m
g0220: cpu_lion ............... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0220:  ...... evoformer_attn[92m[OKAY][0m 
g0220: ......... [93m[NO][0m ....... [93m[NO][0m
g0220: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0220: fused_lamb evoformer_attn.............  .........[92m[YES][0m  [93m[NO][0m......  .......[92m[OKAY][0m 
g0220: [93m[NO][0m
g0220: fused_lamb ............. [92m[YES][0m ......fused_lion  [92m[OKAY][0m.............
g0220:  [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0220: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0220: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0220: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0220: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0220: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0220: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0220: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0220: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0220: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0220: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0220: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0220: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0mragged_ops
g0220:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0random_ltd
g0220:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0220: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0220: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0220: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0220: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0220: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0220: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0220: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0220: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0220: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0220: --------------------------------------------------
g0220: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0220: --------------------------------------------------
g0220: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0220: --------------------------------------------------
g0220: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0220: --------------------------------------------------
g0220: DeepSpeed general environment info:
g0220: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0220: torch version .................... 2.0.1+cu118
g0220: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0220: deepspeed info ................... 0.12.4, unknown, unknown
g0220: torch cuda version ............... 11.8
g0220: torch hip version ................ None
g0220: nvcc version ..................... 11.8
g0220: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0220: shared memory (/dev/shm) size .... 188.13 GB
g0220: DeepSpeed general environment info:
g0220: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0220: torch version .................... 2.0.1+cu118
g0220: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0220: deepspeed info ................... 0.12.4, unknown, unknown
g0220: torch cuda version ............... 11.8
g0220: torch hip version ................ None
g0220: nvcc version ..................... 11.8
g0220: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0220: shared memory (/dev/shm) size .... 188.13 GB
g0220: DeepSpeed general environment info:
g0220: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0220: torch version .................... 2.0.1+cu118
g0220: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0220: deepspeed info ................... 0.12.4, unknown, unknown
g0220: torch cuda version ............... 11.8
g0220: torch hip version ................ None
g0220: nvcc version ..................... 11.8
g0220: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0220: shared memory (/dev/shm) size .... 188.13 GB
g0220: DeepSpeed general environment info:
g0220: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0220: torch version .................... 2.0.1+cu118
g0220: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0220: deepspeed info ................... 0.12.4, unknown, unknown
g0220: torch cuda version ............... 11.8
g0220: torch hip version ................ None
g0220: nvcc version ..................... 11.8
g0220: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0220: shared memory (/dev/shm) size .... 188.13 GB
g0220: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0220: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0220: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0220: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0235: --------------------------------------------------
g0235: DeepSpeed C++/CUDA extension op report
g0235: --------------------------------------------------
g0235: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0235:       runtime if needed. Op compatibility means that your system
g0235:       meet the required dependencies to JIT install the op.
g0235: --------------------------------------------------
g0235: JIT compiled ops requires ninja
g0235: --------------------------------------------------
g0235: DeepSpeed C++/CUDA extension op report
g0235: --------------------------------------------------
g0235: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0235:       runtime if needed. Op compatibility means that your system
g0235:       meet the required dependencies to JIT install the op.
g0235: --------------------------------------------------
g0235: JIT compiled ops requires ninja
g0235: --------------------------------------------------
g0235: DeepSpeed C++/CUDA extension op report
g0235: --------------------------------------------------
g0235: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0235:       runtime if needed. Op compatibility means that your system
g0235:       meet the required dependencies to JIT install the op.
g0235: --------------------------------------------------
g0235: JIT compiled ops requires ninja
g0235: --------------------------------------------------
g0235: DeepSpeed C++/CUDA extension op report
g0235: --------------------------------------------------
g0235: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0235:       runtime if needed. Op compatibility means that your system
g0235:       meet the required dependencies to JIT install the op.
g0235: --------------------------------------------------
g0235: JIT compiled ops requires ninja
g0235: ninjaninjaninja  .................. .................. .................. [92m[OKAY][0m [92m[OKAY][0m
g0235: [92m[OKAY][0m
g0235: 
g0235: ----------------------------------------------------------------------------------------------------
g0235: --------------------------------------------------
g0235: 
g0235: op nameop name op name ................ ................ ................ installed installed installed .. .. .. compatible compatiblecompatible
g0235: 
g0235: 
g0235: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0235: 
g0235: ninja
g0235:  .................. [92m[OKAY][0m
g0235: --------------------------------------------------
g0235: op name ................ installed .. compatible
g0235: --------------------------------------------------
g0235: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0235: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0235: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0220: [2024-08-10 08:09:13,605] [INFO] [comm.py:637:init_distributed] cdb=None
g0235: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0235: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_lion async_io...............  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m......
g0235:  [92m[OKAY][0m
g0235: fused_adam[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0235: ............. evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0235: ....... [93m[NO][0m
g0235: cpu_adam ............... fused_lamb[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0235: ...... [92m[OKAY][0m
g0235: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: fused_lion ............. cpu_lion[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0235: ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0235: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0235: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: [2024-08-10 08:09:13,606] [INFO] [comm.py:637:init_distributed] cdb=None
g0220: [2024-08-10 08:09:13,606] [INFO] [comm.py:637:init_distributed] cdb=None
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [2024-08-10 08:09:13,616] [INFO] [comm.py:637:init_distributed] cdb=None
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0235: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0235: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0235: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0235: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0235: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0235: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0235: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0235: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0mragged_ops
g0235:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0235: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0235: sparse_attn ............ [93m[NO][0m ragged_ops.......  .............[93m[NO][0m 
g0235: [92m[YES][0m ...... [92m[OKAY][0m
g0235: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0235: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0235: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0235: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0235: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0235: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0235: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0235: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0235: --------------------------------------------------
g0235: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0235: --------------------------------------------------
g0235: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0235: --------------------------------------------------
g0235: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0235: --------------------------------------------------
g0235: DeepSpeed general environment info:
g0235: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0235: torch version .................... 2.0.1+cu118
g0235: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0235: deepspeed info ................... 0.12.4, unknown, unknown
g0235: torch cuda version ............... 11.8
g0235: torch hip version ................ None
g0235: nvcc version ..................... 11.8
g0235: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0235: shared memory (/dev/shm) size .... 188.13 GB
g0235: DeepSpeed general environment info:
g0235: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0235: torch version .................... 2.0.1+cu118
g0235: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0235: deepspeed info ................... 0.12.4, unknown, unknown
g0235: torch cuda version ............... 11.8
g0235: torch hip version ................ None
g0235: nvcc version ..................... 11.8
g0235: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0235: shared memory (/dev/shm) size .... 188.13 GB
g0235: DeepSpeed general environment info:
g0235: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0235: torch version .................... 2.0.1+cu118
g0235: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0235: deepspeed info ................... 0.12.4, unknown, unknown
g0235: torch cuda version ............... 11.8
g0235: torch hip version ................ None
g0235: nvcc version ..................... 11.8
g0235: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0235: shared memory (/dev/shm) size .... 188.13 GB
g0235: DeepSpeed general environment info:
g0235: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0235: torch version .................... 2.0.1+cu118
g0235: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0235: deepspeed info ................... 0.12.4, unknown, unknown
g0235: torch cuda version ............... 11.8
g0235: torch hip version ................ None
g0235: nvcc version ..................... 11.8
g0235: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0235: shared memory (/dev/shm) size .... 188.13 GB
g0235: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0235: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0235: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0235: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0236: > setting tensorboard ...
g0236: [2024-08-10 08:09:13,705] [INFO] [comm.py:637:init_distributed] cdb=None
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [2024-08-10 08:09:13,775] [INFO] [comm.py:637:init_distributed] cdb=None
g0235: [2024-08-10 08:09:13,776] [INFO] [comm.py:637:init_distributed] cdb=None
g0235: [2024-08-10 08:09:13,776] [INFO] [comm.py:637:init_distributed] cdb=None
g0235: [2024-08-10 08:09:13,778] [INFO] [comm.py:637:init_distributed] cdb=None
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: --------------------------------------------------
g0234: DeepSpeed C++/CUDA extension op report
g0234: --------------------------------------------------
g0234: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0234:       runtime if needed. Op compatibility means that your system
g0234:       meet the required dependencies to JIT install the op.
g0234: --------------------------------------------------
g0234: JIT compiled ops requires ninja
g0234: --------------------------------------------------
g0234: DeepSpeed C++/CUDA extension op report
g0234: --------------------------------------------------
g0234: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0234:       runtime if needed. Op compatibility means that your system
g0234:       meet the required dependencies to JIT install the op.
g0234: --------------------------------------------------
g0234: JIT compiled ops requires ninja
g0234: --------------------------------------------------
g0234: DeepSpeed C++/CUDA extension op report
g0234: --------------------------------------------------
g0234: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0234:       runtime if needed. Op compatibility means that your system
g0234:       meet the required dependencies to JIT install the op.
g0234: --------------------------------------------------
g0234: JIT compiled ops requires ninja
g0234: --------------------------------------------------
g0234: DeepSpeed C++/CUDA extension op report
g0234: --------------------------------------------------
g0234: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0234:       runtime if needed. Op compatibility means that your system
g0234:       meet the required dependencies to JIT install the op.
g0234: --------------------------------------------------
g0234: JIT compiled ops requires ninja
g0234: ninjaninjaninja  .................................... ninja  ..................[92m[OKAY][0m[92m[OKAY][0m  
g0234: 
g0234: ..................[92m[OKAY][0m 
g0234: ----------------------------------------------------------------------------------------------------[92m[OKAY][0m
g0234: --------------------------------------------------
g0234: 
g0234: 
g0234: op nameop name op name --------------------------------------------------................ ................
g0234:  ................ installed installedop name installed  .. .. .................. compatible 
g0234:  compatibleinstalledcompatible--------------------------------------------------
g0234:  
g0234: 
g0234: ..---------------------------------------------------------------------------------------------------- 
g0234: 
g0234: compatible
g0234: --------------------------------------------------
g0234: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: async_io[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0234: ............... evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0234: ....... [93m[NO][0m
g0234: fused_lamb .............fused_adam  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0234:  [92m[OKAY][0m
g0234: cpu_adam ............... [92m[YES][0m fused_lion......  .............[92m[OKAY][0m 
g0234: [92m[YES][0m ...... cpu_adagrad[92m[OKAY][0m 
g0234: ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0234: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0234: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: async_io ............... [92m[YES][0m ...... fused_lion[92m[OKAY][0m 
g0234: ............. [92m[YES][0m ...... [92m[OKAY][0mfused_adam
g0234:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0234: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0234: async_iofused_lamb  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0234: 
g0234: fused_lionfused_adam  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0234: 
g0234: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0234: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0234: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0234: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0234: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0234: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0234: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: cutlass_opsquantizer  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0234: 
g0234: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0234: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0234: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0234: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0234: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0234: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0234: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0234: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0234: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0234: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0234: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0234: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0234: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0234: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0234: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0234: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0234: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0234: --------------------------------------------------
g0234: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0234: --------------------------------------------------
g0234: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0234: --------------------------------------------------
g0234: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0234: --------------------------------------------------
g0234: DeepSpeed general environment info:
g0234: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0234: torch version .................... 2.0.1+cu118
g0234: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0234: deepspeed info ................... 0.12.4, unknown, unknown
g0234: torch cuda version ............... 11.8
g0234: torch hip version ................ None
g0234: nvcc version ..................... 11.8
g0234: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0234: shared memory (/dev/shm) size .... 188.13 GB
g0234: DeepSpeed general environment info:
g0234: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0234: torch version .................... 2.0.1+cu118
g0234: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0234: deepspeed info ................... 0.12.4, unknown, unknown
g0234: torch cuda version ............... 11.8
g0234: torch hip version ................ None
g0234: nvcc version ..................... 11.8
g0234: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0234: shared memory (/dev/shm) size .... 188.13 GB
g0234: DeepSpeed general environment info:
g0234: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0234: torch version .................... 2.0.1+cu118
g0234: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0234: deepspeed info ................... 0.12.4, unknown, unknown
g0234: torch cuda version ............... 11.8
g0234: torch hip version ................ None
g0234: nvcc version ..................... 11.8
g0234: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0234: shared memory (/dev/shm) size .... 188.13 GB
g0234: DeepSpeed general environment info:
g0234: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0234: torch version .................... 2.0.1+cu118
g0234: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0234: deepspeed info ................... 0.12.4, unknown, unknown
g0234: torch cuda version ............... 11.8
g0234: torch hip version ................ None
g0234: nvcc version ..................... 11.8
g0234: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0234: shared memory (/dev/shm) size .... 188.13 GB
g0232: --------------------------------------------------
g0232: DeepSpeed C++/CUDA extension op report
g0232: --------------------------------------------------
g0232: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0232:       runtime if needed. Op compatibility means that your system
g0232:       meet the required dependencies to JIT install the op.
g0232: --------------------------------------------------
g0232: JIT compiled ops requires ninja
g0232: --------------------------------------------------
g0232: DeepSpeed C++/CUDA extension op report
g0232: --------------------------------------------------
g0232: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0232:       runtime if needed. Op compatibility means that your system
g0232:       meet the required dependencies to JIT install the op.
g0232: --------------------------------------------------
g0232: JIT compiled ops requires ninja
g0232: --------------------------------------------------
g0232: DeepSpeed C++/CUDA extension op report
g0232: --------------------------------------------------
g0232: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0232:       runtime if needed. Op compatibility means that your system
g0232:       meet the required dependencies to JIT install the op.
g0232: --------------------------------------------------
g0232: JIT compiled ops requires ninja
g0232: --------------------------------------------------
g0232: DeepSpeed C++/CUDA extension op report
g0232: --------------------------------------------------
g0232: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0232:       runtime if needed. Op compatibility means that your system
g0232:       meet the required dependencies to JIT install the op.
g0232: --------------------------------------------------
g0232: JIT compiled ops requires ninja
g0234: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0234: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0234: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0234: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0232: ninjaninjaninja  ninja ......................................................    [92m[OKAY][0m..................[92m[OKAY][0m[92m[OKAY][0m
g0232:  
g0232: 
g0232: [92m[OKAY][0m----------------------------------------------------------------------------------------------------
g0232: --------------------------------------------------
g0232: 
g0232: 
g0232: --------------------------------------------------op nameop name
g0232: op name   op name................................................    ................installedinstalledinstalled    installed......    ..compatiblecompatiblecompatible 
g0232: 
g0232: 
g0232: compatible--------------------------------------------------
g0232: ----------------------------------------------------------------------------------------------------
g0232: 
g0232: --------------------------------------------------
g0232: 
g0232: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0232: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0232: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0232: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0232: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0232: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0232: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0232: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0232: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0232: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0232: async_iofused_adam  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0232: 
g0232: cpu_adam ...............fused_adam  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0232:  [92m[OKAY][0m
g0232: cpu_adagrad ............ cpu_adam[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0232: ...... [92m[OKAY][0mcpu_lion
g0232:  ............... cpu_adagrad[92m[YES][0m  ..................  [92m[YES][0m[92m[OKAY][0m 
g0232: ...... [92m[OKAY][0m
g0232: cpu_lion[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0232: ............... [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0232:  ....... [93m[NO][0m
g0232: fused_lamb[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0232: ............. [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0232:  ....... [93m[NO][0m
g0232: fused_lamb fused_lion.............  .............[92m[YES][0m  [92m[YES][0m......async_io  ......[92m[OKAY][0m  
g0232: ...............[92m[OKAY][0m 
g0232: [92m[YES][0m ...... [92m[OKAY][0m
g0232: fused_lion ............. [92m[YES][0m ......fused_adam  [92m[OKAY][0m.............
g0232:  [92m[YES][0m ...... [92m[OKAY][0m
g0232: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0232: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0232: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0232: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0232: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0232: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0232: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: [2024-08-10 08:09:14,047] [INFO] [comm.py:637:init_distributed] cdb=None
g0234: [2024-08-10 08:09:14,049] [INFO] [comm.py:637:init_distributed] cdb=None
g0234: [2024-08-10 08:09:14,049] [INFO] [comm.py:637:init_distributed] cdb=None
g0232: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0232: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0232: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0232: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0232: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0232: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0232: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0232: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0232: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0232: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0232: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0232: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: [2024-08-10 08:09:14,057] [INFO] [comm.py:637:init_distributed] cdb=None
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0232: ragged_device_ops ...... [92m[YES][0m ......ragged_device_ops  [92m[OKAY][0m......
g0232:  [92m[YES][0m ...... [92m[OKAY][0m
g0232: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0232: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0232: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0232: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0232: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0232: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0232: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0232: ragged_ops ............. [92m[YES][0m ...... ragged_ops[92m[OKAY][0m 
g0232: ............. [92m[YES][0m ......random_ltd  [92m[OKAY][0m.............
g0232:  [92m[YES][0m ......random_ltd  [92m[OKAY][0m.............
g0232:  [92m[YES][0m ...... [92m[OKAY][0m
g0232: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0232: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0232: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0232: sparse_attn ............ [93m[NO][0m .......[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible 
g0232: [93m[NO][0m
g0232: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0232: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0232: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0232: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0232: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0232: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0232: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0232: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0232: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0232: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0232: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0232: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0232: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0232: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0232: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0232: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0232: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0232: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0232: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0232: --------------------------------------------------
g0232: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0232: --------------------------------------------------
g0232: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0232: --------------------------------------------------
g0232: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0232: --------------------------------------------------
g0232: DeepSpeed general environment info:
g0232: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0232: torch version .................... 2.0.1+cu118
g0232: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0232: deepspeed info ................... 0.12.4, unknown, unknown
g0232: torch cuda version ............... 11.8
g0232: torch hip version DeepSpeed general environment info:................ 
g0232: None
g0232: torch install pathnvcc version  ....................................  11.8
g0232: ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0232: torch versiondeepspeed wheel compiled w.  ..........................  torch 2.0, cuda 11.82.0.1+cu118
g0232: 
g0232: shared memory (/dev/shm) sizedeepspeed install path  ...............  188.13 GB
g0232: ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0232: deepspeed info ................... 0.12.4, unknown, unknown
g0232: torch cuda version ............... 11.8
g0232: torch hip version ................ None
g0232: nvcc version ..................... 11.8
g0232: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0232: shared memory (/dev/shm) size .... 188.13 GB
g0232: DeepSpeed general environment info:
g0232: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0232: torch version .................... 2.0.1+cu118
g0232: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0232: deepspeed info ................... 0.12.4, unknown, unknown
g0232: torch cuda version ............... 11.8
g0232: torch hip version ................ None
g0232: nvcc version ..................... 11.8
g0232: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0232: shared memory (/dev/shm) size .... 188.13 GB
g0232: DeepSpeed general environment info:
g0232: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0232: torch version .................... 2.0.1+cu118
g0232: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0232: deepspeed info ................... 0.12.4, unknown, unknown
g0232: torch cuda version ............... 11.8
g0232: torch hip version ................ None
g0232: nvcc version ..................... 11.8
g0232: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0232: shared memory (/dev/shm) size .... 188.13 GB
g0232: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0232: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0232: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0232: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0232: [2024-08-10 08:09:14,214] [INFO] [comm.py:637:init_distributed] cdb=None
g0232: [2024-08-10 08:09:14,215] [INFO] [comm.py:637:init_distributed] cdb=None
g0232: [2024-08-10 08:09:14,215] [INFO] [comm.py:637:init_distributed] cdb=None
g0232: [2024-08-10 08:09:14,215] [INFO] [comm.py:637:init_distributed] cdb=None
g0232: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0232: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0232: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0232: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0232: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0232: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0232: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0232: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0214: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0214-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0214: > initialized tensor model parallel with size 1
g0214: > initialized pipeline model parallel with size 8
g0214: > setting random seeds to 1234 ...
g0214: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0214: > compiling dataset index builder ...
g0214: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0214: make: Nothing to be done for 'default'.
g0214: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0214: >>> done with dataset index builder. Compilation time: 0.080 seconds
g0214: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0214: > compiling and loading fused kernels ...
g0214: Detected CUDA files, patching ldflags
g0214: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0214: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0214: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0214: ninja: no work to do.
g0214: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0214: Detected CUDA files, patching ldflags
g0214: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0214: Building extension module scaled_masked_softmax_cuda...
g0214: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0214: ninja: no work to do.
g0214: Loading extension module scaled_masked_softmax_cuda...
g0214: Detected CUDA files, patching ldflags
g0214: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0214: Building extension module scaled_softmax_cuda...
g0214: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0214: ninja: no work to do.
g0214: Loading extension module scaled_softmax_cuda...
g0214: >>> done with compiling and loading fused kernels. Compilation time: 7.754 seconds
g0214: time to initialize megatron (seconds): 22.659
g0214: [after megatron is initialized] datetime: 2024-08-10 08:09:24 
g0225: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0236: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0220: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0233: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0234: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0214: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0235: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0232: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0236: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0236: wandb:  $ pip install wandb --upgrade
g0236: wandb: Tracking run with wandb version 0.17.5
g0236: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080926-qnkwq3k6
g0236: wandb: Run `wandb offline` to turn off syncing.
g0236: wandb: Syncing run g0236.abci.local
g0236: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0236: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/qnkwq3k6
g0233: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0233: wandb:  $ pip install wandb --upgrade
g0233: wandb: Tracking run with wandb version 0.17.5
g0233: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080926-we9afgmy
g0233: wandb: Run `wandb offline` to turn off syncing.
g0220: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0220: wandb:  $ pip install wandb --upgrade
g0220: wandb: Tracking run with wandb version 0.17.5
g0220: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080926-tqmnbrxq
g0220: wandb: Run `wandb offline` to turn off syncing.
g0235: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0235: wandb:  $ pip install wandb --upgrade
g0235: wandb: Tracking run with wandb version 0.17.5
g0235: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080926-qjfh9i14
g0235: wandb: Run `wandb offline` to turn off syncing.
g0214: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0214: wandb:  $ pip install wandb --upgrade
g0214: wandb: Tracking run with wandb version 0.17.5
g0214: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080926-z7kvvyyj
g0214: wandb: Run `wandb offline` to turn off syncing.
g0233: wandb: Syncing run g0233.abci.local
g0233: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0233: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/we9afgmy
g0225: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0225: wandb:  $ pip install wandb --upgrade
g0225: wandb: Tracking run with wandb version 0.17.5
g0225: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080926-zqme59d5
g0225: wandb: Run `wandb offline` to turn off syncing.
g0220: wandb: Syncing run g0220.abci.local
g0220: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0220: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/tqmnbrxq
g0235: wandb: Syncing run g0235.abci.local
g0235: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0235: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/qjfh9i14
g0214: wandb: Syncing run g0214.abci.local
g0214: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0214: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/z7kvvyyj
g0225: wandb: Syncing run g0225.abci.local
g0225: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0225: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/zqme59d5
g0234: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0234: wandb:  $ pip install wandb --upgrade
g0234: wandb: Tracking run with wandb version 0.17.5
g0234: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080926-pi80bp11
g0234: wandb: Run `wandb offline` to turn off syncing.
g0234: wandb: Syncing run g0234.abci.local
g0234: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0234: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/pi80bp11
g0232: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0232: wandb:  $ pip install wandb --upgrade
g0232: wandb: Tracking run with wandb version 0.17.5
g0232: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240810_080926-mul4bfkr
g0232: wandb: Run `wandb offline` to turn off syncing.
g0232: wandb: Syncing run g0232.abci.local
g0232: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0232: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/mul4bfkr
g0214: building GPT model ...
g0214: [2024-08-10 08:09:27,267] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0214: [2024-08-10 08:09:27,267] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0214: [2024-08-10 08:09:27,268] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 57.79 GB, percent = 15.4%
g0214: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0214: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0214: [2024-08-10 08:09:27,795] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0214: stage=0 layers=5
g0214:      0: _to_float16
g0214:      1: EmbeddingPipe
g0214:      2: ParallelTransformerLayerPipe
g0214:      3: ParallelTransformerLayerPipe
g0214:      4: ParallelTransformerLayerPipe
g0214: stage=1 layers=3
g0214:      5: ParallelTransformerLayerPipe
g0214:      6: ParallelTransformerLayerPipe
g0214:      7: ParallelTransformerLayerPipe
g0214: stage=2 layers=3
g0214:      8: ParallelTransformerLayerPipe
g0214:      9: ParallelTransformerLayerPipe
g0214:     10: ParallelTransformerLayerPipe
g0214: stage=3 layers=3
g0214:     11: ParallelTransformerLayerPipe
g0214:     12: ParallelTransformerLayerPipe
g0214:     13: ParallelTransformerLayerPipe
g0214: stage=4 layers=3
g0214:     14: ParallelTransformerLayerPipe
g0214:     15: ParallelTransformerLayerPipe
g0214:     16: ParallelTransformerLayerPipe
g0214: stage=5 layers=3
g0214:     17: ParallelTransformerLayerPipe
g0214:     18: ParallelTransformerLayerPipe
g0214:     19: ParallelTransformerLayerPipe
g0214: stage=6 layers=3
g0214:     20: ParallelTransformerLayerPipe
g0214:     21: ParallelTransformerLayerPipe
g0214:     22: ParallelTransformerLayerPipe
g0214: stage=7 layers=3
g0214:     23: ParallelTransformerLayerPipe
g0214:     24: MixedFusedRMSNorm
g0214:     25: LMHeadPipe
g0214:   loss: CrossEntropy
g0233:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0220:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0232:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0225:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0236:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0235:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0234:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0214: [2024-08-10 08:09:28,347] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0214: [2024-08-10 08:09:28,348] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0214: [2024-08-10 08:09:28,348] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 57.85 GB, percent = 15.4%
g0214:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0214: setting training iterations to 10000000
g0214: > learning rate decay style: cosine
g0214: DeepSpeed is enabled.
g0214: [2024-08-10 08:09:28,350] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0233: [2024-08-10 08:09:28,367] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0233: [2024-08-10 08:09:28,367] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0233: [2024-08-10 08:09:28,367] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0233: [2024-08-10 08:09:28,367] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0220: [2024-08-10 08:09:28,376] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0220: [2024-08-10 08:09:28,376] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0220: [2024-08-10 08:09:28,376] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0220: [2024-08-10 08:09:28,377] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0236: [2024-08-10 08:09:28,391] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0236: [2024-08-10 08:09:28,391] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0236: [2024-08-10 08:09:28,392] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0236: [2024-08-10 08:09:28,392] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0225: [2024-08-10 08:09:28,404] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0225: [2024-08-10 08:09:28,404] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0225: [2024-08-10 08:09:28,405] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0225: [2024-08-10 08:09:28,405] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0235: [2024-08-10 08:09:28,406] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0235: [2024-08-10 08:09:28,406] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0235: [2024-08-10 08:09:28,406] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0235: [2024-08-10 08:09:28,406] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0232: [2024-08-10 08:09:28,410] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0232: [2024-08-10 08:09:28,410] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0232: [2024-08-10 08:09:28,410] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0232: [2024-08-10 08:09:28,411] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0234: [2024-08-10 08:09:28,454] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0234: [2024-08-10 08:09:28,454] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0234: [2024-08-10 08:09:28,454] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0234: [2024-08-10 08:09:28,455] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0214: [2024-08-10 08:09:28,536] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0214: [2024-08-10 08:09:28,537] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0214: [2024-08-10 08:09:28,537] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0214: [2024-08-10 08:09:28,537] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0214: [2024-08-10 08:09:28,538] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0214: [2024-08-10 08:09:28,572] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0214: [2024-08-10 08:09:28,573] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0214: [2024-08-10 08:09:28,573] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0214: [2024-08-10 08:09:28,573] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f21327ce490>
g0214: [2024-08-10 08:09:28,573] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0214: [2024-08-10 08:09:28,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: [2024-08-10 08:09:28,573] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0214: [2024-08-10 08:09:28,573] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0214: [2024-08-10 08:09:28,574] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0214:     "partition_activations": false, 
g0214:     "contiguous_memory_optimization": false, 
g0214:     "cpu_checkpointing": false, 
g0214:     "number_checkpoints": null, 
g0214:     "synchronize_checkpoint_boundary": false, 
g0214:     "profile": false
g0214: }
g0214: [2024-08-10 08:09:28,574] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0214: [2024-08-10 08:09:28,574] [INFO] [config.py:983:print]   amp_enabled .................. False
g0214: [2024-08-10 08:09:28,574] [INFO] [config.py:983:print]   amp_params ................... False
g0214: [2024-08-10 08:09:28,575] [INFO] [config.py:983:print]   autotuning_config ............ {
g0214:     "enabled": false, 
g0214:     "start_step": null, 
g0214:     "end_step": null, 
g0214:     "metric_path": null, 
g0214:     "arg_mappings": null, 
g0214:     "metric": "throughput", 
g0214:     "model_info": null, 
g0214:     "results_dir": "autotuning_results", 
g0214:     "exps_dir": "autotuning_exps", 
g0214:     "overwrite": true, 
g0214:     "fast": true, 
g0214:     "start_profile_step": 3, 
g0214:     "end_profile_step": 5, 
g0214:     "tuner_type": "gridsearch", 
g0214:     "tuner_early_stopping": 5, 
g0214:     "tuner_num_trials": 50, 
g0214:     "model_info_path": null, 
g0214:     "mp_size": 1, 
g0214:     "max_train_batch_size": null, 
g0214:     "min_train_batch_size": 1, 
g0214:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0214:     "min_train_micro_batch_size_per_gpu": 1, 
g0214:     "num_tuning_micro_batch_sizes": 3
g0214: }
g0214: [2024-08-10 08:09:28,575] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0214: [2024-08-10 08:09:28,575] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0214: [2024-08-10 08:09:28,575] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0214: [2024-08-10 08:09:28,575] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0214: [2024-08-10 08:09:28,575] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f20801eed10>
g0214: [2024-08-10 08:09:28,576] [INFO] [config.py:983:print]   communication_data_type ...... None
g0214: [2024-08-10 08:09:28,576] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0214: [2024-08-10 08:09:28,576] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0214: [2024-08-10 08:09:28,576] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0214: [2024-08-10 08:09:28,576] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0214: [2024-08-10 08:09:28,576] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0214: [2024-08-10 08:09:28,576] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0214: [2024-08-10 08:09:28,577] [INFO] [config.py:983:print]   disable_allgather ............ False
g0214: [2024-08-10 08:09:28,577] [INFO] [config.py:983:print]   dump_state ................... False
g0214: [2024-08-10 08:09:28,577] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0214: [2024-08-10 08:09:28,577] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0214: [2024-08-10 08:09:28,577] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0214: [2024-08-10 08:09:28,577] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0214: [2024-08-10 08:09:28,577] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0214: [2024-08-10 08:09:28,578] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0214: [2024-08-10 08:09:28,578] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0214: [2024-08-10 08:09:28,578] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0214: [2024-08-10 08:09:28,578] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0214: [2024-08-10 08:09:28,578] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0214: [2024-08-10 08:09:28,578] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0214:     "enabled": false, 
g0214:     "recompute_fwd_factor": 0.0, 
g0214:     "profile_step": 1, 
g0214:     "module_depth": -1, 
g0214:     "top_modules": 1, 
g0214:     "detailed": true, 
g0214:     "output_file": null
g0214: }
g0214: [2024-08-10 08:09:28,578] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0214: [2024-08-10 08:09:28,578] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0214: [2024-08-10 08:09:28,579] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0214: [2024-08-10 08:09:28,579] [INFO] [config.py:983:print]   global_rank .................. 0
g0214: [2024-08-10 08:09:28,579] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0214: [2024-08-10 08:09:28,579] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0214: [2024-08-10 08:09:28,579] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0214: [2024-08-10 08:09:28,579] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0214: [2024-08-10 08:09:28,579] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0214: [2024-08-10 08:09:28,579] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0214: [2024-08-10 08:09:28,580] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0214: [2024-08-10 08:09:28,580] [INFO] [config.py:983:print]   loss_scale ................... 0
g0214: [2024-08-10 08:09:28,580] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0214: [2024-08-10 08:09:28,580] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0214: [2024-08-10 08:09:28,580] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0214: [2024-08-10 08:09:28,580] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0214: [2024-08-10 08:09:28,580] [INFO] [config.py:983:print]   nebula_config ................ {
g0214:     "enabled": false, 
g0214:     "persistent_storage_path": null, 
g0214:     "persistent_time_interval": 100, 
g0214:     "num_of_version_in_retention": 2, 
g0214:     "enable_nebula_load": true, 
g0214:     "load_path": null
g0214: }
g0214: [2024-08-10 08:09:28,581] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0214: [2024-08-10 08:09:28,581] [INFO] [config.py:983:print]   optimizer_name ............... None
g0214: [2024-08-10 08:09:28,581] [INFO] [config.py:983:print]   optimizer_params ............. None
g0214: [2024-08-10 08:09:28,581] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0214: [2024-08-10 08:09:28,581] [INFO] [config.py:983:print]   pld_enabled .................. False
g0214: [2024-08-10 08:09:28,582] [INFO] [config.py:983:print]   pld_params ................... False
g0214: [2024-08-10 08:09:28,582] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0214: [2024-08-10 08:09:28,582] [INFO] [config.py:983:print]   scheduler_name ............... None
g0214: [2024-08-10 08:09:28,582] [INFO] [config.py:983:print]   scheduler_params ............. None
g0214: [2024-08-10 08:09:28,582] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0214: [2024-08-10 08:09:28,582] [INFO] [config.py:983:print]   sparse_attention ............. None
g0214: [2024-08-10 08:09:28,582] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0214: [2024-08-10 08:09:28,582] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0214: [2024-08-10 08:09:28,583] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0214: [2024-08-10 08:09:28,583] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0214: [2024-08-10 08:09:28,583] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0214: [2024-08-10 08:09:28,583] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0214: [2024-08-10 08:09:28,583] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0214: [2024-08-10 08:09:28,583] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0214: [2024-08-10 08:09:28,584] [INFO] [config.py:983:print]   world_size ................... 4
g0214: [2024-08-10 08:09:28,584] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0214: [2024-08-10 08:09:28,584] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0214: [2024-08-10 08:09:28,584] [INFO] [config.py:983:print]   zero_enabled ................. False
g0214: [2024-08-10 08:09:28,584] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0214: [2024-08-10 08:09:28,584] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0214: [2024-08-10 08:09:28,584] [INFO] [config.py:969:print_user_config]   json = {
g0214:     "train_batch_size": 128, 
g0214:     "train_micro_batch_size_per_gpu": 1, 
g0214:     "steps_per_print": 10, 
g0214:     "zero_optimization": {
g0214:         "stage": 0
g0214:     }, 
g0214:     "gradient_clipping": 1.0, 
g0214:     "prescale_gradients": true, 
g0214:     "fp16": {
g0214:         "enabled": true, 
g0214:         "loss_scale": 0, 
g0214:         "loss_scale_window": 500, 
g0214:         "hysteresis": 2, 
g0214:         "min_loss_scale": 1, 
g0214:         "initial_scale_power": 11
g0214:     }, 
g0214:     "wall_clock_breakdown": false
g0214: }
g0214: [2024-08-10 08:09:28,585] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0214: [2024-08-10 08:09:28,585] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0214: [2024-08-10 08:09:29,312] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0225: [2024-08-10 08:09:29,313] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0235: [2024-08-10 08:09:29,313] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0236: [2024-08-10 08:09:29,313] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0233: [2024-08-10 08:09:29,313] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0234: [2024-08-10 08:09:29,314] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0220: [2024-08-10 08:09:29,314] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0232: [2024-08-10 08:09:29,314] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0236: [2024-08-10 08:09:30,025] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0236: [2024-08-10 08:09:30,025] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0236: [2024-08-10 08:09:30,025] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0236: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0214: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0214: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0214: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0225: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0235: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0232: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0235: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0232: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0235: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0232: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0232: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0234: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0225: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0220: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0233: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0225: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0220: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0225: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0233: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0220: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0214: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0233: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0233: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0234: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0234: [2024-08-10 08:09:30,026] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0220: [2024-08-10 08:09:30,027] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0235: [2024-08-10 08:09:30,027] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0234: [2024-08-10 08:09:30,027] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0225: [2024-08-10 08:09:33,715] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0225: [2024-08-10 08:09:33,715] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0225: [2024-08-10 08:09:33,715] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0225: [2024-08-10 08:09:33,715] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0225: [2024-08-10 08:09:33,723] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt...
g0225: [2024-08-10 08:09:33,724] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt...
g0225: [2024-08-10 08:09:33,725] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt...
g0225: [2024-08-10 08:09:33,725] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt...
g0232: [2024-08-10 08:09:33,754] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0232: [2024-08-10 08:09:33,754] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0232: [2024-08-10 08:09:33,754] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0233: [2024-08-10 08:09:33,759] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0233: [2024-08-10 08:09:33,759] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0233: [2024-08-10 08:09:33,759] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0233: [2024-08-10 08:09:33,760] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0232: [2024-08-10 08:09:33,759] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0232: [2024-08-10 08:09:33,762] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt...
g0232: [2024-08-10 08:09:33,762] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt...
g0232: [2024-08-10 08:09:33,762] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt...
g0232: [2024-08-10 08:09:33,767] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt...
g0233: [2024-08-10 08:09:33,768] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt...
g0233: [2024-08-10 08:09:33,768] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt...
g0233: [2024-08-10 08:09:33,768] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt...
g0233: [2024-08-10 08:09:33,768] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt...
g0220: [2024-08-10 08:09:33,807] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0220: [2024-08-10 08:09:33,807] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0220: [2024-08-10 08:09:33,807] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0220: [2024-08-10 08:09:33,808] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0220: [2024-08-10 08:09:33,815] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt...
g0220: [2024-08-10 08:09:33,815] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt...
g0220: [2024-08-10 08:09:33,815] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt...
g0220: [2024-08-10 08:09:33,816] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt...
g0235: [2024-08-10 08:09:33,863] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0235: [2024-08-10 08:09:33,863] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0235: [2024-08-10 08:09:33,863] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0235: [2024-08-10 08:09:33,863] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0234: [2024-08-10 08:09:33,863] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0234: [2024-08-10 08:09:33,863] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0234: [2024-08-10 08:09:33,864] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0234: [2024-08-10 08:09:33,864] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 08:09:33,869] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 08:09:33,869] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 08:09:33,869] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 08:09:33,870] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0235: [2024-08-10 08:09:33,871] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt...
g0235: [2024-08-10 08:09:33,871] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt...
g0235: [2024-08-10 08:09:33,871] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt...
g0235: [2024-08-10 08:09:33,871] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt...
g0234: [2024-08-10 08:09:33,871] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt...
g0234: [2024-08-10 08:09:33,871] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt...
g0234: [2024-08-10 08:09:33,872] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt...
g0234: [2024-08-10 08:09:33,872] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt...
g0214: [2024-08-10 08:09:33,877] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0214: [2024-08-10 08:09:33,877] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0214: [2024-08-10 08:09:33,878] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0214: [2024-08-10 08:09:33,878] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt...
g0236: [2024-08-10 08:09:33,996] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0236: [2024-08-10 08:09:33,996] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0236: [2024-08-10 08:09:33,996] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0236: [2024-08-10 08:09:33,996] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0236: [2024-08-10 08:09:34,004] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt...
g0236: [2024-08-10 08:09:34,004] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt...
g0236: [2024-08-10 08:09:34,005] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt...
g0236: [2024-08-10 08:09:34,005] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt...
g0232: [2024-08-10 08:09:35,659] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt.
g0232: [2024-08-10 08:09:35,659] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt.
g0232: [2024-08-10 08:09:35,659] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt.
g0232: [2024-08-10 08:09:35,660] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_03_model_states.pt.
g0232: [2024-08-10 08:09:35,660] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0232: [2024-08-10 08:09:35,660] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0232: [2024-08-10 08:09:35,661] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0232: [2024-08-10 08:09:35,661] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0220: [2024-08-10 08:09:35,671] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt.
g0220: [2024-08-10 08:09:35,671] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt.
g0220: [2024-08-10 08:09:35,672] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0220: [2024-08-10 08:09:35,672] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt.
g0220: [2024-08-10 08:09:35,672] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_01_model_states.pt.
g0220: [2024-08-10 08:09:35,672] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0220: [2024-08-10 08:09:35,673] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0220: [2024-08-10 08:09:35,673] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0225: [2024-08-10 08:09:35,818] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt.
g0225: [2024-08-10 08:09:35,818] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt.
g0225: [2024-08-10 08:09:35,819] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt.
g0225: [2024-08-10 08:09:35,819] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0225: [2024-08-10 08:09:35,819] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0225: [2024-08-10 08:09:35,819] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_02_model_states.pt.
g0225: [2024-08-10 08:09:35,819] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0225: [2024-08-10 08:09:35,820] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,085] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt.
g0234: [2024-08-10 08:09:36,085] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt.
g0234: [2024-08-10 08:09:36,085] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt.
g0234: [2024-08-10 08:09:36,085] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_05_model_states.pt.
g0234: [2024-08-10 08:09:36,086] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,086] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,086] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,086] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,167] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,167] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,167] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,167] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,167] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,167] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,167] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,168] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,200] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,201] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,201] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,202] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_05-model_00-model_states.pt.
g0236: [2024-08-10 08:09:36,211] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt.
g0236: [2024-08-10 08:09:36,211] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt.
g0236: [2024-08-10 08:09:36,211] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt.
g0236: [2024-08-10 08:09:36,212] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0236: [2024-08-10 08:09:36,212] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0236: [2024-08-10 08:09:36,213] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0236: [2024-08-10 08:09:36,215] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_07_model_states.pt.
g0220: [2024-08-10 08:09:36,215] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0236: [2024-08-10 08:09:36,216] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,217] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,222] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,223] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0233: [2024-08-10 08:09:36,261] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt.
g0233: [2024-08-10 08:09:36,261] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt.
g0233: [2024-08-10 08:09:36,262] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0233: [2024-08-10 08:09:36,262] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0233: [2024-08-10 08:09:36,263] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt.
g0233: [2024-08-10 08:09:36,265] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0233: [2024-08-10 08:09:36,269] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_04_model_states.pt.
g0233: [2024-08-10 08:09:36,270] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0232: [2024-08-10 08:09:36,305] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0232: [2024-08-10 08:09:36,305] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0232: [2024-08-10 08:09:36,305] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0232: [2024-08-10 08:09:36,305] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0232: [2024-08-10 08:09:36,305] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0232: [2024-08-10 08:09:36,306] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0232: [2024-08-10 08:09:36,306] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0232: [2024-08-10 08:09:36,306] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt...
g0232: [2024-08-10 08:09:36,339] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0232: [2024-08-10 08:09:36,339] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0232: [2024-08-10 08:09:36,342] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0232: [2024-08-10 08:09:36,342] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_11-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,345] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,345] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,345] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,345] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,346] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,346] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,346] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,346] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt...
g0232: [2024-08-10 08:09:36,357] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0232: [2024-08-10 08:09:36,357] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0232: [2024-08-10 08:09:36,362] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0232: [2024-08-10 08:09:36,362] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,378] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,382] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,382] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_17-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,400] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,401] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,403] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,403] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0225: [2024-08-10 08:09:36,433] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0225: [2024-08-10 08:09:36,434] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0225: [2024-08-10 08:09:36,434] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0225: [2024-08-10 08:09:36,434] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0225: [2024-08-10 08:09:36,435] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0225: [2024-08-10 08:09:36,435] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0225: [2024-08-10 08:09:36,435] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0225: [2024-08-10 08:09:36,436] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt...
g0225: [2024-08-10 08:09:36,466] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0225: [2024-08-10 08:09:36,470] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0225: [2024-08-10 08:09:36,470] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0225: [2024-08-10 08:09:36,477] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_08-model_00-model_states.pt.
g0225: [2024-08-10 08:09:36,483] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0225: [2024-08-10 08:09:36,485] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0225: [2024-08-10 08:09:36,487] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0225: [2024-08-10 08:09:36,500] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,531] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,531] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,531] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,531] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,531] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,531] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,532] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,532] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,562] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,562] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,562] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,563] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_06-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,576] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,579] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,580] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,584] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,699] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,699] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,700] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,700] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,700] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,700] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,700] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,700] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt...
g0214: [2024-08-10 08:09:36,715] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 08:09:36,715] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 08:09:36,715] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 08:09:36,715] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 08:09:36,716] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0214: [2024-08-10 08:09:36,716] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0214: [2024-08-10 08:09:36,716] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0214: [2024-08-10 08:09:36,716] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,731] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,731] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,733] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,734] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_18-model_00-model_states.pt.
g0234: [2024-08-10 08:09:36,752] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,752] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,754] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0234: [2024-08-10 08:09:36,755] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0214: [2024-08-10 08:09:36,902] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0214: [2024-08-10 08:09:36,903] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0214: [2024-08-10 08:09:36,909] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0214: [2024-08-10 08:09:36,910] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0214: [2024-08-10 08:09:36,910] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0214: [2024-08-10 08:09:36,910] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0214: [2024-08-10 08:09:36,910] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0214: [2024-08-10 08:09:36,910] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt...
g0235: [2024-08-10 08:09:36,911] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt.
g0235: [2024-08-10 08:09:36,912] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0235: [2024-08-10 08:09:36,912] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt.
g0235: [2024-08-10 08:09:36,912] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt.
g0235: [2024-08-10 08:09:36,912] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/mp_rank_06_model_states.pt.
g0235: [2024-08-10 08:09:36,913] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0235: [2024-08-10 08:09:36,913] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0235: [2024-08-10 08:09:36,913] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0214: [2024-08-10 08:09:36,949] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,954] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,954] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,955] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,955] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,955] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,955] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,955] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,955] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt...
g0214: [2024-08-10 08:09:36,955] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0214: [2024-08-10 08:09:36,960] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0214: [2024-08-10 08:09:36,960] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_01-model_00-model_states.pt.
g0214: [2024-08-10 08:09:36,979] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0236: [2024-08-10 08:09:36,985] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 08:09:36,985] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0214: [2024-08-10 08:09:36,985] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0236: [2024-08-10 08:09:36,985] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 08:09:36,985] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0236: [2024-08-10 08:09:36,985] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 08:09:36,985] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0236: [2024-08-10 08:09:36,986] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0236: [2024-08-10 08:09:36,986] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt...
g0220: [2024-08-10 08:09:36,986] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,986] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,988] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 08:09:36,988] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_07-model_00-model_states.pt.
g0214: [2024-08-10 08:09:36,989] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0214: [2024-08-10 08:09:36,989] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,021] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,021] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,021] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,022] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,037] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,043] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,043] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,043] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,069] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_24-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,070] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,070] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,070] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,070] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,107] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,107] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,107] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,107] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,107] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,107] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,108] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,109] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,138] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,138] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,139] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,139] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_12-model_00-model_states.pt.
g0234: [2024-08-10 08:09:37,145] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 08:09:37,145] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 08:09:37,145] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 08:09:37,146] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 08:09:37,146] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0234: [2024-08-10 08:09:37,146] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0234: [2024-08-10 08:09:37,146] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0234: [2024-08-10 08:09:37,146] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,153] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,154] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,160] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,160] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0234: [2024-08-10 08:09:37,177] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 08:09:37,177] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 08:09:37,178] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 08:09:37,179] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_19-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,207] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,208] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,208] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,208] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,208] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,208] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,209] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,209] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,241] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,241] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,243] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,244] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_20-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,258] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,259] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,262] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,264] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,306] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,307] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,309] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,310] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,310] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,310] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,310] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,310] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt...
g0214: [2024-08-10 08:09:37,352] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0214: [2024-08-10 08:09:37,352] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0214: [2024-08-10 08:09:37,352] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0214: [2024-08-10 08:09:37,353] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0214: [2024-08-10 08:09:37,353] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0214: [2024-08-10 08:09:37,353] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0214: [2024-08-10 08:09:37,353] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0214: [2024-08-10 08:09:37,353] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt...
g0236: [2024-08-10 08:09:37,354] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,354] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,362] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 08:09:37,362] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_25-model_00-model_states.pt.
g0214: [2024-08-10 08:09:37,386] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0214: [2024-08-10 08:09:37,386] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0214: [2024-08-10 08:09:37,389] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0214: [2024-08-10 08:09:37,389] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_02-model_00-model_states.pt.
g0233: [2024-08-10 08:09:37,393] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0233: [2024-08-10 08:09:37,393] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0233: [2024-08-10 08:09:37,393] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0233: [2024-08-10 08:09:37,393] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0233: [2024-08-10 08:09:37,393] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0233: [2024-08-10 08:09:37,393] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0233: [2024-08-10 08:09:37,394] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0233: [2024-08-10 08:09:37,394] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt...
g0214: [2024-08-10 08:09:37,408] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0214: [2024-08-10 08:09:37,408] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0214: [2024-08-10 08:09:37,410] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0214: [2024-08-10 08:09:37,410] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0233: [2024-08-10 08:09:37,428] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0233: [2024-08-10 08:09:37,428] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0233: [2024-08-10 08:09:37,429] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0233: [2024-08-10 08:09:37,430] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_14-model_00-model_states.pt.
g0233: [2024-08-10 08:09:37,445] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0233: [2024-08-10 08:09:37,446] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0233: [2024-08-10 08:09:37,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0233: [2024-08-10 08:09:37,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,664] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,664] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,664] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,664] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,664] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,665] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,665] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,665] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,694] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,694] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,697] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0235: [2024-08-10 08:09:37,698] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_21-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,698] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,699] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,699] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,699] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,699] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,699] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,699] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,700] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,712] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,712] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,716] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0235: [2024-08-10 08:09:37,716] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0232: [2024-08-10 08:09:37,731] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,731] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,732] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0232: [2024-08-10 08:09:37,733] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_13-model_00-model_states.pt.
g0225: [2024-08-10 08:09:37,733] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 08:09:37,733] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 08:09:37,733] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 08:09:37,733] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 08:09:37,733] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0225: [2024-08-10 08:09:37,733] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0225: [2024-08-10 08:09:37,734] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0225: [2024-08-10 08:09:37,734] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt...
g0225: [2024-08-10 08:09:37,763] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 08:09:37,767] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 08:09:37,767] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 08:09:37,767] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 08:09:37,781] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0225: [2024-08-10 08:09:37,781] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0225: [2024-08-10 08:09:37,784] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0225: [2024-08-10 08:09:37,789] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0225: [2024-08-10 08:09:38,062] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0225: [2024-08-10 08:09:38,062] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0225: [2024-08-10 08:09:38,062] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0225: [2024-08-10 08:09:38,063] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0214: [2024-08-10 08:09:38,062] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,062] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,063] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,063] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,063] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0214: [2024-08-10 08:09:38,063] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0214: [2024-08-10 08:09:38,063] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0214: [2024-08-10 08:09:38,063] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt...
g0225: [2024-08-10 08:09:38,064] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0225: [2024-08-10 08:09:38,064] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0225: [2024-08-10 08:09:38,064] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0225: [2024-08-10 08:09:38,064] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt...
g0225: [2024-08-10 08:09:38,092] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,096] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,097] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_03-model_00-model_states.pt.
g0225: [2024-08-10 08:09:38,097] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0225: [2024-08-10 08:09:38,097] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0225: [2024-08-10 08:09:38,097] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_10-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,116] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0214: [2024-08-10 08:09:38,116] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0214: [2024-08-10 08:09:38,117] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0214: [2024-08-10 08:09:38,118] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0233: [2024-08-10 08:09:38,479] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0233: [2024-08-10 08:09:38,479] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0233: [2024-08-10 08:09:38,479] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0233: [2024-08-10 08:09:38,479] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0233: [2024-08-10 08:09:38,480] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0233: [2024-08-10 08:09:38,480] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0233: [2024-08-10 08:09:38,480] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0233: [2024-08-10 08:09:38,480] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt...
g0233: [2024-08-10 08:09:38,511] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0233: [2024-08-10 08:09:38,511] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0233: [2024-08-10 08:09:38,513] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0233: [2024-08-10 08:09:38,513] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_15-model_00-model_states.pt.
g0233: [2024-08-10 08:09:38,530] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0233: [2024-08-10 08:09:38,530] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0233: [2024-08-10 08:09:38,533] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0233: [2024-08-10 08:09:38,533] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0235: [2024-08-10 08:09:38,658] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 08:09:38,658] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 08:09:38,658] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 08:09:38,659] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0235: [2024-08-10 08:09:38,659] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 08:09:38,659] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0235: [2024-08-10 08:09:38,659] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0235: [2024-08-10 08:09:38,659] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt...
g0235: [2024-08-10 08:09:38,689] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 08:09:38,692] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 08:09:38,692] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 08:09:38,695] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_22-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,874] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,874] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,874] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,874] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,875] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0214: [2024-08-10 08:09:38,875] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0214: [2024-08-10 08:09:38,875] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0214: [2024-08-10 08:09:38,875] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt...
g0214: [2024-08-10 08:09:38,907] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,907] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,908] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 08:09:38,908] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_04-model_00-model_states.pt.
g0233: [2024-08-10 08:09:39,227] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 08:09:39,227] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 08:09:39,227] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 08:09:39,228] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 08:09:39,228] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0233: [2024-08-10 08:09:39,228] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0233: [2024-08-10 08:09:39,228] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0233: [2024-08-10 08:09:39,228] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt...
g0233: [2024-08-10 08:09:39,259] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 08:09:39,259] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 08:09:39,262] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 08:09:39,262] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step38000/layer_16-model_00-model_states.pt.
g0214:  > overriding learning rate value to 0.0002
g0214:  > overriding minimum learning rate value to 2e-06
g0214:  > overriding warmup iterations value to 0
g0214:  > overriding warmup tokens value to 3000000000
g0214:  > overriding total number of iterations value to 1280000000
g0214:  > overriding decay tokens value to 300000000000
g0214:  > overriding learning rate decay style value to cosine
g0214:  > overriding start weight decay value to 0.1
g0214:  > overriding end weight decay value to 0.1
g0214:  > overriding total number of weight decay iterations value to 1280000000
g0214:  > overriding weight decay incr style value to constant
g0214:  checkpoint version 3.0
g0214:   successfully loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase at iteration 38000
g0236: (min, max) time across ranks (ms):
g0236:     load-checkpoint ................................: (10415.88, 10417.37)
g0214: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-10 08:09:40 
g0214: > building train, validation, and test datasets ...
g0214:  > datasets target sizes (minimum size):
g0214:     train:      1280000000
g0214:     validation: 128012800
g0214:     test:       12800
g0214: > building train, validation, and test datasets for GPT ...
g0214: Single data path provided for train, valid & test
g0214:  > building dataset index ...
g0214:     reading sizes...
g0214:     reading pointers...
g0214:     reading document index...
g0214:     creating numpy buffer of mmap...
g0214:     creating memory view of numpy buffer...
g0214:  > finished creating indexed dataset in 0.198552 seconds
g0214:     number of documents: 2237032
g0214:  > dataset split:
g0214:     train:
g0214:      document indices in [0, 2122943) total of 2122943 documents
g0214:     validation:
g0214:      document indices in [2122943, 2234795) total of 111852 documents
g0214:     test:
g0214:      document indices in [2234795, 2237032) total of 2237 documents
g0214:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3596e0161783fa35db8c4808febe221a_doc_idx.npy
g0214:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3596e0161783fa35db8c4808febe221a_sample_idx.npy
g0214:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3596e0161783fa35db8c4808febe221a_shuffle_idx.npy
g0214:     loaded indexed file in 0.235 seconds
g0214:     total number of samples: 10749555
g0214:     total number of epochs: 1
g0214:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/f33b588cfa0bd03d2d3e7adf89988baa_doc_idx.npy
g0214:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/f33b588cfa0bd03d2d3e7adf89988baa_sample_idx.npy
g0214:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/f33b588cfa0bd03d2d3e7adf89988baa_shuffle_idx.npy
g0214:     loaded indexed file in 0.307 seconds
g0214:     total number of samples: 128527028
g0214:     total number of epochs: 228
g0214:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/6749304f1e1aa62f2f0d4bf70e8db4aa_doc_idx.npy
g0214:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/6749304f1e1aa62f2f0d4bf70e8db4aa_sample_idx.npy
g0214:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/6749304f1e1aa62f2f0d4bf70e8db4aa_shuffle_idx.npy
g0214:     loaded indexed file in 0.038 seconds
g0214:     total number of samples: 14468
g0214:     total number of epochs: 2
g0214: > finished creating GPT datasets ...
g0214: [after dataloaders are built] datetime: 2024-08-10 08:09:43 
g0214: done with setup ...
g0214: training ...
g0236: (min, max) time across ranks (ms):
g0236:     model-and-optimizer-setup ......................: (13319.92, 13344.30)
g0236:     train/valid/test-data-iterators-setup ..........: (2766.02, 2772.52)
g0214: [before the start of training step] datetime: 2024-08-10 08:09:43 
g0214: [2024-08-10 08:10:44,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=38010, skipped=57, lr=[0.00019973153223664213, 0.00019973153223664213], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38010 loss: 1.0509 iter time (s): 6.048 samples/sec: 21.163
g0236:  iteration    38010/10000000 | consumed samples:      4865280 | consumed tokens:   9964093440 | elapsed time per iteration (ms): 6135.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.010939E+00 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.863 | tokens per gpu per second (tgs): 1335.203 | TFLOPs: 10.74 |
g0232: [Rank 12] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7222.0 | max reserved: 7222.0
g0236: [Rank 28] (after 38010 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0214: [Rank 0] (after 38010 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 10902.0 | max reserved: 10902.0
g0235: [Rank 24] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 5054.0 | max reserved: 5054.0
g0225: [Rank 8] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8116.0 | max reserved: 8116.0
g0234: [Rank 20] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5414.0 | max reserved: 5414.0
g0233: [Rank 16] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6328.0 | max reserved: 6328.0
g0220: [Rank 4] (after 38010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 9010.0 | max reserved: 9010.0
g0214: [2024-08-10 08:11:30,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=38020, skipped=57, lr=[0.00019973133016797814, 0.00019973133016797814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38020 loss: 1.1740 iter time (s): 4.407 samples/sec: 29.042
g0236:  iteration    38020/10000000 | consumed samples:      4866560 | consumed tokens:   9966714880 | elapsed time per iteration (ms): 4574.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.054584E+00 | loss scale: 16384.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.981 | tokens per gpu per second (tgs): 1790.815 | TFLOPs: 14.41 |
g0214: [2024-08-10 08:12:14,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=38030, skipped=57, lr=[0.00019973112802340018, 0.00019973112802340018], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38030 loss: 0.8335 iter time (s): 4.392 samples/sec: 29.141
g0236:  iteration    38030/10000000 | consumed samples:      4867840 | consumed tokens:   9969336320 | elapsed time per iteration (ms): 4425.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.017080E+00 | loss scale: 16384.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.923 | tokens per gpu per second (tgs): 1851.088 | TFLOPs: 14.90 |
g0214: [2024-08-10 08:12:59,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=38040, skipped=57, lr=[0.00019973092580290844, 0.00019973092580290844], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38040 loss: 1.2096 iter time (s): 4.494 samples/sec: 28.482
g0236:  iteration    38040/10000000 | consumed samples:      4869120 | consumed tokens:   9971957760 | elapsed time per iteration (ms): 4527.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.999089E-01 | loss scale: 16384.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.271 | tokens per gpu per second (tgs): 1809.326 | TFLOPs: 14.56 |
g0214: [2024-08-10 08:13:47,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=38050, skipped=57, lr=[0.00019973072350650301, 0.00019973072350650301], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38050 loss: 1.0000 iter time (s): 4.736 samples/sec: 27.025
g0236:  iteration    38050/10000000 | consumed samples:      4870400 | consumed tokens:   9974579200 | elapsed time per iteration (ms): 4768.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.778531E-01 | loss scale: 16384.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.840 | tokens per gpu per second (tgs): 1717.785 | TFLOPs: 13.82 |
g0214: [2024-08-10 08:14:34,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=38060, skipped=57, lr=[0.0001997305211341841, 0.0001997305211341841], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38060 loss: 1.1435 iter time (s): 4.692 samples/sec: 27.278
g0236:  iteration    38060/10000000 | consumed samples:      4871680 | consumed tokens:   9977200640 | elapsed time per iteration (ms): 4725.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.788863E-01 | loss scale: 16384.0 | grad norm: 0.277 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.087 | tokens per gpu per second (tgs): 1733.599 | TFLOPs: 13.95 |
g0214: [2024-08-10 08:15:24,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=38070, skipped=57, lr=[0.00019973031868595186, 0.00019973031868595186], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38070 loss: 0.9924 iter time (s): 4.947 samples/sec: 25.873
g0236:  iteration    38070/10000000 | consumed samples:      4872960 | consumed tokens:   9979822080 | elapsed time per iteration (ms): 4981.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.028003E+00 | loss scale: 16384.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.696 | tokens per gpu per second (tgs): 1644.554 | TFLOPs: 13.23 |
g0214: [2024-08-10 08:16:09,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=38080, skipped=57, lr=[0.0001997301161618064, 0.0001997301161618064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38080 loss: 1.0555 iter time (s): 4.452 samples/sec: 28.750
g0236:  iteration    38080/10000000 | consumed samples:      4874240 | consumed tokens:   9982443520 | elapsed time per iteration (ms): 4485.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.006478E+00 | loss scale: 16384.0 | grad norm: 0.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.534 | tokens per gpu per second (tgs): 1826.196 | TFLOPs: 14.70 |
g0214: [2024-08-10 08:17:00,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=38090, skipped=57, lr=[0.00019972991356174792, 0.00019972991356174792], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38090 loss: 1.1416 iter time (s): 5.018 samples/sec: 25.507
g0236:  iteration    38090/10000000 | consumed samples:      4875520 | consumed tokens:   9985064960 | elapsed time per iteration (ms): 5051.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.859339E-01 | loss scale: 16384.0 | grad norm: 0.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.340 | tokens per gpu per second (tgs): 1621.748 | TFLOPs: 13.05 |
g0214: [2024-08-10 08:17:48,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=38100, skipped=57, lr=[0.00019972971088577657, 0.00019972971088577657], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38100 loss: 1.0355 iter time (s): 4.810 samples/sec: 26.612
g0236:  iteration    38100/10000000 | consumed samples:      4876800 | consumed tokens:   9987686400 | elapsed time per iteration (ms): 4842.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.013719E+00 | loss scale: 16384.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.430 | tokens per gpu per second (tgs): 1691.534 | TFLOPs: 13.61 |
g0214: [2024-08-10 08:18:42,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=38110, skipped=57, lr=[0.0001997295081338925, 0.0001997295081338925], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38110 loss: 1.2734 iter time (s): 5.376 samples/sec: 23.809
g0236:  iteration    38110/10000000 | consumed samples:      4878080 | consumed tokens:   9990307840 | elapsed time per iteration (ms): 5409.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.020328E+00 | loss scale: 16384.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.662 | tokens per gpu per second (tgs): 1514.396 | TFLOPs: 12.19 |
g0214: [2024-08-10 08:19:34,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=38120, skipped=57, lr=[0.00019972930530609586, 0.00019972930530609586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38120 loss: 0.9699 iter time (s): 5.160 samples/sec: 24.805
g0236:  iteration    38120/10000000 | consumed samples:      4879360 | consumed tokens:   9992929280 | elapsed time per iteration (ms): 5193.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.014074E+00 | loss scale: 16384.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.647 | tokens per gpu per second (tgs): 1577.408 | TFLOPs: 12.69 |
g0214: [2024-08-10 08:20:27,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=38130, skipped=57, lr=[0.00019972910240238683, 0.00019972910240238683], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38130 loss: 1.0448 iter time (s): 5.271 samples/sec: 24.285
g0236:  iteration    38130/10000000 | consumed samples:      4880640 | consumed tokens:   9995550720 | elapsed time per iteration (ms): 5303.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.884739E-01 | loss scale: 16384.0 | grad norm: 0.269 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.134 | tokens per gpu per second (tgs): 1544.556 | TFLOPs: 12.43 |
g0235: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0235: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0214: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0214: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0234: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0234: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0236: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0236: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0232: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0233: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0220: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0225: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0233: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0232: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0220: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0220: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0225: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 08:21:12,671] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0234: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0233: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0225: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0233: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0235: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0234: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0236: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0232: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0236: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0220: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0214: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0214: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0235: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0220: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0236: [2024-08-10 08:21:12,672] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0214: [2024-08-10 08:21:12,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=38140, skipped=57, lr=[0.00019972889942276552, 0.00019972889942276552], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38140 loss: 0.8839 iter time (s): 4.476 samples/sec: 28.595
g0236:  iteration    38140/10000000 | consumed samples:      4881920 | consumed tokens:   9998172160 | elapsed time per iteration (ms): 4508.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.695728E-01 | loss scale: 32768.0 | grad norm: 0.099 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.390 | tokens per gpu per second (tgs): 1816.937 | TFLOPs: 14.62 |
g0214: [2024-08-10 08:22:00,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=38150, skipped=57, lr=[0.00019972869636723212, 0.00019972869636723212], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38150 loss: 1.0955 iter time (s): 4.799 samples/sec: 26.671
g0236:  iteration    38150/10000000 | consumed samples:      4883200 | consumed tokens:  10000793600 | elapsed time per iteration (ms): 4831.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.021742E+00 | loss scale: 32768.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.491 | tokens per gpu per second (tgs): 1695.441 | TFLOPs: 13.64 |
g0214: [2024-08-10 08:22:49,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=38160, skipped=57, lr=[0.00019972849323578678, 0.00019972849323578678], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38160 loss: 0.9437 iter time (s): 4.843 samples/sec: 26.432
g0236:  iteration    38160/10000000 | consumed samples:      4884480 | consumed tokens:  10003415040 | elapsed time per iteration (ms): 4876.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.023855E+00 | loss scale: 32768.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.250 | tokens per gpu per second (tgs): 1679.981 | TFLOPs: 13.52 |
g0214: [2024-08-10 08:23:35,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=38170, skipped=57, lr=[0.00019972829002842967, 0.00019972829002842967], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38170 loss: 0.9114 iter time (s): 4.550 samples/sec: 28.129
g0236:  iteration    38170/10000000 | consumed samples:      4885760 | consumed tokens:  10006036480 | elapsed time per iteration (ms): 4583.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.491287E-01 | loss scale: 32768.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.925 | tokens per gpu per second (tgs): 1787.225 | TFLOPs: 14.38 |
g0214: [2024-08-10 08:24:26,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=38180, skipped=57, lr=[0.00019972808674516092, 0.00019972808674516092], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38180 loss: 1.0712 iter time (s): 5.065 samples/sec: 25.271
g0236:  iteration    38180/10000000 | consumed samples:      4887040 | consumed tokens:  10008657920 | elapsed time per iteration (ms): 5098.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.776393E-01 | loss scale: 32768.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.108 | tokens per gpu per second (tgs): 1606.885 | TFLOPs: 12.93 |
g0214: [2024-08-10 08:25:20,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=38190, skipped=57, lr=[0.00019972788338598068, 0.00019972788338598068], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38190 loss: 1.0103 iter time (s): 5.325 samples/sec: 24.039
g0236:  iteration    38190/10000000 | consumed samples:      4888320 | consumed tokens:  10011279360 | elapsed time per iteration (ms): 5357.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.857277E-01 | loss scale: 32768.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.891 | tokens per gpu per second (tgs): 1529.011 | TFLOPs: 12.30 |
g0214: [2024-08-10 08:26:09,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=38200, skipped=57, lr=[0.00019972767995088918, 0.00019972767995088918], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38200 loss: 1.0526 iter time (s): 4.889 samples/sec: 26.184
g0236:  iteration    38200/10000000 | consumed samples:      4889600 | consumed tokens:  10013900800 | elapsed time per iteration (ms): 4922.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.000079E+00 | loss scale: 32768.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.006 | tokens per gpu per second (tgs): 1664.372 | TFLOPs: 13.39 |
g0214: [2024-08-10 08:26:53,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=38210, skipped=57, lr=[0.00019972747643988645, 0.00019972747643988645], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38210 loss: 1.0589 iter time (s): 4.420 samples/sec: 28.962
g0236:  iteration    38210/10000000 | consumed samples:      4890880 | consumed tokens:  10016522240 | elapsed time per iteration (ms): 4452.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.882597E-01 | loss scale: 32768.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.749 | tokens per gpu per second (tgs): 1839.952 | TFLOPs: 14.81 |
g0214: [2024-08-10 08:27:37,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=38220, skipped=57, lr=[0.00019972727285297277, 0.00019972727285297277], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38220 loss: 0.8610 iter time (s): 4.304 samples/sec: 29.743
g0236:  iteration    38220/10000000 | consumed samples:      4892160 | consumed tokens:  10019143680 | elapsed time per iteration (ms): 4336.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.044733E+00 | loss scale: 32768.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.517 | tokens per gpu per second (tgs): 1889.076 | TFLOPs: 15.20 |
g0214: [2024-08-10 08:28:26,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=38230, skipped=57, lr=[0.00019972706919014823, 0.00019972706919014823], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38230 loss: 1.0929 iter time (s): 4.843 samples/sec: 26.429
g0236:  iteration    38230/10000000 | consumed samples:      4893440 | consumed tokens:  10021765120 | elapsed time per iteration (ms): 4876.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.839746E-01 | loss scale: 32768.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.249 | tokens per gpu per second (tgs): 1679.926 | TFLOPs: 13.52 |
g0214: [2024-08-10 08:29:13,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=38240, skipped=57, lr=[0.000199726865451413, 0.000199726865451413], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38240 loss: 0.8764 iter time (s): 4.699 samples/sec: 27.238
g0236:  iteration    38240/10000000 | consumed samples:      4894720 | consumed tokens:  10024386560 | elapsed time per iteration (ms): 4733.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.862205E-01 | loss scale: 32768.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.039 | tokens per gpu per second (tgs): 1730.511 | TFLOPs: 13.93 |
g0214: [2024-08-10 08:30:00,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=38250, skipped=57, lr=[0.00019972666163676722, 0.00019972666163676722], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38250 loss: 0.9022 iter time (s): 4.651 samples/sec: 27.520
g0236:  iteration    38250/10000000 | consumed samples:      4896000 | consumed tokens:  10027008000 | elapsed time per iteration (ms): 4683.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.867936E-01 | loss scale: 32768.0 | grad norm: 0.285 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.328 | tokens per gpu per second (tgs): 1748.988 | TFLOPs: 14.07 |
g0214: [2024-08-10 08:30:49,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=38260, skipped=57, lr=[0.00019972645774621106, 0.00019972645774621106], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38260 loss: 1.0823 iter time (s): 4.863 samples/sec: 26.323
g0236:  iteration    38260/10000000 | consumed samples:      4897280 | consumed tokens:  10029629440 | elapsed time per iteration (ms): 4895.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.679610E-01 | loss scale: 32768.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.146 | tokens per gpu per second (tgs): 1673.326 | TFLOPs: 13.47 |
g0214: [2024-08-10 08:31:35,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=38270, skipped=57, lr=[0.00019972625377974473, 0.00019972625377974473], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38270 loss: 0.9416 iter time (s): 4.551 samples/sec: 28.124
g0236:  iteration    38270/10000000 | consumed samples:      4898560 | consumed tokens:  10032250880 | elapsed time per iteration (ms): 4584.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.663082E-01 | loss scale: 32768.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.919 | tokens per gpu per second (tgs): 1786.817 | TFLOPs: 14.38 |
g0214: [2024-08-10 08:32:24,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=38280, skipped=57, lr=[0.0001997260497373683, 0.0001997260497373683], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38280 loss: 1.0949 iter time (s): 4.906 samples/sec: 26.091
g0236:  iteration    38280/10000000 | consumed samples:      4899840 | consumed tokens:  10034872320 | elapsed time per iteration (ms): 4940.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.005646E+00 | loss scale: 32768.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.911 | tokens per gpu per second (tgs): 1658.295 | TFLOPs: 13.34 |
g0214: [2024-08-10 08:33:19,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=38290, skipped=57, lr=[0.00019972584561908196, 0.00019972584561908196], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38290 loss: 1.0198 iter time (s): 5.456 samples/sec: 23.460
g0236:  iteration    38290/10000000 | consumed samples:      4901120 | consumed tokens:  10037493760 | elapsed time per iteration (ms): 5488.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.744432E-01 | loss scale: 32768.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.320 | tokens per gpu per second (tgs): 1492.500 | TFLOPs: 12.01 |
g0214: [2024-08-10 08:34:10,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=38300, skipped=57, lr=[0.0001997256414248859, 0.0001997256414248859], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38300 loss: 1.0155 iter time (s): 5.072 samples/sec: 25.237
g0236:  iteration    38300/10000000 | consumed samples:      4902400 | consumed tokens:  10040115200 | elapsed time per iteration (ms): 5111.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.849043E-01 | loss scale: 32768.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.040 | tokens per gpu per second (tgs): 1602.570 | TFLOPs: 12.90 |
g0214: [2024-08-10 08:34:56,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=38310, skipped=57, lr=[0.00019972543715478022, 0.00019972543715478022], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38310 loss: 1.1769 iter time (s): 4.531 samples/sec: 28.248
g0236:  iteration    38310/10000000 | consumed samples:      4903680 | consumed tokens:  10042736640 | elapsed time per iteration (ms): 4564.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.022383E+00 | loss scale: 32768.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.045 | tokens per gpu per second (tgs): 1794.884 | TFLOPs: 14.44 |
g0214: [2024-08-10 08:35:43,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=38320, skipped=57, lr=[0.00019972523280876512, 0.00019972523280876512], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38320 loss: 0.9176 iter time (s): 4.741 samples/sec: 26.997
g0236:  iteration    38320/10000000 | consumed samples:      4904960 | consumed tokens:  10045358080 | elapsed time per iteration (ms): 4775.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.928655E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.804 | tokens per gpu per second (tgs): 1715.426 | TFLOPs: 13.80 |
g0214: [2024-08-10 08:36:29,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=38330, skipped=57, lr=[0.00019972502838684075, 0.00019972502838684075], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38330 loss: 0.8579 iter time (s): 4.557 samples/sec: 28.089
g0236:  iteration    38330/10000000 | consumed samples:      4906240 | consumed tokens:  10047979520 | elapsed time per iteration (ms): 4589.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.366109E-01 | loss scale: 32768.0 | grad norm: 0.433 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.888 | tokens per gpu per second (tgs): 1784.830 | TFLOPs: 14.36 |
g0214: [2024-08-10 08:37:14,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=38340, skipped=57, lr=[0.00019972482388900726, 0.00019972482388900726], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38340 loss: 1.2707 iter time (s): 4.446 samples/sec: 28.788
g0236:  iteration    38340/10000000 | consumed samples:      4907520 | consumed tokens:  10050600960 | elapsed time per iteration (ms): 4479.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.939481E-01 | loss scale: 32768.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.577 | tokens per gpu per second (tgs): 1828.903 | TFLOPs: 14.72 |
g0214: [2024-08-10 08:38:02,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=38350, skipped=57, lr=[0.00019972461931526476, 0.00019972461931526476], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38350 loss: 1.1046 iter time (s): 4.808 samples/sec: 26.620
g0236:  iteration    38350/10000000 | consumed samples:      4908800 | consumed tokens:  10053222400 | elapsed time per iteration (ms): 4841.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.850337E-01 | loss scale: 32768.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.440 | tokens per gpu per second (tgs): 1692.176 | TFLOPs: 13.62 |
g0214: [2024-08-10 08:38:49,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=38360, skipped=57, lr=[0.0001997244146656135, 0.0001997244146656135], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38360 loss: 1.0673 iter time (s): 4.583 samples/sec: 27.931
g0236:  iteration    38360/10000000 | consumed samples:      4910080 | consumed tokens:  10055843840 | elapsed time per iteration (ms): 4615.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.007186E+00 | loss scale: 32768.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.734 | tokens per gpu per second (tgs): 1775.005 | TFLOPs: 14.28 |
g0214: [2024-08-10 08:39:36,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=38370, skipped=57, lr=[0.00019972420994005357, 0.00019972420994005357], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38370 loss: 0.9812 iter time (s): 4.691 samples/sec: 27.288
g0236:  iteration    38370/10000000 | consumed samples:      4911360 | consumed tokens:  10058465280 | elapsed time per iteration (ms): 4723.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.047681E+00 | loss scale: 32768.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.098 | tokens per gpu per second (tgs): 1734.301 | TFLOPs: 13.96 |
g0214: [2024-08-10 08:40:26,493] [INFO] [logging.py:96:log_dist] [Rank 0] step=38380, skipped=57, lr=[0.00019972400513858517, 0.00019972400513858517], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38380 loss: 0.9669 iter time (s): 4.987 samples/sec: 25.669
g0236:  iteration    38380/10000000 | consumed samples:      4912640 | consumed tokens:  10061086720 | elapsed time per iteration (ms): 5019.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.055641E+00 | loss scale: 32768.0 | grad norm: 0.291 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.500 | tokens per gpu per second (tgs): 1632.027 | TFLOPs: 13.13 |
g0214: [2024-08-10 08:41:16,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=38390, skipped=57, lr=[0.00019972380026120844, 0.00019972380026120844], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38390 loss: 1.0129 iter time (s): 4.976 samples/sec: 25.723
g0236:  iteration    38390/10000000 | consumed samples:      4913920 | consumed tokens:  10063708160 | elapsed time per iteration (ms): 5009.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.040525E+00 | loss scale: 32768.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.554 | tokens per gpu per second (tgs): 1635.452 | TFLOPs: 13.16 |
g0214: [2024-08-10 08:42:01,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=38400, skipped=57, lr=[0.00019972359530792352, 0.00019972359530792352], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38400 loss: 0.8497 iter time (s): 4.476 samples/sec: 28.595
g0236:  iteration    38400/10000000 | consumed samples:      4915200 | consumed tokens:  10066329600 | elapsed time per iteration (ms): 4509.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.019844E+00 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.388 | tokens per gpu per second (tgs): 1816.813 | TFLOPs: 14.62 |
g0214: [2024-08-10 08:42:56,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=38410, skipped=57, lr=[0.0001997233902787306, 0.0001997233902787306], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38410 loss: 0.9179 iter time (s): 5.445 samples/sec: 23.508
g0236:  iteration    38410/10000000 | consumed samples:      4916480 | consumed tokens:  10068951040 | elapsed time per iteration (ms): 5477.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.895172E-01 | loss scale: 32768.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.368 | tokens per gpu per second (tgs): 1495.580 | TFLOPs: 12.04 |
g0214: [2024-08-10 08:43:45,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=38420, skipped=57, lr=[0.0001997231851736298, 0.0001997231851736298], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38420 loss: 0.9431 iter time (s): 4.827 samples/sec: 26.519
g0236:  iteration    38420/10000000 | consumed samples:      4917760 | consumed tokens:  10071572480 | elapsed time per iteration (ms): 4860.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.945898E-01 | loss scale: 32768.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.334 | tokens per gpu per second (tgs): 1685.383 | TFLOPs: 13.56 |
g0214: [2024-08-10 08:44:30,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=38430, skipped=57, lr=[0.00019972297999262128, 0.00019972297999262128], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38430 loss: 0.9817 iter time (s): 4.490 samples/sec: 28.506
g0236:  iteration    38430/10000000 | consumed samples:      4919040 | consumed tokens:  10074193920 | elapsed time per iteration (ms): 4523.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.338318E-01 | loss scale: 32768.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.298 | tokens per gpu per second (tgs): 1811.044 | TFLOPs: 14.57 |
g0214: [2024-08-10 08:45:21,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=38440, skipped=57, lr=[0.00019972277473570525, 0.00019972277473570525], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38440 loss: 1.0879 iter time (s): 5.069 samples/sec: 25.253
g0236:  iteration    38440/10000000 | consumed samples:      4920320 | consumed tokens:  10076815360 | elapsed time per iteration (ms): 5107.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.005198E+00 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.063 | tokens per gpu per second (tgs): 1604.018 | TFLOPs: 12.91 |
g0214: [2024-08-10 08:46:07,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=38450, skipped=57, lr=[0.0001997225694028818, 0.0001997225694028818], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38450 loss: 1.0625 iter time (s): 4.626 samples/sec: 27.671
g0236:  iteration    38450/10000000 | consumed samples:      4921600 | consumed tokens:  10079436800 | elapsed time per iteration (ms): 4658.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.000778E+00 | loss scale: 32768.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.476 | tokens per gpu per second (tgs): 1758.453 | TFLOPs: 14.15 |
g0214: [2024-08-10 08:46:57,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=38460, skipped=57, lr=[0.00019972236399415114, 0.00019972236399415114], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38460 loss: 0.8953 iter time (s): 4.896 samples/sec: 26.143
g0236:  iteration    38460/10000000 | consumed samples:      4922880 | consumed tokens:  10082058240 | elapsed time per iteration (ms): 4929.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.637166E-01 | loss scale: 32768.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.969 | tokens per gpu per second (tgs): 1661.992 | TFLOPs: 13.37 |
g0214: [2024-08-10 08:47:48,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=38470, skipped=57, lr=[0.0001997221585095134, 0.0001997221585095134], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38470 loss: 0.9258 iter time (s): 5.138 samples/sec: 24.911
g0236:  iteration    38470/10000000 | consumed samples:      4924160 | consumed tokens:  10084679680 | elapsed time per iteration (ms): 5171.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.796669E-01 | loss scale: 32768.0 | grad norm: 0.231 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.753 | tokens per gpu per second (tgs): 1584.178 | TFLOPs: 12.75 |
g0214: [2024-08-10 08:48:38,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=38480, skipped=57, lr=[0.00019972195294896877, 0.00019972195294896877], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38480 loss: 0.9891 iter time (s): 4.951 samples/sec: 25.852
g0236:  iteration    38480/10000000 | consumed samples:      4925440 | consumed tokens:  10087301120 | elapsed time per iteration (ms): 4984.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.003694E+00 | loss scale: 32768.0 | grad norm: 0.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.682 | tokens per gpu per second (tgs): 1643.654 | TFLOPs: 13.23 |
g0214: [2024-08-10 08:49:23,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=38490, skipped=57, lr=[0.00019972174731251735, 0.00019972174731251735], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38490 loss: 1.0267 iter time (s): 4.406 samples/sec: 29.054
g0236:  iteration    38490/10000000 | consumed samples:      4926720 | consumed tokens:  10089922560 | elapsed time per iteration (ms): 4438.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.026807E+00 | loss scale: 32768.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.837 | tokens per gpu per second (tgs): 1845.551 | TFLOPs: 14.85 |
g0214: [2024-08-10 08:50:11,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=38500, skipped=57, lr=[0.00019972154160015937, 0.00019972154160015937], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38500 loss: 0.9848 iter time (s): 4.817 samples/sec: 26.572
g0236:  iteration    38500/10000000 | consumed samples:      4928000 | consumed tokens:  10092544000 | elapsed time per iteration (ms): 4849.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.001608E+00 | loss scale: 32768.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.394 | tokens per gpu per second (tgs): 1689.187 | TFLOPs: 13.59 |
g0214: [2024-08-10 08:50:59,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=38510, skipped=57, lr=[0.00019972133581189493, 0.00019972133581189493], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38510 loss: 1.0264 iter time (s): 4.756 samples/sec: 26.912
g0236:  iteration    38510/10000000 | consumed samples:      4929280 | consumed tokens:  10095165440 | elapsed time per iteration (ms): 4790.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.026938E+00 | loss scale: 32768.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.722 | tokens per gpu per second (tgs): 1710.228 | TFLOPs: 13.76 |
g0214: [2024-08-10 08:51:45,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=38520, skipped=57, lr=[0.00019972112994772422, 0.00019972112994772422], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38520 loss: 0.8991 iter time (s): 4.572 samples/sec: 27.994
g0236:  iteration    38520/10000000 | consumed samples:      4930560 | consumed tokens:  10097786880 | elapsed time per iteration (ms): 4606.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.525936E-01 | loss scale: 32768.0 | grad norm: 0.438 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.788 | tokens per gpu per second (tgs): 1778.411 | TFLOPs: 14.31 |
g0214: [2024-08-10 08:52:34,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=38530, skipped=57, lr=[0.00019972092400764735, 0.00019972092400764735], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38530 loss: 0.9542 iter time (s): 4.803 samples/sec: 26.652
g0236:  iteration    38530/10000000 | consumed samples:      4931840 | consumed tokens:  10100408320 | elapsed time per iteration (ms): 4836.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.648279E-01 | loss scale: 32768.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.467 | tokens per gpu per second (tgs): 1693.917 | TFLOPs: 13.63 |
g0214: [2024-08-10 08:53:32,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=38540, skipped=57, lr=[0.00019972071799166454, 0.00019972071799166454], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38540 loss: 1.0673 iter time (s): 5.863 samples/sec: 21.830
g0236:  iteration    38540/10000000 | consumed samples:      4933120 | consumed tokens:  10103029760 | elapsed time per iteration (ms): 5896.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.043044E+00 | loss scale: 32768.0 | grad norm: 0.261 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.708 | tokens per gpu per second (tgs): 1389.318 | TFLOPs: 11.18 |
g0214: [2024-08-10 08:54:18,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=38550, skipped=57, lr=[0.00019972051189977595, 0.00019972051189977595], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38550 loss: 0.9793 iter time (s): 4.549 samples/sec: 28.139
g0236:  iteration    38550/10000000 | consumed samples:      4934400 | consumed tokens:  10105651200 | elapsed time per iteration (ms): 4581.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.860979E-01 | loss scale: 32768.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.937 | tokens per gpu per second (tgs): 1787.959 | TFLOPs: 14.39 |
g0214: [2024-08-10 08:55:07,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=38560, skipped=57, lr=[0.00019972030573198166, 0.00019972030573198166], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38560 loss: 1.0450 iter time (s): 4.828 samples/sec: 26.514
g0236:  iteration    38560/10000000 | consumed samples:      4935680 | consumed tokens:  10108272640 | elapsed time per iteration (ms): 4860.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.876593E-01 | loss scale: 32768.0 | grad norm: 0.368 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.335 | tokens per gpu per second (tgs): 1685.450 | TFLOPs: 13.56 |
g0214: [2024-08-10 08:55:53,455] [INFO] [logging.py:96:log_dist] [Rank 0] step=38570, skipped=57, lr=[0.00019972009948828192, 0.00019972009948828192], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38570 loss: 0.8909 iter time (s): 4.574 samples/sec: 27.985
g0236:  iteration    38570/10000000 | consumed samples:      4936960 | consumed tokens:  10110894080 | elapsed time per iteration (ms): 4606.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.012251E+00 | loss scale: 32768.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.786 | tokens per gpu per second (tgs): 1778.296 | TFLOPs: 14.31 |
g0214: [2024-08-10 08:56:42,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=38580, skipped=57, lr=[0.00019971989316867685, 0.00019971989316867685], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38580 loss: 0.9806 iter time (s): 4.835 samples/sec: 26.476
g0236:  iteration    38580/10000000 | consumed samples:      4938240 | consumed tokens:  10113515520 | elapsed time per iteration (ms): 4867.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.801276E-01 | loss scale: 32768.0 | grad norm: 0.283 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.299 | tokens per gpu per second (tgs): 1683.120 | TFLOPs: 13.54 |
g0214: [2024-08-10 08:57:31,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=38590, skipped=57, lr=[0.00019971968677316658, 0.00019971968677316658], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38590 loss: 1.0726 iter time (s): 4.874 samples/sec: 26.263
g0236:  iteration    38590/10000000 | consumed samples:      4939520 | consumed tokens:  10116136960 | elapsed time per iteration (ms): 4906.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.035922E+00 | loss scale: 32768.0 | grad norm: 0.339 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.086 | tokens per gpu per second (tgs): 1669.478 | TFLOPs: 13.43 |
g0214: [2024-08-10 08:58:21,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=38600, skipped=57, lr=[0.00019971948030175134, 0.00019971948030175134], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38600 loss: 0.8512 iter time (s): 4.989 samples/sec: 25.656
g0236:  iteration    38600/10000000 | consumed samples:      4940800 | consumed tokens:  10118758400 | elapsed time per iteration (ms): 5021.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.916274E-01 | loss scale: 32768.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.488 | tokens per gpu per second (tgs): 1631.264 | TFLOPs: 13.13 |
g0214: [2024-08-10 08:59:10,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=38610, skipped=57, lr=[0.0001997192737544312, 0.0001997192737544312], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38610 loss: 1.0409 iter time (s): 4.840 samples/sec: 26.445
g0236:  iteration    38610/10000000 | consumed samples:      4942080 | consumed tokens:  10121379840 | elapsed time per iteration (ms): 4873.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.896750E-01 | loss scale: 32768.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.265 | tokens per gpu per second (tgs): 1680.945 | TFLOPs: 13.53 |
g0214: [2024-08-10 08:59:54,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=38620, skipped=57, lr=[0.00019971906713120639, 0.00019971906713120639], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38620 loss: 1.0578 iter time (s): 4.407 samples/sec: 29.048
g0236:  iteration    38620/10000000 | consumed samples:      4943360 | consumed tokens:  10124001280 | elapsed time per iteration (ms): 4439.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.448683E-01 | loss scale: 32768.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.834 | tokens per gpu per second (tgs): 1845.355 | TFLOPs: 14.85 |
g0214: [2024-08-10 09:00:44,761] [INFO] [logging.py:96:log_dist] [Rank 0] step=38630, skipped=57, lr=[0.00019971886043207706, 0.00019971886043207706], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38630 loss: 0.8037 iter time (s): 4.989 samples/sec: 25.654
g0236:  iteration    38630/10000000 | consumed samples:      4944640 | consumed tokens:  10126622720 | elapsed time per iteration (ms): 5021.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.746189E-01 | loss scale: 32768.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.489 | tokens per gpu per second (tgs): 1631.315 | TFLOPs: 13.13 |
g0234: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0214: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0232: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0232: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 09:01:34,820] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0232: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0232: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0214: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0214: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 09:01:34,821] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0214: [2024-08-10 09:01:34,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=38640, skipped=57, lr=[0.0001997186536570433, 0.0001997186536570433], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38640 loss: 1.0667 iter time (s): 4.974 samples/sec: 25.735
g0236:  iteration    38640/10000000 | consumed samples:      4945920 | consumed tokens:  10129244160 | elapsed time per iteration (ms): 5006.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.954255E-01 | loss scale: 65536.0 | grad norm: 0.102 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.565 | tokens per gpu per second (tgs): 1636.156 | TFLOPs: 13.17 |
g0214: [2024-08-10 09:02:25,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=38650, skipped=57, lr=[0.00019971844680610537, 0.00019971844680610537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38650 loss: 0.9560 iter time (s): 5.031 samples/sec: 25.442
g0236:  iteration    38650/10000000 | consumed samples:      4947200 | consumed tokens:  10131865600 | elapsed time per iteration (ms): 5063.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.942032E-01 | loss scale: 65536.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.278 | tokens per gpu per second (tgs): 1617.793 | TFLOPs: 13.02 |
g0214: [2024-08-10 09:03:16,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=38660, skipped=57, lr=[0.00019971823987926336, 0.00019971823987926336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38660 loss: 0.9667 iter time (s): 5.080 samples/sec: 25.198
g0236:  iteration    38660/10000000 | consumed samples:      4948480 | consumed tokens:  10134487040 | elapsed time per iteration (ms): 5112.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.014509E+00 | loss scale: 65536.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.037 | tokens per gpu per second (tgs): 1602.356 | TFLOPs: 12.89 |
g0214: [2024-08-10 09:04:18,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=38670, skipped=57, lr=[0.00019971803287651747, 0.00019971803287651747], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38670 loss: 1.0126 iter time (s): 6.196 samples/sec: 20.659
g0236:  iteration    38670/10000000 | consumed samples:      4949760 | consumed tokens:  10137108480 | elapsed time per iteration (ms): 6228.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.014856E+00 | loss scale: 65536.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.549 | tokens per gpu per second (tgs): 1315.158 | TFLOPs: 10.58 |
g0214: [2024-08-10 09:05:21,156] [INFO] [logging.py:96:log_dist] [Rank 0] step=38680, skipped=57, lr=[0.0001997178257978678, 0.0001997178257978678], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38680 loss: 1.1184 iter time (s): 6.195 samples/sec: 20.663
g0236:  iteration    38680/10000000 | consumed samples:      4951040 | consumed tokens:  10139729920 | elapsed time per iteration (ms): 6227.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.023437E+00 | loss scale: 65536.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.554 | tokens per gpu per second (tgs): 1315.455 | TFLOPs: 10.59 |
g0214: [2024-08-10 09:06:09,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=38690, skipped=57, lr=[0.00019971761864331458, 0.00019971761864331458], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38690 loss: 1.0980 iter time (s): 4.771 samples/sec: 26.829
g0236:  iteration    38690/10000000 | consumed samples:      4952320 | consumed tokens:  10142351360 | elapsed time per iteration (ms): 4803.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.025771E+00 | loss scale: 65536.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.647 | tokens per gpu per second (tgs): 1705.400 | TFLOPs: 13.72 |
g0214: [2024-08-10 09:06:55,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=38700, skipped=57, lr=[0.00019971741141285794, 0.00019971741141285794], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38700 loss: 1.0163 iter time (s): 4.553 samples/sec: 28.113
g0236:  iteration    38700/10000000 | consumed samples:      4953600 | consumed tokens:  10144972800 | elapsed time per iteration (ms): 4585.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.031347E+00 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.913 | tokens per gpu per second (tgs): 1786.433 | TFLOPs: 14.38 |
g0214: [2024-08-10 09:07:44,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=38710, skipped=57, lr=[0.000199717204106498, 0.000199717204106498], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38710 loss: 1.0492 iter time (s): 4.905 samples/sec: 26.095
g0236:  iteration    38710/10000000 | consumed samples:      4954880 | consumed tokens:  10147594240 | elapsed time per iteration (ms): 4938.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.031399E+00 | loss scale: 65536.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.921 | tokens per gpu per second (tgs): 1658.937 | TFLOPs: 13.35 |
g0214: [2024-08-10 09:08:33,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=38720, skipped=57, lr=[0.00019971699672423498, 0.00019971699672423498], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38720 loss: 0.9482 iter time (s): 4.920 samples/sec: 26.014
g0236:  iteration    38720/10000000 | consumed samples:      4956160 | consumed tokens:  10150215680 | elapsed time per iteration (ms): 4953.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.058543E+00 | loss scale: 65536.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.842 | tokens per gpu per second (tgs): 1653.887 | TFLOPs: 13.31 |
g0214: [2024-08-10 09:09:23,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=38730, skipped=57, lr=[0.00019971678926606902, 0.00019971678926606902], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38730 loss: 1.0433 iter time (s): 4.871 samples/sec: 26.278
g0236:  iteration    38730/10000000 | consumed samples:      4957440 | consumed tokens:  10152837120 | elapsed time per iteration (ms): 4904.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.039472E+00 | loss scale: 65536.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.101 | tokens per gpu per second (tgs): 1670.469 | TFLOPs: 13.44 |
g0214: [2024-08-10 09:11:42,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=38740, skipped=57, lr=[0.00019971658173200025, 0.00019971658173200025], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38740 loss: 1.1280 iter time (s): 13.878 samples/sec: 9.223
g0236:  iteration    38740/10000000 | consumed samples:      4958720 | consumed tokens:  10155458560 | elapsed time per iteration (ms): 13913.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.032778E+00 | loss scale: 65536.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.200 | tokens per gpu per second (tgs): 588.774 | TFLOPs: 4.74 |
g0214: [2024-08-10 09:13:58,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=38750, skipped=57, lr=[0.00019971637412202889, 0.00019971637412202889], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38750 loss: 1.0919 iter time (s): 13.569 samples/sec: 9.434
g0236:  iteration    38750/10000000 | consumed samples:      4960000 | consumed tokens:  10158080000 | elapsed time per iteration (ms): 13657.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.469657E-01 | loss scale: 65536.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.372 | tokens per gpu per second (tgs): 599.811 | TFLOPs: 4.83 |
g0214: [2024-08-10 09:16:15,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=38760, skipped=57, lr=[0.00019971616643615502, 0.00019971616643615502], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38760 loss: 0.9074 iter time (s): 13.602 samples/sec: 9.410
g0236:  iteration    38760/10000000 | consumed samples:      4961280 | consumed tokens:  10160701440 | elapsed time per iteration (ms): 13661.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.679823E-01 | loss scale: 65536.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 9.370 | tokens per gpu per second (tgs): 599.665 | TFLOPs: 4.83 |
g0214: [2024-08-10 09:17:45,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=38770, skipped=57, lr=[0.0001997159586743789, 0.0001997159586743789], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38770 loss: 1.1802 iter time (s): 9.005 samples/sec: 14.215
g0236:  iteration    38770/10000000 | consumed samples:      4962560 | consumed tokens:  10163322880 | elapsed time per iteration (ms): 9047.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.963767E-01 | loss scale: 65536.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 14.147 | tokens per gpu per second (tgs): 905.412 | TFLOPs: 7.29 |
g0214: [2024-08-10 09:18:52,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=38780, skipped=57, lr=[0.0001997157508367006, 0.0001997157508367006], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38780 loss: 0.9311 iter time (s): 6.606 samples/sec: 19.375
g0236:  iteration    38780/10000000 | consumed samples:      4963840 | consumed tokens:  10165944320 | elapsed time per iteration (ms): 6639.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.866539E-01 | loss scale: 65536.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.278 | tokens per gpu per second (tgs): 1233.763 | TFLOPs: 9.93 |
g0214: [2024-08-10 09:19:49,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=38790, skipped=57, lr=[0.00019971554292312033, 0.00019971554292312033], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38790 loss: 1.0933 iter time (s): 5.729 samples/sec: 22.341
g0236:  iteration    38790/10000000 | consumed samples:      4965120 | consumed tokens:  10168565760 | elapsed time per iteration (ms): 5761.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.925570E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.215 | tokens per gpu per second (tgs): 1421.765 | TFLOPs: 11.44 |
g0214: [2024-08-10 09:20:42,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=38800, skipped=57, lr=[0.0001997153349336382, 0.0001997153349336382], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38800 loss: 1.1626 iter time (s): 5.218 samples/sec: 24.532
g0236:  iteration    38800/10000000 | consumed samples:      4966400 | consumed tokens:  10171187200 | elapsed time per iteration (ms): 5250.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.045323E+00 | loss scale: 65536.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.380 | tokens per gpu per second (tgs): 1560.352 | TFLOPs: 12.56 |
g0214: [2024-08-10 09:21:32,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=38810, skipped=57, lr=[0.0001997151268682544, 0.0001997151268682544], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38810 loss: 1.0959 iter time (s): 4.948 samples/sec: 25.869
g0236:  iteration    38810/10000000 | consumed samples:      4967680 | consumed tokens:  10173808640 | elapsed time per iteration (ms): 4982.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.013823E+00 | loss scale: 65536.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.692 | tokens per gpu per second (tgs): 1644.315 | TFLOPs: 13.23 |
g0214: [2024-08-10 09:22:24,529] [INFO] [logging.py:96:log_dist] [Rank 0] step=38820, skipped=57, lr=[0.00019971491872696913, 0.00019971491872696913], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38820 loss: 1.0060 iter time (s): 5.206 samples/sec: 24.588
g0236:  iteration    38820/10000000 | consumed samples:      4968960 | consumed tokens:  10176430080 | elapsed time per iteration (ms): 5238.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.568895E-01 | loss scale: 65536.0 | grad norm: 0.236 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.434 | tokens per gpu per second (tgs): 1563.784 | TFLOPs: 12.58 |
g0214: [2024-08-10 09:23:15,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=38830, skipped=57, lr=[0.0001997147105097825, 0.0001997147105097825], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38830 loss: 1.0276 iter time (s): 5.050 samples/sec: 25.348
g0236:  iteration    38830/10000000 | consumed samples:      4970240 | consumed tokens:  10179051520 | elapsed time per iteration (ms): 5082.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.520417E-01 | loss scale: 65536.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.183 | tokens per gpu per second (tgs): 1611.720 | TFLOPs: 12.97 |
g0214: [2024-08-10 09:23:58,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=38840, skipped=57, lr=[0.00019971450221669467, 0.00019971450221669467], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38840 loss: 0.9905 iter time (s): 4.286 samples/sec: 29.865
g0236:  iteration    38840/10000000 | consumed samples:      4971520 | consumed tokens:  10181672960 | elapsed time per iteration (ms): 4318.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.004629E+00 | loss scale: 65536.0 | grad norm: 0.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.638 | tokens per gpu per second (tgs): 1896.858 | TFLOPs: 15.26 |
g0214: [2024-08-10 09:24:58,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=38850, skipped=57, lr=[0.00019971429384770586, 0.00019971429384770586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38850 loss: 0.9199 iter time (s): 5.957 samples/sec: 21.488
g0236:  iteration    38850/10000000 | consumed samples:      4972800 | consumed tokens:  10184294400 | elapsed time per iteration (ms): 5997.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.030516E+00 | loss scale: 65536.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.341 | tokens per gpu per second (tgs): 1365.806 | TFLOPs: 10.99 |
g0214: [2024-08-10 09:26:11,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=38860, skipped=57, lr=[0.00019971408540281612, 0.00019971408540281612], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38860 loss: 1.0772 iter time (s): 7.249 samples/sec: 17.657
g0236:  iteration    38860/10000000 | consumed samples:      4974080 | consumed tokens:  10186915840 | elapsed time per iteration (ms): 7281.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.010524E+00 | loss scale: 65536.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.578 | tokens per gpu per second (tgs): 1124.992 | TFLOPs: 9.05 |
g0214: [2024-08-10 09:27:24,734] [INFO] [logging.py:96:log_dist] [Rank 0] step=38870, skipped=57, lr=[0.0001997138768820257, 0.0001997138768820257], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38870 loss: 1.1177 iter time (s): 7.306 samples/sec: 17.521
g0236:  iteration    38870/10000000 | consumed samples:      4975360 | consumed tokens:  10189537280 | elapsed time per iteration (ms): 7339.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.019080E+00 | loss scale: 65536.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.440 | tokens per gpu per second (tgs): 1116.166 | TFLOPs: 8.98 |
g0214: [2024-08-10 09:28:37,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=38880, skipped=57, lr=[0.00019971366828533475, 0.00019971366828533475], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38880 loss: 1.0754 iter time (s): 7.267 samples/sec: 17.614
g0236:  iteration    38880/10000000 | consumed samples:      4976640 | consumed tokens:  10192158720 | elapsed time per iteration (ms): 7300.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.954597E-01 | loss scale: 65536.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.534 | tokens per gpu per second (tgs): 1122.181 | TFLOPs: 9.03 |
g0214: [2024-08-10 09:29:27,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=38890, skipped=57, lr=[0.0001997134596127434, 0.0001997134596127434], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38890 loss: 1.0625 iter time (s): 4.950 samples/sec: 25.856
g0236:  iteration    38890/10000000 | consumed samples:      4977920 | consumed tokens:  10194780160 | elapsed time per iteration (ms): 4983.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.839314E-01 | loss scale: 65536.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.684 | tokens per gpu per second (tgs): 1643.796 | TFLOPs: 13.23 |
g0214: [2024-08-10 09:30:08,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=38900, skipped=57, lr=[0.00019971325086425183, 0.00019971325086425183], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38900 loss: 0.9287 iter time (s): 4.108 samples/sec: 31.160
g0236:  iteration    38900/10000000 | consumed samples:      4979200 | consumed tokens:  10197401600 | elapsed time per iteration (ms): 4141.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.780867E-01 | loss scale: 65536.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.908 | tokens per gpu per second (tgs): 1978.140 | TFLOPs: 15.92 |
g0214: [2024-08-10 09:30:54,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=38910, skipped=57, lr=[0.0001997130420398602, 0.0001997130420398602], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38910 loss: 0.9942 iter time (s): 4.507 samples/sec: 28.398
g0236:  iteration    38910/10000000 | consumed samples:      4980480 | consumed tokens:  10200023040 | elapsed time per iteration (ms): 4541.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.920942E-01 | loss scale: 65536.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.185 | tokens per gpu per second (tgs): 1803.846 | TFLOPs: 14.52 |
g0214: [2024-08-10 09:31:57,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=38920, skipped=57, lr=[0.00019971283313956865, 0.00019971283313956865], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38920 loss: 0.9297 iter time (s): 6.248 samples/sec: 20.487
g0236:  iteration    38920/10000000 | consumed samples:      4981760 | consumed tokens:  10202644480 | elapsed time per iteration (ms): 6280.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.967781E-01 | loss scale: 65536.0 | grad norm: 0.352 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.379 | tokens per gpu per second (tgs): 1304.286 | TFLOPs: 10.50 |
g0214: [2024-08-10 09:33:11,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=38930, skipped=57, lr=[0.00019971262416337737, 0.00019971262416337737], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38930 loss: 0.9775 iter time (s): 7.418 samples/sec: 17.256
g0236:  iteration    38930/10000000 | consumed samples:      4983040 | consumed tokens:  10205265920 | elapsed time per iteration (ms): 7451.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.678904E-01 | loss scale: 65536.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.178 | tokens per gpu per second (tgs): 1099.421 | TFLOPs: 8.85 |
g0214: [2024-08-10 09:34:30,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=38940, skipped=57, lr=[0.00019971241511128646, 0.00019971241511128646], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38940 loss: 0.9647 iter time (s): 7.813 samples/sec: 16.382
g0236:  iteration    38940/10000000 | consumed samples:      4984320 | consumed tokens:  10207887360 | elapsed time per iteration (ms): 7848.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.860479E-01 | loss scale: 65536.0 | grad norm: 0.290 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.308 | tokens per gpu per second (tgs): 1043.724 | TFLOPs: 8.40 |
g0214: [2024-08-10 09:35:36,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=38950, skipped=57, lr=[0.0001997122059832962, 0.0001997122059832962], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38950 loss: 1.1117 iter time (s): 6.547 samples/sec: 19.551
g0236:  iteration    38950/10000000 | consumed samples:      4985600 | consumed tokens:  10210508800 | elapsed time per iteration (ms): 6580.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.989101E-01 | loss scale: 65536.0 | grad norm: 0.438 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.453 | tokens per gpu per second (tgs): 1244.991 | TFLOPs: 10.02 |
g0214: [2024-08-10 09:36:38,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=38960, skipped=57, lr=[0.00019971199677940667, 0.00019971199677940667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38960 loss: 1.0108 iter time (s): 6.249 samples/sec: 20.484
g0236:  iteration    38960/10000000 | consumed samples:      4986880 | consumed tokens:  10213130240 | elapsed time per iteration (ms): 6281.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.003018E+00 | loss scale: 65536.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.377 | tokens per gpu per second (tgs): 1304.132 | TFLOPs: 10.49 |
g0214: [2024-08-10 09:37:30,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=38970, skipped=57, lr=[0.000199711787499618, 0.000199711787499618], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38970 loss: 1.1381 iter time (s): 5.171 samples/sec: 24.752
g0236:  iteration    38970/10000000 | consumed samples:      4988160 | consumed tokens:  10215751680 | elapsed time per iteration (ms): 5203.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.037863E+00 | loss scale: 65536.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.597 | tokens per gpu per second (tgs): 1574.215 | TFLOPs: 12.67 |
g0214: [2024-08-10 09:38:28,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=38980, skipped=57, lr=[0.00019971157814393042, 0.00019971157814393042], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38980 loss: 0.9590 iter time (s): 5.759 samples/sec: 22.227
g0236:  iteration    38980/10000000 | consumed samples:      4989440 | consumed tokens:  10218373120 | elapsed time per iteration (ms): 5795.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.934420E-01 | loss scale: 65536.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.087 | tokens per gpu per second (tgs): 1413.538 | TFLOPs: 11.37 |
g0214: [2024-08-10 09:39:35,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=38990, skipped=57, lr=[0.00019971136871234406, 0.00019971136871234406], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 38990 loss: 0.9246 iter time (s): 6.602 samples/sec: 19.388
g0236:  iteration    38990/10000000 | consumed samples:      4990720 | consumed tokens:  10220994560 | elapsed time per iteration (ms): 6637.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.983414E-01 | loss scale: 65536.0 | grad norm: 0.314 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.286 | tokens per gpu per second (tgs): 1234.283 | TFLOPs: 9.93 |
g0214: [2024-08-10 09:40:42,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=39000, skipped=57, lr=[0.00019971115920485903, 0.00019971115920485903], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39000 loss: 0.9879 iter time (s): 6.702 samples/sec: 19.099
g0236:  iteration    39000/10000000 | consumed samples:      4992000 | consumed tokens:  10223616000 | elapsed time per iteration (ms): 6737.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.793756E-01 | loss scale: 65536.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.998 | tokens per gpu per second (tgs): 1215.867 | TFLOPs: 9.78 |
g0236: -------------------------------------------------------------------------------------------------
g0236:  validation loss at iteration 39000 | lm loss value: 9.934151E-01 | lm loss PPL: 2.700441E+00 | 
g0236: -------------------------------------------------------------------------------------------------
g0214: saving checkpoint at iteration   39000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214: [2024-08-10 09:50:55,809] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step39000 is about to be saved!
g0214: [2024-08-10 09:50:55,818] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0214: [2024-08-10 09:50:55,818] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0214: [2024-08-10 09:50:55,818] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0220: [2024-08-10 09:50:55,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0220: [2024-08-10 09:50:55,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0220: [2024-08-10 09:50:55,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0232: [2024-08-10 09:50:55,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0232: [2024-08-10 09:50:55,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0232: [2024-08-10 09:50:55,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0236: [2024-08-10 09:50:55,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0236: [2024-08-10 09:50:55,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0236: [2024-08-10 09:50:55,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0234: [2024-08-10 09:50:55,821] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0234: [2024-08-10 09:50:55,821] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0234: [2024-08-10 09:50:55,821] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0235: [2024-08-10 09:50:55,821] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0235: [2024-08-10 09:50:55,821] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0235: [2024-08-10 09:50:55,821] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0225: [2024-08-10 09:50:55,823] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0225: [2024-08-10 09:50:55,823] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0225: [2024-08-10 09:50:55,823] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0233: [2024-08-10 09:50:55,825] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0233: [2024-08-10 09:50:55,825] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0233: [2024-08-10 09:50:55,825] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0236: [2024-08-10 09:50:55,850] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_23-model_00-model_states.pt...
g0232: [2024-08-10 09:50:55,852] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_11-model_00-model_states.pt...
g0220: [2024-08-10 09:50:55,853] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_05-model_00-model_states.pt...
g0235: [2024-08-10 09:50:55,855] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_20-model_00-model_states.pt...
g0234: [2024-08-10 09:50:55,855] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_17-model_00-model_states.pt...
g0233: [2024-08-10 09:50:55,858] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_14-model_00-model_states.pt...
g0225: [2024-08-10 09:50:55,860] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_08-model_00-model_states.pt...
g0214: [2024-08-10 09:50:55,864] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_01-model_00-model_states.pt...
g0220: [2024-08-10 09:50:55,964] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_05-model_00-model_states.pt.
g0220: [2024-08-10 09:50:55,998] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_06-model_00-model_states.pt...
g0233: [2024-08-10 09:50:56,017] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_14-model_00-model_states.pt.
g0236: [2024-08-10 09:50:56,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 09:50:56,036] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 09:50:56,039] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_24-model_00-model_states.pt.
g0235: [2024-08-10 09:50:56,039] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_20-model_00-model_states.pt.
g0233: [2024-08-10 09:50:56,051] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_15-model_00-model_states.pt...
g0234: [2024-08-10 09:50:56,052] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_17-model_00-model_states.pt.
g0232: [2024-08-10 09:50:56,068] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_11-model_00-model_states.pt.
g0235: [2024-08-10 09:50:56,075] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_21-model_00-model_states.pt...
g0234: [2024-08-10 09:50:56,087] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_18-model_00-model_states.pt...
g0236: [2024-08-10 09:50:56,092] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_25-model_00-model_states.pt...
g0232: [2024-08-10 09:50:56,100] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_12-model_00-model_states.pt...
g0220: [2024-08-10 09:50:56,161] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_06-model_00-model_states.pt.
g0234: [2024-08-10 09:50:56,183] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_18-model_00-model_states.pt.
g0220: [2024-08-10 09:50:56,188] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_07-model_00-model_states.pt...
g0235: [2024-08-10 09:50:56,199] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_21-model_00-model_states.pt.
g0233: [2024-08-10 09:50:56,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_15-model_00-model_states.pt.
g0234: [2024-08-10 09:50:56,213] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_19-model_00-model_states.pt...
g0232: [2024-08-10 09:50:56,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_12-model_00-model_states.pt.
g0235: [2024-08-10 09:50:56,230] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_22-model_00-model_states.pt...
g0233: [2024-08-10 09:50:56,240] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_16-model_00-model_states.pt...
g0232: [2024-08-10 09:50:56,252] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_13-model_00-model_states.pt...
g0225: [2024-08-10 09:50:56,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_08-model_00-model_states.pt.
g0225: [2024-08-10 09:50:56,313] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_09-model_00-model_states.pt...
g0236: [2024-08-10 09:50:56,343] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 09:50:56,344] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_07_model_states.pt...
g0234: [2024-08-10 09:50:56,358] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 09:50:56,359] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_05_model_states.pt...
g0225: [2024-08-10 09:50:56,432] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_09-model_00-model_states.pt.
g0214: [2024-08-10 09:50:56,434] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_01-model_00-model_states.pt.
g0232: [2024-08-10 09:50:56,435] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_13-model_00-model_states.pt.
g0232: [2024-08-10 09:50:56,436] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_03_model_states.pt...
g0233: [2024-08-10 09:50:56,443] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 09:50:56,445] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_04_model_states.pt...
g0214: [2024-08-10 09:50:56,462] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_02-model_00-model_states.pt...
g0225: [2024-08-10 09:50:56,465] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_10-model_00-model_states.pt...
g0235: [2024-08-10 09:50:56,515] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 09:50:56,517] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_06_model_states.pt...
g0225: [2024-08-10 09:50:56,591] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_10-model_00-model_states.pt.
g0225: [2024-08-10 09:50:56,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_02_model_states.pt...
g0214: [2024-08-10 09:50:56,617] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_02-model_00-model_states.pt.
g0214: [2024-08-10 09:50:56,649] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_03-model_00-model_states.pt...
g0220: [2024-08-10 09:50:56,666] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 09:50:56,667] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_01_model_states.pt...
g0214: [2024-08-10 09:50:56,790] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_03-model_00-model_states.pt.
g0214: [2024-08-10 09:50:56,820] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_04-model_00-model_states.pt...
g0214: [2024-08-10 09:50:56,995] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 09:50:56,997] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_00_model_states.pt
g0214: [2024-08-10 09:50:56,997] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_00_model_states.pt...
g0236: [2024-08-10 09:50:58,456] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_07_model_states.pt.
g0236: [2024-08-10 09:50:58,457] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0233: [2024-08-10 09:50:58,763] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_04_model_states.pt.
g0233: [2024-08-10 09:50:58,763] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0232: [2024-08-10 09:50:58,903] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_03_model_states.pt.
g0232: [2024-08-10 09:50:58,903] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0234: [2024-08-10 09:50:58,976] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_05_model_states.pt.
g0234: [2024-08-10 09:50:58,977] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0220: [2024-08-10 09:50:59,000] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_01_model_states.pt.
g0220: [2024-08-10 09:50:59,001] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0225: [2024-08-10 09:50:59,114] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_02_model_states.pt.
g0225: [2024-08-10 09:50:59,115] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0235: [2024-08-10 09:50:59,148] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_06_model_states.pt.
g0235: [2024-08-10 09:50:59,148] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0214: [2024-08-10 09:51:00,396] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step39000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 09:51:00,397] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step39000 is ready now!
g0214:   successfully saved checkpoint at iteration   39000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214: Checkpoint Save GB: 22.521, GB/Sec: 4.89, Latency(second): 4.602
g0236: (min, max) time across ranks (ms):
g0236:     save-checkpoint ................................: (4601.90, 4602.25)
g0214: [2024-08-10 09:52:00,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=39010, skipped=57, lr=[0.0001997109496214756, 0.0001997109496214756], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39010 loss: 1.0650 iter time (s): 6.026 samples/sec: 21.243
g0236:  iteration    39010/10000000 | consumed samples:      4993280 | consumed tokens:  10226237440 | elapsed time per iteration (ms): 67838.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.062323E+00 | loss scale: 65536.0 | grad norm: 0.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.887 | tokens per gpu per second (tgs): 120.757 | TFLOPs: 0.97 |
g0214: [2024-08-10 09:52:53,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=39020, skipped=57, lr=[0.00019971073996219386, 0.00019971073996219386], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39020 loss: 0.9302 iter time (s): 5.253 samples/sec: 24.365
g0236:  iteration    39020/10000000 | consumed samples:      4994560 | consumed tokens:  10228858880 | elapsed time per iteration (ms): 5287.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.004689E+00 | loss scale: 65536.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.208 | tokens per gpu per second (tgs): 1549.315 | TFLOPs: 12.47 |
g0214: [2024-08-10 09:53:42,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=39030, skipped=57, lr=[0.00019971053022701398, 0.00019971053022701398], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39030 loss: 1.0385 iter time (s): 4.868 samples/sec: 26.294
g0236:  iteration    39030/10000000 | consumed samples:      4995840 | consumed tokens:  10231480320 | elapsed time per iteration (ms): 4900.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.002723E+00 | loss scale: 65536.0 | grad norm: 0.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.118 | tokens per gpu per second (tgs): 1671.566 | TFLOPs: 13.45 |
g0214: [2024-08-10 09:54:34,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=39040, skipped=57, lr=[0.00019971032041593616, 0.00019971032041593616], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39040 loss: 0.9232 iter time (s): 5.119 samples/sec: 25.003
g0236:  iteration    39040/10000000 | consumed samples:      4997120 | consumed tokens:  10234101760 | elapsed time per iteration (ms): 5152.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.577532E-01 | loss scale: 65536.0 | grad norm: 0.289 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.841 | tokens per gpu per second (tgs): 1589.847 | TFLOPs: 12.79 |
g0214: [2024-08-10 09:55:24,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=39050, skipped=57, lr=[0.0001997101105289605, 0.0001997101105289605], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39050 loss: 1.1372 iter time (s): 4.973 samples/sec: 25.737
g0236:  iteration    39050/10000000 | consumed samples:      4998400 | consumed tokens:  10236723200 | elapsed time per iteration (ms): 5008.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.041123E+00 | loss scale: 65536.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.556 | tokens per gpu per second (tgs): 1635.581 | TFLOPs: 13.16 |
g0214: [2024-08-10 09:56:21,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=39060, skipped=57, lr=[0.00019970990056608718, 0.00019970990056608718], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39060 loss: 0.9818 iter time (s): 5.702 samples/sec: 22.448
g0236:  iteration    39060/10000000 | consumed samples:      4999680 | consumed tokens:  10239344640 | elapsed time per iteration (ms): 5735.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.013610E+00 | loss scale: 65536.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.317 | tokens per gpu per second (tgs): 1428.286 | TFLOPs: 11.49 |
g0214: [2024-08-10 09:57:18,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=39070, skipped=57, lr=[0.0001997096905273164, 0.0001997096905273164], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39070 loss: 1.0225 iter time (s): 5.648 samples/sec: 22.662
g0236:  iteration    39070/10000000 | consumed samples:      5000960 | consumed tokens:  10241966080 | elapsed time per iteration (ms): 5681.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.023135E+00 | loss scale: 65536.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.529 | tokens per gpu per second (tgs): 1441.833 | TFLOPs: 11.60 |
g0214: [2024-08-10 09:58:09,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=39080, skipped=57, lr=[0.00019970948041264826, 0.00019970948041264826], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39080 loss: 0.9781 iter time (s): 5.074 samples/sec: 25.227
g0236:  iteration    39080/10000000 | consumed samples:      5002240 | consumed tokens:  10244587520 | elapsed time per iteration (ms): 5107.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.012070E+00 | loss scale: 65536.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.060 | tokens per gpu per second (tgs): 1603.846 | TFLOPs: 12.91 |
g0214: [2024-08-10 09:58:57,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=39090, skipped=57, lr=[0.00019970927022208297, 0.00019970927022208297], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39090 loss: 1.0288 iter time (s): 4.793 samples/sec: 26.704
g0236:  iteration    39090/10000000 | consumed samples:      5003520 | consumed tokens:  10247208960 | elapsed time per iteration (ms): 4829.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.480666E-01 | loss scale: 65536.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.505 | tokens per gpu per second (tgs): 1696.328 | TFLOPs: 13.65 |
g0214: [2024-08-10 09:59:54,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=39100, skipped=57, lr=[0.0001997090599556207, 0.0001997090599556207], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39100 loss: 1.0856 iter time (s): 5.604 samples/sec: 22.841
g0236:  iteration    39100/10000000 | consumed samples:      5004800 | consumed tokens:  10249830400 | elapsed time per iteration (ms): 5638.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.022855E+00 | loss scale: 65536.0 | grad norm: 0.250 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.701 | tokens per gpu per second (tgs): 1452.850 | TFLOPs: 11.69 |
g0214: [2024-08-10 10:00:42,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=39110, skipped=57, lr=[0.00019970884961326157, 0.00019970884961326157], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39110 loss: 0.8848 iter time (s): 4.769 samples/sec: 26.842
g0236:  iteration    39110/10000000 | consumed samples:      5006080 | consumed tokens:  10252451840 | elapsed time per iteration (ms): 4802.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.626145E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.654 | tokens per gpu per second (tgs): 1705.827 | TFLOPs: 13.73 |
g0214: [2024-08-10 10:01:31,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=39120, skipped=57, lr=[0.00019970863919500576, 0.00019970863919500576], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39120 loss: 1.0993 iter time (s): 4.910 samples/sec: 26.071
g0236:  iteration    39120/10000000 | consumed samples:      5007360 | consumed tokens:  10255073280 | elapsed time per iteration (ms): 4942.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.897697E-01 | loss scale: 65536.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.900 | tokens per gpu per second (tgs): 1657.572 | TFLOPs: 13.34 |
g0214: [2024-08-10 10:02:20,027] [INFO] [logging.py:96:log_dist] [Rank 0] step=39130, skipped=57, lr=[0.00019970842870085343, 0.00019970842870085343], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39130 loss: 1.0332 iter time (s): 4.788 samples/sec: 26.734
g0236:  iteration    39130/10000000 | consumed samples:      5008640 | consumed tokens:  10257694720 | elapsed time per iteration (ms): 4820.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.021760E+00 | loss scale: 65536.0 | grad norm: 0.251 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.552 | tokens per gpu per second (tgs): 1699.341 | TFLOPs: 13.67 |
g0235: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0214: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0214: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0232: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0232: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0232: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 10:03:10,019] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0214: [2024-08-10 10:03:10,018] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0214: [2024-08-10 10:03:10,019] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 10:03:10,019] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0214: [2024-08-10 10:03:10,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=39140, skipped=57, lr=[0.00019970821813080473, 0.00019970821813080473], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39140 loss: 1.2633 iter time (s): 4.967 samples/sec: 25.770
g0236:  iteration    39140/10000000 | consumed samples:      5009920 | consumed tokens:  10260316160 | elapsed time per iteration (ms): 4999.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.030264E+00 | loss scale: 131072.0 | grad norm: 0.151 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.602 | tokens per gpu per second (tgs): 1638.517 | TFLOPs: 13.19 |
g0214: [2024-08-10 10:04:03,528] [INFO] [logging.py:96:log_dist] [Rank 0] step=39150, skipped=57, lr=[0.00019970800748485987, 0.00019970800748485987], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39150 loss: 1.0510 iter time (s): 5.317 samples/sec: 24.072
g0236:  iteration    39150/10000000 | consumed samples:      5011200 | consumed tokens:  10262937600 | elapsed time per iteration (ms): 5350.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.003288E+00 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.923 | tokens per gpu per second (tgs): 1531.090 | TFLOPs: 12.32 |
g0214: [2024-08-10 10:04:52,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=39160, skipped=57, lr=[0.00019970779676301893, 0.00019970779676301893], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39160 loss: 0.9759 iter time (s): 4.827 samples/sec: 26.519
g0236:  iteration    39160/10000000 | consumed samples:      5012480 | consumed tokens:  10265559040 | elapsed time per iteration (ms): 4859.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.972444E-01 | loss scale: 131072.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.340 | tokens per gpu per second (tgs): 1685.789 | TFLOPs: 13.57 |
g0214: [2024-08-10 10:05:43,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=39170, skipped=57, lr=[0.00019970758596528216, 0.00019970758596528216], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39170 loss: 1.0041 iter time (s): 5.054 samples/sec: 25.324
g0236:  iteration    39170/10000000 | consumed samples:      5013760 | consumed tokens:  10268180480 | elapsed time per iteration (ms): 5087.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.700317E-01 | loss scale: 131072.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.158 | tokens per gpu per second (tgs): 1610.089 | TFLOPs: 12.96 |
g0214: [2024-08-10 10:06:32,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=39180, skipped=57, lr=[0.00019970737509164964, 0.00019970737509164964], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39180 loss: 0.9552 iter time (s): 4.861 samples/sec: 26.333
g0236:  iteration    39180/10000000 | consumed samples:      5015040 | consumed tokens:  10270801920 | elapsed time per iteration (ms): 4901.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.001336E+00 | loss scale: 131072.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.117 | tokens per gpu per second (tgs): 1671.492 | TFLOPs: 13.45 |
g0214: [2024-08-10 10:07:18,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=39190, skipped=57, lr=[0.0001997071641421216, 0.0001997071641421216], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39190 loss: 0.9120 iter time (s): 4.665 samples/sec: 27.441
g0236:  iteration    39190/10000000 | consumed samples:      5016320 | consumed tokens:  10273423360 | elapsed time per iteration (ms): 4698.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.019782E+00 | loss scale: 131072.0 | grad norm: 0.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.243 | tokens per gpu per second (tgs): 1743.530 | TFLOPs: 14.03 |
g0214: [2024-08-10 10:08:09,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=39200, skipped=57, lr=[0.00019970695311669816, 0.00019970695311669816], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39200 loss: 1.0519 iter time (s): 4.972 samples/sec: 25.746
g0236:  iteration    39200/10000000 | consumed samples:      5017600 | consumed tokens:  10276044800 | elapsed time per iteration (ms): 5005.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.040934E+00 | loss scale: 131072.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.574 | tokens per gpu per second (tgs): 1636.704 | TFLOPs: 13.17 |
g0214: [2024-08-10 10:08:57,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=39210, skipped=57, lr=[0.00019970674201537952, 0.00019970674201537952], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39210 loss: 0.9434 iter time (s): 4.847 samples/sec: 26.408
g0236:  iteration    39210/10000000 | consumed samples:      5018880 | consumed tokens:  10278666240 | elapsed time per iteration (ms): 4880.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.037617E+00 | loss scale: 131072.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.227 | tokens per gpu per second (tgs): 1678.512 | TFLOPs: 13.51 |
g0214: [2024-08-10 10:09:46,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=39220, skipped=57, lr=[0.0001997065308381658, 0.0001997065308381658], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39220 loss: 1.1023 iter time (s): 4.823 samples/sec: 26.540
g0236:  iteration    39220/10000000 | consumed samples:      5020160 | consumed tokens:  10281287680 | elapsed time per iteration (ms): 4855.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.012925E+00 | loss scale: 131072.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.361 | tokens per gpu per second (tgs): 1687.093 | TFLOPs: 13.58 |
g0214: [2024-08-10 10:10:36,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=39230, skipped=57, lr=[0.0001997063195850572, 0.0001997063195850572], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39230 loss: 0.9597 iter time (s): 4.966 samples/sec: 25.773
g0236:  iteration    39230/10000000 | consumed samples:      5021440 | consumed tokens:  10283909120 | elapsed time per iteration (ms): 4999.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.946414E-01 | loss scale: 131072.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.605 | tokens per gpu per second (tgs): 1638.724 | TFLOPs: 13.19 |
g0214: [2024-08-10 10:11:26,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=39240, skipped=57, lr=[0.00019970610825605385, 0.00019970610825605385], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39240 loss: 0.9722 iter time (s): 4.989 samples/sec: 25.659
g0236:  iteration    39240/10000000 | consumed samples:      5022720 | consumed tokens:  10286530560 | elapsed time per iteration (ms): 5032.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.490410E-01 | loss scale: 131072.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.437 | tokens per gpu per second (tgs): 1627.944 | TFLOPs: 13.10 |
g0214: [2024-08-10 10:12:13,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=39250, skipped=57, lr=[0.00019970589685115595, 0.00019970589685115595], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39250 loss: 0.7759 iter time (s): 4.608 samples/sec: 27.776
g0236:  iteration    39250/10000000 | consumed samples:      5024000 | consumed tokens:  10289152000 | elapsed time per iteration (ms): 4641.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.589510E-01 | loss scale: 131072.0 | grad norm: 0.434 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.577 | tokens per gpu per second (tgs): 1764.950 | TFLOPs: 14.20 |
g0214: [2024-08-10 10:13:00,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=39260, skipped=57, lr=[0.00019970568537036362, 0.00019970568537036362], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39260 loss: 0.9954 iter time (s): 4.722 samples/sec: 27.108
g0236:  iteration    39260/10000000 | consumed samples:      5025280 | consumed tokens:  10291773440 | elapsed time per iteration (ms): 4755.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.010727E+00 | loss scale: 131072.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.914 | tokens per gpu per second (tgs): 1722.487 | TFLOPs: 13.86 |
g0214: [2024-08-10 10:13:48,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=39270, skipped=57, lr=[0.00019970547381367707, 0.00019970547381367707], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39270 loss: 0.9040 iter time (s): 4.736 samples/sec: 27.026
g0236:  iteration    39270/10000000 | consumed samples:      5026560 | consumed tokens:  10294394880 | elapsed time per iteration (ms): 4769.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.009608E+00 | loss scale: 131072.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.836 | tokens per gpu per second (tgs): 1717.483 | TFLOPs: 13.82 |
g0214: [2024-08-10 10:14:34,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=39280, skipped=57, lr=[0.00019970526218109639, 0.00019970526218109639], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39280 loss: 1.0636 iter time (s): 4.567 samples/sec: 28.024
g0236:  iteration    39280/10000000 | consumed samples:      5027840 | consumed tokens:  10297016320 | elapsed time per iteration (ms): 4600.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.843357E-01 | loss scale: 131072.0 | grad norm: 0.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.825 | tokens per gpu per second (tgs): 1780.782 | TFLOPs: 14.33 |
g0214: [2024-08-10 10:15:24,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=39290, skipped=57, lr=[0.0001997050504726218, 0.0001997050504726218], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39290 loss: 1.1453 iter time (s): 5.017 samples/sec: 25.513
g0236:  iteration    39290/10000000 | consumed samples:      5029120 | consumed tokens:  10299637760 | elapsed time per iteration (ms): 5050.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.762521E-01 | loss scale: 131072.0 | grad norm: 0.268 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.345 | tokens per gpu per second (tgs): 1622.061 | TFLOPs: 13.05 |
g0214: [2024-08-10 10:16:13,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=39300, skipped=57, lr=[0.0001997048386882535, 0.0001997048386882535], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39300 loss: 1.1352 iter time (s): 4.779 samples/sec: 26.783
g0236:  iteration    39300/10000000 | consumed samples:      5030400 | consumed tokens:  10302259200 | elapsed time per iteration (ms): 4812.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.009523E+00 | loss scale: 131072.0 | grad norm: 0.259 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.600 | tokens per gpu per second (tgs): 1702.405 | TFLOPs: 13.70 |
g0214: [2024-08-10 10:17:01,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=39310, skipped=57, lr=[0.00019970462682799154, 0.00019970462682799154], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39310 loss: 1.0705 iter time (s): 4.772 samples/sec: 26.826
g0236:  iteration    39310/10000000 | consumed samples:      5031680 | consumed tokens:  10304880640 | elapsed time per iteration (ms): 4806.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.039586E+00 | loss scale: 131072.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.631 | tokens per gpu per second (tgs): 1704.414 | TFLOPs: 13.72 |
g0214: [2024-08-10 10:17:52,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=39320, skipped=57, lr=[0.00019970441489183618, 0.00019970441489183618], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39320 loss: 1.0221 iter time (s): 5.065 samples/sec: 25.269
g0236:  iteration    39320/10000000 | consumed samples:      5032960 | consumed tokens:  10307502080 | elapsed time per iteration (ms): 5099.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.913228E-01 | loss scale: 131072.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.101 | tokens per gpu per second (tgs): 1606.457 | TFLOPs: 12.93 |
g0214: [2024-08-10 10:18:41,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=39330, skipped=57, lr=[0.00019970420287978753, 0.00019970420287978753], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39330 loss: 0.9781 iter time (s): 4.901 samples/sec: 26.117
g0236:  iteration    39330/10000000 | consumed samples:      5034240 | consumed tokens:  10310123520 | elapsed time per iteration (ms): 4933.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.646173E-01 | loss scale: 131072.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.943 | tokens per gpu per second (tgs): 1660.368 | TFLOPs: 13.36 |
g0214: [2024-08-10 10:19:28,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=39340, skipped=57, lr=[0.0001997039907918458, 0.0001997039907918458], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39340 loss: 1.0446 iter time (s): 4.637 samples/sec: 27.606
g0236:  iteration    39340/10000000 | consumed samples:      5035520 | consumed tokens:  10312744960 | elapsed time per iteration (ms): 4672.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.054141E+00 | loss scale: 131072.0 | grad norm: 0.253 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.393 | tokens per gpu per second (tgs): 1753.133 | TFLOPs: 14.11 |
g0214: [2024-08-10 10:20:13,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=39350, skipped=57, lr=[0.0001997037786280111, 0.0001997037786280111], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39350 loss: 1.1017 iter time (s): 4.478 samples/sec: 28.587
g0236:  iteration    39350/10000000 | consumed samples:      5036800 | consumed tokens:  10315366400 | elapsed time per iteration (ms): 4527.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.035102E+00 | loss scale: 131072.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.272 | tokens per gpu per second (tgs): 1809.419 | TFLOPs: 14.56 |
g0214: [2024-08-10 10:20:57,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=39360, skipped=57, lr=[0.00019970356638828363, 0.00019970356638828363], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39360 loss: 1.0631 iter time (s): 4.386 samples/sec: 29.182
g0236:  iteration    39360/10000000 | consumed samples:      5038080 | consumed tokens:  10317987840 | elapsed time per iteration (ms): 4419.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.005432E+00 | loss scale: 131072.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.960 | tokens per gpu per second (tgs): 1853.448 | TFLOPs: 14.92 |
g0214: [2024-08-10 10:21:39,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=39370, skipped=57, lr=[0.00019970335407266355, 0.00019970335407266355], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39370 loss: 1.1184 iter time (s): 4.106 samples/sec: 31.172
g0236:  iteration    39370/10000000 | consumed samples:      5039360 | consumed tokens:  10320609280 | elapsed time per iteration (ms): 4139.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.894622E-01 | loss scale: 131072.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.922 | tokens per gpu per second (tgs): 1978.985 | TFLOPs: 15.93 |
g0214: [2024-08-10 10:22:30,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=39380, skipped=57, lr=[0.000199703141681151, 0.000199703141681151], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39380 loss: 0.9826 iter time (s): 5.126 samples/sec: 24.968
g0236:  iteration    39380/10000000 | consumed samples:      5040640 | consumed tokens:  10323230720 | elapsed time per iteration (ms): 5160.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.029879E+00 | loss scale: 131072.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.805 | tokens per gpu per second (tgs): 1587.535 | TFLOPs: 12.78 |
g0214: [2024-08-10 10:23:20,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=39390, skipped=57, lr=[0.00019970292921374616, 0.00019970292921374616], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39390 loss: 1.2306 iter time (s): 4.961 samples/sec: 25.801
g0236:  iteration    39390/10000000 | consumed samples:      5041920 | consumed tokens:  10325852160 | elapsed time per iteration (ms): 4994.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.013323E+00 | loss scale: 131072.0 | grad norm: 0.299 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.631 | tokens per gpu per second (tgs): 1640.366 | TFLOPs: 13.20 |
g0214: [2024-08-10 10:24:18,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=39400, skipped=57, lr=[0.0001997027166704492, 0.0001997027166704492], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39400 loss: 1.0147 iter time (s): 5.722 samples/sec: 22.370
g0236:  iteration    39400/10000000 | consumed samples:      5043200 | consumed tokens:  10328473600 | elapsed time per iteration (ms): 5762.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.765480E-01 | loss scale: 131072.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.211 | tokens per gpu per second (tgs): 1421.524 | TFLOPs: 11.44 |
g0214: [2024-08-10 10:25:08,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=39410, skipped=57, lr=[0.00019970250405126026, 0.00019970250405126026], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39410 loss: 0.9109 iter time (s): 5.047 samples/sec: 25.359
g0236:  iteration    39410/10000000 | consumed samples:      5044480 | consumed tokens:  10331095040 | elapsed time per iteration (ms): 5081.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.637434E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.191 | tokens per gpu per second (tgs): 1612.216 | TFLOPs: 12.97 |
g0214: [2024-08-10 10:25:59,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=39420, skipped=57, lr=[0.0001997022913561795, 0.0001997022913561795], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39420 loss: 0.9387 iter time (s): 5.027 samples/sec: 25.462
g0236:  iteration    39420/10000000 | consumed samples:      5045760 | consumed tokens:  10333716480 | elapsed time per iteration (ms): 5060.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.807573E-01 | loss scale: 131072.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.296 | tokens per gpu per second (tgs): 1618.943 | TFLOPs: 13.03 |
g0214: [2024-08-10 10:26:52,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=39430, skipped=57, lr=[0.00019970207858520717, 0.00019970207858520717], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39430 loss: 0.9603 iter time (s): 5.305 samples/sec: 24.128
g0236:  iteration    39430/10000000 | consumed samples:      5047040 | consumed tokens:  10336337920 | elapsed time per iteration (ms): 5337.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.917710E-01 | loss scale: 131072.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.980 | tokens per gpu per second (tgs): 1534.719 | TFLOPs: 12.35 |
g0214: [2024-08-10 10:27:44,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=39440, skipped=57, lr=[0.0001997018657383433, 0.0001997018657383433], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39440 loss: 0.9602 iter time (s): 5.079 samples/sec: 25.201
g0236:  iteration    39440/10000000 | consumed samples:      5048320 | consumed tokens:  10338959360 | elapsed time per iteration (ms): 5112.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.996576E-01 | loss scale: 131072.0 | grad norm: 0.328 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.039 | tokens per gpu per second (tgs): 1602.469 | TFLOPs: 12.90 |
g0214: [2024-08-10 10:28:37,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=39450, skipped=57, lr=[0.00019970165281558814, 0.00019970165281558814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39450 loss: 1.0593 iter time (s): 5.279 samples/sec: 24.246
g0236:  iteration    39450/10000000 | consumed samples:      5049600 | consumed tokens:  10341580800 | elapsed time per iteration (ms): 5313.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.876594E-01 | loss scale: 131072.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.090 | tokens per gpu per second (tgs): 1541.767 | TFLOPs: 12.41 |
g0214: [2024-08-10 10:29:24,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=39460, skipped=57, lr=[0.00019970143981694184, 0.00019970143981694184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39460 loss: 0.8706 iter time (s): 4.649 samples/sec: 27.532
g0236:  iteration    39460/10000000 | consumed samples:      5050880 | consumed tokens:  10344202240 | elapsed time per iteration (ms): 4682.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.034538E+00 | loss scale: 131072.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.334 | tokens per gpu per second (tgs): 1749.368 | TFLOPs: 14.08 |
g0214: [2024-08-10 10:30:20,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=39470, skipped=57, lr=[0.00019970122674240457, 0.00019970122674240457], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39470 loss: 0.8694 iter time (s): 5.637 samples/sec: 22.706
g0236:  iteration    39470/10000000 | consumed samples:      5052160 | consumed tokens:  10346823680 | elapsed time per iteration (ms): 5670.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.803205E-01 | loss scale: 131072.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.572 | tokens per gpu per second (tgs): 1444.585 | TFLOPs: 11.62 |
g0214: [2024-08-10 10:31:20,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=39480, skipped=57, lr=[0.00019970101359197644, 0.00019970101359197644], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39480 loss: 1.2392 iter time (s): 5.977 samples/sec: 21.414
g0236:  iteration    39480/10000000 | consumed samples:      5053440 | consumed tokens:  10349445120 | elapsed time per iteration (ms): 6010.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.031398E+00 | loss scale: 131072.0 | grad norm: 0.295 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.296 | tokens per gpu per second (tgs): 1362.939 | TFLOPs: 10.97 |
g0214: [2024-08-10 10:32:17,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=39490, skipped=57, lr=[0.0001997008003656577, 0.0001997008003656577], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39490 loss: 1.0517 iter time (s): 5.666 samples/sec: 22.591
g0236:  iteration    39490/10000000 | consumed samples:      5054720 | consumed tokens:  10352066560 | elapsed time per iteration (ms): 5698.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.732245E-01 | loss scale: 131072.0 | grad norm: 0.288 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.461 | tokens per gpu per second (tgs): 1437.532 | TFLOPs: 11.57 |
g0214: [2024-08-10 10:32:59,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=39500, skipped=57, lr=[0.00019970058706344845, 0.00019970058706344845], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39500 loss: 1.1687 iter time (s): 4.130 samples/sec: 30.992
g0236:  iteration    39500/10000000 | consumed samples:      5056000 | consumed tokens:  10354688000 | elapsed time per iteration (ms): 4163.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.014780E+00 | loss scale: 131072.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.744 | tokens per gpu per second (tgs): 1967.597 | TFLOPs: 15.83 |
g0214: [2024-08-10 10:33:44,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=39510, skipped=57, lr=[0.0001997003736853489, 0.0001997003736853489], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39510 loss: 0.9466 iter time (s): 4.512 samples/sec: 28.366
g0236:  iteration    39510/10000000 | consumed samples:      5057280 | consumed tokens:  10357309440 | elapsed time per iteration (ms): 4545.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.005394E+00 | loss scale: 131072.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.162 | tokens per gpu per second (tgs): 1802.386 | TFLOPs: 14.50 |
g0214: [2024-08-10 10:34:37,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=39520, skipped=57, lr=[0.00019970016023135913, 0.00019970016023135913], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39520 loss: 0.9175 iter time (s): 5.196 samples/sec: 24.633
g0236:  iteration    39520/10000000 | consumed samples:      5058560 | consumed tokens:  10359930880 | elapsed time per iteration (ms): 5229.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.747273E-01 | loss scale: 131072.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.478 | tokens per gpu per second (tgs): 1566.590 | TFLOPs: 12.61 |
g0214: [2024-08-10 10:35:28,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=39530, skipped=57, lr=[0.0001996999467014794, 0.0001996999467014794], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39530 loss: 1.0360 iter time (s): 5.100 samples/sec: 25.099
g0236:  iteration    39530/10000000 | consumed samples:      5059840 | consumed tokens:  10362552320 | elapsed time per iteration (ms): 5133.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.953210E-01 | loss scale: 131072.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.934 | tokens per gpu per second (tgs): 1595.758 | TFLOPs: 12.84 |
g0214: [2024-08-10 10:36:18,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=39540, skipped=57, lr=[0.0001996997330957098, 0.0001996997330957098], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39540 loss: 0.9037 iter time (s): 4.938 samples/sec: 25.920
g0236:  iteration    39540/10000000 | consumed samples:      5061120 | consumed tokens:  10365173760 | elapsed time per iteration (ms): 4971.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.932199E-01 | loss scale: 131072.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.749 | tokens per gpu per second (tgs): 1647.912 | TFLOPs: 13.26 |
g0214: [2024-08-10 10:37:08,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=39550, skipped=57, lr=[0.00019969951941405058, 0.00019969951941405058], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39550 loss: 1.1247 iter time (s): 4.948 samples/sec: 25.869
g0236:  iteration    39550/10000000 | consumed samples:      5062400 | consumed tokens:  10367795200 | elapsed time per iteration (ms): 4980.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.023789E+00 | loss scale: 131072.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.698 | tokens per gpu per second (tgs): 1644.700 | TFLOPs: 13.24 |
g0214: [2024-08-10 10:38:00,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=39560, skipped=57, lr=[0.00019969930565650183, 0.00019969930565650183], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39560 loss: 1.0704 iter time (s): 5.200 samples/sec: 24.614
g0236:  iteration    39560/10000000 | consumed samples:      5063680 | consumed tokens:  10370416640 | elapsed time per iteration (ms): 5233.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.015742E+00 | loss scale: 131072.0 | grad norm: 0.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.459 | tokens per gpu per second (tgs): 1565.385 | TFLOPs: 12.60 |
g0214: [2024-08-10 10:38:53,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=39570, skipped=57, lr=[0.00019969909182306373, 0.00019969909182306373], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39570 loss: 1.0707 iter time (s): 5.273 samples/sec: 24.274
g0236:  iteration    39570/10000000 | consumed samples:      5064960 | consumed tokens:  10373038080 | elapsed time per iteration (ms): 5316.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.038816E+00 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.077 | tokens per gpu per second (tgs): 1540.960 | TFLOPs: 12.40 |
g0214: [2024-08-10 10:39:42,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=39580, skipped=57, lr=[0.00019969887791373647, 0.00019969887791373647], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39580 loss: 1.0131 iter time (s): 4.832 samples/sec: 26.490
g0236:  iteration    39580/10000000 | consumed samples:      5066240 | consumed tokens:  10375659520 | elapsed time per iteration (ms): 4866.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.001962E+00 | loss scale: 131072.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.301 | tokens per gpu per second (tgs): 1683.285 | TFLOPs: 13.55 |
g0214: [2024-08-10 10:40:27,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=39590, skipped=57, lr=[0.00019969866392852018, 0.00019969866392852018], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39590 loss: 0.8901 iter time (s): 4.471 samples/sec: 28.626
g0236:  iteration    39590/10000000 | consumed samples:      5067520 | consumed tokens:  10378280960 | elapsed time per iteration (ms): 4503.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.014803E+00 | loss scale: 131072.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.420 | tokens per gpu per second (tgs): 1818.867 | TFLOPs: 14.64 |
g0214: [2024-08-10 10:41:14,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=39600, skipped=57, lr=[0.00019969844986741502, 0.00019969844986741502], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39600 loss: 0.9403 iter time (s): 4.683 samples/sec: 27.335
g0236:  iteration    39600/10000000 | consumed samples:      5068800 | consumed tokens:  10380902400 | elapsed time per iteration (ms): 4715.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.594207E-01 | loss scale: 131072.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.143 | tokens per gpu per second (tgs): 1737.161 | TFLOPs: 13.98 |
g0214: [2024-08-10 10:42:04,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=39610, skipped=57, lr=[0.00019969823573042124, 0.00019969823573042124], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39610 loss: 1.0982 iter time (s): 5.017 samples/sec: 25.512
g0236:  iteration    39610/10000000 | consumed samples:      5070080 | consumed tokens:  10383523840 | elapsed time per iteration (ms): 5049.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.025133E+00 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.348 | tokens per gpu per second (tgs): 1622.246 | TFLOPs: 13.05 |
g0214: [2024-08-10 10:42:57,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=39620, skipped=57, lr=[0.00019969802151753886, 0.00019969802151753886], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39620 loss: 1.0710 iter time (s): 5.242 samples/sec: 24.416
g0236:  iteration    39620/10000000 | consumed samples:      5071360 | consumed tokens:  10386145280 | elapsed time per iteration (ms): 5274.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.962902E-01 | loss scale: 131072.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.267 | tokens per gpu per second (tgs): 1553.058 | TFLOPs: 12.50 |
g0214: [2024-08-10 10:43:58,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=39630, skipped=57, lr=[0.00019969780722876819, 0.00019969780722876819], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39630 loss: 0.9415 iter time (s): 6.081 samples/sec: 21.051
g0236:  iteration    39630/10000000 | consumed samples:      5072640 | consumed tokens:  10388766720 | elapsed time per iteration (ms): 6115.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.031758E+00 | loss scale: 131072.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.931 | tokens per gpu per second (tgs): 1339.611 | TFLOPs: 10.78 |
g0235: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0214: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0232: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-10 10:44:57,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 10:44:57,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0232: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-10 10:44:57,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-10 10:44:57,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-10 10:44:57,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0214: [2024-08-10 10:44:57,157] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 10:44:57,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0214: [2024-08-10 10:44:57,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0232: [2024-08-10 10:44:57,158] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0214: [2024-08-10 10:44:57,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=39640, skipped=57, lr=[0.0001996975928641093, 0.0001996975928641093], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39640 loss: 1.0920 iter time (s): 5.796 samples/sec: 22.082
g0236:  iteration    39640/10000000 | consumed samples:      5073920 | consumed tokens:  10391388160 | elapsed time per iteration (ms): 5831.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.991337E-01 | loss scale: 262144.0 | grad norm: 0.110 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.952 | tokens per gpu per second (tgs): 1404.909 | TFLOPs: 11.31 |
g0214: [2024-08-10 10:45:47,780] [INFO] [logging.py:96:log_dist] [Rank 0] step=39650, skipped=57, lr=[0.00019969737842356238, 0.00019969737842356238], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39650 loss: 0.9595 iter time (s): 5.023 samples/sec: 25.482
g0236:  iteration    39650/10000000 | consumed samples:      5075200 | consumed tokens:  10394009600 | elapsed time per iteration (ms): 5064.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.877048E-01 | loss scale: 262144.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.273 | tokens per gpu per second (tgs): 1617.449 | TFLOPs: 13.02 |
g0214: [2024-08-10 10:46:34,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=39660, skipped=57, lr=[0.00019969716390712763, 0.00019969716390712763], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39660 loss: 0.8631 iter time (s): 4.628 samples/sec: 27.659
g0236:  iteration    39660/10000000 | consumed samples:      5076480 | consumed tokens:  10396631040 | elapsed time per iteration (ms): 4660.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.799973E-01 | loss scale: 262144.0 | grad norm: 0.265 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.467 | tokens per gpu per second (tgs): 1757.858 | TFLOPs: 14.15 |
g0214: [2024-08-10 10:47:27,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=39670, skipped=57, lr=[0.00019969694931480514, 0.00019969694931480514], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39670 loss: 0.8783 iter time (s): 5.318 samples/sec: 24.068
g0236:  iteration    39670/10000000 | consumed samples:      5077760 | consumed tokens:  10399252480 | elapsed time per iteration (ms): 5350.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.600639E-01 | loss scale: 262144.0 | grad norm: 0.268 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.922 | tokens per gpu per second (tgs): 1530.980 | TFLOPs: 12.32 |
g0214: [2024-08-10 10:48:22,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=39680, skipped=57, lr=[0.00019969673464659515, 0.00019969673464659515], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39680 loss: 1.0098 iter time (s): 5.407 samples/sec: 23.671
g0236:  iteration    39680/10000000 | consumed samples:      5079040 | consumed tokens:  10401873920 | elapsed time per iteration (ms): 5440.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.013282E+00 | loss scale: 262144.0 | grad norm: 0.255 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.529 | tokens per gpu per second (tgs): 1505.885 | TFLOPs: 12.12 |
g0214: [2024-08-10 10:49:16,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=39690, skipped=57, lr=[0.0001996965199024978, 0.0001996965199024978], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39690 loss: 0.9368 iter time (s): 5.346 samples/sec: 23.944
g0236:  iteration    39690/10000000 | consumed samples:      5080320 | consumed tokens:  10404495360 | elapsed time per iteration (ms): 5380.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.031466E+00 | loss scale: 262144.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.788 | tokens per gpu per second (tgs): 1522.449 | TFLOPs: 12.25 |
g0214: [2024-08-10 10:50:05,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=39700, skipped=57, lr=[0.00019969630508251323, 0.00019969630508251323], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39700 loss: 0.9484 iter time (s): 4.920 samples/sec: 26.017
g0236:  iteration    39700/10000000 | consumed samples:      5081600 | consumed tokens:  10407116800 | elapsed time per iteration (ms): 4953.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.694061E-01 | loss scale: 262144.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.839 | tokens per gpu per second (tgs): 1653.687 | TFLOPs: 13.31 |
g0214: [2024-08-10 10:50:55,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=39710, skipped=57, lr=[0.00019969609018664165, 0.00019969609018664165], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39710 loss: 0.9401 iter time (s): 4.947 samples/sec: 25.876
g0236:  iteration    39710/10000000 | consumed samples:      5082880 | consumed tokens:  10409738240 | elapsed time per iteration (ms): 4980.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.019494E+00 | loss scale: 262144.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.702 | tokens per gpu per second (tgs): 1644.906 | TFLOPs: 13.24 |
g0214: [2024-08-10 10:51:46,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=39720, skipped=57, lr=[0.00019969587521488318, 0.00019969587521488318], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39720 loss: 0.9619 iter time (s): 5.040 samples/sec: 25.399
g0236:  iteration    39720/10000000 | consumed samples:      5084160 | consumed tokens:  10412359680 | elapsed time per iteration (ms): 5072.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.560535E-01 | loss scale: 262144.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.236 | tokens per gpu per second (tgs): 1615.100 | TFLOPs: 13.00 |
g0214: [2024-08-10 10:52:32,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=39730, skipped=57, lr=[0.00019969566016723802, 0.00019969566016723802], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39730 loss: 0.9053 iter time (s): 4.607 samples/sec: 27.782
g0236:  iteration    39730/10000000 | consumed samples:      5085440 | consumed tokens:  10414981120 | elapsed time per iteration (ms): 4639.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.980186E-01 | loss scale: 262144.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.590 | tokens per gpu per second (tgs): 1765.756 | TFLOPs: 14.21 |
g0214: [2024-08-10 10:53:22,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=39740, skipped=57, lr=[0.0001996954450437063, 0.0001996954450437063], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39740 loss: 0.8867 iter time (s): 4.939 samples/sec: 25.918
g0236:  iteration    39740/10000000 | consumed samples:      5086720 | consumed tokens:  10417602560 | elapsed time per iteration (ms): 4971.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.005345E+00 | loss scale: 262144.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.746 | tokens per gpu per second (tgs): 1647.775 | TFLOPs: 13.26 |
g0214: [2024-08-10 10:54:16,774] [INFO] [logging.py:96:log_dist] [Rank 0] step=39750, skipped=57, lr=[0.00019969522984428822, 0.00019969522984428822], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39750 loss: 1.0282 iter time (s): 5.414 samples/sec: 23.642
g0236:  iteration    39750/10000000 | consumed samples:      5088000 | consumed tokens:  10420224000 | elapsed time per iteration (ms): 5447.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.824120E-01 | loss scale: 262144.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.499 | tokens per gpu per second (tgs): 1503.909 | TFLOPs: 12.10 |
g0214: [2024-08-10 10:55:05,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=39760, skipped=57, lr=[0.00019969501456898396, 0.00019969501456898396], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39760 loss: 0.9769 iter time (s): 4.802 samples/sec: 26.656
g0236:  iteration    39760/10000000 | consumed samples:      5089280 | consumed tokens:  10422845440 | elapsed time per iteration (ms): 4835.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.004462E+00 | loss scale: 262144.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.470 | tokens per gpu per second (tgs): 1694.077 | TFLOPs: 13.63 |
g0214: [2024-08-10 10:55:55,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=39770, skipped=57, lr=[0.0001996947992177936, 0.0001996947992177936], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39770 loss: 0.8559 iter time (s): 4.994 samples/sec: 25.631
g0236:  iteration    39770/10000000 | consumed samples:      5090560 | consumed tokens:  10425466880 | elapsed time per iteration (ms): 5037.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.297094E-01 | loss scale: 262144.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.409 | tokens per gpu per second (tgs): 1626.198 | TFLOPs: 13.09 |
g0214: [2024-08-10 10:56:45,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=39780, skipped=57, lr=[0.0001996945837907174, 0.0001996945837907174], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39780 loss: 0.9699 iter time (s): 4.983 samples/sec: 25.688
g0236:  iteration    39780/10000000 | consumed samples:      5091840 | consumed tokens:  10428088320 | elapsed time per iteration (ms): 5015.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.887453E-01 | loss scale: 262144.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.522 | tokens per gpu per second (tgs): 1633.432 | TFLOPs: 13.14 |
g0214: [2024-08-10 10:57:34,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=39790, skipped=57, lr=[0.00019969436828775545, 0.00019969436828775545], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39790 loss: 0.9289 iter time (s): 4.857 samples/sec: 26.355
g0236:  iteration    39790/10000000 | consumed samples:      5093120 | consumed tokens:  10430709760 | elapsed time per iteration (ms): 4903.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.013960E+00 | loss scale: 262144.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.104 | tokens per gpu per second (tgs): 1670.658 | TFLOPs: 13.44 |
g0214: [2024-08-10 10:58:29,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=39800, skipped=57, lr=[0.000199694152708908, 0.000199694152708908], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39800 loss: 0.8900 iter time (s): 5.473 samples/sec: 23.389
g0236:  iteration    39800/10000000 | consumed samples:      5094400 | consumed tokens:  10433331200 | elapsed time per iteration (ms): 5506.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.000688E+00 | loss scale: 262144.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.247 | tokens per gpu per second (tgs): 1487.786 | TFLOPs: 11.97 |
g0214: [2024-08-10 10:59:18,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=39810, skipped=57, lr=[0.00019969393705417514, 0.00019969393705417514], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39810 loss: 0.8909 iter time (s): 4.816 samples/sec: 26.580
g0236:  iteration    39810/10000000 | consumed samples:      5095680 | consumed tokens:  10435952640 | elapsed time per iteration (ms): 4848.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.824459E-01 | loss scale: 262144.0 | grad norm: 0.250 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.401 | tokens per gpu per second (tgs): 1689.660 | TFLOPs: 13.60 |
g0214: [2024-08-10 11:00:13,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=39820, skipped=57, lr=[0.0001996937213235571, 0.0001996937213235571], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39820 loss: 1.0289 iter time (s): 5.472 samples/sec: 23.392
g0236:  iteration    39820/10000000 | consumed samples:      5096960 | consumed tokens:  10438574080 | elapsed time per iteration (ms): 5514.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.870530E-01 | loss scale: 262144.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.214 | tokens per gpu per second (tgs): 1485.680 | TFLOPs: 11.96 |
g0214: [2024-08-10 11:01:03,133] [INFO] [logging.py:96:log_dist] [Rank 0] step=39830, skipped=57, lr=[0.00019969350551705397, 0.00019969350551705397], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39830 loss: 1.0156 iter time (s): 4.942 samples/sec: 25.899
g0236:  iteration    39830/10000000 | consumed samples:      5098240 | consumed tokens:  10441195520 | elapsed time per iteration (ms): 4975.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.020730E+00 | loss scale: 262144.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.727 | tokens per gpu per second (tgs): 1646.497 | TFLOPs: 13.25 |
g0214: [2024-08-10 11:01:53,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=39840, skipped=57, lr=[0.000199693289634666, 0.000199693289634666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39840 loss: 1.0410 iter time (s): 5.020 samples/sec: 25.499
g0236:  iteration    39840/10000000 | consumed samples:      5099520 | consumed tokens:  10443816960 | elapsed time per iteration (ms): 5052.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.765000E-01 | loss scale: 262144.0 | grad norm: 0.258 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.335 | tokens per gpu per second (tgs): 1621.456 | TFLOPs: 13.05 |
g0214: [2024-08-10 11:02:46,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=39850, skipped=57, lr=[0.00019969307367639327, 0.00019969307367639327], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39850 loss: 0.8821 iter time (s): 5.271 samples/sec: 24.282
g0236:  iteration    39850/10000000 | consumed samples:      5100800 | consumed tokens:  10446438400 | elapsed time per iteration (ms): 5304.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.994644E-01 | loss scale: 262144.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.131 | tokens per gpu per second (tgs): 1544.373 | TFLOPs: 12.43 |
g0214: [2024-08-10 11:03:37,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=39860, skipped=57, lr=[0.00019969285764223602, 0.00019969285764223602], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39860 loss: 1.1525 iter time (s): 5.079 samples/sec: 25.201
g0236:  iteration    39860/10000000 | consumed samples:      5102080 | consumed tokens:  10449059840 | elapsed time per iteration (ms): 5111.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.778558E-01 | loss scale: 262144.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.041 | tokens per gpu per second (tgs): 1602.604 | TFLOPs: 12.90 |
g0214: [2024-08-10 11:04:37,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=39870, skipped=57, lr=[0.00019969264153219438, 0.00019969264153219438], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39870 loss: 0.9311 iter time (s): 5.948 samples/sec: 21.520
g0236:  iteration    39870/10000000 | consumed samples:      5103360 | consumed tokens:  10451681280 | elapsed time per iteration (ms): 5981.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.030724E+00 | loss scale: 262144.0 | grad norm: 0.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.399 | tokens per gpu per second (tgs): 1369.550 | TFLOPs: 11.02 |
g0214: [2024-08-10 11:05:30,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=39880, skipped=57, lr=[0.00019969242534626853, 0.00019969242534626853], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39880 loss: 1.0061 iter time (s): 5.229 samples/sec: 24.479
g0236:  iteration    39880/10000000 | consumed samples:      5104640 | consumed tokens:  10454302720 | elapsed time per iteration (ms): 5261.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.938162E-01 | loss scale: 262144.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.328 | tokens per gpu per second (tgs): 1557.020 | TFLOPs: 12.53 |
g0214: [2024-08-10 11:06:22,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=39890, skipped=57, lr=[0.0001996922090844586, 0.0001996922090844586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39890 loss: 1.0072 iter time (s): 5.155 samples/sec: 24.831
g0236:  iteration    39890/10000000 | consumed samples:      5105920 | consumed tokens:  10456924160 | elapsed time per iteration (ms): 5187.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.909787E-01 | loss scale: 262144.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.676 | tokens per gpu per second (tgs): 1579.262 | TFLOPs: 12.71 |
g0214: [2024-08-10 11:07:17,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=39900, skipped=57, lr=[0.0001996919927467648, 0.0001996919927467648], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39900 loss: 1.2169 iter time (s): 5.511 samples/sec: 23.228
g0236:  iteration    39900/10000000 | consumed samples:      5107200 | consumed tokens:  10459545600 | elapsed time per iteration (ms): 5543.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.051612E+00 | loss scale: 262144.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.091 | tokens per gpu per second (tgs): 1477.844 | TFLOPs: 11.89 |
g0214: [2024-08-10 11:08:09,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=39910, skipped=57, lr=[0.0001996917763331873, 0.0001996917763331873], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39910 loss: 0.9918 iter time (s): 5.147 samples/sec: 24.868
g0236:  iteration    39910/10000000 | consumed samples:      5108480 | consumed tokens:  10462167040 | elapsed time per iteration (ms): 5179.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.028746E+00 | loss scale: 262144.0 | grad norm: 0.440 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.712 | tokens per gpu per second (tgs): 1581.540 | TFLOPs: 12.73 |
g0214: [2024-08-10 11:08:57,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=39920, skipped=57, lr=[0.00019969155984372622, 0.00019969155984372622], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39920 loss: 1.2287 iter time (s): 4.780 samples/sec: 26.780
g0236:  iteration    39920/10000000 | consumed samples:      5109760 | consumed tokens:  10464788480 | elapsed time per iteration (ms): 4811.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.013028E+00 | loss scale: 262144.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.601 | tokens per gpu per second (tgs): 1702.433 | TFLOPs: 13.70 |
g0214: [2024-08-10 11:09:46,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=39930, skipped=57, lr=[0.00019969134327838176, 0.00019969134327838176], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39930 loss: 1.0798 iter time (s): 4.918 samples/sec: 26.026
g0236:  iteration    39930/10000000 | consumed samples:      5111040 | consumed tokens:  10467409920 | elapsed time per iteration (ms): 4950.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.017376E+00 | loss scale: 262144.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.856 | tokens per gpu per second (tgs): 1654.816 | TFLOPs: 13.32 |
g0214: [2024-08-10 11:10:38,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=39940, skipped=57, lr=[0.0001996911266371541, 0.0001996911266371541], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39940 loss: 0.9739 iter time (s): 5.160 samples/sec: 24.805
g0236:  iteration    39940/10000000 | consumed samples:      5112320 | consumed tokens:  10470031360 | elapsed time per iteration (ms): 5192.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.014291E+00 | loss scale: 262144.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.651 | tokens per gpu per second (tgs): 1577.681 | TFLOPs: 12.70 |
g0214: [2024-08-10 11:11:28,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=39950, skipped=57, lr=[0.00019969090992004335, 0.00019969090992004335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39950 loss: 1.0082 iter time (s): 4.942 samples/sec: 25.902
g0236:  iteration    39950/10000000 | consumed samples:      5113600 | consumed tokens:  10472652800 | elapsed time per iteration (ms): 4974.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.898247E-01 | loss scale: 262144.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.733 | tokens per gpu per second (tgs): 1646.910 | TFLOPs: 13.25 |
g0214: [2024-08-10 11:12:22,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=39960, skipped=57, lr=[0.00019969069312704971, 0.00019969069312704971], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39960 loss: 0.8294 iter time (s): 5.382 samples/sec: 23.784
g0236:  iteration    39960/10000000 | consumed samples:      5114880 | consumed tokens:  10475274240 | elapsed time per iteration (ms): 5415.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.024537E+00 | loss scale: 262144.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.637 | tokens per gpu per second (tgs): 1512.764 | TFLOPs: 12.17 |
g0214: [2024-08-10 11:13:24,087] [INFO] [logging.py:96:log_dist] [Rank 0] step=39970, skipped=57, lr=[0.0001996904762581734, 0.0001996904762581734], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39970 loss: 1.0856 iter time (s): 6.097 samples/sec: 20.994
g0236:  iteration    39970/10000000 | consumed samples:      5116160 | consumed tokens:  10477895680 | elapsed time per iteration (ms): 6129.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.002465E+00 | loss scale: 262144.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.882 | tokens per gpu per second (tgs): 1336.460 | TFLOPs: 10.75 |
g0214: [2024-08-10 11:14:22,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=39980, skipped=57, lr=[0.0001996902593134145, 0.0001996902593134145], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39980 loss: 0.9376 iter time (s): 5.822 samples/sec: 21.987
g0236:  iteration    39980/10000000 | consumed samples:      5117440 | consumed tokens:  10480517120 | elapsed time per iteration (ms): 5872.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.000093E+00 | loss scale: 262144.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.798 | tokens per gpu per second (tgs): 1395.075 | TFLOPs: 11.23 |
g0214: [2024-08-10 11:15:12,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=39990, skipped=57, lr=[0.0001996900422927732, 0.0001996900422927732], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 39990 loss: 1.0154 iter time (s): 4.970 samples/sec: 25.752
g0236:  iteration    39990/10000000 | consumed samples:      5118720 | consumed tokens:  10483138560 | elapsed time per iteration (ms): 5002.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.029690E+00 | loss scale: 262144.0 | grad norm: 0.409 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.588 | tokens per gpu per second (tgs): 1637.604 | TFLOPs: 13.18 |
g0214: [2024-08-10 11:16:06,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=40000, skipped=57, lr=[0.00019968982519624973, 0.00019968982519624973], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40000 loss: 0.9641 iter time (s): 5.304 samples/sec: 24.133
g0236:  iteration    40000/10000000 | consumed samples:      5120000 | consumed tokens:  10485760000 | elapsed time per iteration (ms): 5336.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.622637E-01 | loss scale: 262144.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.985 | tokens per gpu per second (tgs): 1535.069 | TFLOPs: 12.35 |
g0236: -------------------------------------------------------------------------------------------------
g0236:  validation loss at iteration 40000 | lm loss value: 9.664733E-01 | lm loss PPL: 2.628658E+00 | 
g0236: -------------------------------------------------------------------------------------------------
g0214: saving checkpoint at iteration   40000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214: [2024-08-10 11:26:16,916] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step40000 is about to be saved!
g0236: [2024-08-10 11:26:16,928] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0236: [2024-08-10 11:26:16,928] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0236: [2024-08-10 11:26:16,928] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0233: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0214: [2024-08-10 11:26:16,928] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0214: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0220: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0220: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0220: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0233: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0233: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0234: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0234: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0235: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0235: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0235: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0234: [2024-08-10 11:26:16,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0214: [2024-08-10 11:26:16,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0225: [2024-08-10 11:26:16,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0225: [2024-08-10 11:26:16,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0225: [2024-08-10 11:26:16,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0232: [2024-08-10 11:26:16,937] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0232: [2024-08-10 11:26:16,937] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0232: [2024-08-10 11:26:16,938] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0236: [2024-08-10 11:26:16,953] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_23-model_00-model_states.pt...
g0233: [2024-08-10 11:26:16,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_14-model_00-model_states.pt...
g0220: [2024-08-10 11:26:16,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_05-model_00-model_states.pt...
g0234: [2024-08-10 11:26:16,962] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_17-model_00-model_states.pt...
g0235: [2024-08-10 11:26:16,964] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_20-model_00-model_states.pt...
g0225: [2024-08-10 11:26:16,965] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_08-model_00-model_states.pt...
g0232: [2024-08-10 11:26:16,971] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_11-model_00-model_states.pt...
g0214: [2024-08-10 11:26:16,975] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_01-model_00-model_states.pt...
g0232: [2024-08-10 11:26:17,083] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_11-model_00-model_states.pt.
g0232: [2024-08-10 11:26:17,116] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_12-model_00-model_states.pt...
g0225: [2024-08-10 11:26:17,120] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_08-model_00-model_states.pt.
g0220: [2024-08-10 11:26:17,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_05-model_00-model_states.pt.
g0220: [2024-08-10 11:26:17,157] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_06-model_00-model_states.pt...
g0225: [2024-08-10 11:26:17,157] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_09-model_00-model_states.pt...
g0235: [2024-08-10 11:26:17,161] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_20-model_00-model_states.pt.
g0234: [2024-08-10 11:26:17,176] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_17-model_00-model_states.pt.
g0214: [2024-08-10 11:26:17,177] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_01-model_00-model_states.pt.
g0235: [2024-08-10 11:26:17,196] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_21-model_00-model_states.pt...
g0214: [2024-08-10 11:26:17,206] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_02-model_00-model_states.pt...
g0233: [2024-08-10 11:26:17,209] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_14-model_00-model_states.pt.
g0234: [2024-08-10 11:26:17,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_18-model_00-model_states.pt...
g0233: [2024-08-10 11:26:17,243] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_15-model_00-model_states.pt...
g0236: [2024-08-10 11:26:17,261] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 11:26:17,262] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 11:26:17,264] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_24-model_00-model_states.pt.
g0232: [2024-08-10 11:26:17,278] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_12-model_00-model_states.pt.
g0232: [2024-08-10 11:26:17,308] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_13-model_00-model_states.pt...
g0236: [2024-08-10 11:26:17,309] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_25-model_00-model_states.pt...
g0220: [2024-08-10 11:26:17,329] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_06-model_00-model_states.pt.
g0233: [2024-08-10 11:26:17,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_15-model_00-model_states.pt.
g0214: [2024-08-10 11:26:17,355] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_02-model_00-model_states.pt.
g0220: [2024-08-10 11:26:17,359] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_07-model_00-model_states.pt...
g0233: [2024-08-10 11:26:17,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_16-model_00-model_states.pt...
g0214: [2024-08-10 11:26:17,389] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_03-model_00-model_states.pt...
g0234: [2024-08-10 11:26:17,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_18-model_00-model_states.pt.
g0235: [2024-08-10 11:26:17,415] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_21-model_00-model_states.pt.
g0232: [2024-08-10 11:26:17,428] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_13-model_00-model_states.pt.
g0234: [2024-08-10 11:26:17,429] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_19-model_00-model_states.pt...
g0232: [2024-08-10 11:26:17,430] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_03_model_states.pt...
g0235: [2024-08-10 11:26:17,445] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_22-model_00-model_states.pt...
g0233: [2024-08-10 11:26:17,473] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 11:26:17,475] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_04_model_states.pt...
g0220: [2024-08-10 11:26:17,492] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 11:26:17,493] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_01_model_states.pt...
g0236: [2024-08-10 11:26:17,506] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 11:26:17,507] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_07_model_states.pt...
g0225: [2024-08-10 11:26:17,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 11:26:17,541] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_10-model_00-model_states.pt...
g0214: [2024-08-10 11:26:17,562] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_03-model_00-model_states.pt.
g0214: [2024-08-10 11:26:17,587] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_04-model_00-model_states.pt...
g0235: [2024-08-10 11:26:17,613] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 11:26:17,615] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_06_model_states.pt...
g0234: [2024-08-10 11:26:17,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 11:26:17,641] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_05_model_states.pt...
g0225: [2024-08-10 11:26:17,700] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_10-model_00-model_states.pt.
g0225: [2024-08-10 11:26:17,702] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_02_model_states.pt...
g0214: [2024-08-10 11:26:17,794] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 11:26:17,796] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_00_model_states.pt
g0214: [2024-08-10 11:26:17,797] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_00_model_states.pt...
g0220: [2024-08-10 11:26:19,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_01_model_states.pt.
g0220: [2024-08-10 11:26:19,771] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0234: [2024-08-10 11:26:19,934] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_05_model_states.pt.
g0234: [2024-08-10 11:26:19,934] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0233: [2024-08-10 11:26:20,010] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_04_model_states.pt.
g0233: [2024-08-10 11:26:20,011] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0236: [2024-08-10 11:26:20,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_07_model_states.pt.
g0236: [2024-08-10 11:26:20,028] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0225: [2024-08-10 11:26:20,244] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_02_model_states.pt.
g0225: [2024-08-10 11:26:20,245] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0235: [2024-08-10 11:26:21,026] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_06_model_states.pt.
g0235: [2024-08-10 11:26:21,026] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0232: [2024-08-10 11:26:21,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_03_model_states.pt.
g0232: [2024-08-10 11:26:21,187] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0214: [2024-08-10 11:26:21,283] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step40000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 11:26:21,283] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step40000 is ready now!
g0214:   successfully saved checkpoint at iteration   40000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214: Checkpoint Save GB: 22.521, GB/Sec: 5.12, Latency(second): 4.398
g0236: (min, max) time across ranks (ms):
g0236:     save-checkpoint ................................: (4397.00, 4398.37)
g0214: [2024-08-10 11:27:21,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=40010, skipped=57, lr=[0.00019968960802384416, 0.00019968960802384416], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40010 loss: 0.9396 iter time (s): 5.960 samples/sec: 21.477
g0236:  iteration    40010/10000000 | consumed samples:      5121280 | consumed tokens:  10488381440 | elapsed time per iteration (ms): 67506.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.801959E-01 | loss scale: 262144.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 1.896 | tokens per gpu per second (tgs): 121.352 | TFLOPs: 0.98 |
g0214: [2024-08-10 11:28:11,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=40020, skipped=57, lr=[0.00019968939077555673, 0.00019968939077555673], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40020 loss: 1.0260 iter time (s): 4.973 samples/sec: 25.741
g0236:  iteration    40020/10000000 | consumed samples:      5122560 | consumed tokens:  10491002880 | elapsed time per iteration (ms): 5007.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.060559E+00 | loss scale: 262144.0 | grad norm: 0.258 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.560 | tokens per gpu per second (tgs): 1635.859 | TFLOPs: 13.16 |
g0214: [2024-08-10 11:29:04,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=40030, skipped=57, lr=[0.00019968917345138758, 0.00019968917345138758], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40030 loss: 1.1508 iter time (s): 5.271 samples/sec: 24.283
g0236:  iteration    40030/10000000 | consumed samples:      5123840 | consumed tokens:  10493624320 | elapsed time per iteration (ms): 5303.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.791676E-01 | loss scale: 262144.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.133 | tokens per gpu per second (tgs): 1544.537 | TFLOPs: 12.43 |
g0214: [2024-08-10 11:29:54,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=40040, skipped=57, lr=[0.0001996889560513369, 0.0001996889560513369], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40040 loss: 1.2055 iter time (s): 5.013 samples/sec: 25.533
g0236:  iteration    40040/10000000 | consumed samples:      5125120 | consumed tokens:  10496245760 | elapsed time per iteration (ms): 5045.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.910967E-01 | loss scale: 262144.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.368 | tokens per gpu per second (tgs): 1623.581 | TFLOPs: 13.07 |
g0214: [2024-08-10 11:30:45,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=40050, skipped=57, lr=[0.0001996887385754048, 0.0001996887385754048], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40050 loss: 0.9465 iter time (s): 5.084 samples/sec: 25.178
g0236:  iteration    40050/10000000 | consumed samples:      5126400 | consumed tokens:  10498867200 | elapsed time per iteration (ms): 5116.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.058828E+00 | loss scale: 262144.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.018 | tokens per gpu per second (tgs): 1601.133 | TFLOPs: 12.88 |
g0214: [2024-08-10 11:31:39,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=40060, skipped=57, lr=[0.0001996885210235915, 0.0001996885210235915], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40060 loss: 0.8633 iter time (s): 5.276 samples/sec: 24.261
g0236:  iteration    40060/10000000 | consumed samples:      5127680 | consumed tokens:  10501488640 | elapsed time per iteration (ms): 5308.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.819258E-01 | loss scale: 262144.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.111 | tokens per gpu per second (tgs): 1543.123 | TFLOPs: 12.42 |
g0214: [2024-08-10 11:32:33,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=40070, skipped=57, lr=[0.0001996883033958971, 0.0001996883033958971], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40070 loss: 0.9163 iter time (s): 5.456 samples/sec: 23.460
g0236:  iteration    40070/10000000 | consumed samples:      5128960 | consumed tokens:  10504110080 | elapsed time per iteration (ms): 5488.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.841770E-01 | loss scale: 262144.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.321 | tokens per gpu per second (tgs): 1492.519 | TFLOPs: 12.01 |
g0235: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40071
g0235: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40071
g0235: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40071
g0214: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40071
g0235: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0214: Grad overflow on iteration 40071
g0214: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40071
g0214: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0214: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40071
g0214: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40071
g0236: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40071
g0225: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: Grad overflow on iteration 40071
g0233: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 40071
g0233: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40071
g0233: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 40071
g0225: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: Grad overflow on iteration 40071
g0225: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40071
g0225: Grad overflow on iteration 40071
g0232: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0232: Grad overflow on iteration 40071
g0233: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40071
g0232: Grad overflow on iteration 40071
g0220: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: Grad overflow on iteration 40071
g0232: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: Grad overflow on iteration 40071
g0220: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0214: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 40071
g0214: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40071
g0214: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40071
g0220: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40071
g0225: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: Grad overflow on iteration 40071
g0220: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0232: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40071
g0225: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0232: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 40071
g0233: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 40071
g0233: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40071
g0232: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 40071
g0236: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0232: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0214: [2024-08-10 11:32:44,592] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0234: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 40071
g0234: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-10 11:32:44,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0214: [2024-08-10 11:33:24,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=40080, skipped=58, lr=[0.0001996880856923219, 0.0001996880856923219], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40080 loss: 0.8459 iter time (s): 4.978 samples/sec: 25.712
g0236:  iteration    40080/10000000 | consumed samples:      5130240 | consumed tokens:  10506731520 | elapsed time per iteration (ms): 5011.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.506972E-01 | loss scale: 131072.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.543 | tokens per gpu per second (tgs): 1634.732 | TFLOPs: 13.15 |
g0214: [2024-08-10 11:34:17,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=40090, skipped=58, lr=[0.0001996878679128659, 0.0001996878679128659], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40090 loss: 0.9621 iter time (s): 5.329 samples/sec: 24.021
g0236:  iteration    40090/10000000 | consumed samples:      5131520 | consumed tokens:  10509352960 | elapsed time per iteration (ms): 5375.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.043846E+00 | loss scale: 131072.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.811 | tokens per gpu per second (tgs): 1523.930 | TFLOPs: 12.26 |
g0214: [2024-08-10 11:35:12,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=40100, skipped=58, lr=[0.00019968765005752944, 0.00019968765005752944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40100 loss: 1.0741 iter time (s): 5.409 samples/sec: 23.663
g0236:  iteration    40100/10000000 | consumed samples:      5132800 | consumed tokens:  10511974400 | elapsed time per iteration (ms): 5441.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.004859E+00 | loss scale: 131072.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.521 | tokens per gpu per second (tgs): 1505.362 | TFLOPs: 12.11 |
g0214: [2024-08-10 11:36:04,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=40110, skipped=58, lr=[0.00019968743212631256, 0.00019968743212631256], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40110 loss: 1.0634 iter time (s): 5.196 samples/sec: 24.633
g0236:  iteration    40110/10000000 | consumed samples:      5134080 | consumed tokens:  10514595840 | elapsed time per iteration (ms): 5228.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.626004E-01 | loss scale: 131072.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.480 | tokens per gpu per second (tgs): 1566.730 | TFLOPs: 12.61 |
g0214: [2024-08-10 11:36:57,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=40120, skipped=58, lr=[0.00019968721411921546, 0.00019968721411921546], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40120 loss: 0.8666 iter time (s): 5.217 samples/sec: 24.536
g0236:  iteration    40120/10000000 | consumed samples:      5135360 | consumed tokens:  10517217280 | elapsed time per iteration (ms): 5249.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.758440E-01 | loss scale: 131072.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.382 | tokens per gpu per second (tgs): 1560.417 | TFLOPs: 12.56 |
g0214: [2024-08-10 11:37:57,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=40130, skipped=58, lr=[0.00019968699603623832, 0.00019968699603623832], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40130 loss: 0.9800 iter time (s): 6.054 samples/sec: 21.142
g0236:  iteration    40130/10000000 | consumed samples:      5136640 | consumed tokens:  10519838720 | elapsed time per iteration (ms): 6087.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.539345E-01 | loss scale: 131072.0 | grad norm: 0.433 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.027 | tokens per gpu per second (tgs): 1345.756 | TFLOPs: 10.83 |
g0214: [2024-08-10 11:39:04,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=40140, skipped=58, lr=[0.0001996867778773813, 0.0001996867778773813], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40140 loss: 0.9450 iter time (s): 6.675 samples/sec: 19.177
g0236:  iteration    40140/10000000 | consumed samples:      5137920 | consumed tokens:  10522460160 | elapsed time per iteration (ms): 6707.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.845150E-01 | loss scale: 131072.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.084 | tokens per gpu per second (tgs): 1221.369 | TFLOPs: 9.83 |
g0214: [2024-08-10 11:40:05,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=40150, skipped=58, lr=[0.00019968655964264456, 0.00019968655964264456], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40150 loss: 0.9594 iter time (s): 6.044 samples/sec: 21.179
g0236:  iteration    40150/10000000 | consumed samples:      5139200 | consumed tokens:  10525081600 | elapsed time per iteration (ms): 6076.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.005211E+00 | loss scale: 131072.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.064 | tokens per gpu per second (tgs): 1348.108 | TFLOPs: 10.85 |
g0214: [2024-08-10 11:40:57,104] [INFO] [logging.py:96:log_dist] [Rank 0] step=40160, skipped=58, lr=[0.0001996863413320283, 0.0001996863413320283], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40160 loss: 0.9813 iter time (s): 5.102 samples/sec: 25.090
g0236:  iteration    40160/10000000 | consumed samples:      5140480 | consumed tokens:  10527703040 | elapsed time per iteration (ms): 5134.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.006814E+00 | loss scale: 131072.0 | grad norm: 0.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.928 | tokens per gpu per second (tgs): 1595.360 | TFLOPs: 12.84 |
g0214: [2024-08-10 11:41:48,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=40170, skipped=58, lr=[0.00019968612294553266, 0.00019968612294553266], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40170 loss: 1.0934 iter time (s): 5.139 samples/sec: 24.906
g0236:  iteration    40170/10000000 | consumed samples:      5141760 | consumed tokens:  10530324480 | elapsed time per iteration (ms): 5172.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.012404E+00 | loss scale: 131072.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.748 | tokens per gpu per second (tgs): 1583.886 | TFLOPs: 12.75 |
g0214: [2024-08-10 11:42:44,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=40180, skipped=58, lr=[0.00019968590448315781, 0.00019968590448315781], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40180 loss: 1.0768 iter time (s): 5.570 samples/sec: 22.978
g0236:  iteration    40180/10000000 | consumed samples:      5143040 | consumed tokens:  10532945920 | elapsed time per iteration (ms): 5602.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.039853E+00 | loss scale: 131072.0 | grad norm: 0.246 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.845 | tokens per gpu per second (tgs): 1462.109 | TFLOPs: 11.77 |
g0214: [2024-08-10 11:43:35,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=40190, skipped=58, lr=[0.00019968568594490392, 0.00019968568594490392], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40190 loss: 0.9488 iter time (s): 5.005 samples/sec: 25.574
g0236:  iteration    40190/10000000 | consumed samples:      5144320 | consumed tokens:  10535567360 | elapsed time per iteration (ms): 5038.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.572294E-01 | loss scale: 131072.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.407 | tokens per gpu per second (tgs): 1626.023 | TFLOPs: 13.08 |
g0214: [2024-08-10 11:44:24,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=40200, skipped=58, lr=[0.00019968546733077118, 0.00019968546733077118], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40200 loss: 0.9405 iter time (s): 4.869 samples/sec: 26.288
g0236:  iteration    40200/10000000 | consumed samples:      5145600 | consumed tokens:  10538188800 | elapsed time per iteration (ms): 4903.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.003645E+00 | loss scale: 131072.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.101 | tokens per gpu per second (tgs): 1670.491 | TFLOPs: 13.44 |
g0214: [2024-08-10 11:45:23,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=40210, skipped=58, lr=[0.00019968524864075968, 0.00019968524864075968], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40210 loss: 0.9005 iter time (s): 5.936 samples/sec: 21.564
g0236:  iteration    40210/10000000 | consumed samples:      5146880 | consumed tokens:  10540810240 | elapsed time per iteration (ms): 5970.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.004926E+00 | loss scale: 131072.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.441 | tokens per gpu per second (tgs): 1372.200 | TFLOPs: 11.04 |
g0214: [2024-08-10 11:46:12,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=40220, skipped=58, lr=[0.0001996850298748697, 0.0001996850298748697], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40220 loss: 1.0753 iter time (s): 4.820 samples/sec: 26.558
g0236:  iteration    40220/10000000 | consumed samples:      5148160 | consumed tokens:  10543431680 | elapsed time per iteration (ms): 4853.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.010082E+00 | loss scale: 131072.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.374 | tokens per gpu per second (tgs): 1687.946 | TFLOPs: 13.58 |
g0214: [2024-08-10 11:47:01,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=40230, skipped=58, lr=[0.00019968481103310135, 0.00019968481103310135], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40230 loss: 1.0037 iter time (s): 4.842 samples/sec: 26.435
g0236:  iteration    40230/10000000 | consumed samples:      5149440 | consumed tokens:  10546053120 | elapsed time per iteration (ms): 4875.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.826523E-01 | loss scale: 131072.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.255 | tokens per gpu per second (tgs): 1680.343 | TFLOPs: 13.52 |
g0214: [2024-08-10 11:47:50,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=40240, skipped=58, lr=[0.00019968459211545478, 0.00019968459211545478], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40240 loss: 1.1543 iter time (s): 4.843 samples/sec: 26.429
g0236:  iteration    40240/10000000 | consumed samples:      5150720 | consumed tokens:  10548674560 | elapsed time per iteration (ms): 4876.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.834265E-01 | loss scale: 131072.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.249 | tokens per gpu per second (tgs): 1679.960 | TFLOPs: 13.52 |
g0214: [2024-08-10 11:48:44,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=40250, skipped=58, lr=[0.0001996843731219302, 0.0001996843731219302], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40250 loss: 1.0674 iter time (s): 5.401 samples/sec: 23.701
g0236:  iteration    40250/10000000 | consumed samples:      5152000 | consumed tokens:  10551296000 | elapsed time per iteration (ms): 5434.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.642030E-01 | loss scale: 131072.0 | grad norm: 0.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.552 | tokens per gpu per second (tgs): 1507.323 | TFLOPs: 12.13 |
g0214: [2024-08-10 11:49:34,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=40260, skipped=58, lr=[0.00019968415405252778, 0.00019968415405252778], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40260 loss: 0.8457 iter time (s): 4.949 samples/sec: 25.865
g0236:  iteration    40260/10000000 | consumed samples:      5153280 | consumed tokens:  10553917440 | elapsed time per iteration (ms): 4983.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.697817E-01 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.684 | tokens per gpu per second (tgs): 1643.771 | TFLOPs: 13.23 |
g0214: [2024-08-10 11:50:20,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=40270, skipped=58, lr=[0.00019968393490724763, 0.00019968393490724763], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40270 loss: 0.9429 iter time (s): 4.608 samples/sec: 27.775
g0236:  iteration    40270/10000000 | consumed samples:      5154560 | consumed tokens:  10556538880 | elapsed time per iteration (ms): 4641.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.897146E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.578 | tokens per gpu per second (tgs): 1764.973 | TFLOPs: 14.20 |
g0214: [2024-08-10 11:51:05,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=40280, skipped=58, lr=[0.00019968371568608996, 0.00019968371568608996], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40280 loss: 0.9439 iter time (s): 4.478 samples/sec: 28.586
g0236:  iteration    40280/10000000 | consumed samples:      5155840 | consumed tokens:  10559160320 | elapsed time per iteration (ms): 4517.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.009978E+00 | loss scale: 131072.0 | grad norm: 0.160 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.337 | tokens per gpu per second (tgs): 1813.554 | TFLOPs: 14.59 |
g0214: [2024-08-10 11:51:56,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=40290, skipped=58, lr=[0.00019968349638905495, 0.00019968349638905495], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40290 loss: 0.9555 iter time (s): 5.032 samples/sec: 25.435
g0236:  iteration    40290/10000000 | consumed samples:      5157120 | consumed tokens:  10561781760 | elapsed time per iteration (ms): 5066.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.669834E-01 | loss scale: 131072.0 | grad norm: 0.266 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.266 | tokens per gpu per second (tgs): 1617.046 | TFLOPs: 13.01 |
g0214: [2024-08-10 11:52:46,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=40300, skipped=58, lr=[0.00019968327701614275, 0.00019968327701614275], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40300 loss: 0.8617 iter time (s): 4.923 samples/sec: 25.998
g0236:  iteration    40300/10000000 | consumed samples:      5158400 | consumed tokens:  10564403200 | elapsed time per iteration (ms): 4969.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.012025E+00 | loss scale: 131072.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.758 | tokens per gpu per second (tgs): 1648.500 | TFLOPs: 13.27 |
g0214: [2024-08-10 11:53:38,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=40310, skipped=58, lr=[0.00019968305756735353, 0.00019968305756735353], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40310 loss: 0.9037 iter time (s): 5.195 samples/sec: 24.641
g0236:  iteration    40310/10000000 | consumed samples:      5159680 | consumed tokens:  10567024640 | elapsed time per iteration (ms): 5241.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.962684E-01 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.419 | tokens per gpu per second (tgs): 1562.847 | TFLOPs: 12.58 |
g0214: [2024-08-10 11:54:27,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=40320, skipped=58, lr=[0.00019968283804268745, 0.00019968283804268745], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40320 loss: 1.1902 iter time (s): 4.836 samples/sec: 26.467
g0236:  iteration    40320/10000000 | consumed samples:      5160960 | consumed tokens:  10569646080 | elapsed time per iteration (ms): 4877.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.937840E-01 | loss scale: 131072.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.245 | tokens per gpu per second (tgs): 1679.681 | TFLOPs: 13.52 |
g0214: [2024-08-10 11:55:19,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=40330, skipped=58, lr=[0.00019968261844214468, 0.00019968261844214468], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40330 loss: 0.9353 iter time (s): 5.183 samples/sec: 24.694
g0236:  iteration    40330/10000000 | consumed samples:      5162240 | consumed tokens:  10572267520 | elapsed time per iteration (ms): 5216.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.018371E+00 | loss scale: 131072.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.538 | tokens per gpu per second (tgs): 1570.438 | TFLOPs: 12.64 |
g0214: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40330
g0214: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40330
g0236: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40330
g0236: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40330
g0236: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40330
g0225: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 40330
g0236: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 40330
g0225: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40330
g0236: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: Grad overflow on iteration 40330
g0225: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 40330
g0225: Grad overflow on iteration 40330
g0232: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40330
g0236: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0232: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: Grad overflow on iteration 40330
g0225: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0232: Grad overflow on iteration 40330
g0234: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40330
g0234: Grad overflow on iteration 40330
g0233: Grad overflow on iteration 40330
g0214: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40330
g0234: Grad overflow on iteration 40330
g0233: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0214: Grad overflow on iteration 40330
g0235: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0214: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: Grad overflow on iteration 40330
g0235: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: Grad overflow on iteration 40330
g0233: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 40330
g0232: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0232: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0232: Grad overflow on iteration 40330
g0234: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: Grad overflow on iteration 40330
g0234: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0232: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0214: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: Grad overflow on iteration 40330
g0233: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 40330
g0214: Grad overflow on iteration 40330
g0233: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0214: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0214: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: Grad overflow on iteration 40330
g0233: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 11:55:24,135] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
g0220: Grad overflow on iteration 40330
g0220: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40330
g0220: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40330
g0220: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 11:55:24,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 11:55:24,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0214: [2024-08-10 11:56:15,154] [INFO] [logging.py:96:log_dist] [Rank 0] step=40340, skipped=59, lr=[0.00019968239876572545, 0.00019968239876572545], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40340 loss: 1.1720 iter time (s): 5.522 samples/sec: 23.178
g0236:  iteration    40340/10000000 | consumed samples:      5163520 | consumed tokens:  10574888960 | elapsed time per iteration (ms): 5565.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.003398E+00 | loss scale: 65536.0 | grad norm: 0.428 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.000 | tokens per gpu per second (tgs): 1472.031 | TFLOPs: 11.85 |
g0214: [2024-08-10 11:57:06,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=40350, skipped=59, lr=[0.00019968217901342983, 0.00019968217901342983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40350 loss: 0.9436 iter time (s): 5.072 samples/sec: 25.235
g0236:  iteration    40350/10000000 | consumed samples:      5164800 | consumed tokens:  10577510400 | elapsed time per iteration (ms): 5104.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.766334E-01 | loss scale: 65536.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.074 | tokens per gpu per second (tgs): 1604.748 | TFLOPs: 12.91 |
g0214: [2024-08-10 11:57:56,626] [INFO] [logging.py:96:log_dist] [Rank 0] step=40360, skipped=59, lr=[0.00019968195918525805, 0.00019968195918525805], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40360 loss: 0.9543 iter time (s): 5.010 samples/sec: 25.550
g0236:  iteration    40360/10000000 | consumed samples:      5166080 | consumed tokens:  10580131840 | elapsed time per iteration (ms): 5042.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.858359E-01 | loss scale: 65536.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.385 | tokens per gpu per second (tgs): 1624.625 | TFLOPs: 13.07 |
g0214: [2024-08-10 11:58:45,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=40370, skipped=59, lr=[0.00019968173928121028, 0.00019968173928121028], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40370 loss: 0.9061 iter time (s): 4.832 samples/sec: 26.492
g0236:  iteration    40370/10000000 | consumed samples:      5167360 | consumed tokens:  10582753280 | elapsed time per iteration (ms): 4865.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.824096E-01 | loss scale: 65536.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.306 | tokens per gpu per second (tgs): 1683.590 | TFLOPs: 13.55 |
g0214: [2024-08-10 11:59:38,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=40380, skipped=59, lr=[0.00019968151930128665, 0.00019968151930128665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40380 loss: 0.9574 iter time (s): 5.243 samples/sec: 24.415
g0236:  iteration    40380/10000000 | consumed samples:      5168640 | consumed tokens:  10585374720 | elapsed time per iteration (ms): 5289.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.982008E-01 | loss scale: 65536.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.199 | tokens per gpu per second (tgs): 1548.749 | TFLOPs: 12.46 |
g0214: [2024-08-10 12:00:37,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=40390, skipped=59, lr=[0.00019968129924548736, 0.00019968129924548736], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40390 loss: 1.1010 iter time (s): 5.911 samples/sec: 21.653
g0236:  iteration    40390/10000000 | consumed samples:      5169920 | consumed tokens:  10587996160 | elapsed time per iteration (ms): 5944.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.048733E+00 | loss scale: 65536.0 | grad norm: 0.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.531 | tokens per gpu per second (tgs): 1377.992 | TFLOPs: 11.09 |
g0214: [2024-08-10 12:01:28,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=40400, skipped=59, lr=[0.00019968107911381262, 0.00019968107911381262], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40400 loss: 0.9159 iter time (s): 5.093 samples/sec: 25.134
g0236:  iteration    40400/10000000 | consumed samples:      5171200 | consumed tokens:  10590617600 | elapsed time per iteration (ms): 5125.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.934891E-01 | loss scale: 65536.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.973 | tokens per gpu per second (tgs): 1598.290 | TFLOPs: 12.86 |
g0214: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40407
g0236: Grad overflow on iteration 40407
g0236: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40407
g0236: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0214: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40407
g0225: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: Grad overflow on iteration 40407
g0225: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 40407
g0220: Grad overflow on iteration 40407
g0214: Grad overflow on iteration 40407
g0236: Grad overflow on iteration 40407
g0232: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40407
g0232: Grad overflow on iteration 40407
g0225: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0232: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: Grad overflow on iteration 40407
g0234: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0232: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40407
g0236: Grad overflow on iteration 40407
g0232: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0232: Grad overflow on iteration 40407
g0232: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0232: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40407
g0220: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 40407
g0232: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40407
g0232: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: Grad overflow on iteration 40407
g0220: Grad overflow on iteration 40407
g0236: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: Grad overflow on iteration 40407
g0236: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: Grad overflow on iteration 40407
g0235: [2024-08-10 12:02:14,463] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: Grad overflow on iteration 40407
g0235: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: Grad overflow on iteration 40407
g0233: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 40407
g0220: Grad overflow on iteration 40407
g0225: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 40407
g0220: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: Grad overflow on iteration 40407
g0233: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 40407
g0234: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 40407
g0233: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40407
g0225: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0214: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0214: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 40407
g0233: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0214: [2024-08-10 12:02:14,464] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0225: [2024-08-10 12:02:14,464] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0214: [2024-08-10 12:02:25,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=40410, skipped=60, lr=[0.0001996808589062625, 0.0001996808589062625], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40410 loss: 0.9751 iter time (s): 5.631 samples/sec: 22.731
g0236:  iteration    40410/10000000 | consumed samples:      5172480 | consumed tokens:  10593239040 | elapsed time per iteration (ms): 5667.8 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 32768.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.584 | tokens per gpu per second (tgs): 1445.349 | TFLOPs: 11.63 |
g0214: [2024-08-10 12:03:17,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=40420, skipped=60, lr=[0.00019968063862283724, 0.00019968063862283724], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40420 loss: 0.9072 iter time (s): 5.121 samples/sec: 24.996
g0236:  iteration    40420/10000000 | consumed samples:      5173760 | consumed tokens:  10595860480 | elapsed time per iteration (ms): 5153.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.819884E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.838 | tokens per gpu per second (tgs): 1589.623 | TFLOPs: 12.79 |
g0214: [2024-08-10 12:04:06,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=40430, skipped=60, lr=[0.00019968041826353696, 0.00019968041826353696], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40430 loss: 1.0884 iter time (s): 4.876 samples/sec: 26.250
g0236:  iteration    40430/10000000 | consumed samples:      5175040 | consumed tokens:  10598481920 | elapsed time per iteration (ms): 4909.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.008161E+00 | loss scale: 32768.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.071 | tokens per gpu per second (tgs): 1668.534 | TFLOPs: 13.43 |
g0214: [2024-08-10 12:04:58,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=40440, skipped=60, lr=[0.0001996801978283619, 0.0001996801978283619], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40440 loss: 0.9503 iter time (s): 5.154 samples/sec: 24.833
g0236:  iteration    40440/10000000 | consumed samples:      5176320 | consumed tokens:  10601103360 | elapsed time per iteration (ms): 5187.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.922740E-01 | loss scale: 32768.0 | grad norm: 0.279 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.675 | tokens per gpu per second (tgs): 1579.206 | TFLOPs: 12.71 |
g0214: [2024-08-10 12:05:54,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=40450, skipped=60, lr=[0.00019967997731731214, 0.00019967997731731214], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40450 loss: 0.9622 iter time (s): 5.587 samples/sec: 22.911
g0236:  iteration    40450/10000000 | consumed samples:      5177600 | consumed tokens:  10603724800 | elapsed time per iteration (ms): 5619.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.872890E-01 | loss scale: 32768.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.778 | tokens per gpu per second (tgs): 1457.767 | TFLOPs: 11.73 |
g0214: [2024-08-10 12:06:51,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=40460, skipped=60, lr=[0.00019967975673038796, 0.00019967975673038796], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40460 loss: 1.0623 iter time (s): 5.678 samples/sec: 22.545
g0236:  iteration    40460/10000000 | consumed samples:      5178880 | consumed tokens:  10606346240 | elapsed time per iteration (ms): 5709.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.020619E+00 | loss scale: 32768.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.418 | tokens per gpu per second (tgs): 1434.722 | TFLOPs: 11.55 |
g0214: [2024-08-10 12:07:52,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=40470, skipped=60, lr=[0.00019967953606758943, 0.00019967953606758943], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40470 loss: 1.0109 iter time (s): 6.034 samples/sec: 21.214
g0236:  iteration    40470/10000000 | consumed samples:      5180160 | consumed tokens:  10608967680 | elapsed time per iteration (ms): 6066.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.013469E+00 | loss scale: 32768.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.099 | tokens per gpu per second (tgs): 1350.322 | TFLOPs: 10.87 |
g0214: [2024-08-10 12:08:47,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=40480, skipped=60, lr=[0.00019967931532891676, 0.00019967931532891676], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40480 loss: 0.8421 iter time (s): 5.467 samples/sec: 23.412
g0236:  iteration    40480/10000000 | consumed samples:      5181440 | consumed tokens:  10611589120 | elapsed time per iteration (ms): 5499.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.009305E+00 | loss scale: 32768.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.275 | tokens per gpu per second (tgs): 1489.584 | TFLOPs: 11.99 |
g0214: [2024-08-10 12:09:36,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=40490, skipped=60, lr=[0.00019967909451437015, 0.00019967909451437015], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40490 loss: 0.9970 iter time (s): 4.950 samples/sec: 25.859
g0236:  iteration    40490/10000000 | consumed samples:      5182720 | consumed tokens:  10614210560 | elapsed time per iteration (ms): 4982.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.016470E+00 | loss scale: 32768.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.690 | tokens per gpu per second (tgs): 1644.139 | TFLOPs: 13.23 |
g0214: [2024-08-10 12:10:25,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=40500, skipped=60, lr=[0.00019967887362394972, 0.00019967887362394972], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40500 loss: 0.8563 iter time (s): 4.795 samples/sec: 26.694
g0236:  iteration    40500/10000000 | consumed samples:      5184000 | consumed tokens:  10616832000 | elapsed time per iteration (ms): 4829.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.599021E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.506 | tokens per gpu per second (tgs): 1696.389 | TFLOPs: 13.65 |
g0214: [2024-08-10 12:11:21,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=40510, skipped=60, lr=[0.00019967865265765566, 0.00019967865265765566], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40510 loss: 0.8194 iter time (s): 5.572 samples/sec: 22.972
g0236:  iteration    40510/10000000 | consumed samples:      5185280 | consumed tokens:  10619453440 | elapsed time per iteration (ms): 5604.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.413941E-01 | loss scale: 32768.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.839 | tokens per gpu per second (tgs): 1461.673 | TFLOPs: 11.76 |
g0214: [2024-08-10 12:12:16,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=40520, skipped=60, lr=[0.00019967843161548812, 0.00019967843161548812], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40520 loss: 0.9404 iter time (s): 5.451 samples/sec: 23.483
g0236:  iteration    40520/10000000 | consumed samples:      5186560 | consumed tokens:  10622074880 | elapsed time per iteration (ms): 5483.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.877327E-01 | loss scale: 32768.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.343 | tokens per gpu per second (tgs): 1493.975 | TFLOPs: 12.02 |
g0214: [2024-08-10 12:13:12,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=40530, skipped=60, lr=[0.00019967821049744728, 0.00019967821049744728], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40530 loss: 0.9454 iter time (s): 5.599 samples/sec: 22.862
g0236:  iteration    40530/10000000 | consumed samples:      5187840 | consumed tokens:  10624696320 | elapsed time per iteration (ms): 5631.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.535836E-01 | loss scale: 32768.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.730 | tokens per gpu per second (tgs): 1454.699 | TFLOPs: 11.71 |
g0214: [2024-08-10 12:14:10,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=40540, skipped=60, lr=[0.00019967798930353335, 0.00019967798930353335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40540 loss: 1.1354 iter time (s): 5.771 samples/sec: 22.179
g0236:  iteration    40540/10000000 | consumed samples:      5189120 | consumed tokens:  10627317760 | elapsed time per iteration (ms): 5804.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.027673E+00 | loss scale: 32768.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.052 | tokens per gpu per second (tgs): 1411.357 | TFLOPs: 11.36 |
g0214: [2024-08-10 12:15:04,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=40550, skipped=60, lr=[0.00019967776803374646, 0.00019967776803374646], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40550 loss: 1.2082 iter time (s): 5.358 samples/sec: 23.890
g0236:  iteration    40550/10000000 | consumed samples:      5190400 | consumed tokens:  10629939200 | elapsed time per iteration (ms): 5391.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.002922E+00 | loss scale: 32768.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.743 | tokens per gpu per second (tgs): 1519.556 | TFLOPs: 12.23 |
g0214: [2024-08-10 12:16:01,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=40560, skipped=60, lr=[0.00019967754668808677, 0.00019967754668808677], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40560 loss: 1.1846 iter time (s): 5.701 samples/sec: 22.452
g0236:  iteration    40560/10000000 | consumed samples:      5191680 | consumed tokens:  10632560640 | elapsed time per iteration (ms): 5734.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.049427E+00 | loss scale: 32768.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.323 | tokens per gpu per second (tgs): 1428.641 | TFLOPs: 11.50 |
g0214: [2024-08-10 12:17:01,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=40570, skipped=60, lr=[0.00019967732526655451, 0.00019967732526655451], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40570 loss: 0.8962 iter time (s): 5.969 samples/sec: 21.443
g0236:  iteration    40570/10000000 | consumed samples:      5192960 | consumed tokens:  10635182080 | elapsed time per iteration (ms): 6002.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.004872E+00 | loss scale: 32768.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.325 | tokens per gpu per second (tgs): 1364.802 | TFLOPs: 10.98 |
g0214: [2024-08-10 12:17:51,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=40580, skipped=60, lr=[0.0001996771037691498, 0.0001996771037691498], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40580 loss: 0.9918 iter time (s): 4.990 samples/sec: 25.652
g0236:  iteration    40580/10000000 | consumed samples:      5194240 | consumed tokens:  10637803520 | elapsed time per iteration (ms): 5023.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.701197E-01 | loss scale: 32768.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.480 | tokens per gpu per second (tgs): 1630.743 | TFLOPs: 13.12 |
g0214: [2024-08-10 12:18:40,108] [INFO] [logging.py:96:log_dist] [Rank 0] step=40590, skipped=60, lr=[0.00019967688219587278, 0.00019967688219587278], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40590 loss: 0.7748 iter time (s): 4.788 samples/sec: 26.731
g0236:  iteration    40590/10000000 | consumed samples:      5195520 | consumed tokens:  10640424960 | elapsed time per iteration (ms): 4821.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.543899E-01 | loss scale: 32768.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.546 | tokens per gpu per second (tgs): 1698.918 | TFLOPs: 13.67 |
g0214: [2024-08-10 12:19:29,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=40600, skipped=60, lr=[0.00019967666054672366, 0.00019967666054672366], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40600 loss: 1.0266 iter time (s): 4.911 samples/sec: 26.064
g0236:  iteration    40600/10000000 | consumed samples:      5196800 | consumed tokens:  10643046400 | elapsed time per iteration (ms): 4943.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.594902E-01 | loss scale: 32768.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.893 | tokens per gpu per second (tgs): 1657.163 | TFLOPs: 13.34 |
g0214: [2024-08-10 12:20:24,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=40610, skipped=60, lr=[0.00019967643882170264, 0.00019967643882170264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40610 loss: 0.9480 iter time (s): 5.448 samples/sec: 23.496
g0236:  iteration    40610/10000000 | consumed samples:      5198080 | consumed tokens:  10645667840 | elapsed time per iteration (ms): 5481.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.841143E-01 | loss scale: 32768.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.351 | tokens per gpu per second (tgs): 1494.439 | TFLOPs: 12.03 |
g0214: [2024-08-10 12:21:15,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=40620, skipped=60, lr=[0.00019967621702080983, 0.00019967621702080983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40620 loss: 1.1503 iter time (s): 5.084 samples/sec: 25.176
g0236:  iteration    40620/10000000 | consumed samples:      5199360 | consumed tokens:  10648289280 | elapsed time per iteration (ms): 5116.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.000865E+00 | loss scale: 32768.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.016 | tokens per gpu per second (tgs): 1601.023 | TFLOPs: 12.88 |
g0214: [2024-08-10 12:22:04,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=40630, skipped=60, lr=[0.00019967599514404547, 0.00019967599514404547], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40630 loss: 1.0326 iter time (s): 4.842 samples/sec: 26.434
g0236:  iteration    40630/10000000 | consumed samples:      5200640 | consumed tokens:  10650910720 | elapsed time per iteration (ms): 4874.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.046470E+00 | loss scale: 32768.0 | grad norm: 0.292 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.260 | tokens per gpu per second (tgs): 1680.619 | TFLOPs: 13.52 |
g0214: [2024-08-10 12:22:59,076] [INFO] [logging.py:96:log_dist] [Rank 0] step=40640, skipped=60, lr=[0.00019967577319140968, 0.00019967577319140968], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40640 loss: 1.0478 iter time (s): 5.448 samples/sec: 23.495
g0236:  iteration    40640/10000000 | consumed samples:      5201920 | consumed tokens:  10653532160 | elapsed time per iteration (ms): 5480.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.643477E-01 | loss scale: 32768.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.356 | tokens per gpu per second (tgs): 1494.770 | TFLOPs: 12.03 |
g0214: [2024-08-10 12:23:58,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=40650, skipped=60, lr=[0.00019967555116290263, 0.00019967555116290263], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40650 loss: 0.9952 iter time (s): 5.932 samples/sec: 21.579
g0236:  iteration    40650/10000000 | consumed samples:      5203200 | consumed tokens:  10656153600 | elapsed time per iteration (ms): 5966.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.793591E-01 | loss scale: 32768.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.453 | tokens per gpu per second (tgs): 1372.993 | TFLOPs: 11.05 |
g0214: [2024-08-10 12:24:57,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=40660, skipped=60, lr=[0.0001996753290585245, 0.0001996753290585245], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40660 loss: 1.1534 iter time (s): 5.884 samples/sec: 21.755
g0236:  iteration    40660/10000000 | consumed samples:      5204480 | consumed tokens:  10658775040 | elapsed time per iteration (ms): 5916.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.007936E+00 | loss scale: 32768.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.633 | tokens per gpu per second (tgs): 1384.516 | TFLOPs: 11.14 |
g0214: [2024-08-10 12:25:52,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=40670, skipped=60, lr=[0.00019967510687827548, 0.00019967510687827548], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40670 loss: 0.9463 iter time (s): 5.461 samples/sec: 23.440
g0236:  iteration    40670/10000000 | consumed samples:      5205760 | consumed tokens:  10661396480 | elapsed time per iteration (ms): 5494.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.043862E+00 | loss scale: 32768.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.298 | tokens per gpu per second (tgs): 1491.073 | TFLOPs: 12.00 |
g0220: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40678
g0235: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40678
g0235: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0220: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40678
g0220: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0235: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0235: Grad overflow on iteration 40678
g0235: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40678
g0235: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0235: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0225: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 40678
g0225: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 40678
g0235: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40678
g0214: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40678
g0220: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40678
g0232: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40678
g0225: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0236: Grad overflow on iteration 40678
g0234: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0236: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0214: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0233: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0234: Grad overflow on iteration 40678
g0225: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0214: Grad overflow on iteration 40678
g0234: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0233: Grad overflow on iteration 40678
g0225: Grad overflow on iteration 40678
g0233: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40678
g0225: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0220: Grad overflow on iteration 40678
g0234: Grad overflow on iteration 40678
g0234: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0214: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0234: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 40678
g0236: Grad overflow on iteration 40678
g0234: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0236: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0234: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 40678
g0236: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0236: Grad overflow on iteration 40678
g0236: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0234: Grad overflow on iteration 40678
g0236: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40678
g0233: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0233: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 40678
g0232: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0233: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0232: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 40678
g0232: Grad overflow on iteration 40678
g0233: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 40678
g0232: [2024-08-10 12:26:43,734] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40678
g0232: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0232: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0232: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40678
g0234: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0235: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0220: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0233: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0214: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40678
g0232: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0236: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0225: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0214: [2024-08-10 12:26:43,735] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0214: [2024-08-10 12:26:43,736] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
g0214: [2024-08-10 12:26:50,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=40680, skipped=61, lr=[0.0001996748846221557, 0.0001996748846221557], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40680 loss: 1.1093 iter time (s): 5.706 samples/sec: 22.431
g0236:  iteration    40680/10000000 | consumed samples:      5207040 | consumed tokens:  10664017920 | elapsed time per iteration (ms): 5739.1 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 16384.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.303 | tokens per gpu per second (tgs): 1427.396 | TFLOPs: 11.49 |
g0214: [2024-08-10 12:27:47,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=40690, skipped=61, lr=[0.00019967466229016536, 0.00019967466229016536], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40690 loss: 1.0348 iter time (s): 5.706 samples/sec: 22.432
g0236:  iteration    40690/10000000 | consumed samples:      5208320 | consumed tokens:  10666639360 | elapsed time per iteration (ms): 5739.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.974948E-01 | loss scale: 16384.0 | grad norm: 0.348 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.304 | tokens per gpu per second (tgs): 1427.435 | TFLOPs: 11.49 |
g0214: [2024-08-10 12:28:39,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=40700, skipped=61, lr=[0.00019967443988230463, 0.00019967443988230463], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40700 loss: 1.1598 iter time (s): 5.198 samples/sec: 24.627
g0236:  iteration    40700/10000000 | consumed samples:      5209600 | consumed tokens:  10669260800 | elapsed time per iteration (ms): 5233.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.013888E+00 | loss scale: 16384.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.459 | tokens per gpu per second (tgs): 1565.360 | TFLOPs: 12.60 |
g0214: [2024-08-10 12:29:39,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=40710, skipped=61, lr=[0.0001996742173985737, 0.0001996742173985737], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40710 loss: 0.9385 iter time (s): 5.886 samples/sec: 21.746
g0236:  iteration    40710/10000000 | consumed samples:      5210880 | consumed tokens:  10671882240 | elapsed time per iteration (ms): 5936.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.058842E+00 | loss scale: 16384.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.562 | tokens per gpu per second (tgs): 1379.974 | TFLOPs: 11.10 |
g0214: [2024-08-10 12:30:38,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=40720, skipped=61, lr=[0.00019967399483897266, 0.00019967399483897266], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40720 loss: 1.0682 iter time (s): 5.886 samples/sec: 21.745
g0236:  iteration    40720/10000000 | consumed samples:      5212160 | consumed tokens:  10674503680 | elapsed time per iteration (ms): 5923.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.753265E-01 | loss scale: 16384.0 | grad norm: 0.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.610 | tokens per gpu per second (tgs): 1383.044 | TFLOPs: 11.13 |
g0214: [2024-08-10 12:31:33,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=40730, skipped=61, lr=[0.00019967377220350178, 0.00019967377220350178], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40730 loss: 0.7947 iter time (s): 5.502 samples/sec: 23.266
g0236:  iteration    40730/10000000 | consumed samples:      5213440 | consumed tokens:  10677125120 | elapsed time per iteration (ms): 5534.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.456551E-01 | loss scale: 16384.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.126 | tokens per gpu per second (tgs): 1480.089 | TFLOPs: 11.91 |
g0214: [2024-08-10 12:32:27,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=40740, skipped=61, lr=[0.0001996735494921612, 0.0001996735494921612], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40740 loss: 0.9857 iter time (s): 5.321 samples/sec: 24.055
g0236:  iteration    40740/10000000 | consumed samples:      5214720 | consumed tokens:  10679746560 | elapsed time per iteration (ms): 5356.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.806722E-01 | loss scale: 16384.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.896 | tokens per gpu per second (tgs): 1529.366 | TFLOPs: 12.31 |
g0214: [2024-08-10 12:33:20,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=40750, skipped=61, lr=[0.00019967332670495108, 0.00019967332670495108], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40750 loss: 1.0251 iter time (s): 5.297 samples/sec: 24.166
g0236:  iteration    40750/10000000 | consumed samples:      5216000 | consumed tokens:  10682368000 | elapsed time per iteration (ms): 5330.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.930873E-01 | loss scale: 16384.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.013 | tokens per gpu per second (tgs): 1536.822 | TFLOPs: 12.37 |
g0214: [2024-08-10 12:34:11,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=40760, skipped=61, lr=[0.00019967310384187158, 0.00019967310384187158], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40760 loss: 1.0258 iter time (s): 5.042 samples/sec: 25.388
g0236:  iteration    40760/10000000 | consumed samples:      5217280 | consumed tokens:  10684989440 | elapsed time per iteration (ms): 5083.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.017732E+00 | loss scale: 16384.0 | grad norm: 0.319 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.179 | tokens per gpu per second (tgs): 1611.463 | TFLOPs: 12.97 |
g0214: [2024-08-10 12:35:04,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=40770, skipped=61, lr=[0.0001996728809029229, 0.0001996728809029229], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40770 loss: 0.9046 iter time (s): 5.268 samples/sec: 24.299
g0236:  iteration    40770/10000000 | consumed samples:      5218560 | consumed tokens:  10687610880 | elapsed time per iteration (ms): 5312.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.485123E-01 | loss scale: 16384.0 | grad norm: 0.358 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.092 | tokens per gpu per second (tgs): 1541.913 | TFLOPs: 12.41 |
g0214: [2024-08-10 12:35:57,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=40780, skipped=61, lr=[0.00019967265788810518, 0.00019967265788810518], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40780 loss: 1.0573 iter time (s): 5.290 samples/sec: 24.196
g0236:  iteration    40780/10000000 | consumed samples:      5219840 | consumed tokens:  10690232320 | elapsed time per iteration (ms): 5323.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.715634E-01 | loss scale: 16384.0 | grad norm: 0.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.044 | tokens per gpu per second (tgs): 1538.800 | TFLOPs: 12.38 |
g0214: [2024-08-10 12:36:55,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=40790, skipped=61, lr=[0.0001996724347974186, 0.0001996724347974186], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40790 loss: 0.9821 iter time (s): 5.728 samples/sec: 22.348
g0236:  iteration    40790/10000000 | consumed samples:      5221120 | consumed tokens:  10692853760 | elapsed time per iteration (ms): 5759.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.864763E-01 | loss scale: 16384.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.223 | tokens per gpu per second (tgs): 1422.247 | TFLOPs: 11.45 |
g0214: [2024-08-10 12:37:47,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=40800, skipped=61, lr=[0.00019967221163086333, 0.00019967221163086333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40800 loss: 0.9070 iter time (s): 5.139 samples/sec: 24.906
g0236:  iteration    40800/10000000 | consumed samples:      5222400 | consumed tokens:  10695475200 | elapsed time per iteration (ms): 5172.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.436054E-01 | loss scale: 16384.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.749 | tokens per gpu per second (tgs): 1583.925 | TFLOPs: 12.75 |
g0214: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40805
g0214: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40805
g0214: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40805
g0214: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0214: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40805
g0214: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0214: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0236: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40805
g0236: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0236: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40805
g0236: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0236: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40805
g0235: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0235: Grad overflow on iteration 40805
g0235: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0236: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40805
g0235: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40805
g0235: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40805
g0235: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0235: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40805
g0235: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0225: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 40805
g0234: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40805
g0225: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0233: Grad overflow on iteration 40805
g0232: Grad overflow on iteration 40805
g0234: Grad overflow on iteration 40805
g0220: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0232: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0234: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 40805
g0234: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 40805
g0233: Grad overflow on iteration 40805
g0214: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0235: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0234: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0234: Grad overflow on iteration 40805
g0232: Grad overflow on iteration 40805
g0234: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0234: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0225: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0232: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0232: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0232: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40805
g0220: Grad overflow on iteration 40805
g0225: Grad overflow on iteration 40805
g0220: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0232: Grad overflow on iteration 40805
g0232: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0232: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0225: Grad overflow on iteration 40805
g0233: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0220: Grad overflow on iteration 40805
g0233: Grad overflow on iteration 40805
g0225: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0220: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 40805
g0220: Grad overflow on iteration 40805
g0220: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0225: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0220: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0233: Grad overflow on iteration 40805
g0233: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0220: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0233: [2024-08-10 12:38:20,338] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0214: [2024-08-10 12:38:20,338] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0
g0236: [2024-08-10 12:38:20,339] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0
g0214: [2024-08-10 12:38:40,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=40810, skipped=62, lr=[0.00019967198838843957, 0.00019967198838843957], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40810 loss: 0.8883 iter time (s): 5.266 samples/sec: 24.306
g0236:  iteration    40810/10000000 | consumed samples:      5223680 | consumed tokens:  10698096640 | elapsed time per iteration (ms): 5299.4 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 8192.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.154 | tokens per gpu per second (tgs): 1545.841 | TFLOPs: 12.44 |
g0214: [2024-08-10 12:39:38,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=40820, skipped=62, lr=[0.00019967176507014746, 0.00019967176507014746], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40820 loss: 1.0360 iter time (s): 5.793 samples/sec: 22.095
g0236:  iteration    40820/10000000 | consumed samples:      5224960 | consumed tokens:  10700718080 | elapsed time per iteration (ms): 5825.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.016768E+00 | loss scale: 8192.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.972 | tokens per gpu per second (tgs): 1406.184 | TFLOPs: 11.32 |
g0214: [2024-08-10 12:40:26,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=40830, skipped=62, lr=[0.00019967154167598722, 0.00019967154167598722], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40830 loss: 1.0249 iter time (s): 4.745 samples/sec: 26.976
g0236:  iteration    40830/10000000 | consumed samples:      5226240 | consumed tokens:  10703339520 | elapsed time per iteration (ms): 4777.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.003557E+00 | loss scale: 8192.0 | grad norm: 0.292 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.793 | tokens per gpu per second (tgs): 1714.734 | TFLOPs: 13.80 |
g0214: [2024-08-10 12:41:18,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=40840, skipped=62, lr=[0.00019967131820595896, 0.00019967131820595896], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40840 loss: 0.9216 iter time (s): 5.191 samples/sec: 24.659
g0236:  iteration    40840/10000000 | consumed samples:      5227520 | consumed tokens:  10705960960 | elapsed time per iteration (ms): 5223.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.006749E+00 | loss scale: 8192.0 | grad norm: 0.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.504 | tokens per gpu per second (tgs): 1568.233 | TFLOPs: 12.62 |
g0214: [2024-08-10 12:42:07,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=40850, skipped=62, lr=[0.0001996710946600629, 0.0001996710946600629], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40850 loss: 0.8832 iter time (s): 4.841 samples/sec: 26.442
g0236:  iteration    40850/10000000 | consumed samples:      5228800 | consumed tokens:  10708582400 | elapsed time per iteration (ms): 4873.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.858423E-01 | loss scale: 8192.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.266 | tokens per gpu per second (tgs): 1681.003 | TFLOPs: 13.53 |
g0214: [2024-08-10 12:43:00,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=40860, skipped=62, lr=[0.00019967087103829912, 0.00019967087103829912], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40860 loss: 1.0043 iter time (s): 5.257 samples/sec: 24.348
g0236:  iteration    40860/10000000 | consumed samples:      5230080 | consumed tokens:  10711203840 | elapsed time per iteration (ms): 5290.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.043869E+00 | loss scale: 8192.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.196 | tokens per gpu per second (tgs): 1548.535 | TFLOPs: 12.46 |
g0214: [2024-08-10 12:43:56,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=40870, skipped=62, lr=[0.00019967064734066793, 0.00019967064734066793], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40870 loss: 1.0200 iter time (s): 5.584 samples/sec: 22.921
g0236:  iteration    40870/10000000 | consumed samples:      5231360 | consumed tokens:  10713825280 | elapsed time per iteration (ms): 5618.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.648755E-01 | loss scale: 8192.0 | grad norm: 0.414 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.784 | tokens per gpu per second (tgs): 1458.146 | TFLOPs: 11.73 |
g0214: [2024-08-10 12:44:55,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=40880, skipped=62, lr=[0.0001996704235671694, 0.0001996704235671694], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40880 loss: 1.1535 iter time (s): 5.884 samples/sec: 21.753
g0236:  iteration    40880/10000000 | consumed samples:      5232640 | consumed tokens:  10716446720 | elapsed time per iteration (ms): 5917.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.890594E-01 | loss scale: 8192.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.633 | tokens per gpu per second (tgs): 1384.493 | TFLOPs: 11.14 |
g0214: [2024-08-10 12:45:49,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=40890, skipped=62, lr=[0.00019967019971780373, 0.00019967019971780373], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40890 loss: 0.9225 iter time (s): 5.312 samples/sec: 24.099
g0236:  iteration    40890/10000000 | consumed samples:      5233920 | consumed tokens:  10719068160 | elapsed time per iteration (ms): 5345.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.024289E+00 | loss scale: 8192.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.945 | tokens per gpu per second (tgs): 1532.476 | TFLOPs: 12.33 |
g0214: [2024-08-10 12:46:44,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=40900, skipped=62, lr=[0.00019966997579257112, 0.00019966997579257112], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40900 loss: 1.0380 iter time (s): 5.470 samples/sec: 23.401
g0236:  iteration    40900/10000000 | consumed samples:      5235200 | consumed tokens:  10721689600 | elapsed time per iteration (ms): 5502.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.745213E-01 | loss scale: 8192.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.262 | tokens per gpu per second (tgs): 1488.760 | TFLOPs: 11.98 |
g0214: [2024-08-10 12:47:41,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=40910, skipped=62, lr=[0.00019966975179147172, 0.00019966975179147172], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40910 loss: 1.1948 iter time (s): 5.665 samples/sec: 22.595
g0236:  iteration    40910/10000000 | consumed samples:      5236480 | consumed tokens:  10724311040 | elapsed time per iteration (ms): 5698.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.025795E+00 | loss scale: 8192.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.462 | tokens per gpu per second (tgs): 1437.593 | TFLOPs: 11.57 |
g0214: [2024-08-10 12:48:35,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=40920, skipped=62, lr=[0.00019966952771450572, 0.00019966952771450572], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40920 loss: 0.8892 iter time (s): 5.430 samples/sec: 23.574
g0236:  iteration    40920/10000000 | consumed samples:      5237760 | consumed tokens:  10726932480 | elapsed time per iteration (ms): 5463.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.972848E-01 | loss scale: 8192.0 | grad norm: 0.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.429 | tokens per gpu per second (tgs): 1499.429 | TFLOPs: 12.07 |
g0214: [2024-08-10 12:49:29,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=40930, skipped=62, lr=[0.00019966930356167322, 0.00019966930356167322], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40930 loss: 1.0495 iter time (s): 5.307 samples/sec: 24.120
g0236:  iteration    40930/10000000 | consumed samples:      5239040 | consumed tokens:  10729553920 | elapsed time per iteration (ms): 5344.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.551863E-01 | loss scale: 8192.0 | grad norm: 0.279 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.948 | tokens per gpu per second (tgs): 1532.675 | TFLOPs: 12.33 |
g0214: [2024-08-10 12:50:18,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=40940, skipped=62, lr=[0.00019966907933297446, 0.00019966907933297446], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40940 loss: 1.0071 iter time (s): 4.910 samples/sec: 26.071
g0236:  iteration    40940/10000000 | consumed samples:      5240320 | consumed tokens:  10732175360 | elapsed time per iteration (ms): 4943.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.766899E-01 | loss scale: 8192.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.892 | tokens per gpu per second (tgs): 1657.072 | TFLOPs: 13.33 |
g0214: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40943
g0214: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40943
g0220: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40943
g0214: Grad overflow on iteration 40943
g0220: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40943
g0235: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 40943
g0235: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0232: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40943
g0220: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 40943
g0235: Grad overflow on iteration 40943
g0232: Grad overflow on iteration 40943
g0220: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0232: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40943
g0214: Grad overflow on iteration 40943
g0234: Grad overflow on iteration 40943
g0232: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0236: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0234: Grad overflow on iteration 40943
g0235: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0220: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40943
g0234: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 40943
g0233: Grad overflow on iteration 40943
g0220: Grad overflow on iteration 40943
g0234: Grad overflow on iteration 40943
g0235: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0236: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0234: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0225: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0233: Grad overflow on iteration 40943
g0214: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0225: Grad overflow on iteration 40943
g0233: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 40943
g0234: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0214: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0232: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0236: Grad overflow on iteration 40943
g0232: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0236: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0232: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0233: Grad overflow on iteration 40943
g0236: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0214: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0232: Grad overflow on iteration 40943
g0236: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40943
g0236: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 40943
g0236: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0233: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0225: Grad overflow on iteration 40943
g0233: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0225: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0233: Grad overflow on iteration 40943
g0232: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0233: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0233: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0225: Grad overflow on iteration 40943
g0225: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0234: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0234: Grad overflow on iteration 40943
g0214: [2024-08-10 12:50:36,699] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
g0225: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 40943
g0225: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0234: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0234: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0236: [2024-08-10 12:50:36,699] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0225: [2024-08-10 12:50:36,700] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0214: [2024-08-10 12:51:04,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=40950, skipped=63, lr=[0.00019966885502840963, 0.00019966885502840963], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40950 loss: 0.8652 iter time (s): 4.599 samples/sec: 27.833
g0236:  iteration    40950/10000000 | consumed samples:      5241600 | consumed tokens:  10734796800 | elapsed time per iteration (ms): 4632.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.668986E-01 | loss scale: 4096.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.630 | tokens per gpu per second (tgs): 1768.340 | TFLOPs: 14.23 |
g0214: [2024-08-10 12:51:55,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=40960, skipped=63, lr=[0.00019966863064797887, 0.00019966863064797887], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40960 loss: 0.9074 iter time (s): 5.013 samples/sec: 25.536
g0236:  iteration    40960/10000000 | consumed samples:      5242880 | consumed tokens:  10737418240 | elapsed time per iteration (ms): 5045.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.414986E-01 | loss scale: 4096.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.369 | tokens per gpu per second (tgs): 1623.611 | TFLOPs: 13.07 |
g0214: [2024-08-10 12:52:45,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=40970, skipped=63, lr=[0.00019966840619168233, 0.00019966840619168233], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40970 loss: 0.9473 iter time (s): 4.971 samples/sec: 25.749
g0236:  iteration    40970/10000000 | consumed samples:      5244160 | consumed tokens:  10740039680 | elapsed time per iteration (ms): 5003.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.815837E-01 | loss scale: 4096.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.581 | tokens per gpu per second (tgs): 1637.158 | TFLOPs: 13.17 |
g0214: [2024-08-10 12:53:32,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=40980, skipped=63, lr=[0.00019966818165952023, 0.00019966818165952023], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40980 loss: 1.0633 iter time (s): 4.639 samples/sec: 27.593
g0236:  iteration    40980/10000000 | consumed samples:      5245440 | consumed tokens:  10742661120 | elapsed time per iteration (ms): 4671.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.835951E-01 | loss scale: 4096.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.398 | tokens per gpu per second (tgs): 1753.477 | TFLOPs: 14.11 |
g0214: [2024-08-10 12:54:18,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=40990, skipped=63, lr=[0.0001996679570514927, 0.0001996679570514927], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 40990 loss: 1.0299 iter time (s): 4.577 samples/sec: 27.965
g0236:  iteration    40990/10000000 | consumed samples:      5246720 | consumed tokens:  10745282560 | elapsed time per iteration (ms): 4612.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.673769E-01 | loss scale: 4096.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.750 | tokens per gpu per second (tgs): 1775.970 | TFLOPs: 14.29 |
g0214: [2024-08-10 12:55:08,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=41000, skipped=63, lr=[0.0001996677323675999, 0.0001996677323675999], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41000 loss: 1.0837 iter time (s): 5.044 samples/sec: 25.378
g0236:  iteration    41000/10000000 | consumed samples:      5248000 | consumed tokens:  10747904000 | elapsed time per iteration (ms): 5079.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.447356E-01 | loss scale: 4096.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.200 | tokens per gpu per second (tgs): 1612.781 | TFLOPs: 12.98 |
g0236: -------------------------------------------------------------------------------------------------
g0236:  validation loss at iteration 41000 | lm loss value: 9.879920E-01 | lm loss PPL: 2.685836E+00 | 
g0236: -------------------------------------------------------------------------------------------------
g0214: saving checkpoint at iteration   41000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214: [2024-08-10 13:04:50,873] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step41000 is about to be saved!
g0214: [2024-08-10 13:04:50,882] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0214: [2024-08-10 13:04:50,883] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0214: [2024-08-10 13:04:50,883] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0236: [2024-08-10 13:04:50,883] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0236: [2024-08-10 13:04:50,883] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0236: [2024-08-10 13:04:50,883] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0232: [2024-08-10 13:04:50,883] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0232: [2024-08-10 13:04:50,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0232: [2024-08-10 13:04:50,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0225: [2024-08-10 13:04:50,883] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0225: [2024-08-10 13:04:50,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0225: [2024-08-10 13:04:50,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0233: [2024-08-10 13:04:50,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0233: [2024-08-10 13:04:50,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0233: [2024-08-10 13:04:50,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0235: [2024-08-10 13:04:50,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0235: [2024-08-10 13:04:50,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0235: [2024-08-10 13:04:50,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0234: [2024-08-10 13:04:50,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0234: [2024-08-10 13:04:50,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0234: [2024-08-10 13:04:50,887] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0220: [2024-08-10 13:04:50,891] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0220: [2024-08-10 13:04:50,891] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0220: [2024-08-10 13:04:50,891] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0236: [2024-08-10 13:04:50,906] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_23-model_00-model_states.pt...
g0232: [2024-08-10 13:04:50,915] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_11-model_00-model_states.pt...
g0235: [2024-08-10 13:04:50,917] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_20-model_00-model_states.pt...
g0233: [2024-08-10 13:04:50,917] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_14-model_00-model_states.pt...
g0225: [2024-08-10 13:04:50,918] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_08-model_00-model_states.pt...
g0234: [2024-08-10 13:04:50,919] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_17-model_00-model_states.pt...
g0220: [2024-08-10 13:04:50,923] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_05-model_00-model_states.pt...
g0214: [2024-08-10 13:04:50,933] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_01-model_00-model_states.pt...
g0232: [2024-08-10 13:04:51,036] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_11-model_00-model_states.pt.
g0225: [2024-08-10 13:04:51,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_08-model_00-model_states.pt.
g0235: [2024-08-10 13:04:51,058] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_20-model_00-model_states.pt.
g0232: [2024-08-10 13:04:51,069] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_12-model_00-model_states.pt...
g0225: [2024-08-10 13:04:51,082] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_09-model_00-model_states.pt...
g0220: [2024-08-10 13:04:51,090] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_05-model_00-model_states.pt.
g0235: [2024-08-10 13:04:51,093] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_21-model_00-model_states.pt...
g0233: [2024-08-10 13:04:51,114] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_14-model_00-model_states.pt.
g0220: [2024-08-10 13:04:51,123] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_06-model_00-model_states.pt...
g0236: [2024-08-10 13:04:51,126] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 13:04:51,126] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 13:04:51,128] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_24-model_00-model_states.pt.
g0234: [2024-08-10 13:04:51,140] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_17-model_00-model_states.pt.
g0233: [2024-08-10 13:04:51,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_15-model_00-model_states.pt...
g0236: [2024-08-10 13:04:51,175] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_25-model_00-model_states.pt...
g0234: [2024-08-10 13:04:51,176] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_18-model_00-model_states.pt...
g0214: [2024-08-10 13:04:51,185] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_01-model_00-model_states.pt.
g0232: [2024-08-10 13:04:51,206] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_12-model_00-model_states.pt.
g0214: [2024-08-10 13:04:51,216] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_02-model_00-model_states.pt...
g0225: [2024-08-10 13:04:51,221] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_09-model_00-model_states.pt.
g0220: [2024-08-10 13:04:51,227] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_06-model_00-model_states.pt.
g0232: [2024-08-10 13:04:51,236] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_13-model_00-model_states.pt...
g0225: [2024-08-10 13:04:51,252] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_10-model_00-model_states.pt...
g0220: [2024-08-10 13:04:51,257] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_07-model_00-model_states.pt...
g0234: [2024-08-10 13:04:51,318] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_18-model_00-model_states.pt.
g0236: [2024-08-10 13:04:51,338] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 13:04:51,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_07_model_states.pt...
g0234: [2024-08-10 13:04:51,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_19-model_00-model_states.pt...
g0214: [2024-08-10 13:04:51,364] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_02-model_00-model_states.pt.
g0233: [2024-08-10 13:04:51,374] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_15-model_00-model_states.pt.
g0220: [2024-08-10 13:04:51,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 13:04:51,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_01_model_states.pt...
g0225: [2024-08-10 13:04:51,390] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_10-model_00-model_states.pt.
g0225: [2024-08-10 13:04:51,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_02_model_states.pt...
g0233: [2024-08-10 13:04:51,400] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_16-model_00-model_states.pt...
g0214: [2024-08-10 13:04:51,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_03-model_00-model_states.pt...
g0235: [2024-08-10 13:04:51,435] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_21-model_00-model_states.pt.
g0234: [2024-08-10 13:04:51,456] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 13:04:51,458] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_05_model_states.pt...
g0235: [2024-08-10 13:04:51,465] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_22-model_00-model_states.pt...
g0232: [2024-08-10 13:04:51,488] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_13-model_00-model_states.pt.
g0232: [2024-08-10 13:04:51,489] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_03_model_states.pt...
g0233: [2024-08-10 13:04:51,519] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 13:04:51,521] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_04_model_states.pt...
g0214: [2024-08-10 13:04:51,556] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_03-model_00-model_states.pt.
g0214: [2024-08-10 13:04:51,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_04-model_00-model_states.pt...
g0235: [2024-08-10 13:04:51,616] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 13:04:51,618] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_06_model_states.pt...
g0214: [2024-08-10 13:04:51,760] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 13:04:51,762] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_00_model_states.pt
g0214: [2024-08-10 13:04:51,762] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_00_model_states.pt...
g0236: [2024-08-10 13:04:53,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_07_model_states.pt.
g0236: [2024-08-10 13:04:53,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0225: [2024-08-10 13:04:53,753] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_02_model_states.pt.
g0225: [2024-08-10 13:04:53,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0234: [2024-08-10 13:04:53,761] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_05_model_states.pt.
g0234: [2024-08-10 13:04:53,762] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0233: [2024-08-10 13:04:53,878] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_04_model_states.pt.
g0233: [2024-08-10 13:04:53,878] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0232: [2024-08-10 13:04:53,879] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_03_model_states.pt.
g0232: [2024-08-10 13:04:53,879] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0235: [2024-08-10 13:04:53,951] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_06_model_states.pt.
g0235: [2024-08-10 13:04:53,951] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0220: [2024-08-10 13:04:54,055] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_01_model_states.pt.
g0220: [2024-08-10 13:04:54,055] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0214: [2024-08-10 13:04:55,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step41000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 13:04:55,255] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step41000 is ready now!
g0214:   successfully saved checkpoint at iteration   41000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214: Checkpoint Save GB: 22.521, GB/Sec: 5.12, Latency(second): 4.398
g0236: (min, max) time across ranks (ms):
g0236:     save-checkpoint ................................: (4396.54, 4398.13)
g0214: [2024-08-10 13:05:45,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=41010, skipped=63, lr=[0.00019966750760784207, 0.00019966750760784207], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41010 loss: 1.0770 iter time (s): 4.974 samples/sec: 25.733
g0236:  iteration    41010/10000000 | consumed samples:      5249280 | consumed tokens:  10750525440 | elapsed time per iteration (ms): 63632.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.038175E+00 | loss scale: 4096.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.012 | tokens per gpu per second (tgs): 128.740 | TFLOPs: 1.04 |
g0214: [2024-08-10 13:06:35,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=41020, skipped=63, lr=[0.00019966728277221935, 0.00019966728277221935], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41020 loss: 1.0094 iter time (s): 4.986 samples/sec: 25.673
g0236:  iteration    41020/10000000 | consumed samples:      5250560 | consumed tokens:  10753146880 | elapsed time per iteration (ms): 5018.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.038658E+00 | loss scale: 4096.0 | grad norm: 0.263 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.503 | tokens per gpu per second (tgs): 1632.220 | TFLOPs: 13.13 |
g0214: [2024-08-10 13:07:22,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=41030, skipped=63, lr=[0.0001996670578607319, 0.0001996670578607319], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41030 loss: 0.9354 iter time (s): 4.667 samples/sec: 27.426
g0236:  iteration    41030/10000000 | consumed samples:      5251840 | consumed tokens:  10755768320 | elapsed time per iteration (ms): 4699.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.866275E-01 | loss scale: 4096.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.235 | tokens per gpu per second (tgs): 1743.025 | TFLOPs: 14.03 |
g0214: [2024-08-10 13:08:12,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=41040, skipped=63, lr=[0.00019966683287337995, 0.00019966683287337995], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41040 loss: 0.9273 iter time (s): 5.006 samples/sec: 25.569
g0236:  iteration    41040/10000000 | consumed samples:      5253120 | consumed tokens:  10758389760 | elapsed time per iteration (ms): 5039.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.968975E-01 | loss scale: 4096.0 | grad norm: 0.409 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.402 | tokens per gpu per second (tgs): 1625.702 | TFLOPs: 13.08 |
g0214: [2024-08-10 13:09:04,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=41050, skipped=63, lr=[0.00019966660781016356, 0.00019966660781016356], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41050 loss: 0.9167 iter time (s): 5.117 samples/sec: 25.016
g0236:  iteration    41050/10000000 | consumed samples:      5254400 | consumed tokens:  10761011200 | elapsed time per iteration (ms): 5149.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.816230E-01 | loss scale: 4096.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.855 | tokens per gpu per second (tgs): 1590.749 | TFLOPs: 12.80 |
g0214: [2024-08-10 13:09:52,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=41060, skipped=63, lr=[0.000199666382671083, 0.000199666382671083], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41060 loss: 1.1483 iter time (s): 4.806 samples/sec: 26.631
g0236:  iteration    41060/10000000 | consumed samples:      5255680 | consumed tokens:  10763632640 | elapsed time per iteration (ms): 4840.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.005304E+00 | loss scale: 4096.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.446 | tokens per gpu per second (tgs): 1692.552 | TFLOPs: 13.62 |
g0214: [2024-08-10 13:10:47,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=41070, skipped=63, lr=[0.0001996661574561384, 0.0001996661574561384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41070 loss: 1.2532 iter time (s): 5.452 samples/sec: 23.476
g0236:  iteration    41070/10000000 | consumed samples:      5256960 | consumed tokens:  10766254080 | elapsed time per iteration (ms): 5486.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.002518E+00 | loss scale: 4096.0 | grad norm: 0.420 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.331 | tokens per gpu per second (tgs): 1493.158 | TFLOPs: 12.02 |
g0214: [2024-08-10 13:11:38,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=41080, skipped=63, lr=[0.00019966593216532994, 0.00019966593216532994], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41080 loss: 1.1295 iter time (s): 5.065 samples/sec: 25.270
g0236:  iteration    41080/10000000 | consumed samples:      5258240 | consumed tokens:  10768875520 | elapsed time per iteration (ms): 5098.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.001265E+00 | loss scale: 4096.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.105 | tokens per gpu per second (tgs): 1606.724 | TFLOPs: 12.93 |
g0214: [2024-08-10 13:12:26,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=41090, skipped=63, lr=[0.00019966570679865782, 0.00019966570679865782], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41090 loss: 1.0361 iter time (s): 4.739 samples/sec: 27.010
g0236:  iteration    41090/10000000 | consumed samples:      5259520 | consumed tokens:  10771496960 | elapsed time per iteration (ms): 4772.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.005479E+00 | loss scale: 4096.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.822 | tokens per gpu per second (tgs): 1716.612 | TFLOPs: 13.81 |
g0214: [2024-08-10 13:13:14,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=41100, skipped=63, lr=[0.0001996654813561222, 0.0001996654813561222], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41100 loss: 0.9413 iter time (s): 4.780 samples/sec: 26.776
g0236:  iteration    41100/10000000 | consumed samples:      5260800 | consumed tokens:  10774118400 | elapsed time per iteration (ms): 4814.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.405565E-01 | loss scale: 4096.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.587 | tokens per gpu per second (tgs): 1701.551 | TFLOPs: 13.69 |
g0214: [2024-08-10 13:14:02,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=41110, skipped=63, lr=[0.0001996652558377232, 0.0001996652558377232], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41110 loss: 1.0095 iter time (s): 4.741 samples/sec: 27.001
g0236:  iteration    41110/10000000 | consumed samples:      5262080 | consumed tokens:  10776739840 | elapsed time per iteration (ms): 4773.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.761726E-01 | loss scale: 4096.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.816 | tokens per gpu per second (tgs): 1716.222 | TFLOPs: 13.81 |
g0214: [2024-08-10 13:14:51,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=41120, skipped=63, lr=[0.00019966503024346108, 0.00019966503024346108], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41120 loss: 1.0223 iter time (s): 4.888 samples/sec: 26.188
g0236:  iteration    41120/10000000 | consumed samples:      5263360 | consumed tokens:  10779361280 | elapsed time per iteration (ms): 4921.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.550488E-01 | loss scale: 4096.0 | grad norm: 0.284 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.010 | tokens per gpu per second (tgs): 1664.657 | TFLOPs: 13.40 |
g0214: [2024-08-10 13:15:44,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=41130, skipped=63, lr=[0.00019966480457333594, 0.00019966480457333594], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41130 loss: 0.8975 iter time (s): 5.233 samples/sec: 24.459
g0236:  iteration    41130/10000000 | consumed samples:      5264640 | consumed tokens:  10781982720 | elapsed time per iteration (ms): 5265.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.640217E-01 | loss scale: 4096.0 | grad norm: 0.268 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.308 | tokens per gpu per second (tgs): 1555.689 | TFLOPs: 12.52 |
g0214: [2024-08-10 13:16:33,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=41140, skipped=63, lr=[0.00019966457882734804, 0.00019966457882734804], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41140 loss: 0.8872 iter time (s): 4.937 samples/sec: 25.925
g0236:  iteration    41140/10000000 | consumed samples:      5265920 | consumed tokens:  10784604160 | elapsed time per iteration (ms): 4970.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.635073E-01 | loss scale: 4096.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.753 | tokens per gpu per second (tgs): 1648.186 | TFLOPs: 13.26 |
g0214: [2024-08-10 13:17:20,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=41150, skipped=63, lr=[0.00019966435300549745, 0.00019966435300549745], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41150 loss: 0.8674 iter time (s): 4.675 samples/sec: 27.381
g0236:  iteration    41150/10000000 | consumed samples:      5267200 | consumed tokens:  10787225600 | elapsed time per iteration (ms): 4707.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.535078E-01 | loss scale: 4096.0 | grad norm: 0.163 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.190 | tokens per gpu per second (tgs): 1740.192 | TFLOPs: 14.00 |
g0214: [2024-08-10 13:18:10,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=41160, skipped=63, lr=[0.0001996641271077844, 0.0001996641271077844], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41160 loss: 1.0280 iter time (s): 4.969 samples/sec: 25.760
g0236:  iteration    41160/10000000 | consumed samples:      5268480 | consumed tokens:  10789847040 | elapsed time per iteration (ms): 5001.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.894206E-01 | loss scale: 4096.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.590 | tokens per gpu per second (tgs): 1637.762 | TFLOPs: 13.18 |
g0214: [2024-08-10 13:19:05,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=41170, skipped=63, lr=[0.00019966390113420904, 0.00019966390113420904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41170 loss: 0.9732 iter time (s): 5.376 samples/sec: 23.809
g0236:  iteration    41170/10000000 | consumed samples:      5269760 | consumed tokens:  10792468480 | elapsed time per iteration (ms): 5409.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.965503E-01 | loss scale: 4096.0 | grad norm: 0.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.664 | tokens per gpu per second (tgs): 1514.484 | TFLOPs: 12.19 |
g0214: [2024-08-10 13:19:55,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=41180, skipped=63, lr=[0.0001996636750847716, 0.0001996636750847716], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41180 loss: 0.8742 iter time (s): 5.038 samples/sec: 25.409
g0236:  iteration    41180/10000000 | consumed samples:      5271040 | consumed tokens:  10795089920 | elapsed time per iteration (ms): 5070.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.037907E+00 | loss scale: 4096.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.243 | tokens per gpu per second (tgs): 1615.568 | TFLOPs: 13.00 |
g0214: [2024-08-10 13:20:40,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=41190, skipped=63, lr=[0.00019966344895947218, 0.00019966344895947218], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41190 loss: 0.9288 iter time (s): 4.449 samples/sec: 28.771
g0236:  iteration    41190/10000000 | consumed samples:      5272320 | consumed tokens:  10797711360 | elapsed time per iteration (ms): 4482.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.779262E-01 | loss scale: 4096.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.556 | tokens per gpu per second (tgs): 1827.604 | TFLOPs: 14.71 |
g0214: [2024-08-10 13:21:29,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=41200, skipped=63, lr=[0.000199663222758311, 0.000199663222758311], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41200 loss: 0.9735 iter time (s): 4.830 samples/sec: 26.500
g0236:  iteration    41200/10000000 | consumed samples:      5273600 | consumed tokens:  10800332800 | elapsed time per iteration (ms): 4863.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.374846E-01 | loss scale: 4096.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.320 | tokens per gpu per second (tgs): 1684.495 | TFLOPs: 13.56 |
g0214: [2024-08-10 13:22:21,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=41210, skipped=63, lr=[0.00019966299648128825, 0.00019966299648128825], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41210 loss: 1.0249 iter time (s): 5.251 samples/sec: 24.378
g0236:  iteration    41210/10000000 | consumed samples:      5274880 | consumed tokens:  10802954240 | elapsed time per iteration (ms): 5283.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.872547E-01 | loss scale: 4096.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.226 | tokens per gpu per second (tgs): 1550.477 | TFLOPs: 12.48 |
g0214: [2024-08-10 13:23:14,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=41220, skipped=63, lr=[0.00019966277012840406, 0.00019966277012840406], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41220 loss: 1.0426 iter time (s): 5.231 samples/sec: 24.468
g0236:  iteration    41220/10000000 | consumed samples:      5276160 | consumed tokens:  10805575680 | elapsed time per iteration (ms): 5264.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.613516E-01 | loss scale: 4096.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.312 | tokens per gpu per second (tgs): 1555.990 | TFLOPs: 12.52 |
g0214: [2024-08-10 13:24:07,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=41230, skipped=63, lr=[0.0001996625436996586, 0.0001996625436996586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41230 loss: 0.8969 iter time (s): 5.241 samples/sec: 24.421
g0236:  iteration    41230/10000000 | consumed samples:      5277440 | consumed tokens:  10808197120 | elapsed time per iteration (ms): 5276.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.000612E+00 | loss scale: 4096.0 | grad norm: 0.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.260 | tokens per gpu per second (tgs): 1552.613 | TFLOPs: 12.49 |
g0214: [2024-08-10 13:24:58,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=41240, skipped=63, lr=[0.00019966231719505207, 0.00019966231719505207], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41240 loss: 0.9303 iter time (s): 5.110 samples/sec: 25.047
g0236:  iteration    41240/10000000 | consumed samples:      5278720 | consumed tokens:  10810818560 | elapsed time per iteration (ms): 5143.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.640018E-01 | loss scale: 4096.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.885 | tokens per gpu per second (tgs): 1592.649 | TFLOPs: 12.82 |
g0214: [2024-08-10 13:25:48,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=41250, skipped=63, lr=[0.00019966209061458466, 0.00019966209061458466], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41250 loss: 1.0123 iter time (s): 4.894 samples/sec: 26.152
g0236:  iteration    41250/10000000 | consumed samples:      5280000 | consumed tokens:  10813440000 | elapsed time per iteration (ms): 4928.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.879013E-01 | loss scale: 4096.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.970 | tokens per gpu per second (tgs): 1662.068 | TFLOPs: 13.37 |
g0214: [2024-08-10 13:26:37,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=41260, skipped=63, lr=[0.0001996618639582565, 0.0001996618639582565], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41260 loss: 0.9633 iter time (s): 4.932 samples/sec: 25.951
g0236:  iteration    41260/10000000 | consumed samples:      5281280 | consumed tokens:  10816061440 | elapsed time per iteration (ms): 4966.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.025318E+00 | loss scale: 4096.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.770 | tokens per gpu per second (tgs): 1649.304 | TFLOPs: 13.27 |
g0214: [2024-08-10 13:27:29,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=41270, skipped=63, lr=[0.00019966163722606777, 0.00019966163722606777], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41270 loss: 1.0094 iter time (s): 5.096 samples/sec: 25.120
g0236:  iteration    41270/10000000 | consumed samples:      5282560 | consumed tokens:  10818682880 | elapsed time per iteration (ms): 5128.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.010999E+00 | loss scale: 4096.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.959 | tokens per gpu per second (tgs): 1597.355 | TFLOPs: 12.85 |
g0214: [2024-08-10 13:28:14,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=41280, skipped=63, lr=[0.0001996614104180187, 0.0001996614104180187], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41280 loss: 0.9072 iter time (s): 4.515 samples/sec: 28.348
g0236:  iteration    41280/10000000 | consumed samples:      5283840 | consumed tokens:  10821304320 | elapsed time per iteration (ms): 4548.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.995986E-01 | loss scale: 4096.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.143 | tokens per gpu per second (tgs): 1801.123 | TFLOPs: 14.49 |
g0214: [2024-08-10 13:29:11,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=41290, skipped=63, lr=[0.0001996611835341094, 0.0001996611835341094], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41290 loss: 0.9973 iter time (s): 5.642 samples/sec: 22.686
g0236:  iteration    41290/10000000 | consumed samples:      5285120 | consumed tokens:  10823925760 | elapsed time per iteration (ms): 5684.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.864548E-01 | loss scale: 4096.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.517 | tokens per gpu per second (tgs): 1441.071 | TFLOPs: 11.60 |
g0214: [2024-08-10 13:30:00,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=41300, skipped=63, lr=[0.00019966095657434008, 0.00019966095657434008], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41300 loss: 1.0572 iter time (s): 4.872 samples/sec: 26.273
g0236:  iteration    41300/10000000 | consumed samples:      5286400 | consumed tokens:  10826547200 | elapsed time per iteration (ms): 4905.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.026896E+00 | loss scale: 4096.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.096 | tokens per gpu per second (tgs): 1670.117 | TFLOPs: 13.44 |
g0214: [2024-08-10 13:30:46,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=41310, skipped=63, lr=[0.0001996607295387109, 0.0001996607295387109], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41310 loss: 0.8940 iter time (s): 4.595 samples/sec: 27.854
g0236:  iteration    41310/10000000 | consumed samples:      5287680 | consumed tokens:  10829168640 | elapsed time per iteration (ms): 4628.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.664062E-01 | loss scale: 4096.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.657 | tokens per gpu per second (tgs): 1770.056 | TFLOPs: 14.24 |
g0214: [2024-08-10 13:31:35,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=41320, skipped=63, lr=[0.00019966050242722206, 0.00019966050242722206], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41320 loss: 0.9201 iter time (s): 4.816 samples/sec: 26.578
g0236:  iteration    41320/10000000 | consumed samples:      5288960 | consumed tokens:  10831790080 | elapsed time per iteration (ms): 4848.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.462437E-01 | loss scale: 4096.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.399 | tokens per gpu per second (tgs): 1689.556 | TFLOPs: 13.60 |
g0214: [2024-08-10 13:32:21,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=41330, skipped=63, lr=[0.0001996602752398737, 0.0001996602752398737], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41330 loss: 0.9242 iter time (s): 4.638 samples/sec: 27.601
g0236:  iteration    41330/10000000 | consumed samples:      5290240 | consumed tokens:  10834411520 | elapsed time per iteration (ms): 4671.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.693100E-01 | loss scale: 4096.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.402 | tokens per gpu per second (tgs): 1753.760 | TFLOPs: 14.11 |
g0214: [2024-08-10 13:33:12,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=41340, skipped=63, lr=[0.00019966004797666598, 0.00019966004797666598], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41340 loss: 0.9746 iter time (s): 5.009 samples/sec: 25.554
g0236:  iteration    41340/10000000 | consumed samples:      5291520 | consumed tokens:  10837032960 | elapsed time per iteration (ms): 5041.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.590536E-01 | loss scale: 4096.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.388 | tokens per gpu per second (tgs): 1624.821 | TFLOPs: 13.08 |
g0214: [2024-08-10 13:34:01,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=41350, skipped=63, lr=[0.00019965982063759911, 0.00019965982063759911], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41350 loss: 0.9034 iter time (s): 4.875 samples/sec: 26.257
g0236:  iteration    41350/10000000 | consumed samples:      5292800 | consumed tokens:  10839654400 | elapsed time per iteration (ms): 4908.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.888831E-01 | loss scale: 4096.0 | grad norm: 0.261 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.075 | tokens per gpu per second (tgs): 1668.802 | TFLOPs: 13.43 |
g0214: [2024-08-10 13:34:48,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=41360, skipped=63, lr=[0.0001996595932226733, 0.0001996595932226733], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41360 loss: 1.0184 iter time (s): 4.630 samples/sec: 27.644
g0236:  iteration    41360/10000000 | consumed samples:      5294080 | consumed tokens:  10842275840 | elapsed time per iteration (ms): 4678.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.014541E+00 | loss scale: 4096.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.362 | tokens per gpu per second (tgs): 1751.186 | TFLOPs: 14.09 |
g0214: [2024-08-10 13:35:33,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=41370, skipped=63, lr=[0.00019965936573188863, 0.00019965936573188863], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41370 loss: 0.9585 iter time (s): 4.464 samples/sec: 28.672
g0236:  iteration    41370/10000000 | consumed samples:      5295360 | consumed tokens:  10844897280 | elapsed time per iteration (ms): 4497.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.002349E+00 | loss scale: 4096.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.458 | tokens per gpu per second (tgs): 1821.318 | TFLOPs: 14.66 |
g0214: [2024-08-10 13:36:20,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=41380, skipped=63, lr=[0.00019965913816524537, 0.00019965913816524537], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41380 loss: 1.0733 iter time (s): 4.684 samples/sec: 27.326
g0236:  iteration    41380/10000000 | consumed samples:      5296640 | consumed tokens:  10847518720 | elapsed time per iteration (ms): 4736.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.849069E-01 | loss scale: 4096.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.027 | tokens per gpu per second (tgs): 1729.726 | TFLOPs: 13.92 |
g0214: [2024-08-10 13:37:09,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=41390, skipped=63, lr=[0.00019965891052274362, 0.00019965891052274362], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41390 loss: 0.9259 iter time (s): 4.821 samples/sec: 26.552
g0236:  iteration    41390/10000000 | consumed samples:      5297920 | consumed tokens:  10850140160 | elapsed time per iteration (ms): 4854.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.746675E-01 | loss scale: 4096.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.366 | tokens per gpu per second (tgs): 1687.450 | TFLOPs: 13.58 |
g0214: [2024-08-10 13:37:56,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=41400, skipped=63, lr=[0.0001996586828043836, 0.0001996586828043836], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41400 loss: 1.1760 iter time (s): 4.659 samples/sec: 27.475
g0236:  iteration    41400/10000000 | consumed samples:      5299200 | consumed tokens:  10852761600 | elapsed time per iteration (ms): 4691.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.004798E+00 | loss scale: 4096.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.282 | tokens per gpu per second (tgs): 1746.021 | TFLOPs: 14.05 |
g0214: [2024-08-10 13:38:49,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=41410, skipped=63, lr=[0.0001996584550101655, 0.0001996584550101655], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41410 loss: 0.9572 iter time (s): 5.318 samples/sec: 24.070
g0236:  iteration    41410/10000000 | consumed samples:      5300480 | consumed tokens:  10855383040 | elapsed time per iteration (ms): 5351.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.518733E-01 | loss scale: 4096.0 | grad norm: 0.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.919 | tokens per gpu per second (tgs): 1530.831 | TFLOPs: 12.32 |
g0214: [2024-08-10 13:39:36,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=41420, skipped=63, lr=[0.00019965822714008942, 0.00019965822714008942], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41420 loss: 1.1325 iter time (s): 4.697 samples/sec: 27.252
g0236:  iteration    41420/10000000 | consumed samples:      5301760 | consumed tokens:  10858004480 | elapsed time per iteration (ms): 4730.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.026757E+00 | loss scale: 4096.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.062 | tokens per gpu per second (tgs): 1731.941 | TFLOPs: 13.94 |
g0214: [2024-08-10 13:40:29,081] [INFO] [logging.py:96:log_dist] [Rank 0] step=41430, skipped=63, lr=[0.00019965799919415561, 0.00019965799919415561], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41430 loss: 0.9510 iter time (s): 5.189 samples/sec: 24.668
g0236:  iteration    41430/10000000 | consumed samples:      5303040 | consumed tokens:  10860625920 | elapsed time per iteration (ms): 5223.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.023909E+00 | loss scale: 4096.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.505 | tokens per gpu per second (tgs): 1568.326 | TFLOPs: 12.62 |
g0214: [2024-08-10 13:41:17,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=41440, skipped=63, lr=[0.00019965777117236422, 0.00019965777117236422], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41440 loss: 1.0197 iter time (s): 4.802 samples/sec: 26.657
g0236:  iteration    41440/10000000 | consumed samples:      5304320 | consumed tokens:  10863247360 | elapsed time per iteration (ms): 4835.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.489505E-01 | loss scale: 4096.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.472 | tokens per gpu per second (tgs): 1694.193 | TFLOPs: 13.63 |
g0234: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0234: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0234: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0220: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0220: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0236: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0236: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0232: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0232: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0225: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0233: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0233: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0233: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0232: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0233: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0214: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0214: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0225: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 13:41:40,657] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0214: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0214: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0232: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0235: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0225: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0236: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0233: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0235: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0236: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0233: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0225: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0225: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0220: [2024-08-10 13:41:40,658] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0236: [2024-08-10 13:41:40,659] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0
g0214: [2024-08-10 13:42:05,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=41450, skipped=63, lr=[0.0001996575430747154, 0.0001996575430747154], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41450 loss: 1.0143 iter time (s): 4.793 samples/sec: 26.706
g0236:  iteration    41450/10000000 | consumed samples:      5305600 | consumed tokens:  10865868800 | elapsed time per iteration (ms): 4826.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.906466E-01 | loss scale: 8192.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.518 | tokens per gpu per second (tgs): 1697.159 | TFLOPs: 13.66 |
g0214: [2024-08-10 13:42:52,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=41460, skipped=63, lr=[0.00019965731490120936, 0.00019965731490120936], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41460 loss: 1.0014 iter time (s): 4.673 samples/sec: 27.392
g0236:  iteration    41460/10000000 | consumed samples:      5306880 | consumed tokens:  10868490240 | elapsed time per iteration (ms): 4709.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.501100E-01 | loss scale: 8192.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.177 | tokens per gpu per second (tgs): 1739.303 | TFLOPs: 14.00 |
g0214: [2024-08-10 13:43:37,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=41470, skipped=63, lr=[0.00019965708665184632, 0.00019965708665184632], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41470 loss: 0.9132 iter time (s): 4.444 samples/sec: 28.805
g0236:  iteration    41470/10000000 | consumed samples:      5308160 | consumed tokens:  10871111680 | elapsed time per iteration (ms): 4478.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.522477E-01 | loss scale: 8192.0 | grad norm: 0.319 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.581 | tokens per gpu per second (tgs): 1829.186 | TFLOPs: 14.72 |
g0214: [2024-08-10 13:44:20,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=41480, skipped=63, lr=[0.00019965685832662631, 0.00019965685832662631], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41480 loss: 0.9447 iter time (s): 4.288 samples/sec: 29.853
g0236:  iteration    41480/10000000 | consumed samples:      5309440 | consumed tokens:  10873733120 | elapsed time per iteration (ms): 4320.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.674216E-01 | loss scale: 8192.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.625 | tokens per gpu per second (tgs): 1896.010 | TFLOPs: 15.26 |
g0214: [2024-08-10 13:45:11,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=41490, skipped=63, lr=[0.00019965662992554964, 0.00019965662992554964], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41490 loss: 0.9313 iter time (s): 5.070 samples/sec: 25.248
g0236:  iteration    41490/10000000 | consumed samples:      5310720 | consumed tokens:  10876354560 | elapsed time per iteration (ms): 5102.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.823223E-01 | loss scale: 8192.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.085 | tokens per gpu per second (tgs): 1605.451 | TFLOPs: 12.92 |
g0214: [2024-08-10 13:45:56,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=41500, skipped=63, lr=[0.00019965640144861644, 0.00019965640144861644], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41500 loss: 1.1477 iter time (s): 4.439 samples/sec: 28.836
g0236:  iteration    41500/10000000 | consumed samples:      5312000 | consumed tokens:  10878976000 | elapsed time per iteration (ms): 4477.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.851503E-01 | loss scale: 8192.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.587 | tokens per gpu per second (tgs): 1829.552 | TFLOPs: 14.72 |
g0214: [2024-08-10 13:46:42,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=41510, skipped=63, lr=[0.0001996561728958269, 0.0001996561728958269], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41510 loss: 0.9127 iter time (s): 4.572 samples/sec: 27.996
g0236:  iteration    41510/10000000 | consumed samples:      5313280 | consumed tokens:  10881597440 | elapsed time per iteration (ms): 4605.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.028452E+00 | loss scale: 8192.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.794 | tokens per gpu per second (tgs): 1778.832 | TFLOPs: 14.31 |
g0214: [2024-08-10 13:47:34,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=41520, skipped=63, lr=[0.00019965594426718116, 0.00019965594426718116], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41520 loss: 0.9972 iter time (s): 5.106 samples/sec: 25.068
g0236:  iteration    41520/10000000 | consumed samples:      5314560 | consumed tokens:  10884218880 | elapsed time per iteration (ms): 5139.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.449545E-01 | loss scale: 8192.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.904 | tokens per gpu per second (tgs): 1593.846 | TFLOPs: 12.83 |
g0214: [2024-08-10 13:48:22,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=41530, skipped=63, lr=[0.00019965571556267942, 0.00019965571556267942], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41530 loss: 1.0350 iter time (s): 4.791 samples/sec: 26.717
g0236:  iteration    41530/10000000 | consumed samples:      5315840 | consumed tokens:  10886840320 | elapsed time per iteration (ms): 4823.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.006020E+00 | loss scale: 8192.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.536 | tokens per gpu per second (tgs): 1698.292 | TFLOPs: 13.67 |
g0214: [2024-08-10 13:49:10,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=41540, skipped=63, lr=[0.00019965548678232186, 0.00019965548678232186], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41540 loss: 0.9226 iter time (s): 4.793 samples/sec: 26.706
g0236:  iteration    41540/10000000 | consumed samples:      5317120 | consumed tokens:  10889461760 | elapsed time per iteration (ms): 4826.0 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.523298E-01 | loss scale: 8192.0 | grad norm: 0.316 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.523 | tokens per gpu per second (tgs): 1697.463 | TFLOPs: 13.66 |
g0214: [2024-08-10 13:49:57,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=41550, skipped=63, lr=[0.00019965525792610867, 0.00019965525792610867], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41550 loss: 1.0611 iter time (s): 4.700 samples/sec: 27.236
g0236:  iteration    41550/10000000 | consumed samples:      5318400 | consumed tokens:  10892083200 | elapsed time per iteration (ms): 4732.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.020938E+00 | loss scale: 8192.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.046 | tokens per gpu per second (tgs): 1730.930 | TFLOPs: 13.93 |
g0214: [2024-08-10 13:50:41,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=41560, skipped=63, lr=[0.00019965502899403997, 0.00019965502899403997], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41560 loss: 0.9535 iter time (s): 4.339 samples/sec: 29.501
g0236:  iteration    41560/10000000 | consumed samples:      5319680 | consumed tokens:  10894704640 | elapsed time per iteration (ms): 4371.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.890974E-01 | loss scale: 8192.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.278 | tokens per gpu per second (tgs): 1873.778 | TFLOPs: 15.08 |
g0214: [2024-08-10 13:51:32,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=41570, skipped=63, lr=[0.000199654799986116, 0.000199654799986116], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41570 loss: 0.9925 iter time (s): 5.040 samples/sec: 25.396
g0236:  iteration    41570/10000000 | consumed samples:      5320960 | consumed tokens:  10897326080 | elapsed time per iteration (ms): 5078.1 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.964050E-01 | loss scale: 8192.0 | grad norm: 0.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.206 | tokens per gpu per second (tgs): 1613.197 | TFLOPs: 12.98 |
g0214: [2024-08-10 13:52:17,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=41580, skipped=63, lr=[0.00019965457090233686, 0.00019965457090233686], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41580 loss: 0.9029 iter time (s): 4.519 samples/sec: 28.324
g0236:  iteration    41580/10000000 | consumed samples:      5322240 | consumed tokens:  10899947520 | elapsed time per iteration (ms): 4552.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.545336E-01 | loss scale: 8192.0 | grad norm: 0.323 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.115 | tokens per gpu per second (tgs): 1799.357 | TFLOPs: 14.48 |
g0214: [2024-08-10 13:53:09,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=41590, skipped=63, lr=[0.0001996543417427028, 0.0001996543417427028], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41590 loss: 0.9569 iter time (s): 5.126 samples/sec: 24.973
g0236:  iteration    41590/10000000 | consumed samples:      5323520 | consumed tokens:  10902568960 | elapsed time per iteration (ms): 5159.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.590704E-01 | loss scale: 8192.0 | grad norm: 0.297 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.809 | tokens per gpu per second (tgs): 1587.793 | TFLOPs: 12.78 |
g0214: [2024-08-10 13:53:58,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=41600, skipped=63, lr=[0.000199654112507214, 0.000199654112507214], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41600 loss: 1.0738 iter time (s): 4.848 samples/sec: 26.403
g0236:  iteration    41600/10000000 | consumed samples:      5324800 | consumed tokens:  10905190400 | elapsed time per iteration (ms): 4880.9 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.698238E-01 | loss scale: 8192.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.225 | tokens per gpu per second (tgs): 1678.379 | TFLOPs: 13.51 |
g0214: [2024-08-10 13:54:45,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=41610, skipped=63, lr=[0.0001996538831958706, 0.0001996538831958706], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41610 loss: 0.8657 iter time (s): 4.696 samples/sec: 27.259
g0236:  iteration    41610/10000000 | consumed samples:      5326080 | consumed tokens:  10907811840 | elapsed time per iteration (ms): 4730.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.905870E-01 | loss scale: 8192.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.058 | tokens per gpu per second (tgs): 1731.709 | TFLOPs: 13.94 |
g0214: [2024-08-10 13:55:32,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=41620, skipped=63, lr=[0.00019965365380867278, 0.00019965365380867278], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41620 loss: 1.1384 iter time (s): 4.694 samples/sec: 27.266
g0236:  iteration    41620/10000000 | consumed samples:      5327360 | consumed tokens:  10910433280 | elapsed time per iteration (ms): 4727.7 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.917069E-01 | loss scale: 8192.0 | grad norm: 0.157 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.075 | tokens per gpu per second (tgs): 1732.784 | TFLOPs: 13.94 |
g0214: [2024-08-10 13:56:23,372] [INFO] [logging.py:96:log_dist] [Rank 0] step=41630, skipped=63, lr=[0.00019965342434562068, 0.00019965342434562068], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41630 loss: 0.8821 iter time (s): 5.015 samples/sec: 25.522
g0236:  iteration    41630/10000000 | consumed samples:      5328640 | consumed tokens:  10913054720 | elapsed time per iteration (ms): 5047.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.788193E-01 | loss scale: 8192.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.357 | tokens per gpu per second (tgs): 1622.872 | TFLOPs: 13.06 |
g0214: [2024-08-10 13:57:11,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=41640, skipped=63, lr=[0.00019965319480671456, 0.00019965319480671456], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41640 loss: 1.0355 iter time (s): 4.779 samples/sec: 26.786
g0236:  iteration    41640/10000000 | consumed samples:      5329920 | consumed tokens:  10915676160 | elapsed time per iteration (ms): 4812.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.005091E+00 | loss scale: 8192.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.599 | tokens per gpu per second (tgs): 1702.306 | TFLOPs: 13.70 |
g0214: [2024-08-10 13:58:01,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=41650, skipped=63, lr=[0.00019965296519195452, 0.00019965296519195452], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41650 loss: 1.0769 iter time (s): 4.953 samples/sec: 25.843
g0236:  iteration    41650/10000000 | consumed samples:      5331200 | consumed tokens:  10918297600 | elapsed time per iteration (ms): 4987.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.022937E+00 | loss scale: 8192.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.666 | tokens per gpu per second (tgs): 1642.621 | TFLOPs: 13.22 |
g0214: [2024-08-10 13:58:49,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=41660, skipped=63, lr=[0.0001996527355013408, 0.0001996527355013408], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41660 loss: 0.8938 iter time (s): 4.818 samples/sec: 26.565
g0236:  iteration    41660/10000000 | consumed samples:      5332480 | consumed tokens:  10920919040 | elapsed time per iteration (ms): 4855.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.842372E-01 | loss scale: 8192.0 | grad norm: 0.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.363 | tokens per gpu per second (tgs): 1687.249 | TFLOPs: 13.58 |
g0235: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41669
g0233: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41669
g0234: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41669
g0233: Grad overflow on iteration 41669
g0236: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0236: Grad overflow on iteration 41669
g0233: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41669
g0234: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41669
g0233: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0235: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0214: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41669
g0214: Grad overflow on iteration 41669
g0234: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41669
g0234: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0235: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0234: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0236: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41669
g0234: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41669
g0214: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41669
g0233: Grad overflow on iteration 41669
g0234: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0220: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0220: Grad overflow on iteration 41669
g0236: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0214: Grad overflow on iteration 41669
g0233: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0232: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0234: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0214: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0214: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0232: Grad overflow on iteration 41669
g0235: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41669
g0220: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41669
g0235: Grad overflow on iteration 41669
g0220: Grad overflow on iteration 41669
g0236: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0225: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41669
g0236: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0236: Grad overflow on iteration 41669
g0214: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41669
g0214: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41669
g0232: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0232: Grad overflow on iteration 41669
g0236: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0220: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0236: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0225: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41669
g0232: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0225: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0232: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0225: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41669
g0232: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0232: Grad overflow on iteration 41669
g0232: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41669
g0225: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0232: Grad overflow on iteration 41669
g0235: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0220: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0220: Grad overflow on iteration 41669
g0232: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0220: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0220: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0214: [2024-08-10 13:59:36,313] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0236: [2024-08-10 13:59:36,314] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0
g0214: [2024-08-10 13:59:36,314] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0
g0214: [2024-08-10 13:59:36,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=41670, skipped=64, lr=[0.00019965252871493366, 0.00019965252871493366], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41670 loss: 1.0472 iter time (s): 4.605 samples/sec: 27.797
g0236:  iteration    41670/10000000 | consumed samples:      5333760 | consumed tokens:  10923540480 | elapsed time per iteration (ms): 4638.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.763322E-01 | loss scale: 4096.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.596 | tokens per gpu per second (tgs): 1766.121 | TFLOPs: 14.21 |
g0214: [2024-08-10 14:00:26,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=41680, skipped=64, lr=[0.0001996522758925529, 0.0001996522758925529], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41680 loss: 0.9521 iter time (s): 4.935 samples/sec: 25.935
g0236:  iteration    41680/10000000 | consumed samples:      5335040 | consumed tokens:  10926161920 | elapsed time per iteration (ms): 4984.8 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.758885E-01 | loss scale: 4096.0 | grad norm: 5.975 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.678 | tokens per gpu per second (tgs): 1643.405 | TFLOPs: 13.22 |
g0214: [2024-08-10 14:01:15,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=41690, skipped=64, lr=[0.00019965204597437907, 0.00019965204597437907], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41690 loss: 0.8999 iter time (s): 4.862 samples/sec: 26.324
g0236:  iteration    41690/10000000 | consumed samples:      5336320 | consumed tokens:  10928783360 | elapsed time per iteration (ms): 4904.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.637657E-01 | loss scale: 4096.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.100 | tokens per gpu per second (tgs): 1670.417 | TFLOPs: 13.44 |
g0214: [2024-08-10 14:02:07,892] [INFO] [logging.py:96:log_dist] [Rank 0] step=41700, skipped=64, lr=[0.00019965181598035226, 0.00019965181598035226], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41700 loss: 0.8781 iter time (s): 5.237 samples/sec: 24.443
g0236:  iteration    41700/10000000 | consumed samples:      5337600 | consumed tokens:  10931404800 | elapsed time per iteration (ms): 5269.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.013896E+00 | loss scale: 4096.0 | grad norm: 2.947 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.290 | tokens per gpu per second (tgs): 1554.581 | TFLOPs: 12.51 |
g0220: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41700
g0220: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0235: Grad overflow on iteration 41700
g0235: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41700
g0235: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41700
g0235: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0220: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41700
g0225: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0214: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41700
g0234: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41700
g0214: Grad overflow on iteration 41700
g0234: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41700
g0236: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41700
g0233: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41700
g0235: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0220: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0235: Grad overflow on iteration 41700
g0220: Grad overflow on iteration 41700
g0234: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41700
g0235: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0234: Grad overflow on iteration 41700
g0220: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0233: Grad overflow on iteration 41700
g0225: Grad overflow on iteration 41700
g0220: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0214: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0234: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41700
g0234: Grad overflow on iteration 41700
g0232: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41700
g0233: Grad overflow on iteration 41700
g0232: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0234: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0236: Grad overflow on iteration 41700
g0220: Grad overflow on iteration 41700
g0233: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0236: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0220: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0236: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0236: Grad overflow on iteration 41700
g0234: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0225: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41700
g0225: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0232: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0225: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0225: Grad overflow on iteration 41700
g0236: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0232: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0236: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41700
g0233: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41700
g0225: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0233: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0233: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0235: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0225: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0232: Grad overflow on iteration 41700
g0232: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0232: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0214: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41700
g0214: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0214: [2024-08-10 14:02:14,631] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0
g0214: [2024-08-10 14:02:14,630] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41700
g0214: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0236: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0214: [2024-08-10 14:02:14,631] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41700
g0214: [2024-08-10 14:02:14,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0
g0220: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41706
g0220: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41706
g0220: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41706
g0220: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0220: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0235: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41706
g0235: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41706
g0235: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0235: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41706
g0234: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41706
g0234: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0220: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41706
g0220: Grad overflow on iteration 41706
g0235: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0225: Grad overflow on iteration 41706
g0235: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0225: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41706
g0234: Grad overflow on iteration 41706
g0225: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0225: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41706
g0225: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0234: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41706
g0234: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0225: Grad overflow on iteration 41706
g0225: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0225: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0225: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0214: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41706
g0233: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0214: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0232: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41706
g0236: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41706
g0220: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0214: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0232: Grad overflow on iteration 41706
g0233: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0232: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41706
g0232: Grad overflow on iteration 41706
g0214: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41706
g0214: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0233: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41706
g0214: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0233: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41706
g0233: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0233: Grad overflow on iteration 41706
g0232: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0232: Grad overflow on iteration 41706
g0233: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0236: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41706
g0236: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0236: Grad overflow on iteration 41706
g0236: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0236: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41706
g0236: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0232: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41706
g0232: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0236: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0232: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0232: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0232: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0235: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41706
g0235: [2024-08-10 14:02:43,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0214: [2024-08-10 14:02:43,832] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41706
g0214: [2024-08-10 14:02:43,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2048.0 to 1024.0
g0214: [2024-08-10 14:02:43,835] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2048.0, reducing to 1024.0
g0235: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41708
g0235: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0235: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41708
g0235: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0235: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41708
g0235: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0236: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41708
g0236: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41708
g0236: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0232: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41708
g0220: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41708
g0232: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41708
g0220: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0236: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0236: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41708
g0236: Grad overflow on iteration 41708
g0220: Grad overflow on iteration 41708
g0234: Grad overflow on iteration 41708
g0225: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41708
g0225: Grad overflow on iteration 41708
g0232: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0214: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0233: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0225: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41708
g0234: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0232: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41708
g0225: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0232: Grad overflow on iteration 41708
g0225: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0232: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0233: Grad overflow on iteration 41708
g0232: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0232: Grad overflow on iteration 41708
g0235: Grad overflow on iteration 41708
g0232: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0214: Grad overflow on iteration 41708
g0236: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0225: Grad overflow on iteration 41708
g0232: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0214: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0234: Grad overflow on iteration 41708
g0214: Grad overflow on iteration 41708
g0225: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0214: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0233: Grad overflow on iteration 41708
g0214: Grad overflow on iteration 41708
g0233: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41708
g0234: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41708
g0233: Grad overflow on iteration 41708
g0234: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0234: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41708
g0220: Grad overflow on iteration 41708
g0234: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0234: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0235: [2024-08-10 14:02:52,387] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0214: Grad overflow on iteration 41708
g0233: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0214: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0214: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0233: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0214: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0233: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0214: [2024-08-10 14:02:52,386] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0220: [2024-08-10 14:02:52,387] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0214: [2024-08-10 14:02:52,387] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1024.0, reducing to 512.0
g0214: [2024-08-10 14:02:57,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=41710, skipped=67, lr=[0.00019965158591047264, 0.00019965158591047264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41710 loss: 0.9723 iter time (s): 4.890 samples/sec: 26.175
g0236:  iteration    41710/10000000 | consumed samples:      5338880 | consumed tokens:  10934026240 | elapsed time per iteration (ms): 4923.5 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 512.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.998 | tokens per gpu per second (tgs): 1663.873 | TFLOPs: 13.39 |
g0214: [2024-08-10 14:03:45,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=41720, skipped=67, lr=[0.00019965135576474033, 0.00019965135576474033], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41720 loss: 0.8632 iter time (s): 4.755 samples/sec: 26.917
g0236:  iteration    41720/10000000 | consumed samples:      5340160 | consumed tokens:  10936647680 | elapsed time per iteration (ms): 4789.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.491265E-01 | loss scale: 512.0 | grad norm: 1.986 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.727 | tokens per gpu per second (tgs): 1710.525 | TFLOPs: 13.76 |
g0225: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41724
g0225: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41724
g0225: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0225: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0225: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41724
g0225: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41724
g0225: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0235: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41724
g0220: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41724
g0235: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41724
g0214: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41724
g0220: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41724
g0214: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41724
g0214: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41724
g0214: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0235: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0220: Grad overflow on iteration 41724
g0220: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0220: Grad overflow on iteration 41724
g0225: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0220: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0233: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41724
g0214: Grad overflow on iteration 41724
g0232: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41724
g0234: Grad overflow on iteration 41724
g0234: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41724
g0234: Grad overflow on iteration 41724
g0236: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0233: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41724
g0220: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0233: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0232: Grad overflow on iteration 41724
g0233: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0236: Grad overflow on iteration 41724
g0220: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41724
g0233: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0236: Grad overflow on iteration 41724
g0220: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0234: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0233: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41724
g0220: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0233: Grad overflow on iteration 41724
g0235: Grad overflow on iteration 41724
g0232: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0214: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0232: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41724
g0214: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0232: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0236: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0235: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41724
g0232: Grad overflow on iteration 41724
g0236: Grad overflow on iteration 41724
g0235: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0236: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0235: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0234: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 14:04:10,095] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 512.0, reducing to 256.0
g0236: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0234: Grad overflow on iteration 41724
g0235: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0232: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0233: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0236: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0234: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0234: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0234: [2024-08-10 14:04:10,095] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0225: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41729
g0225: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41729
g0225: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0225: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41729
g0225: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0225: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0225: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41729
g0234: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41729
g0234: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41729
g0234: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0225: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0234: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41729
g0234: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0234: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0214: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41729
g0220: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41729
g0220: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41729
g0220: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0214: Grad overflow on iteration 41729
g0234: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41729
g0234: Grad overflow on iteration 41729
g0214: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41729
g0220: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0214: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0220: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0234: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0233: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0235: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41729
g0232: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0232: Grad overflow on iteration 41729
g0220: Grad overflow on iteration 41729
g0232: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0220: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0232: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41729
g0233: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0235: Grad overflow on iteration 41729
g0236: Grad overflow on iteration 41729
g0233: Grad overflow on iteration 41729
g0214: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0233: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0233: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41729
g0233: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41729
g0233: Grad overflow on iteration 41729
g0232: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41729
g0236: Grad overflow on iteration 41729
g0233: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0232: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41729
g0235: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0236: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0232: Grad overflow on iteration 41729
g0236: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0235: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0235: Grad overflow on iteration 41729
g0235: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41729
g0236: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0232: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0235: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0235: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0235: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0236: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41729
g0236: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0233: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0214: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41729
g0214: [2024-08-10 14:04:35,145] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0214: [2024-08-10 14:04:35,145] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 256.0, reducing to 128.0
g0214: [2024-08-10 14:04:35,146] [INFO] [logging.py:96:log_dist] [Rank 0] step=41730, skipped=69, lr=[0.0001996511485687274, 0.0001996511485687274], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41730 loss: nan iter time (s): 4.978 samples/sec: 25.712
g0236:  iteration    41730/10000000 | consumed samples:      5341440 | consumed tokens:  10939269120 | elapsed time per iteration (ms): 5011.7 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 128.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.540 | tokens per gpu per second (tgs): 1634.583 | TFLOPs: 13.15 |
g0214: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41731
g0214: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0214: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41731
g0214: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 41731
g0214: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0235: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41731
g0235: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41731
g0235: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41731
g0235: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0235: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0235: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0225: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41731
g0233: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41731
g0233: Grad overflow on iteration 41731
g0220: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41731
g0233: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0235: Grad overflow on iteration 41731
g0225: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0225: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41731
g0233: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0236: Grad overflow on iteration 41731
g0233: Grad overflow on iteration 41731
g0236: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0225: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41731
g0236: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0232: Grad overflow on iteration 41731
g0232: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41731
g0233: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41731
g0233: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0233: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0234: Grad overflow on iteration 41731
g0232: Grad overflow on iteration 41731
g0234: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41731
g0225: Grad overflow on iteration 41731
g0225: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0220: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0225: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0232: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0232: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 41731
g0236: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41731
g0232: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0236: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0236: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41731
g0236: Grad overflow on iteration 41731
g0220: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0234: Grad overflow on iteration 41731
g0220: Grad overflow on iteration 41731
g0236: [2024-08-10 14:04:44,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0220: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0220: Grad overflow on iteration 41731
g0234: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0220: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0234: Grad overflow on iteration 41731
g0214: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0214: Grad overflow on iteration 41731
g0234: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0214: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0220: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0214: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0234: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0234: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41731
g0234: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0234: [2024-08-10 14:04:44,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0235: [2024-08-10 14:04:44,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0214: [2024-08-10 14:04:44,033] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 128.0, reducing to 64.0
g0214: [2024-08-10 14:05:22,473] [INFO] [logging.py:96:log_dist] [Rank 0] step=41740, skipped=70, lr=[0.0001996508952457185, 0.0001996508952457185], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41740 loss: 1.1439 iter time (s): 4.700 samples/sec: 27.233
g0236:  iteration    41740/10000000 | consumed samples:      5342720 | consumed tokens:  10941890560 | elapsed time per iteration (ms): 4733.7 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 64.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.040 | tokens per gpu per second (tgs): 1730.561 | TFLOPs: 13.93 |
g0214: [2024-08-10 14:06:12,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=41750, skipped=70, lr=[0.00019965066487242933, 0.00019965066487242933], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41750 loss: 0.9858 iter time (s): 4.996 samples/sec: 25.619
g0236:  iteration    41750/10000000 | consumed samples:      5344000 | consumed tokens:  10944512000 | elapsed time per iteration (ms): 5034.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.008093E+00 | loss scale: 64.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.424 | tokens per gpu per second (tgs): 1627.127 | TFLOPs: 13.09 |
g0214: [2024-08-10 14:07:03,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=41760, skipped=70, lr=[0.0001996504344232882, 0.0001996504344232882], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41760 loss: 1.0816 iter time (s): 5.079 samples/sec: 25.203
g0236:  iteration    41760/10000000 | consumed samples:      5345280 | consumed tokens:  10947133440 | elapsed time per iteration (ms): 5112.6 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 1.026690E+00 | loss scale: 64.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.036 | tokens per gpu per second (tgs): 1602.303 | TFLOPs: 12.89 |
g0214: [2024-08-10 14:07:47,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=41770, skipped=70, lr=[0.0001996502038982953, 0.0001996502038982953], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41770 loss: 1.0345 iter time (s): 4.310 samples/sec: 29.696
g0236:  iteration    41770/10000000 | consumed samples:      5346560 | consumed tokens:  10949754880 | elapsed time per iteration (ms): 4344.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.813284E-01 | loss scale: 64.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.465 | tokens per gpu per second (tgs): 1885.746 | TFLOPs: 15.17 |
g0214: [2024-08-10 14:08:42,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=41780, skipped=70, lr=[0.00019964997329745084, 0.00019964997329745084], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41780 loss: 0.9871 iter time (s): 5.481 samples/sec: 23.355
g0236:  iteration    41780/10000000 | consumed samples:      5347840 | consumed tokens:  10952376320 | elapsed time per iteration (ms): 5513.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.032612E+00 | loss scale: 64.0 | grad norm: 0.432 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.215 | tokens per gpu per second (tgs): 1485.731 | TFLOPs: 11.96 |
g0214: [2024-08-10 14:09:30,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=41790, skipped=70, lr=[0.00019964974262075494, 0.00019964974262075494], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41790 loss: 1.2082 iter time (s): 4.754 samples/sec: 26.925
g0236:  iteration    41790/10000000 | consumed samples:      5349120 | consumed tokens:  10954997760 | elapsed time per iteration (ms): 4787.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 9.525201E-01 | loss scale: 64.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.738 | tokens per gpu per second (tgs): 1711.219 | TFLOPs: 13.77 |
g0214: [2024-08-10 14:10:21,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=41800, skipped=70, lr=[0.00019964951186820783, 0.00019964951186820783], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41800 loss: 1.0984 iter time (s): 5.054 samples/sec: 25.328
g0236:  iteration    41800/10000000 | consumed samples:      5350400 | consumed tokens:  10957619200 | elapsed time per iteration (ms): 5087.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.019342E+00 | loss scale: 64.0 | grad norm: 0.330 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.161 | tokens per gpu per second (tgs): 1610.294 | TFLOPs: 12.96 |
g0214: [2024-08-10 14:11:16,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=41810, skipped=70, lr=[0.00019964928103980965, 0.00019964928103980965], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41810 loss: 0.8647 iter time (s): 5.440 samples/sec: 23.528
g0236:  iteration    41810/10000000 | consumed samples:      5351680 | consumed tokens:  10960240640 | elapsed time per iteration (ms): 5473.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.002775E+00 | loss scale: 64.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.386 | tokens per gpu per second (tgs): 1496.720 | TFLOPs: 12.04 |
g0214: [2024-08-10 14:12:09,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=41820, skipped=70, lr=[0.0001996490501355606, 0.0001996490501355606], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41820 loss: 1.0955 iter time (s): 5.362 samples/sec: 23.870
g0236:  iteration    41820/10000000 | consumed samples:      5352960 | consumed tokens:  10962862080 | elapsed time per iteration (ms): 5395.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.009481E+00 | loss scale: 64.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.724 | tokens per gpu per second (tgs): 1518.331 | TFLOPs: 12.22 |
g0214: [2024-08-10 14:13:02,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=41830, skipped=70, lr=[0.00019964881915546083, 0.00019964881915546083], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41830 loss: 0.9804 iter time (s): 5.186 samples/sec: 24.683
g0236:  iteration    41830/10000000 | consumed samples:      5354240 | consumed tokens:  10965483520 | elapsed time per iteration (ms): 5220.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.020100E+00 | loss scale: 64.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.519 | tokens per gpu per second (tgs): 1569.248 | TFLOPs: 12.63 |
g0214: [2024-08-10 14:13:56,692] [INFO] [logging.py:96:log_dist] [Rank 0] step=41840, skipped=70, lr=[0.00019964858809951055, 0.00019964858809951055], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41840 loss: 0.9554 iter time (s): 5.420 samples/sec: 23.618
g0236:  iteration    41840/10000000 | consumed samples:      5355520 | consumed tokens:  10968104960 | elapsed time per iteration (ms): 5453.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 9.412409E-01 | loss scale: 64.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.473 | tokens per gpu per second (tgs): 1502.282 | TFLOPs: 12.09 |
g0214: [2024-08-10 14:14:47,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=41850, skipped=70, lr=[0.00019964835696770992, 0.00019964835696770992], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41850 loss: 1.1732 iter time (s): 5.057 samples/sec: 25.313
g0236:  iteration    41850/10000000 | consumed samples:      5356800 | consumed tokens:  10970726400 | elapsed time per iteration (ms): 5090.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.047645E+00 | loss scale: 64.0 | grad norm: 0.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.148 | tokens per gpu per second (tgs): 1609.441 | TFLOPs: 12.95 |
g0214: [2024-08-10 14:15:36,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=41860, skipped=70, lr=[0.00019964812576005913, 0.00019964812576005913], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41860 loss: 1.0765 iter time (s): 4.866 samples/sec: 26.303
g0236:  iteration    41860/10000000 | consumed samples:      5358080 | consumed tokens:  10973347840 | elapsed time per iteration (ms): 4899.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.031054E+00 | loss scale: 64.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.125 | tokens per gpu per second (tgs): 1672.006 | TFLOPs: 13.45 |
g0214: [2024-08-10 14:16:25,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=41870, skipped=70, lr=[0.00019964789447655834, 0.00019964789447655834], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41870 loss: 1.0925 iter time (s): 4.881 samples/sec: 26.222
g0236:  iteration    41870/10000000 | consumed samples:      5359360 | consumed tokens:  10975969280 | elapsed time per iteration (ms): 4915.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 9.359877E-01 | loss scale: 64.0 | grad norm: 0.290 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.042 | tokens per gpu per second (tgs): 1666.703 | TFLOPs: 13.41 |
g0214: [2024-08-10 14:17:14,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=41880, skipped=70, lr=[0.00019964766311720775, 0.00019964766311720775], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41880 loss: 0.8898 iter time (s): 4.875 samples/sec: 26.257
g0236:  iteration    41880/10000000 | consumed samples:      5360640 | consumed tokens:  10978590720 | elapsed time per iteration (ms): 4907.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.007296E+00 | loss scale: 64.0 | grad norm: 0.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.082 | tokens per gpu per second (tgs): 1669.258 | TFLOPs: 13.43 |
g0214: [2024-08-10 14:18:07,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=41890, skipped=70, lr=[0.0001996474316820075, 0.0001996474316820075], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41890 loss: 0.9856 iter time (s): 5.238 samples/sec: 24.437
g0236:  iteration    41890/10000000 | consumed samples:      5361920 | consumed tokens:  10981212160 | elapsed time per iteration (ms): 5270.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.032725E+00 | loss scale: 64.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.285 | tokens per gpu per second (tgs): 1554.256 | TFLOPs: 12.51 |
g0214: [2024-08-10 14:19:01,848] [INFO] [logging.py:96:log_dist] [Rank 0] step=41900, skipped=70, lr=[0.00019964720017095783, 0.00019964720017095783], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41900 loss: 1.0585 iter time (s): 5.399 samples/sec: 23.709
g0236:  iteration    41900/10000000 | consumed samples:      5363200 | consumed tokens:  10983833600 | elapsed time per iteration (ms): 5432.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 9.924485E-01 | loss scale: 64.0 | grad norm: 0.238 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.561 | tokens per gpu per second (tgs): 1507.912 | TFLOPs: 12.13 |
g0214: [2024-08-10 14:19:51,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=41910, skipped=70, lr=[0.00019964696858405884, 0.00019964696858405884], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41910 loss: 0.9401 iter time (s): 4.934 samples/sec: 25.945
g0236:  iteration    41910/10000000 | consumed samples:      5364480 | consumed tokens:  10986455040 | elapsed time per iteration (ms): 4968.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.012825E+00 | loss scale: 64.0 | grad norm: 0.313 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.764 | tokens per gpu per second (tgs): 1648.890 | TFLOPs: 13.27 |
g0236: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41912
g0235: Grad overflow on iteration 41912
g0214: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41912
g0235: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0236: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0236: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41912
g0236: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0235: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41912
g0214: Grad overflow on iteration 41912
g0214: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: Grad overflow on iteration 41912
g0236: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0236: Grad overflow on iteration 41912
g0235: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41912
g0214: Grad overflow on iteration 41912
g0225: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 41912
g0220: Grad overflow on iteration 41912
g0225: Grad overflow on iteration 41912
g0235: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0225: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0225: Grad overflow on iteration 41912
g0220: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 41912
g0220: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0220: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0233: Grad overflow on iteration 41912
g0220: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0220: Grad overflow on iteration 41912
g0233: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0232: Grad overflow on iteration 41912
g0225: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0234: Grad overflow on iteration 41912
g0233: Grad overflow on iteration 41912
g0234: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0225: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0225: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 41912
g0225: Grad overflow on iteration 41912
g0232: Grad overflow on iteration 41912
g0220: [2024-08-10 14:20:05,560] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0232: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0225: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 41912
g0236: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0232: Grad overflow on iteration 41912
g0232: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0234: Grad overflow on iteration 41912
g0234: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 14:20:05,560] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0234: Grad overflow on iteration 41912
g0232: Grad overflow on iteration 41912
g0233: Grad overflow on iteration 41912
g0225: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0233: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0232: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0234: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0234: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0234: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 41912
g0234: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: Grad overflow on iteration 41912
g0234: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0233: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 41912
g0214: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: [2024-08-10 14:20:05,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: [2024-08-10 14:20:05,560] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 64.0, reducing to 32.0
g0225: [2024-08-10 14:20:05,560] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0233: [2024-08-10 14:20:05,560] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: [2024-08-10 14:20:40,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=41920, skipped=71, lr=[0.00019964673692131078, 0.00019964673692131078], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41920 loss: 1.0852 iter time (s): 4.819 samples/sec: 26.561
g0236:  iteration    41920/10000000 | consumed samples:      5365760 | consumed tokens:  10989076480 | elapsed time per iteration (ms): 4852.2 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 32.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.380 | tokens per gpu per second (tgs): 1688.309 | TFLOPs: 13.59 |
g0214: [2024-08-10 14:21:29,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=41930, skipped=71, lr=[0.00019964650518271377, 0.00019964650518271377], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41930 loss: 0.9813 iter time (s): 4.884 samples/sec: 26.210
g0236:  iteration    41930/10000000 | consumed samples:      5367040 | consumed tokens:  10991697920 | elapsed time per iteration (ms): 4917.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 9.879885E-01 | loss scale: 32.0 | grad norm: 0.256 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.032 | tokens per gpu per second (tgs): 1666.035 | TFLOPs: 13.41 |
g0214: [2024-08-10 14:22:14,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=41940, skipped=71, lr=[0.00019964627336826806, 0.00019964627336826806], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41940 loss: 1.0839 iter time (s): 4.483 samples/sec: 28.550
g0236:  iteration    41940/10000000 | consumed samples:      5368320 | consumed tokens:  10994319360 | elapsed time per iteration (ms): 4517.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.045496E+00 | loss scale: 32.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.333 | tokens per gpu per second (tgs): 1813.295 | TFLOPs: 14.59 |
g0214: [2024-08-10 14:22:58,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=41950, skipped=71, lr=[0.00019964604147797377, 0.00019964604147797377], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41950 loss: 1.2279 iter time (s): 4.352 samples/sec: 29.414
g0236:  iteration    41950/10000000 | consumed samples:      5369600 | consumed tokens:  10996940800 | elapsed time per iteration (ms): 4384.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.058321E+00 | loss scale: 32.0 | grad norm: 0.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.194 | tokens per gpu per second (tgs): 1868.446 | TFLOPs: 15.04 |
g0214: [2024-08-10 14:23:48,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=41960, skipped=71, lr=[0.00019964580951183108, 0.00019964580951183108], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41960 loss: 1.0554 iter time (s): 4.986 samples/sec: 25.671
g0236:  iteration    41960/10000000 | consumed samples:      5370880 | consumed tokens:  10999562240 | elapsed time per iteration (ms): 5023.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.021880E+00 | loss scale: 32.0 | grad norm: 0.266 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.479 | tokens per gpu per second (tgs): 1630.658 | TFLOPs: 13.12 |
g0214: [2024-08-10 14:24:34,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=41970, skipped=71, lr=[0.00019964557746984018, 0.00019964557746984018], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41970 loss: 0.8024 iter time (s): 4.601 samples/sec: 27.822
g0236:  iteration    41970/10000000 | consumed samples:      5372160 | consumed tokens:  11002183680 | elapsed time per iteration (ms): 4633.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 9.726606E-01 | loss scale: 32.0 | grad norm: 0.284 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.624 | tokens per gpu per second (tgs): 1767.940 | TFLOPs: 14.23 |
g0214: [2024-08-10 14:25:25,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=41980, skipped=71, lr=[0.00019964534535200126, 0.00019964534535200126], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41980 loss: 1.0097 iter time (s): 4.994 samples/sec: 25.632
g0236:  iteration    41980/10000000 | consumed samples:      5373440 | consumed tokens:  11004805120 | elapsed time per iteration (ms): 5027.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 9.698364E-01 | loss scale: 32.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.461 | tokens per gpu per second (tgs): 1629.521 | TFLOPs: 13.11 |
g0214: [2024-08-10 14:26:16,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=41990, skipped=71, lr=[0.00019964511315831446, 0.00019964511315831446], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 41990 loss: 0.9259 iter time (s): 5.142 samples/sec: 24.891
g0236:  iteration    41990/10000000 | consumed samples:      5374720 | consumed tokens:  11007426560 | elapsed time per iteration (ms): 5176.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.006292E+00 | loss scale: 32.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.730 | tokens per gpu per second (tgs): 1582.692 | TFLOPs: 12.74 |
g0214: [2024-08-10 14:27:03,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=42000, skipped=71, lr=[0.00019964488088878, 0.00019964488088878], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42000 loss: 0.8886 iter time (s): 4.670 samples/sec: 27.407
g0236:  iteration    42000/10000000 | consumed samples:      5376000 | consumed tokens:  11010048000 | elapsed time per iteration (ms): 4713.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 9.983244E-01 | loss scale: 32.0 | grad norm: 0.316 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.155 | tokens per gpu per second (tgs): 1737.907 | TFLOPs: 13.99 |
g0236: -------------------------------------------------------------------------------------------------
g0236:  validation loss at iteration 42000 | lm loss value: 9.993436E-01 | lm loss PPL: 2.716498E+00 | 
g0236: -------------------------------------------------------------------------------------------------
g0214: saving checkpoint at iteration   42000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214: [2024-08-10 14:36:34,139] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step42000 is about to be saved!
g0214: [2024-08-10 14:36:34,144] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0214: [2024-08-10 14:36:34,144] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0214: [2024-08-10 14:36:34,144] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0236: [2024-08-10 14:36:34,145] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0236: [2024-08-10 14:36:34,145] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0236: [2024-08-10 14:36:34,146] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0220: [2024-08-10 14:36:34,147] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0220: [2024-08-10 14:36:34,148] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0220: [2024-08-10 14:36:34,148] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0225: [2024-08-10 14:36:34,148] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0225: [2024-08-10 14:36:34,148] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0225: [2024-08-10 14:36:34,148] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0233: [2024-08-10 14:36:34,149] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0233: [2024-08-10 14:36:34,149] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0233: [2024-08-10 14:36:34,149] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0234: [2024-08-10 14:36:34,150] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0234: [2024-08-10 14:36:34,150] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0234: [2024-08-10 14:36:34,150] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0235: [2024-08-10 14:36:34,151] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0235: [2024-08-10 14:36:34,151] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0235: [2024-08-10 14:36:34,151] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0236: [2024-08-10 14:36:34,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0220: [2024-08-10 14:36:34,181] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0233: [2024-08-10 14:36:34,182] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0234: [2024-08-10 14:36:34,182] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0232: [2024-08-10 14:36:34,183] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0232: [2024-08-10 14:36:34,183] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0232: [2024-08-10 14:36:34,183] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0225: [2024-08-10 14:36:34,183] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0235: [2024-08-10 14:36:34,184] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0214: [2024-08-10 14:36:34,193] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0232: [2024-08-10 14:36:34,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0235: [2024-08-10 14:36:34,312] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0234: [2024-08-10 14:36:34,324] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0220: [2024-08-10 14:36:34,339] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0235: [2024-08-10 14:36:34,348] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0233: [2024-08-10 14:36:34,350] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0234: [2024-08-10 14:36:34,360] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0220: [2024-08-10 14:36:34,372] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0233: [2024-08-10 14:36:34,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0236: [2024-08-10 14:36:34,409] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 14:36:34,410] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0236: [2024-08-10 14:36:34,412] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0232: [2024-08-10 14:36:34,422] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0214: [2024-08-10 14:36:34,431] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0232: [2024-08-10 14:36:34,456] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0236: [2024-08-10 14:36:34,458] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0214: [2024-08-10 14:36:34,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0225: [2024-08-10 14:36:34,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0220: [2024-08-10 14:36:34,499] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0235: [2024-08-10 14:36:34,506] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0234: [2024-08-10 14:36:34,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0225: [2024-08-10 14:36:34,517] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0220: [2024-08-10 14:36:34,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0234: [2024-08-10 14:36:34,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0235: [2024-08-10 14:36:34,537] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0232: [2024-08-10 14:36:34,595] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0214: [2024-08-10 14:36:34,598] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0233: [2024-08-10 14:36:34,608] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0220: [2024-08-10 14:36:34,618] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 14:36:34,619] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0232: [2024-08-10 14:36:34,624] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0236: [2024-08-10 14:36:34,631] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 14:36:34,632] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0214: [2024-08-10 14:36:34,635] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0233: [2024-08-10 14:36:34,638] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0225: [2024-08-10 14:36:34,651] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0234: [2024-08-10 14:36:34,656] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 14:36:34,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0225: [2024-08-10 14:36:34,683] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0235: [2024-08-10 14:36:34,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 14:36:34,697] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0214: [2024-08-10 14:36:34,773] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0232: [2024-08-10 14:36:34,785] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0232: [2024-08-10 14:36:34,787] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0225: [2024-08-10 14:36:34,792] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0225: [2024-08-10 14:36:34,795] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0214: [2024-08-10 14:36:34,801] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0233: [2024-08-10 14:36:34,801] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 14:36:34,804] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0214: [2024-08-10 14:36:34,907] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 14:36:34,909] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt
g0214: [2024-08-10 14:36:34,909] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0220: [2024-08-10 14:36:36,910] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0220: [2024-08-10 14:36:36,911] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0236: [2024-08-10 14:36:36,922] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0236: [2024-08-10 14:36:36,922] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0234: [2024-08-10 14:36:36,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0234: [2024-08-10 14:36:36,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0235: [2024-08-10 14:36:36,998] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0235: [2024-08-10 14:36:36,998] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0232: [2024-08-10 14:36:37,078] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0232: [2024-08-10 14:36:37,079] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0225: [2024-08-10 14:36:37,123] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0225: [2024-08-10 14:36:37,124] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0233: [2024-08-10 14:36:37,255] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0233: [2024-08-10 14:36:37,256] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0214: [2024-08-10 14:36:38,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 14:36:38,336] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step42000 is ready now!
g0214:   successfully saved checkpoint at iteration   42000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214: Checkpoint Save GB: 22.521, GB/Sec: 5.35, Latency(second): 4.212
g0236: (min, max) time across ranks (ms):
g0236:     save-checkpoint ................................: (4212.15, 4212.37)
g0214: [2024-08-10 14:37:29,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=42010, skipped=71, lr=[0.00019964464854339806, 0.00019964464854339806], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42010 loss: 1.2145 iter time (s): 5.107 samples/sec: 25.066
g0236:  iteration    42010/10000000 | consumed samples:      5377280 | consumed tokens:  11012669440 | elapsed time per iteration (ms): 62570.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.065875E+00 | loss scale: 32.0 | grad norm: 0.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.046 | tokens per gpu per second (tgs): 130.923 | TFLOPs: 1.05 |
g0214: [2024-08-10 14:38:14,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=42020, skipped=71, lr=[0.0001996444161221688, 0.0001996444161221688], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42020 loss: 0.9147 iter time (s): 4.445 samples/sec: 28.799
g0236:  iteration    42020/10000000 | consumed samples:      5378560 | consumed tokens:  11015290880 | elapsed time per iteration (ms): 4476.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.013335E+00 | loss scale: 32.0 | grad norm: 0.321 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.591 | tokens per gpu per second (tgs): 1829.819 | TFLOPs: 14.72 |
g0214: [2024-08-10 14:39:07,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=42030, skipped=71, lr=[0.0001996441836250924, 0.0001996441836250924], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42030 loss: 1.0218 iter time (s): 5.317 samples/sec: 24.076
g0236:  iteration    42030/10000000 | consumed samples:      5379840 | consumed tokens:  11017912320 | elapsed time per iteration (ms): 5348.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.002518E+00 | loss scale: 32.0 | grad norm: 0.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.930 | tokens per gpu per second (tgs): 1531.529 | TFLOPs: 12.32 |
g0214: [2024-08-10 14:40:00,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=42040, skipped=71, lr=[0.00019964395105216904, 0.00019964395105216904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42040 loss: 1.0016 iter time (s): 5.252 samples/sec: 24.370
g0236:  iteration    42040/10000000 | consumed samples:      5381120 | consumed tokens:  11020533760 | elapsed time per iteration (ms): 5298.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.037753E+00 | loss scale: 32.0 | grad norm: 0.349 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.157 | tokens per gpu per second (tgs): 1546.060 | TFLOPs: 12.44 |
g0214: [2024-08-10 14:40:48,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=42050, skipped=71, lr=[0.00019964371840339892, 0.00019964371840339892], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42050 loss: 0.9455 iter time (s): 4.724 samples/sec: 27.095
g0236:  iteration    42050/10000000 | consumed samples:      5382400 | consumed tokens:  11023155200 | elapsed time per iteration (ms): 4756.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.079649E+00 | loss scale: 32.0 | grad norm: 0.338 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.910 | tokens per gpu per second (tgs): 1722.241 | TFLOPs: 13.86 |
g0214: [2024-08-10 14:41:40,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=42060, skipped=71, lr=[0.00019964348567878216, 0.00019964348567878216], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42060 loss: 0.9229 iter time (s): 5.175 samples/sec: 24.735
g0236:  iteration    42060/10000000 | consumed samples:      5383680 | consumed tokens:  11025776640 | elapsed time per iteration (ms): 5214.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.010711E+00 | loss scale: 32.0 | grad norm: 0.257 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.549 | tokens per gpu per second (tgs): 1571.134 | TFLOPs: 12.64 |
g0214: [2024-08-10 14:42:30,858] [INFO] [logging.py:96:log_dist] [Rank 0] step=42070, skipped=71, lr=[0.00019964325287831898, 0.00019964325287831898], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42070 loss: 1.1925 iter time (s): 4.988 samples/sec: 25.663
g0236:  iteration    42070/10000000 | consumed samples:      5384960 | consumed tokens:  11028398080 | elapsed time per iteration (ms): 5020.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.055009E+00 | loss scale: 32.0 | grad norm: 0.289 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.494 | tokens per gpu per second (tgs): 1631.639 | TFLOPs: 13.13 |
g0214: [2024-08-10 14:43:27,345] [INFO] [logging.py:96:log_dist] [Rank 0] step=42080, skipped=71, lr=[0.0001996430200020096, 0.0001996430200020096], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42080 loss: 0.9458 iter time (s): 5.616 samples/sec: 22.793
g0236:  iteration    42080/10000000 | consumed samples:      5386240 | consumed tokens:  11031019520 | elapsed time per iteration (ms): 5648.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 9.580911E-01 | loss scale: 32.0 | grad norm: 0.309 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.660 | tokens per gpu per second (tgs): 1450.252 | TFLOPs: 11.67 |
g0214: [2024-08-10 14:44:17,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=42090, skipped=71, lr=[0.0001996427870498541, 0.0001996427870498541], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42090 loss: 1.2606 iter time (s): 4.989 samples/sec: 25.655
g0236:  iteration    42090/10000000 | consumed samples:      5387520 | consumed tokens:  11033640960 | elapsed time per iteration (ms): 5021.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.072459E+00 | loss scale: 32.0 | grad norm: 0.269 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.489 | tokens per gpu per second (tgs): 1631.310 | TFLOPs: 13.13 |
g0214: [2024-08-10 14:45:07,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=42100, skipped=71, lr=[0.00019964255402185274, 0.00019964255402185274], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42100 loss: 0.9409 iter time (s): 4.958 samples/sec: 25.819
g0236:  iteration    42100/10000000 | consumed samples:      5388800 | consumed tokens:  11036262400 | elapsed time per iteration (ms): 4989.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.078581E+00 | loss scale: 32.0 | grad norm: 0.400 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.652 | tokens per gpu per second (tgs): 1641.708 | TFLOPs: 13.21 |
g0214: [2024-08-10 14:45:55,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=42110, skipped=71, lr=[0.00019964232091800567, 0.00019964232091800567], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42110 loss: 1.0780 iter time (s): 4.754 samples/sec: 26.926
g0236:  iteration    42110/10000000 | consumed samples:      5390080 | consumed tokens:  11038883840 | elapsed time per iteration (ms): 4791.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.065036E+00 | loss scale: 32.0 | grad norm: 0.523 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.717 | tokens per gpu per second (tgs): 1709.860 | TFLOPs: 13.76 |
g0214: [2024-08-10 14:46:56,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=42120, skipped=71, lr=[0.00019964208773831307, 0.00019964208773831307], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42120 loss: 1.0568 iter time (s): 6.119 samples/sec: 20.920
g0236:  iteration    42120/10000000 | consumed samples:      5391360 | consumed tokens:  11041505280 | elapsed time per iteration (ms): 6151.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.045378E+00 | loss scale: 32.0 | grad norm: 0.512 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.809 | tokens per gpu per second (tgs): 1331.792 | TFLOPs: 10.72 |
g0214: [2024-08-10 14:47:58,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=42130, skipped=71, lr=[0.00019964185448277514, 0.00019964185448277514], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42130 loss: 0.9951 iter time (s): 6.078 samples/sec: 21.060
g0236:  iteration    42130/10000000 | consumed samples:      5392640 | consumed tokens:  11044126720 | elapsed time per iteration (ms): 6111.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.088941E+00 | loss scale: 32.0 | grad norm: 0.665 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.943 | tokens per gpu per second (tgs): 1340.357 | TFLOPs: 10.79 |
g0214: [2024-08-10 14:49:16,345] [INFO] [logging.py:96:log_dist] [Rank 0] step=42140, skipped=71, lr=[0.00019964162115139204, 0.00019964162115139204], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42140 loss: 1.1374 iter time (s): 7.801 samples/sec: 16.409
g0236:  iteration    42140/10000000 | consumed samples:      5393920 | consumed tokens:  11046748160 | elapsed time per iteration (ms): 7834.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.077640E+00 | loss scale: 32.0 | grad norm: 0.618 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.339 | tokens per gpu per second (tgs): 1045.666 | TFLOPs: 8.41 |
g0214: [2024-08-10 14:50:24,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=42150, skipped=71, lr=[0.00019964138774416392, 0.00019964138774416392], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42150 loss: 1.0937 iter time (s): 6.828 samples/sec: 18.748
g0236:  iteration    42150/10000000 | consumed samples:      5395200 | consumed tokens:  11049369600 | elapsed time per iteration (ms): 6859.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.082629E+00 | loss scale: 32.0 | grad norm: 0.481 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.659 | tokens per gpu per second (tgs): 1194.181 | TFLOPs: 9.61 |
g0214: [2024-08-10 14:51:34,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=42160, skipped=71, lr=[0.000199641154261091, 0.000199641154261091], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42160 loss: 0.9624 iter time (s): 6.880 samples/sec: 18.606
g0236:  iteration    42160/10000000 | consumed samples:      5396480 | consumed tokens:  11051991040 | elapsed time per iteration (ms): 6912.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.075801E+00 | loss scale: 32.0 | grad norm: 0.668 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.518 | tokens per gpu per second (tgs): 1185.135 | TFLOPs: 9.54 |
g0214: [2024-08-10 14:52:33,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=42170, skipped=71, lr=[0.0001996409207021735, 0.0001996409207021735], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42170 loss: 1.2067 iter time (s): 5.871 samples/sec: 21.803
g0236:  iteration    42170/10000000 | consumed samples:      5397760 | consumed tokens:  11054612480 | elapsed time per iteration (ms): 5903.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.116047E+00 | loss scale: 32.0 | grad norm: 0.685 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.682 | tokens per gpu per second (tgs): 1387.649 | TFLOPs: 11.17 |
g0214: [2024-08-10 14:53:53,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=42180, skipped=71, lr=[0.0001996406870674115, 0.0001996406870674115], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42180 loss: 1.0224 iter time (s): 7.966 samples/sec: 16.068
g0236:  iteration    42180/10000000 | consumed samples:      5399040 | consumed tokens:  11057233920 | elapsed time per iteration (ms): 8000.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.068820E+00 | loss scale: 32.0 | grad norm: 0.741 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.000 | tokens per gpu per second (tgs): 1023.971 | TFLOPs: 8.24 |
g0214: [2024-08-10 14:55:10,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=42190, skipped=71, lr=[0.00019964045335680526, 0.00019964045335680526], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42190 loss: 1.0794 iter time (s): 7.686 samples/sec: 16.654
g0236:  iteration    42190/10000000 | consumed samples:      5400320 | consumed tokens:  11059855360 | elapsed time per iteration (ms): 7718.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.109704E+00 | loss scale: 32.0 | grad norm: 0.658 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.584 | tokens per gpu per second (tgs): 1061.359 | TFLOPs: 8.54 |
g0214: [2024-08-10 14:56:11,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=42200, skipped=71, lr=[0.0001996402195703549, 0.0001996402195703549], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42200 loss: 1.0451 iter time (s): 6.078 samples/sec: 21.059
g0236:  iteration    42200/10000000 | consumed samples:      5401600 | consumed tokens:  11062476800 | elapsed time per iteration (ms): 6112.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.071796E+00 | loss scale: 32.0 | grad norm: 0.394 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.941 | tokens per gpu per second (tgs): 1340.254 | TFLOPs: 10.79 |
g0214: [2024-08-10 14:57:31,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=42210, skipped=71, lr=[0.00019963998570806063, 0.00019963998570806063], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42210 loss: 1.0613 iter time (s): 7.925 samples/sec: 16.151
g0236:  iteration    42210/10000000 | consumed samples:      5402880 | consumed tokens:  11065098240 | elapsed time per iteration (ms): 7959.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.046731E+00 | loss scale: 32.0 | grad norm: 0.581 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.081 | tokens per gpu per second (tgs): 1029.201 | TFLOPs: 8.28 |
g0214: [2024-08-10 14:58:48,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=42220, skipped=71, lr=[0.00019963975176992264, 0.00019963975176992264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42220 loss: 0.9250 iter time (s): 7.672 samples/sec: 16.685
g0236:  iteration    42220/10000000 | consumed samples:      5404160 | consumed tokens:  11067719680 | elapsed time per iteration (ms): 7705.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.033187E+00 | loss scale: 32.0 | grad norm: 0.928 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.611 | tokens per gpu per second (tgs): 1063.105 | TFLOPs: 8.55 |
g0214: [2024-08-10 14:59:59,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=42230, skipped=71, lr=[0.0001996395177559411, 0.0001996395177559411], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42230 loss: 1.1750 iter time (s): 7.124 samples/sec: 17.967
g0236:  iteration    42230/10000000 | consumed samples:      5405440 | consumed tokens:  11070341120 | elapsed time per iteration (ms): 7158.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.076158E+00 | loss scale: 32.0 | grad norm: 0.509 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.880 | tokens per gpu per second (tgs): 1144.337 | TFLOPs: 9.21 |
g0214: [2024-08-10 15:01:05,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=42240, skipped=71, lr=[0.0001996392836661162, 0.0001996392836661162], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42240 loss: 1.2586 iter time (s): 6.543 samples/sec: 19.563
g0236:  iteration    42240/10000000 | consumed samples:      5406720 | consumed tokens:  11072962560 | elapsed time per iteration (ms): 6576.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.079790E+00 | loss scale: 32.0 | grad norm: 0.527 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.464 | tokens per gpu per second (tgs): 1245.694 | TFLOPs: 10.02 |
g0214: [2024-08-10 15:02:06,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=42250, skipped=71, lr=[0.00019963904950044808, 0.00019963904950044808], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42250 loss: 0.9984 iter time (s): 6.092 samples/sec: 21.011
g0236:  iteration    42250/10000000 | consumed samples:      5408000 | consumed tokens:  11075584000 | elapsed time per iteration (ms): 6125.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.096548E+00 | loss scale: 32.0 | grad norm: 0.594 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.896 | tokens per gpu per second (tgs): 1337.318 | TFLOPs: 10.76 |
g0214: [2024-08-10 15:03:29,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=42260, skipped=71, lr=[0.000199638815258937, 0.000199638815258937], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42260 loss: 0.9669 iter time (s): 8.215 samples/sec: 15.581
g0236:  iteration    42260/10000000 | consumed samples:      5409280 | consumed tokens:  11078205440 | elapsed time per iteration (ms): 8248.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.059669E+00 | loss scale: 32.0 | grad norm: 3.092 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.518 | tokens per gpu per second (tgs): 993.167 | TFLOPs: 7.99 |
g0214: [2024-08-10 15:04:50,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=42270, skipped=71, lr=[0.000199638580941583, 0.000199638580941583], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42270 loss: 1.0375 iter time (s): 8.145 samples/sec: 15.715
g0236:  iteration    42270/10000000 | consumed samples:      5410560 | consumed tokens:  11080826880 | elapsed time per iteration (ms): 8180.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.068807E+00 | loss scale: 32.0 | grad norm: 4.663 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.646 | tokens per gpu per second (tgs): 1001.372 | TFLOPs: 8.06 |
g0214: [2024-08-10 15:05:56,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=42280, skipped=71, lr=[0.0001996383465483864, 0.0001996383465483864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42280 loss: 1.0844 iter time (s): 6.567 samples/sec: 19.492
g0236:  iteration    42280/10000000 | consumed samples:      5411840 | consumed tokens:  11083448320 | elapsed time per iteration (ms): 6603.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.113155E+00 | loss scale: 32.0 | grad norm: 1.507 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.385 | tokens per gpu per second (tgs): 1240.609 | TFLOPs: 9.98 |
g0214: [2024-08-10 15:07:16,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=42290, skipped=71, lr=[0.00019963811207934735, 0.00019963811207934735], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42290 loss: 1.0092 iter time (s): 7.929 samples/sec: 16.143
g0236:  iteration    42290/10000000 | consumed samples:      5413120 | consumed tokens:  11086069760 | elapsed time per iteration (ms): 7964.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.062072E+00 | loss scale: 32.0 | grad norm: 1.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.071 | tokens per gpu per second (tgs): 1028.564 | TFLOPs: 8.28 |
g0214: [2024-08-10 15:08:42,734] [INFO] [logging.py:96:log_dist] [Rank 0] step=42300, skipped=71, lr=[0.00019963787753446598, 0.00019963787753446598], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42300 loss: 0.9384 iter time (s): 8.576 samples/sec: 14.926
g0236:  iteration    42300/10000000 | consumed samples:      5414400 | consumed tokens:  11088691200 | elapsed time per iteration (ms): 8609.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.071278E+00 | loss scale: 32.0 | grad norm: 0.548 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 14.867 | tokens per gpu per second (tgs): 951.509 | TFLOPs: 7.66 |
g0214: [2024-08-10 15:10:03,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=42310, skipped=71, lr=[0.00019963764291374252, 0.00019963764291374252], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42310 loss: 0.9680 iter time (s): 8.090 samples/sec: 15.821
g0236:  iteration    42310/10000000 | consumed samples:      5415680 | consumed tokens:  11091312640 | elapsed time per iteration (ms): 8123.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.097155E+00 | loss scale: 32.0 | grad norm: 1.150 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.756 | tokens per gpu per second (tgs): 1008.384 | TFLOPs: 8.11 |
g0214: [2024-08-10 15:11:24,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=42320, skipped=71, lr=[0.0001996374082171771, 0.0001996374082171771], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42320 loss: 1.1379 iter time (s): 8.027 samples/sec: 15.945
g0236:  iteration    42320/10000000 | consumed samples:      5416960 | consumed tokens:  11093934080 | elapsed time per iteration (ms): 8060.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.088406E+00 | loss scale: 32.0 | grad norm: 0.409 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.880 | tokens per gpu per second (tgs): 1016.341 | TFLOPs: 8.18 |
g0214: [2024-08-10 15:12:46,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=42330, skipped=71, lr=[0.00019963717344476998, 0.00019963717344476998], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42330 loss: 1.0735 iter time (s): 8.143 samples/sec: 15.719
g0236:  iteration    42330/10000000 | consumed samples:      5418240 | consumed tokens:  11096555520 | elapsed time per iteration (ms): 8175.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.041497E+00 | loss scale: 32.0 | grad norm: 0.738 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.657 | tokens per gpu per second (tgs): 1002.017 | TFLOPs: 8.06 |
g0214: [2024-08-10 15:14:13,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=42340, skipped=71, lr=[0.00019963693859652122, 0.00019963693859652122], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42340 loss: 1.3154 iter time (s): 8.707 samples/sec: 14.700
g0236:  iteration    42340/10000000 | consumed samples:      5419520 | consumed tokens:  11099176960 | elapsed time per iteration (ms): 8741.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.138146E+00 | loss scale: 32.0 | grad norm: 7.088 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 14.643 | tokens per gpu per second (tgs): 937.147 | TFLOPs: 7.54 |
g0214: [2024-08-10 15:15:28,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=42350, skipped=71, lr=[0.0001996367036724311, 0.0001996367036724311], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42350 loss: 1.0601 iter time (s): 7.455 samples/sec: 17.170
g0236:  iteration    42350/10000000 | consumed samples:      5420800 | consumed tokens:  11101798400 | elapsed time per iteration (ms): 7488.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.142473E+00 | loss scale: 32.0 | grad norm: 1.888 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.094 | tokens per gpu per second (tgs): 1094.019 | TFLOPs: 8.80 |
g0214: [2024-08-10 15:16:41,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=42360, skipped=71, lr=[0.00019963646867249981, 0.00019963646867249981], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42360 loss: 1.0691 iter time (s): 7.255 samples/sec: 17.643
g0236:  iteration    42360/10000000 | consumed samples:      5422080 | consumed tokens:  11104419840 | elapsed time per iteration (ms): 7287.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.059776E+00 | loss scale: 32.0 | grad norm: 4.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.564 | tokens per gpu per second (tgs): 1124.092 | TFLOPs: 9.05 |
g0214: [2024-08-10 15:18:09,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=42370, skipped=71, lr=[0.00019963623359672746, 0.00019963623359672746], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42370 loss: 1.0982 iter time (s): 8.770 samples/sec: 14.596
g0236:  iteration    42370/10000000 | consumed samples:      5423360 | consumed tokens:  11107041280 | elapsed time per iteration (ms): 8802.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.080683E+00 | loss scale: 32.0 | grad norm: 1.783 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 14.541 | tokens per gpu per second (tgs): 930.602 | TFLOPs: 7.49 |
g0214: [2024-08-10 15:19:34,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=42380, skipped=71, lr=[0.00019963599844511427, 0.00019963599844511427], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42380 loss: 1.1285 iter time (s): 8.474 samples/sec: 15.105
g0236:  iteration    42380/10000000 | consumed samples:      5424640 | consumed tokens:  11109662720 | elapsed time per iteration (ms): 8507.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.096515E+00 | loss scale: 32.0 | grad norm: 2.278 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.046 | tokens per gpu per second (tgs): 962.940 | TFLOPs: 7.75 |
g0214: [2024-08-10 15:21:00,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=42390, skipped=71, lr=[0.0001996357632176604, 0.0001996357632176604], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42390 loss: 1.0696 iter time (s): 8.523 samples/sec: 15.018
g0236:  iteration    42390/10000000 | consumed samples:      5425920 | consumed tokens:  11112284160 | elapsed time per iteration (ms): 8555.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.078437E+00 | loss scale: 32.0 | grad norm: 10.974 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 14.961 | tokens per gpu per second (tgs): 957.488 | TFLOPs: 7.71 |
g0214: [2024-08-10 15:22:21,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=42400, skipped=71, lr=[0.0001996355279143661, 0.0001996355279143661], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42400 loss: 1.0353 iter time (s): 8.090 samples/sec: 15.821
g0236:  iteration    42400/10000000 | consumed samples:      5427200 | consumed tokens:  11114905600 | elapsed time per iteration (ms): 8122.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.108031E+00 | loss scale: 32.0 | grad norm: 2.770 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.758 | tokens per gpu per second (tgs): 1008.507 | TFLOPs: 8.12 |
g0214: [2024-08-10 15:23:43,820] [INFO] [logging.py:96:log_dist] [Rank 0] step=42410, skipped=71, lr=[0.00019963529253523143, 0.00019963529253523143], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42410 loss: 0.9449 iter time (s): 8.209 samples/sec: 15.593
g0236:  iteration    42410/10000000 | consumed samples:      5428480 | consumed tokens:  11117527040 | elapsed time per iteration (ms): 8242.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.131811E+00 | loss scale: 32.0 | grad norm: 5.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.529 | tokens per gpu per second (tgs): 993.876 | TFLOPs: 8.00 |
g0214: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0225: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0225: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0233: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0233: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0235: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0235: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0233: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0214: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0225: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0220: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0236: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0214: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0232: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0232: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0233: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0234: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0235: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0225: [2024-08-10 15:24:14,844] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0234: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0234: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0214: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 15:24:14,844] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0236: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0234: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0234: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 15:24:14,843] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0220: [2024-08-10 15:24:14,844] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0232: [2024-08-10 15:24:14,844] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0236: [2024-08-10 15:24:14,844] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0214: [2024-08-10 15:24:14,844] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0234: [2024-08-10 15:24:14,844] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0234: [2024-08-10 15:24:14,844] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0235: [2024-08-10 15:24:14,844] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32.0 to 64.0
g0214: [2024-08-10 15:25:09,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=42420, skipped=71, lr=[0.00019963505708025668, 0.00019963505708025668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42420 loss: 1.2207 iter time (s): 8.536 samples/sec: 14.996
g0236:  iteration    42420/10000000 | consumed samples:      5429760 | consumed tokens:  11120148480 | elapsed time per iteration (ms): 8568.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.143703E+00 | loss scale: 64.0 | grad norm: 9.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 14.938 | tokens per gpu per second (tgs): 956.046 | TFLOPs: 7.69 |
g0214: [2024-08-10 15:26:15,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=42430, skipped=71, lr=[0.00019963482154944194, 0.00019963482154944194], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42430 loss: 1.1202 iter time (s): 6.576 samples/sec: 19.466
g0236:  iteration    42430/10000000 | consumed samples:      5431040 | consumed tokens:  11122769920 | elapsed time per iteration (ms): 6608.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.195960E+00 | loss scale: 64.0 | grad norm: 1.560 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.369 | tokens per gpu per second (tgs): 1239.609 | TFLOPs: 9.98 |
g0214: [2024-08-10 15:27:39,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=42440, skipped=71, lr=[0.0001996345859427875, 0.0001996345859427875], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42440 loss: 1.1997 iter time (s): 8.339 samples/sec: 15.350
g0236:  iteration    42440/10000000 | consumed samples:      5432320 | consumed tokens:  11125391360 | elapsed time per iteration (ms): 8372.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.128221E+00 | loss scale: 64.0 | grad norm: 1.922 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.289 | tokens per gpu per second (tgs): 978.471 | TFLOPs: 7.87 |
g0214: [2024-08-10 15:29:17,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=42450, skipped=71, lr=[0.00019963435026029345, 0.00019963435026029345], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42450 loss: 1.1428 iter time (s): 9.762 samples/sec: 13.112
g0236:  iteration    42450/10000000 | consumed samples:      5433600 | consumed tokens:  11128012800 | elapsed time per iteration (ms): 9799.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.131214E+00 | loss scale: 64.0 | grad norm: 5.014 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.062 | tokens per gpu per second (tgs): 835.939 | TFLOPs: 6.73 |
g0214: [2024-08-10 15:30:54,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=42460, skipped=71, lr=[0.00019963411450196, 0.00019963411450196], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42460 loss: 1.2766 iter time (s): 9.617 samples/sec: 13.310
g0236:  iteration    42460/10000000 | consumed samples:      5434880 | consumed tokens:  11130634240 | elapsed time per iteration (ms): 9693.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.151063E+00 | loss scale: 64.0 | grad norm: 0.841 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.205 | tokens per gpu per second (tgs): 845.091 | TFLOPs: 6.80 |
g0214: [2024-08-10 15:32:32,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=42470, skipped=71, lr=[0.00019963387866778732, 0.00019963387866778732], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42470 loss: 1.0100 iter time (s): 9.823 samples/sec: 13.031
g0236:  iteration    42470/10000000 | consumed samples:      5436160 | consumed tokens:  11133255680 | elapsed time per iteration (ms): 9873.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.069377E+00 | loss scale: 64.0 | grad norm: 0.450 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 12.965 | tokens per gpu per second (tgs): 829.730 | TFLOPs: 6.68 |
g0214: [2024-08-10 15:33:31,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=42480, skipped=71, lr=[0.00019963364275777564, 0.00019963364275777564], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42480 loss: 1.2238 iter time (s): 5.798 samples/sec: 22.077
g0236:  iteration    42480/10000000 | consumed samples:      5437440 | consumed tokens:  11135877120 | elapsed time per iteration (ms): 5831.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.092476E+00 | loss scale: 64.0 | grad norm: 1.450 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.951 | tokens per gpu per second (tgs): 1404.853 | TFLOPs: 11.31 |
g0214: [2024-08-10 15:34:24,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=42490, skipped=71, lr=[0.00019963340677192507, 0.00019963340677192507], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42490 loss: 1.1242 iter time (s): 5.267 samples/sec: 24.300
g0236:  iteration    42490/10000000 | consumed samples:      5438720 | consumed tokens:  11138498560 | elapsed time per iteration (ms): 5300.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.088297E+00 | loss scale: 64.0 | grad norm: 0.747 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.150 | tokens per gpu per second (tgs): 1545.624 | TFLOPs: 12.44 |
g0214: [2024-08-10 15:35:16,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=42500, skipped=71, lr=[0.00019963317071023586, 0.00019963317071023586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42500 loss: 0.9844 iter time (s): 5.226 samples/sec: 24.492
g0236:  iteration    42500/10000000 | consumed samples:      5440000 | consumed tokens:  11141120000 | elapsed time per iteration (ms): 5259.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.065044E+00 | loss scale: 64.0 | grad norm: 0.854 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.336 | tokens per gpu per second (tgs): 1557.486 | TFLOPs: 12.53 |
g0214: [2024-08-10 15:36:13,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=42510, skipped=71, lr=[0.00019963293457270814, 0.00019963293457270814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42510 loss: 0.9600 iter time (s): 5.577 samples/sec: 22.951
g0236:  iteration    42510/10000000 | consumed samples:      5441280 | consumed tokens:  11143741440 | elapsed time per iteration (ms): 5611.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.076452E+00 | loss scale: 64.0 | grad norm: 0.965 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.811 | tokens per gpu per second (tgs): 1459.889 | TFLOPs: 11.75 |
g0214: [2024-08-10 15:36:59,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=42520, skipped=71, lr=[0.00019963269835934208, 0.00019963269835934208], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42520 loss: 1.2128 iter time (s): 4.638 samples/sec: 27.599
g0236:  iteration    42520/10000000 | consumed samples:      5442560 | consumed tokens:  11146362880 | elapsed time per iteration (ms): 4670.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.062713E+00 | loss scale: 64.0 | grad norm: 9.672 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.404 | tokens per gpu per second (tgs): 1753.856 | TFLOPs: 14.11 |
g0214: [2024-08-10 15:37:51,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=42530, skipped=71, lr=[0.00019963246207013795, 0.00019963246207013795], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42530 loss: 1.4118 iter time (s): 5.165 samples/sec: 24.781
g0236:  iteration    42530/10000000 | consumed samples:      5443840 | consumed tokens:  11148984320 | elapsed time per iteration (ms): 5198.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.100324E+00 | loss scale: 64.0 | grad norm: 1.566 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.621 | tokens per gpu per second (tgs): 1575.769 | TFLOPs: 12.68 |
g0214: [2024-08-10 15:38:40,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=42540, skipped=71, lr=[0.00019963222570509585, 0.00019963222570509585], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42540 loss: 0.9970 iter time (s): 4.869 samples/sec: 26.287
g0236:  iteration    42540/10000000 | consumed samples:      5445120 | consumed tokens:  11151605760 | elapsed time per iteration (ms): 4902.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.073519E+00 | loss scale: 64.0 | grad norm: 0.704 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.109 | tokens per gpu per second (tgs): 1670.957 | TFLOPs: 13.45 |
g0214: [2024-08-10 15:39:26,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=42550, skipped=71, lr=[0.00019963198926421598, 0.00019963198926421598], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42550 loss: 1.0617 iter time (s): 4.540 samples/sec: 28.193
g0236:  iteration    42550/10000000 | consumed samples:      5446400 | consumed tokens:  11154227200 | elapsed time per iteration (ms): 4573.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.094326E+00 | loss scale: 64.0 | grad norm: 1.783 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.989 | tokens per gpu per second (tgs): 1791.323 | TFLOPs: 14.42 |
g0214: [2024-08-10 15:40:10,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=42560, skipped=71, lr=[0.0001996317527474985, 0.0001996317527474985], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42560 loss: 0.9427 iter time (s): 4.359 samples/sec: 29.366
g0236:  iteration    42560/10000000 | consumed samples:      5447680 | consumed tokens:  11156848640 | elapsed time per iteration (ms): 4392.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.030093E+00 | loss scale: 64.0 | grad norm: 1.001 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.141 | tokens per gpu per second (tgs): 1865.035 | TFLOPs: 15.01 |
g0214: [2024-08-10 15:41:00,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=42570, skipped=71, lr=[0.00019963151615494366, 0.00019963151615494366], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42570 loss: 1.0326 iter time (s): 5.024 samples/sec: 25.478
g0236:  iteration    42570/10000000 | consumed samples:      5448960 | consumed tokens:  11159470080 | elapsed time per iteration (ms): 5057.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.077955E+00 | loss scale: 64.0 | grad norm: 0.594 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.310 | tokens per gpu per second (tgs): 1619.865 | TFLOPs: 13.04 |
g0214: [2024-08-10 15:41:51,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=42580, skipped=71, lr=[0.0001996312794865516, 0.0001996312794865516], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42580 loss: 0.9187 iter time (s): 5.047 samples/sec: 25.361
g0236:  iteration    42580/10000000 | consumed samples:      5450240 | consumed tokens:  11162091520 | elapsed time per iteration (ms): 5082.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.093239E+00 | loss scale: 64.0 | grad norm: 0.399 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.185 | tokens per gpu per second (tgs): 1611.845 | TFLOPs: 12.97 |
g0214: [2024-08-10 15:42:38,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=42590, skipped=71, lr=[0.00019963104274232249, 0.00019963104274232249], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42590 loss: 1.0635 iter time (s): 4.654 samples/sec: 27.501
g0236:  iteration    42590/10000000 | consumed samples:      5451520 | consumed tokens:  11164712960 | elapsed time per iteration (ms): 4688.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.090100E+00 | loss scale: 64.0 | grad norm: 1.999 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.303 | tokens per gpu per second (tgs): 1747.384 | TFLOPs: 14.06 |
g0214: [2024-08-10 15:43:27,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=42600, skipped=71, lr=[0.00019963080592225654, 0.00019963080592225654], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42600 loss: 0.9841 iter time (s): 4.874 samples/sec: 26.264
g0236:  iteration    42600/10000000 | consumed samples:      5452800 | consumed tokens:  11167334400 | elapsed time per iteration (ms): 4906.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.082658E+00 | loss scale: 64.0 | grad norm: 1.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.088 | tokens per gpu per second (tgs): 1669.638 | TFLOPs: 13.44 |
g0214: [2024-08-10 15:44:17,514] [INFO] [logging.py:96:log_dist] [Rank 0] step=42610, skipped=71, lr=[0.00019963056902635388, 0.00019963056902635388], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42610 loss: 1.0829 iter time (s): 4.946 samples/sec: 25.882
g0236:  iteration    42610/10000000 | consumed samples:      5454080 | consumed tokens:  11169955840 | elapsed time per iteration (ms): 4978.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.090307E+00 | loss scale: 64.0 | grad norm: 3.355 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.710 | tokens per gpu per second (tgs): 1645.444 | TFLOPs: 13.24 |
g0214: [2024-08-10 15:45:09,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=42620, skipped=71, lr=[0.00019963033205461475, 0.00019963033205461475], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42620 loss: 1.1391 iter time (s): 5.142 samples/sec: 24.894
g0236:  iteration    42620/10000000 | consumed samples:      5455360 | consumed tokens:  11172577280 | elapsed time per iteration (ms): 5181.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.073131E+00 | loss scale: 64.0 | grad norm: 0.756 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.704 | tokens per gpu per second (tgs): 1581.061 | TFLOPs: 12.72 |
g0214: [2024-08-10 15:45:58,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=42630, skipped=71, lr=[0.00019963009500703933, 0.00019963009500703933], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42630 loss: 1.0397 iter time (s): 4.884 samples/sec: 26.207
g0236:  iteration    42630/10000000 | consumed samples:      5456640 | consumed tokens:  11175198720 | elapsed time per iteration (ms): 4917.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.031813E+00 | loss scale: 64.0 | grad norm: 4.352 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.031 | tokens per gpu per second (tgs): 1665.996 | TFLOPs: 13.41 |
g0214: [2024-08-10 15:46:51,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=42640, skipped=71, lr=[0.00019962985788362777, 0.00019962985788362777], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42640 loss: 1.1369 iter time (s): 5.289 samples/sec: 24.200
g0236:  iteration    42640/10000000 | consumed samples:      5457920 | consumed tokens:  11177820160 | elapsed time per iteration (ms): 5322.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.065121E+00 | loss scale: 64.0 | grad norm: 1.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.049 | tokens per gpu per second (tgs): 1539.104 | TFLOPs: 12.39 |
g0214: [2024-08-10 15:47:38,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=42650, skipped=71, lr=[0.00019962962068438024, 0.00019962962068438024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42650 loss: 0.9711 iter time (s): 4.625 samples/sec: 27.678
g0236:  iteration    42650/10000000 | consumed samples:      5459200 | consumed tokens:  11180441600 | elapsed time per iteration (ms): 4657.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.043774E+00 | loss scale: 64.0 | grad norm: 1.358 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.480 | tokens per gpu per second (tgs): 1758.720 | TFLOPs: 14.15 |
g0214: [2024-08-10 15:48:30,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=42660, skipped=71, lr=[0.00019962938340929698, 0.00019962938340929698], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42660 loss: 0.9584 iter time (s): 5.171 samples/sec: 24.755
g0236:  iteration    42660/10000000 | consumed samples:      5460480 | consumed tokens:  11183063040 | elapsed time per iteration (ms): 5204.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.040526E+00 | loss scale: 64.0 | grad norm: 3.837 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.592 | tokens per gpu per second (tgs): 1573.889 | TFLOPs: 12.67 |
g0214: [2024-08-10 15:49:23,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=42670, skipped=71, lr=[0.00019962914605837814, 0.00019962914605837814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42670 loss: 1.0449 iter time (s): 5.301 samples/sec: 24.149
g0236:  iteration    42670/10000000 | consumed samples:      5461760 | consumed tokens:  11185684480 | elapsed time per iteration (ms): 5333.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.025696E+00 | loss scale: 64.0 | grad norm: 1.544 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.999 | tokens per gpu per second (tgs): 1535.953 | TFLOPs: 12.36 |
g0214: [2024-08-10 15:50:31,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=42680, skipped=71, lr=[0.0001996289086316239, 0.0001996289086316239], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42680 loss: 1.0884 iter time (s): 6.705 samples/sec: 19.089
g0236:  iteration    42680/10000000 | consumed samples:      5463040 | consumed tokens:  11188305920 | elapsed time per iteration (ms): 6740.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.068998E+00 | loss scale: 64.0 | grad norm: 1.628 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.989 | tokens per gpu per second (tgs): 1215.278 | TFLOPs: 9.78 |
g0214: [2024-08-10 15:51:31,066] [INFO] [logging.py:96:log_dist] [Rank 0] step=42690, skipped=71, lr=[0.00019962867112903443, 0.00019962867112903443], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42690 loss: 1.1935 iter time (s): 5.963 samples/sec: 21.464
g0236:  iteration    42690/10000000 | consumed samples:      5464320 | consumed tokens:  11190927360 | elapsed time per iteration (ms): 5997.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.088985E+00 | loss scale: 64.0 | grad norm: 2.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.344 | tokens per gpu per second (tgs): 1366.020 | TFLOPs: 10.99 |
g0214: [2024-08-10 15:52:37,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=42700, skipped=71, lr=[0.00019962843355060994, 0.00019962843355060994], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42700 loss: 0.9504 iter time (s): 6.618 samples/sec: 19.342
g0236:  iteration    42700/10000000 | consumed samples:      5465600 | consumed tokens:  11193548800 | elapsed time per iteration (ms): 6650.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.091518E+00 | loss scale: 64.0 | grad norm: 3.850 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.246 | tokens per gpu per second (tgs): 1231.745 | TFLOPs: 9.91 |
g0214: [2024-08-10 15:53:46,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=42710, skipped=71, lr=[0.0001996281958963506, 0.0001996281958963506], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42710 loss: 1.0292 iter time (s): 6.851 samples/sec: 18.684
g0236:  iteration    42710/10000000 | consumed samples:      5466880 | consumed tokens:  11196170240 | elapsed time per iteration (ms): 6883.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.053559E+00 | loss scale: 64.0 | grad norm: 0.908 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.595 | tokens per gpu per second (tgs): 1190.054 | TFLOPs: 9.58 |
g0214: [2024-08-10 15:54:54,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=42720, skipped=71, lr=[0.0001996279581662566, 0.0001996279581662566], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42720 loss: 1.1738 iter time (s): 6.798 samples/sec: 18.830
g0236:  iteration    42720/10000000 | consumed samples:      5468160 | consumed tokens:  11198791680 | elapsed time per iteration (ms): 6830.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.073084E+00 | loss scale: 64.0 | grad norm: 1.043 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.738 | tokens per gpu per second (tgs): 1199.248 | TFLOPs: 9.65 |
g0214: [2024-08-10 15:55:59,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=42730, skipped=71, lr=[0.00019962772036032813, 0.00019962772036032813], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42730 loss: 0.9731 iter time (s): 6.459 samples/sec: 19.816
g0236:  iteration    42730/10000000 | consumed samples:      5469440 | consumed tokens:  11201413120 | elapsed time per iteration (ms): 6492.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.088936E+00 | loss scale: 64.0 | grad norm: 1.317 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.716 | tokens per gpu per second (tgs): 1261.840 | TFLOPs: 10.15 |
g0214: [2024-08-10 15:56:55,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=42740, skipped=71, lr=[0.00019962748247856534, 0.00019962748247856534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42740 loss: 1.2038 iter time (s): 5.531 samples/sec: 23.140
g0236:  iteration    42740/10000000 | consumed samples:      5470720 | consumed tokens:  11204034560 | elapsed time per iteration (ms): 5565.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.082195E+00 | loss scale: 64.0 | grad norm: 7.663 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.001 | tokens per gpu per second (tgs): 1472.032 | TFLOPs: 11.85 |
g0214: [2024-08-10 15:57:59,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=42750, skipped=71, lr=[0.0001996272445209684, 0.0001996272445209684], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42750 loss: 1.0779 iter time (s): 6.428 samples/sec: 19.913
g0236:  iteration    42750/10000000 | consumed samples:      5472000 | consumed tokens:  11206656000 | elapsed time per iteration (ms): 6461.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.038397E+00 | loss scale: 64.0 | grad norm: 3.889 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.810 | tokens per gpu per second (tgs): 1267.839 | TFLOPs: 10.20 |
g0214: [2024-08-10 15:58:59,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=42760, skipped=71, lr=[0.00019962700648753758, 0.00019962700648753758], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42760 loss: 1.0499 iter time (s): 5.897 samples/sec: 21.707
g0236:  iteration    42760/10000000 | consumed samples:      5473280 | consumed tokens:  11209277440 | elapsed time per iteration (ms): 5930.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.038694E+00 | loss scale: 64.0 | grad norm: 2.314 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.583 | tokens per gpu per second (tgs): 1381.304 | TFLOPs: 11.12 |
g0214: [2024-08-10 16:00:12,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=42770, skipped=71, lr=[0.00019962676837827296, 0.00019962676837827296], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42770 loss: 0.9787 iter time (s): 7.337 samples/sec: 17.446
g0236:  iteration    42770/10000000 | consumed samples:      5474560 | consumed tokens:  11211898880 | elapsed time per iteration (ms): 7370.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.052691E+00 | loss scale: 64.0 | grad norm: 1.903 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.368 | tokens per gpu per second (tgs): 1111.528 | TFLOPs: 8.94 |
g0214: [2024-08-10 16:01:14,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=42780, skipped=71, lr=[0.00019962653019317482, 0.00019962653019317482], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42780 loss: 1.1257 iter time (s): 6.162 samples/sec: 20.774
g0236:  iteration    42780/10000000 | consumed samples:      5475840 | consumed tokens:  11214520320 | elapsed time per iteration (ms): 6195.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.084732E+00 | loss scale: 64.0 | grad norm: 21.856 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.661 | tokens per gpu per second (tgs): 1322.313 | TFLOPs: 10.64 |
g0214: [2024-08-10 16:02:20,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=42790, skipped=71, lr=[0.00019962629193224327, 0.00019962629193224327], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42790 loss: 0.9823 iter time (s): 6.498 samples/sec: 19.698
g0236:  iteration    42790/10000000 | consumed samples:      5477120 | consumed tokens:  11217141760 | elapsed time per iteration (ms): 6531.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.026053E+00 | loss scale: 64.0 | grad norm: 2.385 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.596 | tokens per gpu per second (tgs): 1254.160 | TFLOPs: 10.09 |
g0214: [2024-08-10 16:03:22,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=42800, skipped=71, lr=[0.0001996260535954785, 0.0001996260535954785], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42800 loss: 1.1870 iter time (s): 6.160 samples/sec: 20.779
g0236:  iteration    42800/10000000 | consumed samples:      5478400 | consumed tokens:  11219763200 | elapsed time per iteration (ms): 6193.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.089196E+00 | loss scale: 64.0 | grad norm: 1.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.668 | tokens per gpu per second (tgs): 1322.759 | TFLOPs: 10.64 |
g0214: [2024-08-10 16:04:37,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=42810, skipped=71, lr=[0.00019962581518288074, 0.00019962581518288074], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42810 loss: 0.9456 iter time (s): 7.457 samples/sec: 17.166
g0236:  iteration    42810/10000000 | consumed samples:      5479680 | consumed tokens:  11222384640 | elapsed time per iteration (ms): 7492.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.090952E+00 | loss scale: 64.0 | grad norm: 1.903 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.083 | tokens per gpu per second (tgs): 1093.326 | TFLOPs: 8.80 |
g0214: [2024-08-10 16:05:36,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=42820, skipped=71, lr=[0.00019962557669445013, 0.00019962557669445013], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42820 loss: 1.0040 iter time (s): 5.948 samples/sec: 21.519
g0236:  iteration    42820/10000000 | consumed samples:      5480960 | consumed tokens:  11225006080 | elapsed time per iteration (ms): 5981.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.021051E+00 | loss scale: 64.0 | grad norm: 2.426 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.399 | tokens per gpu per second (tgs): 1369.534 | TFLOPs: 11.02 |
g0214: [2024-08-10 16:06:38,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=42830, skipped=71, lr=[0.0001996253381301869, 0.0001996253381301869], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42830 loss: 1.0048 iter time (s): 6.113 samples/sec: 20.940
g0236:  iteration    42830/10000000 | consumed samples:      5482240 | consumed tokens:  11227627520 | elapsed time per iteration (ms): 6146.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.091183E+00 | loss scale: 64.0 | grad norm: 1.876 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.827 | tokens per gpu per second (tgs): 1332.897 | TFLOPs: 10.73 |
g0214: [2024-08-10 16:07:44,359] [INFO] [logging.py:96:log_dist] [Rank 0] step=42840, skipped=71, lr=[0.00019962509949009115, 0.00019962509949009115], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42840 loss: 0.9862 iter time (s): 6.570 samples/sec: 19.482
g0236:  iteration    42840/10000000 | consumed samples:      5483520 | consumed tokens:  11230248960 | elapsed time per iteration (ms): 6603.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.060016E+00 | loss scale: 64.0 | grad norm: 11.905 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.383 | tokens per gpu per second (tgs): 1240.506 | TFLOPs: 9.98 |
g0214: [2024-08-10 16:08:52,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=42850, skipped=71, lr=[0.00019962486077416313, 0.00019962486077416313], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42850 loss: 1.0013 iter time (s): 6.768 samples/sec: 18.912
g0236:  iteration    42850/10000000 | consumed samples:      5484800 | consumed tokens:  11232870400 | elapsed time per iteration (ms): 6801.1 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.069302E+00 | loss scale: 64.0 | grad norm: 7.057 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.821 | tokens per gpu per second (tgs): 1204.513 | TFLOPs: 9.69 |
g0214: [2024-08-10 16:10:00,725] [INFO] [logging.py:96:log_dist] [Rank 0] step=42860, skipped=71, lr=[0.00019962462198240303, 0.00019962462198240303], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42860 loss: 1.1008 iter time (s): 6.797 samples/sec: 18.833
g0236:  iteration    42860/10000000 | consumed samples:      5486080 | consumed tokens:  11235491840 | elapsed time per iteration (ms): 6837.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.116788E+00 | loss scale: 64.0 | grad norm: 1.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.721 | tokens per gpu per second (tgs): 1198.150 | TFLOPs: 9.64 |
g0214: [2024-08-10 16:11:28,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=42870, skipped=71, lr=[0.000199624383114811, 0.000199624383114811], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42870 loss: 1.1469 iter time (s): 8.734 samples/sec: 14.656
g0236:  iteration    42870/10000000 | consumed samples:      5487360 | consumed tokens:  11238113280 | elapsed time per iteration (ms): 8775.9 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.107698E+00 | loss scale: 64.0 | grad norm: 1.688 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 14.585 | tokens per gpu per second (tgs): 933.467 | TFLOPs: 7.51 |
g0214: [2024-08-10 16:12:36,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=42880, skipped=71, lr=[0.00019962414417138723, 0.00019962414417138723], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42880 loss: 1.1410 iter time (s): 6.723 samples/sec: 19.041
g0236:  iteration    42880/10000000 | consumed samples:      5488640 | consumed tokens:  11240734720 | elapsed time per iteration (ms): 6759.6 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.070146E+00 | loss scale: 64.0 | grad norm: 2.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.936 | tokens per gpu per second (tgs): 1211.897 | TFLOPs: 9.75 |
g0214: [2024-08-10 16:13:50,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=42890, skipped=71, lr=[0.0001996239051521319, 0.0001996239051521319], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42890 loss: 1.0319 iter time (s): 7.421 samples/sec: 17.248
g0236:  iteration    42890/10000000 | consumed samples:      5489920 | consumed tokens:  11243356160 | elapsed time per iteration (ms): 7457.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.123186E+00 | loss scale: 64.0 | grad norm: 8.785 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.165 | tokens per gpu per second (tgs): 1098.568 | TFLOPs: 8.84 |
g0214: [2024-08-10 16:15:19,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=42900, skipped=71, lr=[0.00019962366605704522, 0.00019962366605704522], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42900 loss: 1.2591 iter time (s): 8.862 samples/sec: 14.445
g0236:  iteration    42900/10000000 | consumed samples:      5491200 | consumed tokens:  11245977600 | elapsed time per iteration (ms): 8895.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.090463E+00 | loss scale: 64.0 | grad norm: 0.978 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 14.389 | tokens per gpu per second (tgs): 920.912 | TFLOPs: 7.41 |
g0214: [2024-08-10 16:16:33,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=42910, skipped=71, lr=[0.00019962342688612737, 0.00019962342688612737], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42910 loss: 1.0140 iter time (s): 7.359 samples/sec: 17.393
g0236:  iteration    42910/10000000 | consumed samples:      5492480 | consumed tokens:  11248599040 | elapsed time per iteration (ms): 7392.2 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.085184E+00 | loss scale: 64.0 | grad norm: 1.689 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.316 | tokens per gpu per second (tgs): 1108.196 | TFLOPs: 8.92 |
g0235: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0233: [2024-08-10 16:17:05,215] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 16:17:05,215] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0225: [2024-08-10 16:17:05,215] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0225: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0220: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0225: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0214: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0236: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0220: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0220: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0220: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0236: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0236: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0234: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0232: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0234: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0232: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0235: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0233: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0220: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0214: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0232: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0234: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0225: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0236: [2024-08-10 16:17:05,216] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0225: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0235: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0232: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0232: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0236: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0233: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0234: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0214: [2024-08-10 16:17:05,220] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0214: [2024-08-10 16:17:05,221] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 64.0 to 128.0
g0214: [2024-08-10 16:17:59,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=42920, skipped=71, lr=[0.0001996231876393785, 0.0001996231876393785], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42920 loss: 0.9763 iter time (s): 8.606 samples/sec: 14.873
g0236:  iteration    42920/10000000 | consumed samples:      5493760 | consumed tokens:  11251220480 | elapsed time per iteration (ms): 8639.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.087078E+00 | loss scale: 128.0 | grad norm: 3.293 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 14.816 | tokens per gpu per second (tgs): 948.229 | TFLOPs: 7.63 |
g0214: [2024-08-10 16:19:34,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=42930, skipped=71, lr=[0.0001996229483167988, 0.0001996229483167988], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42930 loss: 1.0778 iter time (s): 9.434 samples/sec: 13.568
g0236:  iteration    42930/10000000 | consumed samples:      5495040 | consumed tokens:  11253841920 | elapsed time per iteration (ms): 9466.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.062556E+00 | loss scale: 128.0 | grad norm: 1.966 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.521 | tokens per gpu per second (tgs): 865.337 | TFLOPs: 6.96 |
g0214: [2024-08-10 16:21:19,929] [INFO] [logging.py:96:log_dist] [Rank 0] step=42940, skipped=71, lr=[0.0001996227089183885, 0.0001996227089183885], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42940 loss: 0.9863 iter time (s): 10.496 samples/sec: 12.195
g0236:  iteration    42940/10000000 | consumed samples:      5496320 | consumed tokens:  11256463360 | elapsed time per iteration (ms): 10531.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.064859E+00 | loss scale: 128.0 | grad norm: 0.867 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 12.154 | tokens per gpu per second (tgs): 777.836 | TFLOPs: 6.26 |
g0214: [2024-08-10 16:23:02,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=42950, skipped=71, lr=[0.00019962246944414775, 0.00019962246944414775], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42950 loss: 1.2517 iter time (s): 10.182 samples/sec: 12.571
g0236:  iteration    42950/10000000 | consumed samples:      5497600 | consumed tokens:  11259084800 | elapsed time per iteration (ms): 10216.4 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.089532E+00 | loss scale: 128.0 | grad norm: 1.333 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 12.529 | tokens per gpu per second (tgs): 801.846 | TFLOPs: 6.45 |
g0214: [2024-08-10 16:24:54,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=42960, skipped=71, lr=[0.00019962222989407672, 0.00019962222989407672], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42960 loss: 1.1872 iter time (s): 11.200 samples/sec: 11.429
g0236:  iteration    42960/10000000 | consumed samples:      5498880 | consumed tokens:  11261706240 | elapsed time per iteration (ms): 11232.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.114155E+00 | loss scale: 128.0 | grad norm: 0.448 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 11.395 | tokens per gpu per second (tgs): 729.311 | TFLOPs: 5.87 |
g0214: [2024-08-10 16:26:23,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=42970, skipped=71, lr=[0.0001996219902681756, 0.0001996219902681756], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42970 loss: 1.1447 iter time (s): 8.862 samples/sec: 14.443
g0236:  iteration    42970/10000000 | consumed samples:      5500160 | consumed tokens:  11264327680 | elapsed time per iteration (ms): 8894.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.163239E+00 | loss scale: 128.0 | grad norm: 6.781 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 14.391 | tokens per gpu per second (tgs): 921.015 | TFLOPs: 7.41 |
g0214: [2024-08-10 16:27:43,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=42980, skipped=71, lr=[0.0001996217505664446, 0.0001996217505664446], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42980 loss: 1.1176 iter time (s): 8.012 samples/sec: 15.975
g0236:  iteration    42980/10000000 | consumed samples:      5501440 | consumed tokens:  11266949120 | elapsed time per iteration (ms): 8044.5 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.091356E+00 | loss scale: 128.0 | grad norm: 0.601 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.912 | tokens per gpu per second (tgs): 1018.342 | TFLOPs: 8.19 |
g0214: [2024-08-10 16:29:05,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=42990, skipped=71, lr=[0.0001996215107888839, 0.0001996215107888839], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 42990 loss: 1.5825 iter time (s): 8.103 samples/sec: 15.798
g0236:  iteration    42990/10000000 | consumed samples:      5502720 | consumed tokens:  11269570560 | elapsed time per iteration (ms): 8145.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.113862E+00 | loss scale: 128.0 | grad norm: 40.581 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 15.715 | tokens per gpu per second (tgs): 1005.739 | TFLOPs: 8.09 |
g0214: [2024-08-10 16:30:39,589] [INFO] [logging.py:96:log_dist] [Rank 0] step=43000, skipped=71, lr=[0.00019962127093549366, 0.00019962127093549366], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 43000 loss: 1.0760 iter time (s): 9.400 samples/sec: 13.617
g0236:  iteration    43000/10000000 | consumed samples:      5504000 | consumed tokens:  11272192000 | elapsed time per iteration (ms): 9432.8 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.136817E+00 | loss scale: 128.0 | grad norm: 0.654 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.570 | tokens per gpu per second (tgs): 868.461 | TFLOPs: 6.99 |
g0236: -------------------------------------------------------------------------------------------------
g0236:  validation loss at iteration 43000 | lm loss value: 1.052947E+00 | lm loss PPL: 2.866086E+00 | 
g0236: -------------------------------------------------------------------------------------------------
g0214: saving checkpoint at iteration   43000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214: [2024-08-10 16:39:44,298] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step43000 is about to be saved!
g0214: [2024-08-10 16:39:44,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0214: [2024-08-10 16:39:44,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0236: [2024-08-10 16:39:44,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0214: [2024-08-10 16:39:44,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0236: [2024-08-10 16:39:44,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0236: [2024-08-10 16:39:44,305] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0225: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0225: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0225: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0233: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0233: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0233: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0234: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0234: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0234: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0235: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0235: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0235: [2024-08-10 16:39:44,307] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0220: [2024-08-10 16:39:44,308] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0220: [2024-08-10 16:39:44,308] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0220: [2024-08-10 16:39:44,308] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0232: [2024-08-10 16:39:44,308] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0232: [2024-08-10 16:39:44,308] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0232: [2024-08-10 16:39:44,308] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0236: [2024-08-10 16:39:44,333] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_23-model_00-model_states.pt...
g0232: [2024-08-10 16:39:44,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_11-model_00-model_states.pt...
g0220: [2024-08-10 16:39:44,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_05-model_00-model_states.pt...
g0233: [2024-08-10 16:39:44,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_14-model_00-model_states.pt...
g0234: [2024-08-10 16:39:44,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_17-model_00-model_states.pt...
g0235: [2024-08-10 16:39:44,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_20-model_00-model_states.pt...
g0225: [2024-08-10 16:39:44,342] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_08-model_00-model_states.pt...
g0214: [2024-08-10 16:39:44,351] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_01-model_00-model_states.pt...
g0225: [2024-08-10 16:39:44,454] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_08-model_00-model_states.pt.
g0220: [2024-08-10 16:39:44,473] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_05-model_00-model_states.pt.
g0225: [2024-08-10 16:39:44,491] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_09-model_00-model_states.pt...
g0236: [2024-08-10 16:39:44,496] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_23-model_00-model_states.pt.
g0236: [2024-08-10 16:39:44,497] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_24-model_00-model_states.pt...
g0235: [2024-08-10 16:39:44,499] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_20-model_00-model_states.pt.
g0236: [2024-08-10 16:39:44,500] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_24-model_00-model_states.pt.
g0220: [2024-08-10 16:39:44,506] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_06-model_00-model_states.pt...
g0233: [2024-08-10 16:39:44,533] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_14-model_00-model_states.pt.
g0235: [2024-08-10 16:39:44,534] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_21-model_00-model_states.pt...
g0214: [2024-08-10 16:39:44,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_01-model_00-model_states.pt.
g0236: [2024-08-10 16:39:44,553] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_25-model_00-model_states.pt...
g0214: [2024-08-10 16:39:44,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_02-model_00-model_states.pt...
g0233: [2024-08-10 16:39:44,569] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_15-model_00-model_states.pt...
g0234: [2024-08-10 16:39:44,587] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_17-model_00-model_states.pt.
g0234: [2024-08-10 16:39:44,622] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_18-model_00-model_states.pt...
g0232: [2024-08-10 16:39:44,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_11-model_00-model_states.pt.
g0220: [2024-08-10 16:39:44,658] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_06-model_00-model_states.pt.
g0235: [2024-08-10 16:39:44,669] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_21-model_00-model_states.pt.
g0232: [2024-08-10 16:39:44,687] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_12-model_00-model_states.pt...
g0220: [2024-08-10 16:39:44,687] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_07-model_00-model_states.pt...
g0225: [2024-08-10 16:39:44,692] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_09-model_00-model_states.pt.
g0235: [2024-08-10 16:39:44,698] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_22-model_00-model_states.pt...
g0233: [2024-08-10 16:39:44,701] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_15-model_00-model_states.pt.
g0225: [2024-08-10 16:39:44,725] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_10-model_00-model_states.pt...
g0233: [2024-08-10 16:39:44,731] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_16-model_00-model_states.pt...
g0214: [2024-08-10 16:39:44,740] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_02-model_00-model_states.pt.
g0214: [2024-08-10 16:39:44,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_03-model_00-model_states.pt...
g0234: [2024-08-10 16:39:44,824] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_18-model_00-model_states.pt.
g0232: [2024-08-10 16:39:44,827] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_12-model_00-model_states.pt.
g0220: [2024-08-10 16:39:44,833] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_07-model_00-model_states.pt.
g0220: [2024-08-10 16:39:44,834] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_01_model_states.pt...
g0225: [2024-08-10 16:39:44,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_10-model_00-model_states.pt.
g0225: [2024-08-10 16:39:44,853] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_02_model_states.pt...
g0234: [2024-08-10 16:39:44,854] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_19-model_00-model_states.pt...
g0232: [2024-08-10 16:39:44,856] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_13-model_00-model_states.pt...
g0233: [2024-08-10 16:39:44,856] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_16-model_00-model_states.pt.
g0233: [2024-08-10 16:39:44,858] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_04_model_states.pt...
g0235: [2024-08-10 16:39:44,908] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_22-model_00-model_states.pt.
g0235: [2024-08-10 16:39:44,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_06_model_states.pt...
g0214: [2024-08-10 16:39:44,912] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_03-model_00-model_states.pt.
g0214: [2024-08-10 16:39:44,937] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_04-model_00-model_states.pt...
g0232: [2024-08-10 16:39:44,954] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_13-model_00-model_states.pt.
g0232: [2024-08-10 16:39:44,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_03_model_states.pt...
g0234: [2024-08-10 16:39:44,969] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_19-model_00-model_states.pt.
g0234: [2024-08-10 16:39:44,971] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_05_model_states.pt...
g0214: [2024-08-10 16:39:45,035] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_04-model_00-model_states.pt.
g0214: [2024-08-10 16:39:45,037] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_00_model_states.pt
g0214: [2024-08-10 16:39:45,037] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_00_model_states.pt...
g0236: [2024-08-10 16:39:45,087] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/layer_25-model_00-model_states.pt.
g0236: [2024-08-10 16:39:45,088] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_07_model_states.pt...
g0220: [2024-08-10 16:39:47,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_01_model_states.pt.
g0220: [2024-08-10 16:39:47,107] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0235: [2024-08-10 16:39:47,193] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_06_model_states.pt.
g0235: [2024-08-10 16:39:47,193] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0233: [2024-08-10 16:39:47,253] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_04_model_states.pt.
g0233: [2024-08-10 16:39:47,254] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0234: [2024-08-10 16:39:47,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_05_model_states.pt.
g0234: [2024-08-10 16:39:47,254] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0232: [2024-08-10 16:39:47,296] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_03_model_states.pt.
g0232: [2024-08-10 16:39:47,297] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0236: [2024-08-10 16:39:47,303] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_07_model_states.pt.
g0236: [2024-08-10 16:39:47,303] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0225: [2024-08-10 16:39:47,380] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_02_model_states.pt.
g0225: [2024-08-10 16:39:47,381] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0214: [2024-08-10 16:39:48,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step43000/mp_rank_00_model_states.pt.
g0214: [2024-08-10 16:39:48,399] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step43000 is ready now!
g0214:   successfully saved checkpoint at iteration   43000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0214: Checkpoint Save GB: 22.521, GB/Sec: 5.46, Latency(second): 4.121
g0236: (min, max) time across ranks (ms):
g0236:     save-checkpoint ................................: (4121.16, 4121.38)
g0214: [2024-08-10 16:41:01,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=43010, skipped=71, lr=[0.00019962103100627408, 0.00019962103100627408], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 43010 loss: 0.9379 iter time (s): 7.306 samples/sec: 17.520
g0236:  iteration    43010/10000000 | consumed samples:      5505280 | consumed tokens:  11274813440 | elapsed time per iteration (ms): 62216.3 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.063341E+00 | loss scale: 128.0 | grad norm: 0.722 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.057 | tokens per gpu per second (tgs): 131.670 | TFLOPs: 1.06 |
g0214: [2024-08-10 16:42:10,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=43020, skipped=71, lr=[0.00019962079100122534, 0.00019962079100122534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 43020 loss: 1.2207 iter time (s): 6.872 samples/sec: 18.626
g0236:  iteration    43020/10000000 | consumed samples:      5506560 | consumed tokens:  11277434880 | elapsed time per iteration (ms): 6906.0 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 1.093089E+00 | loss scale: 128.0 | grad norm: 0.673 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.535 | tokens per gpu per second (tgs): 1186.210 | TFLOPs: 9.55 |
g0214: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43024
g0214: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43024
g0214: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0214: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0214: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43024
g0214: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0235: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43024
g0235: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43024
g0235: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0233: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0233: Grad overflow on iteration 43024
g0235: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43024
g0234: Grad overflow on iteration 43024
g0232: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43024
g0236: Grad overflow on iteration 43024
g0233: Grad overflow on iteration 43024
g0236: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43024
g0233: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0233: Grad overflow on iteration 43024
g0225: Grad overflow on iteration 43024
g0233: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0225: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43024
g0234: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0232: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0233: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0232: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0225: Grad overflow on iteration 43024
g0220: Grad overflow on iteration 43024
g0225: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43024
g0235: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0234: Grad overflow on iteration 43024
g0234: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0234: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43024
g0234: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0220: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0234: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0220: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0232: Grad overflow on iteration 43024
g0220: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0220: Grad overflow on iteration 43024
g0232: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0220: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0236: Grad overflow on iteration 43024
g0236: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0225: Grad overflow on iteration 43024
g0225: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0225: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0225: [2024-08-10 16:43:00,217] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0234: [2024-08-10 16:43:00,227] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:00,227] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:00,227] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43024
g0234: [2024-08-10 16:43:00,227] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0233: Grad overflow on iteration 43024
g0235: Grad overflow on iteration 43024
g0233: [2024-08-10 16:43:00,227] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0235: [2024-08-10 16:43:00,227] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0220: [2024-08-10 16:43:00,227] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:00,227] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43024
g0236: [2024-08-10 16:43:00,227] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43024
g0225: Grad overflow on iteration 43024
g0225: [2024-08-10 16:43:00,227] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0232: [2024-08-10 16:43:00,227] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43024
g0232: [2024-08-10 16:43:00,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0220: [2024-08-10 16:43:00,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0236: [2024-08-10 16:43:00,228] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0214: [2024-08-10 16:43:00,230] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43024
g0214: [2024-08-10 16:43:00,231] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0214: [2024-08-10 16:43:00,231] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 128.0, reducing to 64.0
g0220: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43025
g0220: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43025
g0220: Grad overflow on iteration 43025
g0214: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43025
g0236: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43025
g0220: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43025
g0234: Grad overflow on iteration 43025
g0225: Grad overflow on iteration 43025
g0214: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0225: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43025
g0225: Grad overflow on iteration 43025
g0225: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43025
g0232: Grad overflow on iteration 43025
g0225: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0225: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43025
g0225: Grad overflow on iteration 43025
g0220: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0234: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43025
g0220: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0233: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0233: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0234: Grad overflow on iteration 43025
g0225: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0233: Grad overflow on iteration 43025
g0220: Grad overflow on iteration 43025
g0234: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0232: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0233: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0236: Grad overflow on iteration 43025
g0233: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43025
g0235: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0235: Grad overflow on iteration 43025
g0233: Grad overflow on iteration 43025
g0234: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0232: Grad overflow on iteration 43025
g0234: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0232: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0234: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43025
g0234: Grad overflow on iteration 43025
g0232: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0234: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0236: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0236: Grad overflow on iteration 43025
g0233: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43025
g0232: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0232: Grad overflow on iteration 43025
g0214: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0235: Grad overflow on iteration 43025
g0233: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0236: Grad overflow on iteration 43025
g0235: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0234: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0236: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0235: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43025
g0236: Grad overflow on iteration 43025
g0235: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0232: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0236: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0232: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0236: [2024-08-10 16:43:07,692] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: [2024-08-10 16:43:07,693] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0214: [2024-08-10 16:43:07,693] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 64.0, reducing to 32.0
g0220: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43027
g0220: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43027
g0233: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43027
g0220: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0233: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0236: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43027
g0233: Grad overflow on iteration 43027
g0233: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0236: Grad overflow on iteration 43027
g0220: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0233: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0236: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0233: Grad overflow on iteration 43027
g0214: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43027
g0232: Grad overflow on iteration 43027
g0233: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0233: Grad overflow on iteration 43027
g0233: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0225: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0225: Grad overflow on iteration 43027
g0234: Grad overflow on iteration 43027
g0225: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0236: Grad overflow on iteration 43027
g0236: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0234: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43027
g0233: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0235: Grad overflow on iteration 43027
g0235: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43027
g0225: Grad overflow on iteration 43027
g0235: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0236: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43027
g0234: Grad overflow on iteration 43027
g0234: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0214: Grad overflow on iteration 43027
g0225: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0214: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43027
g0220: Grad overflow on iteration 43027
g0214: Grad overflow on iteration 43027
g0220: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0214: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0220: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43027
g0214: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0214: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43027
g0232: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0232: Grad overflow on iteration 43027
g0232: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0235: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0236: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43027
g0236: Grad overflow on iteration 43027
g0232: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43027
g0236: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0225: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0225: Grad overflow on iteration 43027
g0225: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0232: Grad overflow on iteration 43027
g0225: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0232: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0232: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0235: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0214: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43027
g0234: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0214: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0214: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0236: [2024-08-10 16:43:24,967] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0214: [2024-08-10 16:43:24,968] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32.0, reducing to 16.0
g0225: [2024-08-10 16:43:30,045] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43028
g0232: [2024-08-10 16:43:30,045] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0220: [2024-08-10 16:43:30,045] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43028
g0220: Grad overflow on iteration 43028
g0225: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0236: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43028
g0225: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0225: Grad overflow on iteration 43028
g0214: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43028
g0235: [2024-08-10 16:43:30,045] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43028
g0235: Grad overflow on iteration 43028
g0214: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:30,045] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43028
g0232: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43028
g0234: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43028
g0234: Grad overflow on iteration 43028
g0234: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0234: Grad overflow on iteration 43028
g0232: Grad overflow on iteration 43028
g0234: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0220: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0232: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43028
g0236: Grad overflow on iteration 43028
g0232: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0233: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0234: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43028
g0235: Grad overflow on iteration 43028
g0236: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0225: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0232: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0233: Grad overflow on iteration 43028
g0234: Grad overflow on iteration 43028
g0233: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0233: Grad overflow on iteration 43028
g0236: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43028
g0214: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43028
g0225: Grad overflow on iteration 43028
g0214: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43028
g0220: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0235: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0233: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43028
g0232: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0234: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0233: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0220: Grad overflow on iteration 43028
g0220: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0214: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0235: Grad overflow on iteration 43028
g0225: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0225: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0220: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43028
g0214: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0234: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0234: Grad overflow on iteration 43028
g0235: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0235: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0234: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0235: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0220: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0236: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0235: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0235: Grad overflow on iteration 43028
g0235: [2024-08-10 16:43:30,046] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0214: [2024-08-10 16:43:30,046] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16.0, reducing to 8.0
g0236: [2024-08-10 16:43:30,053] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43028
g0236: [2024-08-10 16:43:30,054] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0235: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43029
g0220: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0234: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43029
g0235: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0235: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43029
g0235: Grad overflow on iteration 43029
g0214: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0232: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0236: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43029
g0236: Grad overflow on iteration 43029
g0232: Grad overflow on iteration 43029
g0220: Grad overflow on iteration 43029
g0214: Grad overflow on iteration 43029
g0225: Grad overflow on iteration 43029
g0214: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43029
g0233: Grad overflow on iteration 43029
g0225: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43029
g0232: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0220: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0232: Grad overflow on iteration 43029
g0236: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0236: Grad overflow on iteration 43029
g0232: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0234: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0232: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0214: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0232: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0225: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0236: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0225: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43029
g0236: Grad overflow on iteration 43029
g0236: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0232: Grad overflow on iteration 43029
g0214: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43029
g0214: Grad overflow on iteration 43029
g0225: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0214: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0233: Grad overflow on iteration 43029
g0233: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0234: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43029
g0220: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0234: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0233: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0232: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0220: Grad overflow on iteration 43029
g0220: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0233: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43029
g0233: [2024-08-10 16:43:37,795] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0214: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43029
g0232: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43029
g0220: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43029
g0234: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43029
g0236: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43029
g0232: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0220: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0234: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0225: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43029
g0225: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0235: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43029
g0214: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0233: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43029
g0233: [2024-08-10 16:43:37,798] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0236: [2024-08-10 16:43:37,799] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0214: [2024-08-10 16:43:37,799] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8.0, reducing to 4.0
g0235: [2024-08-10 16:43:37,799] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0214: [2024-08-10 16:43:37,799] [INFO] [logging.py:96:log_dist] [Rank 0] step=43030, skipped=76, lr=[0.00019962062295257298, 0.00019962062295257298], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 43030 loss: nan iter time (s): 8.665 samples/sec: 14.772
g0236:  iteration    43030/10000000 | consumed samples:      5507840 | consumed tokens:  11280056320 | elapsed time per iteration (ms): 8697.4 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 4.0 | grad norm: 0.984 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 14.717 | tokens per gpu per second (tgs): 941.887 | TFLOPs: 7.58 |
g0232: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43030
g0232: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0232: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43030
g0220: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0225: Grad overflow on iteration 43030
g0225: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43030
g0225: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0236: Grad overflow on iteration 43030
g0220: Grad overflow on iteration 43030
g0225: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0236: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0232: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43030
g0220: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0225: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43030
g0225: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43030
g0225: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0214: Grad overflow on iteration 43030
g0214: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0214: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43030
g0214: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43030
g0214: Grad overflow on iteration 43030
g0232: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0235: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0220: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43030
g0235: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0234: Grad overflow on iteration 43030
g0214: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43030
g0214: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0233: Grad overflow on iteration 43030
g0214: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0236: Grad overflow on iteration 43030
g0220: Grad overflow on iteration 43030
g0220: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0220: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43030
g0234: Grad overflow on iteration 43030
g0233: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0236: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43030
g0236: Grad overflow on iteration 43030
g0233: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43030
g0232: Grad overflow on iteration 43030
g0225: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0232: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0220: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0236: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0232: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0236: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43030
g0235: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0233: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43030
g0233: Grad overflow on iteration 43030
g0234: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43030
g0235: Grad overflow on iteration 43030
g0235: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0214: [2024-08-10 16:43:46,007] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4.0, reducing to 2.0
g0220: Grad overflow on iteration 43030
g0234: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0234: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0234: Grad overflow on iteration 43030
g0234: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0236: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0233: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0234: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0233: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0233: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0233: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0220: [2024-08-10 16:43:46,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0225: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43031
g0225: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0225: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43031
g0232: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43031
g0232: Grad overflow on iteration 43031
g0235: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43031
g0214: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0220: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0235: Grad overflow on iteration 43031
g0220: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0220: Grad overflow on iteration 43031
g0236: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0235: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43031
g0234: Grad overflow on iteration 43031
g0232: Grad overflow on iteration 43031
g0225: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0236: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0234: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43031
g0214: Grad overflow on iteration 43031
g0236: Grad overflow on iteration 43031
g0232: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0234: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43031
g0214: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0225: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43031
g0233: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43031
g0232: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0234: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0214: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0233: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0232: Grad overflow on iteration 43031
g0233: [2024-08-10 16:43:53,965] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43031
g0220: Grad overflow on iteration 43031
g0232: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0214: Grad overflow on iteration 43031
g0220: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0225: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0235: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0220: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0233: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43031
g0233: Grad overflow on iteration 43031
g0233: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0236: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43031
g0236: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0236: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0235: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0214: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43031
g0214: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0214: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0214: [2024-08-10 16:43:53,966] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2.0, reducing to 1.0
g0214: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43031
g0236: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43031
g0232: Grad overflow on iteration 43031
g0225: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0225: Grad overflow on iteration 43031
g0235: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0233: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0236: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0235: Grad overflow on iteration 43031
g0233: Grad overflow on iteration 43031
g0220: Grad overflow on iteration 43031
g0220: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0233: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0235: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0234: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43031
g0234: [2024-08-10 16:43:53,966] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0233: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43032
g0233: Grad overflow on iteration 43032
g0232: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43032
g0233: Grad overflow on iteration 43032
g0214: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0233: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43032
g0233: Grad overflow on iteration 43032
g0233: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0214: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43032
g0225: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43032
g0232: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43032
g0235: Grad overflow on iteration 43032
g0225: Grad overflow on iteration 43032
g0214: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43032
g0225: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0214: Grad overflow on iteration 43032
g0214: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0214: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0214: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43032
g0214: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0233: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0233: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43032
g0214: [2024-08-10 16:44:00,349] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0233: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0220: Grad overflow on iteration 43032
g0233: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0234: Grad overflow on iteration 43032
g0232: Grad overflow on iteration 43032
g0234: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43032
g0225: Grad overflow on iteration 43032
g0234: Grad overflow on iteration 43032
g0225: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43032
g0220: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0236: Grad overflow on iteration 43032
g0220: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43032
g0234: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0220: Grad overflow on iteration 43032
g0236: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0232: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0234: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0234: Grad overflow on iteration 43032
g0220: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0234: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0220: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0234: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0235: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0225: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0232: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0234: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43032
g0235: Grad overflow on iteration 43032
g0235: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0232: Grad overflow on iteration 43032
g0214: [2024-08-10 16:44:00,349] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1.0, reducing to 1
g0236: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0225: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43032
g0225: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0236: Grad overflow on iteration 43032
g0225: [2024-08-10 16:44:00,349] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0236: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0232: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0234: [2024-08-10 16:44:00,349] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0232: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43032
g0232: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0236: [2024-08-10 16:44:00,349] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0232: [2024-08-10 16:44:00,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0236: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43033
g0225: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43033
g0220: Grad overflow on iteration 43033
g0214: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43033
g0233: Grad overflow on iteration 43033
g0220: Grad overflow on iteration 43033
g0233: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43033
g0232: Grad overflow on iteration 43033
g0214: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43033
g0214: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: Grad overflow on iteration 43033
g0225: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43033
g0220: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43033
g0233: Grad overflow on iteration 43033
g0232: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: Grad overflow on iteration 43033
g0233: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43033
g0225: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43033
g0220: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43033
g0220: Grad overflow on iteration 43033
g0235: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43033
g0214: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43033
g0232: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43033
g0232: Grad overflow on iteration 43033
g0235: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43033
g0232: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43033
g0232: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: Grad overflow on iteration 43033
g0235: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43033
g0236: Grad overflow on iteration 43033
g0233: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43033
g0235: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43033
g0236: Grad overflow on iteration 43033
g0234: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43033
g0234: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43033
g0234: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:05,439] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:05,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:05,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:05,440] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0225: [2024-08-10 16:44:05,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:13,806] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43034
g0225: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43034
g0225: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43034
g0233: [2024-08-10 16:44:13,806] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43034
g0233: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43034
g0225: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:13,806] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43034
g0233: Grad overflow on iteration 43034
g0233: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43034
g0232: Grad overflow on iteration 43034
g0232: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43034
g0232: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43034
g0214: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:13,806] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43034
g0232: Grad overflow on iteration 43034
g0214: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: Grad overflow on iteration 43034
g0235: [2024-08-10 16:44:13,806] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43034
g0214: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:13,806] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43034
g0235: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43034
g0235: Grad overflow on iteration 43034
g0235: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43034
g0214: Grad overflow on iteration 43034
g0234: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43034
g0234: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43034
g0220: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43034
g0236: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43034
g0233: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43034
g0236: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43034
g0235: Grad overflow on iteration 43034
g0234: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43034
g0234: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43034
g0235: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43034
g0236: Grad overflow on iteration 43034
g0232: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43034
g0220: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:13,807] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:13,807] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0232: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43035
g0233: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43035
g0235: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43035
g0232: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43035
g0214: Grad overflow on iteration 43035
g0232: Grad overflow on iteration 43035
g0233: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43035
g0235: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: Grad overflow on iteration 43035
g0233: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43035
g0225: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43035
g0225: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43035
g0232: Grad overflow on iteration 43035
g0220: Grad overflow on iteration 43035
g0234: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43035
g0236: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43035
g0235: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43035
g0214: Grad overflow on iteration 43035
g0225: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: Grad overflow on iteration 43035
g0220: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43035
g0214: Grad overflow on iteration 43035
g0236: Grad overflow on iteration 43035
g0214: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43035
g0236: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43035
g0233: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43035
g0232: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43035
g0232: Grad overflow on iteration 43035
g0236: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43035
g0232: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43035
g0233: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43035
g0220: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43035
g0235: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43035
g0214: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:20,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:20,458] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43035
g0214: [2024-08-10 16:44:20,459] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0225: [2024-08-10 16:44:20,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:20,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43036
g0214: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43036
g0214: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43036
g0235: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:26,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43036
g0220: Grad overflow on iteration 43036
g0232: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43036
g0220: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43036
g0235: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43036
g0235: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43036
g0232: Grad overflow on iteration 43036
g0232: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43036
g0220: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:26,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43036
g0220: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:26,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43036
g0233: Grad overflow on iteration 43036
g0235: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:26,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43036
g0235: Grad overflow on iteration 43036
g0225: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:26,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: Grad overflow on iteration 43036
g0236: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43036
g0233: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43036
g0225: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43036
g0233: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:26,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43036
g0234: [2024-08-10 16:44:26,562] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:26,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43036
g0233: Grad overflow on iteration 43036
g0234: [2024-08-10 16:44:26,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:26,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:26,561] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43036
g0236: [2024-08-10 16:44:26,562] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43036
g0225: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43036
g0235: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43036
g0220: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43036
g0232: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43036
g0234: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43036
g0232: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43036
g0225: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43036
g0214: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:26,565] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:26,566] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0225: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43037
g0225: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43037
g0225: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43037
g0220: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43037
g0232: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43037
g0232: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43037
g0232: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43037
g0235: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43037
g0234: Grad overflow on iteration 43037
g0220: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43037
g0220: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43037
g0236: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43037
g0236: Grad overflow on iteration 43037
g0234: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: Grad overflow on iteration 43037
g0235: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43037
g0234: Grad overflow on iteration 43037
g0236: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43037
g0232: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43037
g0234: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43037
g0236: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43037
g0233: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43037
g0233: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43037
g0233: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: Grad overflow on iteration 43037
g0214: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43037
g0233: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43037
g0232: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43037
g0225: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43037
g0220: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43037
g0220: Grad overflow on iteration 43037
g0214: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43037
g0234: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43037
g0235: [2024-08-10 16:44:34,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43037
g0232: [2024-08-10 16:44:34,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:34,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:34,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:34,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:34,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:34,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:34,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:34,044] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:34,044] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0214: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43038
g0214: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43038
g0214: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43038
g0225: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43038
g0232: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43038
g0232: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: Grad overflow on iteration 43038
g0220: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43038
g0225: Grad overflow on iteration 43038
g0234: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43038
g0234: Grad overflow on iteration 43038
g0225: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43038
g0225: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43038
g0233: Grad overflow on iteration 43038
g0225: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43038
g0233: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43038
g0232: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43038
g0220: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43038
g0235: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43038
g0220: Grad overflow on iteration 43038
g0232: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43038
g0232: Grad overflow on iteration 43038
g0236: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43038
g0234: Grad overflow on iteration 43038
g0233: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43038
g0235: Grad overflow on iteration 43038
g0234: Grad overflow on iteration 43038
g0214: Grad overflow on iteration 43038
g0234: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43038
g0214: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43038
g0236: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43038
g0225: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43038
g0236: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43038
g0233: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:39,012] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:39,012] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0225: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43039
g0232: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43039
g0232: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43039
g0225: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43039
g0232: Grad overflow on iteration 43039
g0232: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43039
g0232: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:48,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43039
g0232: Grad overflow on iteration 43039
g0232: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43039
g0235: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: Grad overflow on iteration 43039
g0235: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43039
g0233: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43039
g0235: Grad overflow on iteration 43039
g0220: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43039
g0234: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43039
g0235: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43039
g0214: Grad overflow on iteration 43039
g0214: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43039
g0214: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43039
g0236: Grad overflow on iteration 43039
g0236: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43039
g0236: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43039
g0220: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43039
g0235: Grad overflow on iteration 43039
g0236: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43039
g0220: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43039
g0220: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: Grad overflow on iteration 43039
g0214: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43039
g0233: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43039
g0233: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43039
g0234: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43039
g0233: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43039
g0234: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:48,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:48,084] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0214: [2024-08-10 16:44:48,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=43040, skipped=86, lr=[0.00019962062295257298, 0.00019962062295257298], mom=[(0.9, 0.95), (0.9, 0.95)]
g0214: steps: 43040 loss: nan iter time (s): 6.996 samples/sec: 18.297
g0236:  iteration    43040/10000000 | consumed samples:      5509120 | consumed tokens:  11282677760 | elapsed time per iteration (ms): 7028.7 | learning rate: 1.996E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 0.984 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.211 | tokens per gpu per second (tgs): 1165.510 | TFLOPs: 9.38 |
g0225: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43040
g0225: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43040
g0225: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43040
g0232: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43040
g0232: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43040
g0232: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43040
g0214: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43040
g0235: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43040
g0235: Grad overflow on iteration 43040
g0214: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43040
g0214: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: Grad overflow on iteration 43040
g0236: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43040
g0233: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43040
g0235: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43040
g0234: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43040
g0214: Grad overflow on iteration 43040
g0220: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43040
g0214: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43040
g0220: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43040
g0220: Grad overflow on iteration 43040
g0233: Grad overflow on iteration 43040
g0233: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43040
g0220: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43040
g0233: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43040
g0234: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43040
g0233: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43040
g0236: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:44:53,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43040
g0233: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: Grad overflow on iteration 43040
g0234: Grad overflow on iteration 43040
g0234: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43040
g0234: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43040
g0233: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43040
g0214: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:44:53,476] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0225: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:44:53,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43041
g0235: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43041
g0232: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43041
g0214: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43041
g0214: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43041
g0214: Grad overflow on iteration 43041
g0225: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43041
g0234: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43041
g0220: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43041
g0220: Grad overflow on iteration 43041
g0234: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43041
g0232: Grad overflow on iteration 43041
g0233: Grad overflow on iteration 43041
g0232: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43041
g0233: Grad overflow on iteration 43041
g0236: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43041
g0235: Grad overflow on iteration 43041
g0233: Grad overflow on iteration 43041
g0233: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43041
g0225: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43041
g0233: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43041
g0225: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: Grad overflow on iteration 43041
g0236: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43041
g0234: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43041
g0232: Grad overflow on iteration 43041
g0234: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43041
g0220: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43041
g0232: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43041
g0236: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:06,947] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43041
g0214: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43041
g0214: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43041
g0235: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:06,948] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:06,948] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0225: [2024-08-10 16:45:06,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43041
g0225: [2024-08-10 16:45:06,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43042
g0232: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43042
g0232: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43042
g0225: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43042
g0225: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43042
g0214: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43042
g0214: Grad overflow on iteration 43042
g0214: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43042
g0225: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43042
g0220: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43042
g0235: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43042
g0220: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43042
g0234: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43042
g0236: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43042
g0214: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43042
g0236: Grad overflow on iteration 43042
g0235: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43042
g0233: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43042
g0220: Grad overflow on iteration 43042
g0233: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43042
g0232: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43042
g0232: Grad overflow on iteration 43042
g0232: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43042
g0233: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43042
g0220: Grad overflow on iteration 43042
g0236: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43042
g0232: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43042
g0236: Grad overflow on iteration 43042
g0220: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43042
g0236: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43042
g0214: [2024-08-10 16:45:13,890] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:45:13,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: Grad overflow on iteration 43042
g0214: [2024-08-10 16:45:13,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:45:13,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:13,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:13,891] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43042
g0214: [2024-08-10 16:45:13,891] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0236: [2024-08-10 16:45:13,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:13,891] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43043
g0214: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43043
g0214: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43043
g0214: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43043
g0225: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43043
g0225: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43043
g0225: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43043
g0234: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43043
g0232: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: Grad overflow on iteration 43043
g0234: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43043
g0234: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43043
g0220: Grad overflow on iteration 43043
g0234: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: Grad overflow on iteration 43043
g0220: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43043
g0220: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43043
g0233: Grad overflow on iteration 43043
g0220: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43043
g0232: Grad overflow on iteration 43043
g0233: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43043
g0220: Grad overflow on iteration 43043
g0232: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 43043
g0234: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43043
g0233: Grad overflow on iteration 43043
g0233: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 43043
g0233: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43043
g0232: Grad overflow on iteration 43043
g0214: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: Grad overflow on iteration 43043
g0232: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43043
g0232: Grad overflow on iteration 43043
g0236: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43043
g0236: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43043
g0236: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:19,525] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0225: [2024-08-10 16:45:19,524] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43043
g0225: [2024-08-10 16:45:19,525] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43044
g0220: Grad overflow on iteration 43044
g0236: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43044
g0233: Grad overflow on iteration 43044
g0225: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43044
g0220: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43044
g0236: Grad overflow on iteration 43044
g0225: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43044
g0214: Grad overflow on iteration 43044
g0233: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0225: Grad overflow on iteration 43044
g0225: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 43044
g0214: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: Grad overflow on iteration 43044
g0232: Grad overflow on iteration 43044
g0234: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: Grad overflow on iteration 43044
g0234: Grad overflow on iteration 43044
g0234: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0232: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43044
g0235: Grad overflow on iteration 43044
g0225: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0233: Grad overflow on iteration 43044
g0233: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: Grad overflow on iteration 43044
g0232: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: Grad overflow on iteration 43044
g0234: Grad overflow on iteration 43044
g0232: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 43044
g0235: Grad overflow on iteration 43044
g0220: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0232: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43044
g0233: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0220: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0235: Grad overflow on iteration 43044
g0235: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 43044
g0235: Grad overflow on iteration 43044
g0214: Grad overflow on iteration 43044
g0235: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: Grad overflow on iteration 43044
g0235: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 43044
g0236: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0236: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:344:_update_scale] 
g0214: Grad overflow on iteration 43044
g0234: Grad overflow on iteration 43044
g0214: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0234: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:24,861] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0214: [2024-08-10 16:45:24,861] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
