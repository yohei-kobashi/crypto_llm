
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0108
    HostName g0108
    Port 2222
    StrictHostKeyChecking no

Host g0113
    HostName g0113
    Port 2222
    StrictHostKeyChecking no

Host g0115
    HostName g0115
    Port 2222
    StrictHostKeyChecking no

Host g0119
    HostName g0119
    Port 2222
    StrictHostKeyChecking no

Host g0120
    HostName g0120
    Port 2222
    StrictHostKeyChecking no

Host g0121
    HostName g0121
    Port 2222
    StrictHostKeyChecking no

Host g0123
    HostName g0123
    Port 2222
    StrictHostKeyChecking no

Host g0124
    HostName g0124
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42830265
g0108 slots=4
g0113 slots=4
g0115 slots=4
g0119 slots=4
g0120 slots=4
g0121 slots=4
g0123 slots=4
g0124 slots=4

[2024-08-12 11:22:27,287] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-12 11:22:33,330] [INFO] [runner.py:463:main] Using IP address of 10.1.4.6 for node g0108
[2024-08-12 11:22:33,332] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0108,g0113,g0115,g0119,g0120,g0121,g0123,g0124
[2024-08-12 11:22:33,332] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0108,g0113,g0115,g0119,g0120,g0121,g0123,g0124 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDEwOCI6IFswLCAxLCAyLCAzXSwgImcwMTEzIjogWzAsIDEsIDIsIDNdLCAiZzAxMTUiOiBbMCwgMSwgMiwgM10sICJnMDExOSI6IFswLCAxLCAyLCAzXSwgImcwMTIwIjogWzAsIDEsIDIsIDNdLCAiZzAxMjEiOiBbMCwgMSwgMiwgM10sICJnMDEyMyI6IFswLCAxLCAyLCAzXSwgImcwMTI0IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.4.6 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '13631488000' --train-samples '6656000' --lr '2.0e-4' --min-lr '2.0e-6' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True' --wandb_tag 'other_gpu'
g0108: [2024-08-12 11:22:36,768] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 11:22:38,967] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0108: [2024-08-12 11:22:38,967] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0115': [0, 1, 2, 3], 'g0119': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0124': [0, 1, 2, 3]}
g0108: [2024-08-12 11:22:38,967] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0108: [2024-08-12 11:22:38,967] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0115': [8, 9, 10, 11], 'g0119': [12, 13, 14, 15], 'g0120': [16, 17, 18, 19], 'g0121': [20, 21, 22, 23], 'g0123': [24, 25, 26, 27], 'g0124': [28, 29, 30, 31]})
g0108: [2024-08-12 11:22:38,967] [INFO] [launch.py:163:main] dist_world_size=32
g0108: [2024-08-12 11:22:38,967] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0108: [2024-08-12 11:22:42,123] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 11:22:42,123] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 11:22:42,123] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 11:22:42,350] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 11:22:44,392] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 11:22:44,790] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 11:22:44,831] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 11:22:44,835] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0115: [2024-08-12 11:22:44,838] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0119: [2024-08-12 11:22:45,029] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0124: [2024-08-12 11:22:45,047] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: ninja .................. [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: op name ................ installed .. compatible
g0108: --------------------------------------------------
g0108: ninjaninja  ....................................  [92m[OKAY][0m[92m[OKAY][0m
g0108: 
g0108: ----------------------------------------------------------------------------------------------------
g0108: 
g0108: op nameop name  ................................  installedinstalled  ....  compatiblecompatible
g0108: 
g0108: ----------------------------------------------------------------------------------------------------
g0108: 
g0108: ninja .................. [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: op name ................ installed .. compatible
g0108: --------------------------------------------------
g0108: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0108: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0108: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0masync_io
g0108:  ............... [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0108: [92m[OKAY][0mevoformer_attn
g0108:  ......... [93m[NO][0m ....... [93m[NO][0m
g0108: fused_lamb fused_adam.............  [92m[YES][0m............. ......  [92m[YES][0m[92m[OKAY][0m 
g0108: ...... [92m[OKAY][0m
g0108: fused_lioncpu_adam  ............................ [92m[YES][0m  [92m[YES][0m......  [92m[OKAY][0m......
g0108:  [92m[OKAY][0m
g0108: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0108: fused_adam .............evoformer_attn [92m[YES][0m  ...............  [92m[OKAY][0m[93m[NO][0m
g0108:  ....... cpu_adam[93m[NO][0m ...............
g0108:  [92m[YES][0m ...... [92m[OKAY][0mfused_lamb
g0108:  .............cpu_adagrad  [92m[YES][0m............  [92m[YES][0m......  ...... [92m[OKAY][0m[92m[OKAY][0m
g0108: 
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_lion ............. [92m[YES][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......
g0108:  evoformer_attn[92m[OKAY][0m 
g0108: ......... [93m[NO][0m ....... [93m[NO][0m
g0108: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: inference_core_ops inference_core_ops.....  [92m[YES][0m ...........  inference_core_ops[92m[OKAY][0m[92m[YES][0m 
g0108:  ........... [92m[YES][0m  [92m[OKAY][0m...... 
g0108: [92m[OKAY][0m
g0108: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0mcutlass_ops  ..................  [92m[YES][0m[92m[OKAY][0m ......
g0108:  [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0108: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0108: using torch.float32 for parameters ...
g0108: ------------------------ arguments ------------------------
g0108:   accumulate_allreduce_grads_in_fp32 .............. False
g0108:   adam_beta1 ...................................... 0.9
g0108:   adam_beta2 ...................................... 0.95
g0108:   adam_eps ........................................ 1e-08
g0108:   add_bias_linear ................................. False
g0108:   add_position_embedding .......................... False
g0108:   adlr_autoresume ................................. False
g0108:   adlr_autoresume_interval ........................ 1000
g0108:   aml_data_download_path .......................... None
g0108:   apply_layernorm_1p .............................. False
g0108:   apply_query_key_layer_scaling ................... False
g0108:   apply_residual_connection_post_layernorm ........ False
g0108:   async_tensor_model_parallel_allreduce ........... False
g0108:   attention_dropout ............................... 0.0
g0108:   attention_softmax_in_fp32 ....................... False
g0108:   barrier_with_L1_time ............................ True
g0108:   bert_binary_head ................................ True
g0108:   bert_embedder_type .............................. megatron
g0108:   bert_load ....................................... None
g0108:   bf16 ............................................ False
g0108:   bias_dropout_fusion ............................. True
g0108:   bias_gelu_fusion ................................ False
g0108:   biencoder_projection_dim ........................ 0
g0108:   biencoder_shared_query_context_model ............ False
g0108:   block_data_path ................................. None
g0108:   checkpoint_activations .......................... False
g0108:   checkpoint_in_cpu ............................... False
g0108:   checkpoint_num_layers ........................... 1
g0108:   classes_fraction ................................ 1.0
g0108:   clip_grad ....................................... 1.0
g0108:   compression_training ............................ False
g0108:   consumed_train_samples .......................... 0
g0108:   consumed_train_tokens ........................... 0
g0108:   consumed_valid_samples .......................... 0
g0108:   contigious_checkpointing ........................ False
g0108:   cpu_optimizer ................................... False
g0108:   cpu_torch_adam .................................. False
g0108:   create_moe_param_group .......................... False
g0108:   curriculum_learning_legacy ...................... False
g0108:   data_cache_path ................................. None
g0108:   data_efficiency_curriculum_learning ............. False
g0108:   data_impl ....................................... mmap
g0108:   data_parallel_random_init ....................... False
g0108:   data_parallel_size .............................. 4
g0108:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_no_encryption_text_document']
g0108:   data_per_class_fraction ......................... 1.0
g0108:   data_sharding ................................... True
g0108:   dataloader_type ................................. single
g0108:   DDP_impl ........................................ local
g0108:   decoder_num_layers .............................. None
g0108:   decoder_seq_length .............................. None
g0108:   deepscale ....................................... False
g0108:   deepscale_config ................................ None
g0108:   deepspeed ....................................... True
g0108:   deepspeed_activation_checkpointing .............. False
g0108:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0108:   deepspeed_mpi ................................... False
g0108:   dino_bottleneck_size ............................ 256
g0108:   dino_freeze_last_layer .......................... 1
g0108:   dino_head_hidden_size ........................... 2048
g0108:   dino_local_crops_number ......................... 10
g0108:   dino_local_img_size ............................. 96
g0108:   dino_norm_last_layer ............................ False
g0108:   dino_teacher_temp ............................... 0.07
g0108:   dino_warmup_teacher_temp ........................ 0.04
g0108:   dino_warmup_teacher_temp_epochs ................. 30
g0108:   distribute_checkpointed_activations ............. False
g0108:   distribute_saved_activations .................... False
g0108:   distributed_backend ............................. nccl
g0108:   distributed_timeout_minutes ..................... 10
g0108:   ds_fused_adam ................................... False
g0108:   ds_inference .................................... False
g0108:   ds_pipeline_enabled ............................. True
g0108:   ds_sequence_parallel_size ....................... 1
g0108:   embedding_path .................................. None
g0108:   embedding_weights_in_fp32 ....................... False
g0108:   empty_unused_memory_level ....................... 0
g0108:   enable_expert_tensor_parallelism ................ False
g0108:   encoder_num_layers .............................. 22
g0108:   encoder_seq_length .............................. 2048
g0108:   end_weight_decay ................................ 0.1
g0108:   eod_mask_loss ................................... False
g0108:   eval_interval ................................... 1000
g0108:   eval_iters ...................................... 100
g0108:   evidence_data_path .............................. None
g0108:   exit_duration_in_mins ........................... 30000000
g0108:   exit_interval ................................... None
g0108:   exit_on_missing_checkpoint ...................... False
g0108:   exit_signal_handler ............................. False
g0108:   expert_interval ................................. 2
g0108:   ffn_hidden_size ................................. 5632
g0108:   finetune ........................................ False
g0108:   force_ds_sequence_parallel ...................... False
g0108:   fp16 ............................................ False
g0108:   fp16_lm_cross_entropy ........................... False
g0108:   fp32_residual_connection ........................ False
g0108:   fp8_amax_compute_algo ........................... most_recent
g0108:   fp8_amax_history_len ............................ 1
g0108:   fp8_e4m3 ........................................ False
g0108:   fp8_hybrid ...................................... False
g0108:   fp8_interval .................................... 1
g0108:   fp8_margin ...................................... 0
g0108:   fp8_wgrad ....................................... True
g0108:   global_batch_size ............................... 128
g0108:   gradient_accumulation_fusion .................... True
g0108:   head_lr_mult .................................... 1.0
g0108:   hidden_dropout .................................. 0.0
g0108:   hidden_size ..................................... 2048
g0108:   hidden_size_teacher ............................. None
g0108:   hysteresis ...................................... 2
g0108:   ict_head_size ................................... None
g0108:   ict_load ........................................ None
g0108:   img_h ........................................... 224
g0108:   img_w ........................................... 224
g0108:   indexer_batch_size .............................. 128
g0108:   indexer_log_interval ............................ 1000
g0108:   inference ....................................... False
g0108:   inference_batch_times_seqlen_threshold .......... 512
g0108:   init_method_std ................................. 0.013
g0108:   init_method_xavier_uniform ...................... False
g0108:   initial_loss_scale .............................. 4294967296
g0108:   iter_per_epoch .................................. 1250
g0108:   kd .............................................. False
g0108:   kd_alpha_ce ..................................... 1
g0108:   kd_beta_ce ...................................... 1
g0108:   kd_temp ......................................... 1.0
g0108:   kv_channels ..................................... 128
g0108:   layernorm_epsilon ............................... 1e-05
g0108:   lazy_mpu_init ................................... None
g0108:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0108:   load_teacher .................................... None
g0108:   local_rank ...................................... 0
g0108:   log_batch_size_to_tensorboard ................... True
g0108:   log_interval .................................... 10
g0108:   log_learning_rate_to_tensorboard ................ True
g0108:   log_loss_scale_to_tensorboard ................... True
g0108:   log_memory_to_tensorboard ....................... False
g0108:   log_num_zeros_in_grad ........................... False
g0108:   log_optimizer_states_to_tensorboard ............. True
g0108:   log_params_norm ................................. False
g0108:   log_timers_to_tensorboard ....................... True
g0108:   log_validation_ppl_to_tensorboard ............... True
g0108:   log_world_size_to_tensorboard ................... False
g0108:   loss_scale ...................................... None
g0108:   loss_scale_window ............................... 1000
g0108:   lr .............................................. 0.0002
g0108:   lr_decay_iters .................................. None
g0108:   lr_decay_samples ................................ None
g0108:   lr_decay_style .................................. cosine
g0108:   lr_decay_tokens ................................. 300000000000
g0108:   lr_warmup_fraction .............................. None
g0108:   lr_warmup_iters ................................. 0
g0108:   lr_warmup_samples ............................... 0
g0108:   lr_warmup_tokens ................................ 3000000000
g0108:   make_vocab_size_divisible_by .................... 128
g0108:   mask_factor ..................................... 1.0
g0108:   mask_prob ....................................... 0.15
g0108:   mask_type ....................................... random
g0108:   masked_softmax_fusion ........................... True
g0108:   max_position_embeddings ......................... 2048
g0108:   max_tokens_to_oom ............................... 12000
g0108:   mem_efficient_ln ................................ True
g0108:   memory_centric_tiled_linear ..................... False
g0108:   merge_file ...................................... None
g0108:   micro_batch_size ................................ 1
g0108:   min_loss_scale .................................. 1.0
g0108:   min_lr .......................................... 2e-06
g0108:   mlp_type ........................................ standard
g0108:   mmap_warmup ..................................... False
g0108:   moe_eval_capacity_factor ........................ 1.0
g0108:   moe_expert_parallel_size ........................ 1
g0108:   moe_loss_coeff .................................. 0.1
g0108:   moe_min_capacity ................................ 4
g0108:   moe_token_dropping .............................. True
g0108:   moe_train_capacity_factor ....................... 1.0
g0108:   mos ............................................. False
g0108:   no_load_lr_state ................................ False
g0108:   no_load_optim ................................... None
g0108:   no_load_rng ..................................... None
g0108:   no_persist_layer_norm ........................... False
g0108:   no_pipeline_parallel ............................ False
g0108:   no_save_optim ................................... None
g0108:   no_save_rng ..................................... None
g0108:   normalization ................................... rmsnorm
g0108:   num_attention_heads ............................. 16
g0108:   num_attention_heads_teacher ..................... None
g0108:   num_channels .................................... 3
g0108:   num_classes ..................................... 1000
g0108:   num_experts ..................................... [1]
g0108:   num_experts_switch .............................. None
g0108:   num_experts_teacher ............................. [1]
g0108:   num_key_value_heads ............................. 4
g0108:   num_layers ...................................... 22
g0108:   num_layers_per_virtual_pipeline_stage ........... None
g0108:   num_layers_teacher .............................. None
g0108:   num_workers ..................................... 0
g0108:   onnx_safe ....................................... None
g0108:   openai_gelu ..................................... False
g0108:   optimizer ....................................... adam
g0108:   output_bert_embeddings .......................... False
g0108:   overlap_p2p_comm ................................ False
g0108:   override_opt_param_scheduler .................... True
g0108:   params_dtype .................................... torch.float32
g0108:   partition_activations ........................... False
g0108:   patch_dim ....................................... 16
g0108:   perform_initialization .......................... True
g0108:   pipeline_model_parallel_size .................... 8
g0108:   pipeline_model_parallel_split_rank .............. None
g0108:   profile_backward ................................ False
g0108:   query_in_block_prob ............................. 0.1
g0108:   rampup_batch_size ............................... None
g0108:   random_ltd ...................................... False
g0108:   rank ............................................ 0
g0108:   recompute_granularity ........................... None
g0108:   recompute_method ................................ None
g0108:   recompute_num_layers ............................ 1
g0108:   remote_device ................................... none
g0108:   repeated_dataloader ............................. False
g0108:   reset_attention_mask ............................ False
g0108:   reset_iteration ................................. False
g0108:   reset_position_ids .............................. False
g0108:   retriever_report_topk_accuracies ................ []
g0108:   retriever_score_scaling ......................... False
g0108:   retriever_seq_length ............................ 256
g0108:   retro_add_retriever ............................. False
g0108:   retro_cyclic_train_iters ........................ None
g0108:   retro_encoder_attention_dropout ................. 0.1
g0108:   retro_encoder_hidden_dropout .................... 0.1
g0108:   retro_encoder_layers ............................ 2
g0108:   retro_num_neighbors ............................. 2
g0108:   retro_num_retrieved_chunks ...................... 2
g0108:   retro_return_doc_ids ............................ False
g0108:   retro_workdir ................................... None
g0108:   return_data_index ............................... False
g0108:   rotary_percent .................................. 1.0
g0108:   sample_rate ..................................... 1.0
g0108:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0108:   save_interval ................................... 1000
g0108:   scatter_gather_tensors_in_pipeline .............. True
g0108:   scattered_embeddings ............................ False
g0108:   seed ............................................ 1234
g0108:   seq_length ...................................... 2048
g0108:   sequence_parallel ............................... False
g0108:   sgd_momentum .................................... 0.9
g0108:   short_seq_prob .................................. 0.1
g0108:   skip_train ...................................... False
g0108:   split ........................................... 949,50,1
g0108:   split_transformers .............................. False
g0108:   squared_relu .................................... False
g0108:   standalone_embedding_stage ...................... False
g0108:   start_weight_decay .............................. 0.1
g0108:   swiglu .......................................... True
g0108:   swin_backbone_type .............................. tiny
g0108:   synchronize_each_layer .......................... False
g0108:   tensor_model_parallel_size ...................... 1
g0108:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True
g0108:   tensorboard_log_interval ........................ 1
g0108:   tensorboard_queue_size .......................... 1
g0108:   test_data_path .................................. None
g0108:   tf32 ............................................ False
g0108:   tile_factor ..................................... 1
g0108:   timing_log_level ................................ 0
g0108:   timing_log_option ............................... minmax
g0108:   titles_data_path ................................ None
g0108:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
g0108:   tokenizer_type .................................. SentencePieceTokenizer
g0108:   topk ............................................ 1
g0108:   train_data_exact_num_epochs ..................... None
g0108:   train_data_path ................................. None
g0108:   train_desc_path ................................. None
g0108:   train_doc_idx_path .............................. None
g0108:   train_idx_path .................................. None
g0108:   train_iters ..................................... None
g0108:   train_sample_idx_path ........................... None
g0108:   train_samples ................................... 6656000
g0108:   train_shuffle_idx_path .......................... None
g0108:   train_tokens .................................... 13631488000
g0108:   transformer_impl ................................ local
g0108:   transformer_pipeline_model_parallel_size ........ 8
g0108:   universal_checkpoint ............................ False
g0108:   untie_embeddings_and_output_weights ............. True
g0108:   use_checkpoint_args ............................. False
g0108:   use_checkpoint_opt_param_scheduler .............. False
g0108:   use_contiguous_buffers_in_local_ddp ............. True
g0108:   use_cpu_initialization .......................... None
g0108:   use_dataset_only ................................ False
g0108:   use_distributed_optimizer ....................... False
g0108:   use_flash_attn .................................. False
g0108:   use_flash_attn_triton ........................... False
g0108:   use_flash_attn_v1 ............................... False
g0108:   use_flash_attn_v2 ............................... False
g0108:   use_one_sent_docs ............................... False
g0108:   use_pin_memory .................................. False
g0108:   use_ring_exchange_p2p ........................... False
g0108:   use_rotary_position_embeddings .................. True
g0108:   use_tutel ....................................... False
g0108:   use_wandb ....................................... True
g0108:   valid_data_path ................................. None
g0108:   variable_seq_lengths ............................ False
g0108:   virtual_pipeline_model_parallel_size ............ None
g0108:   vision_backbone_type ............................ vit
g0108:   vision_pretraining .............................. False
g0108:   vision_pretraining_type ......................... classify
g0108:   vocab_extra_ids ................................. 0
g0108:   vocab_file ...................................... None
g0108:   vocab_size ...................................... None
g0108:   wandb_entity .................................... yohei-kobashi
g0108:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True
g0108:   wandb_project ................................... encrypted_data_LLM
g0108:   wandb_tag ....................................... other_gpu
g0108:   weight_decay .................................... 0.1
g0108:   weight_decay_incr_style ......................... constant
g0108:   world_size ...................................... 32
g0108:   zero_allgather_bucket_size ...................... 0.0
g0108:   zero_contigious_gradients ....................... False
g0108:   zero_reduce_bucket_size ......................... 0.0
g0108:   zero_reduce_scatter ............................. False
g0108:   zero_stage ...................................... 0
g0108: -------------------- end of arguments ---------------------
g0108: setting number of micro-batches to constant 32
g0108: > building SentencePieceTokenizer tokenizer ...
g0108:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0108: > initializing torch distributed ...
g0108: [2024-08-12 11:22:46,776] [INFO] [comm.py:637:init_distributed] cdb=None
g0108: [2024-08-12 11:22:46,776] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0108: [2024-08-12 11:22:46,781] [INFO] [comm.py:637:init_distributed] cdb=None
g0108: [2024-08-12 11:22:46,781] [INFO] [comm.py:637:init_distributed] cdb=None
g0108: [2024-08-12 11:22:46,782] [INFO] [comm.py:637:init_distributed] cdb=None
g0108: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [2024-08-12 11:22:48,568] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0123: [2024-08-12 11:22:48,569] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0115': [0, 1, 2, 3], 'g0119': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0124': [0, 1, 2, 3]}
g0123: [2024-08-12 11:22:48,569] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0123: [2024-08-12 11:22:48,569] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0115': [8, 9, 10, 11], 'g0119': [12, 13, 14, 15], 'g0120': [16, 17, 18, 19], 'g0121': [20, 21, 22, 23], 'g0123': [24, 25, 26, 27], 'g0124': [28, 29, 30, 31]})
g0123: [2024-08-12 11:22:48,569] [INFO] [launch.py:163:main] dist_world_size=32
g0123: [2024-08-12 11:22:48,569] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0121: [2024-08-12 11:22:48,818] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0121: [2024-08-12 11:22:48,818] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0115': [0, 1, 2, 3], 'g0119': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0124': [0, 1, 2, 3]}
g0121: [2024-08-12 11:22:48,819] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0121: [2024-08-12 11:22:48,819] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0115': [8, 9, 10, 11], 'g0119': [12, 13, 14, 15], 'g0120': [16, 17, 18, 19], 'g0121': [20, 21, 22, 23], 'g0123': [24, 25, 26, 27], 'g0124': [28, 29, 30, 31]})
g0121: [2024-08-12 11:22:48,819] [INFO] [launch.py:163:main] dist_world_size=32
g0121: [2024-08-12 11:22:48,819] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0113: [2024-08-12 11:22:48,879] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0113: [2024-08-12 11:22:48,879] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0115': [0, 1, 2, 3], 'g0119': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0124': [0, 1, 2, 3]}
g0113: [2024-08-12 11:22:48,879] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0113: [2024-08-12 11:22:48,879] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0115': [8, 9, 10, 11], 'g0119': [12, 13, 14, 15], 'g0120': [16, 17, 18, 19], 'g0121': [20, 21, 22, 23], 'g0123': [24, 25, 26, 27], 'g0124': [28, 29, 30, 31]})
g0113: [2024-08-12 11:22:48,879] [INFO] [launch.py:163:main] dist_world_size=32
g0113: [2024-08-12 11:22:48,879] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0115: [2024-08-12 11:22:48,910] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0115: [2024-08-12 11:22:48,910] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0115': [0, 1, 2, 3], 'g0119': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0124': [0, 1, 2, 3]}
g0115: [2024-08-12 11:22:48,910] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0115: [2024-08-12 11:22:48,910] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0115': [8, 9, 10, 11], 'g0119': [12, 13, 14, 15], 'g0120': [16, 17, 18, 19], 'g0121': [20, 21, 22, 23], 'g0123': [24, 25, 26, 27], 'g0124': [28, 29, 30, 31]})
g0115: [2024-08-12 11:22:48,910] [INFO] [launch.py:163:main] dist_world_size=32
g0115: [2024-08-12 11:22:48,910] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0120: [2024-08-12 11:22:48,939] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0120: [2024-08-12 11:22:48,939] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0115': [0, 1, 2, 3], 'g0119': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0124': [0, 1, 2, 3]}
g0120: [2024-08-12 11:22:48,939] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0120: [2024-08-12 11:22:48,939] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0115': [8, 9, 10, 11], 'g0119': [12, 13, 14, 15], 'g0120': [16, 17, 18, 19], 'g0121': [20, 21, 22, 23], 'g0123': [24, 25, 26, 27], 'g0124': [28, 29, 30, 31]})
g0120: [2024-08-12 11:22:48,939] [INFO] [launch.py:163:main] dist_world_size=32
g0120: [2024-08-12 11:22:48,939] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0119: [2024-08-12 11:22:49,106] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0119: [2024-08-12 11:22:49,106] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0115': [0, 1, 2, 3], 'g0119': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0124': [0, 1, 2, 3]}
g0119: [2024-08-12 11:22:49,106] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0119: [2024-08-12 11:22:49,106] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0115': [8, 9, 10, 11], 'g0119': [12, 13, 14, 15], 'g0120': [16, 17, 18, 19], 'g0121': [20, 21, 22, 23], 'g0123': [24, 25, 26, 27], 'g0124': [28, 29, 30, 31]})
g0119: [2024-08-12 11:22:49,106] [INFO] [launch.py:163:main] dist_world_size=32
g0119: [2024-08-12 11:22:49,106] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0124: [2024-08-12 11:22:49,133] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0124: [2024-08-12 11:22:49,134] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0115': [0, 1, 2, 3], 'g0119': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0124': [0, 1, 2, 3]}
g0124: [2024-08-12 11:22:49,134] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0124: [2024-08-12 11:22:49,134] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0115': [8, 9, 10, 11], 'g0119': [12, 13, 14, 15], 'g0120': [16, 17, 18, 19], 'g0121': [20, 21, 22, 23], 'g0123': [24, 25, 26, 27], 'g0124': [28, 29, 30, 31]})
g0124: [2024-08-12 11:22:49,134] [INFO] [launch.py:163:main] dist_world_size=32
g0124: [2024-08-12 11:22:49,134] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0123: [2024-08-12 11:22:51,776] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 11:22:51,777] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 11:22:51,777] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 11:22:51,920] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 11:22:51,921] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 11:22:51,921] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0115: [2024-08-12 11:22:52,026] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 11:22:52,035] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 11:22:52,043] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 11:22:52,057] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 11:22:52,059] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 11:22:52,061] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 11:22:52,069] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0115: [2024-08-12 11:22:52,076] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 11:22:52,081] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0115: [2024-08-12 11:22:52,093] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0115: [2024-08-12 11:22:52,096] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 11:22:52,132] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 11:22:52,200] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 11:22:52,200] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0119: [2024-08-12 11:22:52,216] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0119: [2024-08-12 11:22:52,217] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0119: [2024-08-12 11:22:52,217] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0124: [2024-08-12 11:22:52,219] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0124: [2024-08-12 11:22:52,219] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0124: [2024-08-12 11:22:52,221] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0119: [2024-08-12 11:22:52,316] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0124: [2024-08-12 11:22:52,430] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0123: ninja .................. [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: op name ................ installed .. compatible
g0123: --------------------------------------------------
g0123: ninjaninja  ....................................  [92m[OKAY][0m[92m[OKAY][0m
g0123: 
g0123: ----------------------------------------------------------------------------------------------------
g0123: 
g0123: op nameop name  ................................  installedinstalled  ....  compatiblecompatible
g0123: 
g0123: ----------------------------------------------------------------------------------------------------
g0123: 
g0123: ninja .................. [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: op name ................ installed .. compatible
g0123: --------------------------------------------------
g0123: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lambasync_io .............  ...............[92m[YES][0m  [92m[YES][0m......  ...... [92m[OKAY][0m[92m[OKAY][0m
g0123: 
g0123: fused_adam .............fused_lion [92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m ...... [92m[OKAY][0m
g0123: 
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: inference_core_ops inference_core_ops.....  [92m[YES][0m.....  ......[92m[YES][0m  [92m[OKAY][0m......
g0123:  [92m[OKAY][0m
g0123: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0123: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0123: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0123: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0123: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: ragged_ops ............. [92m[YES][0m ...... [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible[92m[OKAY][0m
g0123: 
g0123: sparse_attn ............random_ltd  [93m[NO][0m.............  .......[92m[YES][0m  [93m[NO][0m......
g0123:  [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0123: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0120: --------------------------------------------------
g0120: DeepSpeed C++/CUDA extension op report
g0120: --------------------------------------------------
g0120: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.
g0120: --------------------------------------------------
g0120: JIT compiled ops requires ninja
g0120: --------------------------------------------------
g0120: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0120: --------------------------------------------------
g0120: 
g0120: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.DeepSpeed C++/CUDA extension op report
g0120: 
g0120: ----------------------------------------------------------------------------------------------------
g0120: 
g0120: JIT compiled ops requires ninjaNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.
g0120: 
g0120: --------------------------------------------------
g0120: JIT compiled ops requires ninja
g0120: --------------------------------------------------
g0120: DeepSpeed C++/CUDA extension op report
g0120: --------------------------------------------------
g0120: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.
g0120: --------------------------------------------------
g0120: JIT compiled ops requires ninja
g0115: --------------------------------------------------
g0115: DeepSpeed C++/CUDA extension op report
g0115: --------------------------------------------------
g0115: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0115:       runtime if needed. Op compatibility means that your system
g0115:       meet the required dependencies to JIT install the op.
g0115: ----------------------------------------------------------------------------------------------------
g0115: JIT compiled ops requires ninja
g0115: 
g0115: DeepSpeed C++/CUDA extension op report
g0115: --------------------------------------------------
g0115: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0115:       runtime if needed. Op compatibility means that your system
g0115:       meet the required dependencies to JIT install the op.
g0115: --------------------------------------------------
g0115: JIT compiled ops requires ninja
g0115: --------------------------------------------------
g0115: DeepSpeed C++/CUDA extension op report
g0115: --------------------------------------------------
g0115: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0115:       runtime if needed. Op compatibility means that your system
g0115:       meet the required dependencies to JIT install the op.
g0115: --------------------------------------------------
g0115: JIT compiled ops requires ninja
g0115: --------------------------------------------------
g0115: DeepSpeed C++/CUDA extension op report
g0115: --------------------------------------------------
g0115: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0115:       runtime if needed. Op compatibility means that your system
g0115:       meet the required dependencies to JIT install the op.
g0115: --------------------------------------------------
g0115: JIT compiled ops requires ninja
g0120: ninjaninjaninjaninja   .................. .................................... ..................  [92m[OKAY][0m[92m[OKAY][0m [92m[OKAY][0m
g0120: 
g0120: [92m[OKAY][0m
g0120: 
g0120: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0120: --------------------------------------------------
g0120: 
g0120: 
g0120: op nameop name op nameop name ................   ................................................installed    installedinstalledinstalled..    ...... compatible  compatible
g0120: compatiblecompatible
g0120: 
g0120: 
g0120: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
g0120: 
g0120: 
g0120: 
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op report
g0121: --------------------------------------------------
g0121: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.
g0121: --------------------------------------------------
g0121: JIT compiled ops requires ninja
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0121: 
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op reportNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.
g0121: 
g0121: ----------------------------------------------------------------------------------------------------
g0121: 
g0121: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.JIT compiled ops requires ninja
g0121: 
g0121: --------------------------------------------------
g0121: JIT compiled ops requires ninja
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op report
g0121: --------------------------------------------------
g0121: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.
g0121: --------------------------------------------------
g0121: JIT compiled ops requires ninja
g0115: ninjaninjaninjaninja   .................. .................................... ..................  [92m[OKAY][0m [92m[OKAY][0m[92m[OKAY][0m
g0115: [92m[OKAY][0m
g0115: 
g0115: 
g0115: ----------------------------------------------------------------------------------------------------
g0115: ----------------------------------------------------------------------------------------------------
g0115: 
g0115: op name
g0115: op name op nameop name ................  ................ ................................ installed  installed installedinstalled ..  .. .... compatible  compatible
g0115: compatiblecompatible
g0115: 
g0115: 
g0115: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0115: --------------------------------------------------
g0115: 
g0115: 
g0121: ninjaninjaninjaninja    ........................................................................    [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0121: 
g0121: 
g0121: 
g0121: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
g0121: 
g0121: 
g0121: 
g0121: op nameop nameop name op name  ................ ................................ ................  installed installedinstalled installed  .. ....   compatible..compatiblecompatible
g0121:  
g0121: 
g0121: compatible----------------------------------------------------------------------------------------------------
g0121: 
g0121: --------------------------------------------------
g0121: 
g0121: --------------------------------------------------
g0115: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0115: fused_adam ............. [92m[YES][0masync_io  .....................  [92m[OKAY][0m[92m[YES][0m
g0115:  ...... cpu_adam[92m[OKAY][0m 
g0115: ............... [92m[YES][0m ...... [92m[OKAY][0mfused_adam
g0120: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0115:  ............. cpu_adagrad[92m[YES][0m  ..................  [92m[YES][0m[92m[OKAY][0m 
g0120: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0115: ...... [92m[OKAY][0m
g0115: cpu_adam ............... cpu_lion[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0115: ...... [92m[OKAY][0m
g0120: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0115: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0115: 
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0115: evoformer_attncpu_lion  ........................  [93m[NO][0m[92m[YES][0m  .............  [93m[NO][0m[92m[OKAY][0m
g0115: 
g0120: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0120: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0120: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: fused_lamb ............. [92m[YES][0m ...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m
g0120: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: 
g0120: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0115: evoformer_attn ......... [93m[NO][0m ....... fused_lion[93m[NO][0m
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0115:  ............. fused_lamb[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0115: ...... [92m[OKAY][0m
g0115: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0115: async_iofused_adam  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0120: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0120: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: 
g0120: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0115: cpu_adam ............... [92m[YES][0mfused_adam  ...................  [92m[OKAY][0m[92m[YES][0m
g0120: async_io[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0120: ............... evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m .......
g0115:  ...... cpu_adagrad[92m[OKAY][0m 
g0115: ............ [92m[YES][0mcpu_adam  .....................  [92m[OKAY][0m[92m[YES][0m
g0120:  [93m[NO][0m
g0115:  ...... cpu_lion[92m[OKAY][0m 
g0115: ............... [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0m[92m[YES][0m
g0115:  ...... [92m[OKAY][0m
g0120: fused_lambfused_adam  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0115: cpu_lion ...............[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0120: 
g0115: [92m[YES][0m evoformer_attn......  .........[92m[OKAY][0m 
g0115: [93m[NO][0m ....... [93m[NO][0m
g0115: fused_lamb[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0120: cpu_adam ............... [92m[YES][0m fused_lion......  .............[92m[OKAY][0m 
g0115: ............. [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0120: [92m[YES][0m ......cpu_adagrad  [92m[OKAY][0m............
g0120:  [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0115:  ....... [93m[NO][0m
g0120: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0115: fused_lamb .............fused_lion  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0115:  [92m[OKAY][0m
g0120: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0120: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0121: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m ....... async_io[93m[NO][0m
g0121:  ............... fused_lamb[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0121: ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0mfused_lion  ...................  [92m[OKAY][0m[92m[YES][0m
g0121:  ...... [92m[OKAY][0m
g0121: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m async_io.......  [93m[NO][0m...............
g0121:  [92m[YES][0m ......fused_lamb  [92m[OKAY][0m.............
g0121:  [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0m ...... fused_lion[92m[OKAY][0m 
g0121: ............. [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0121: [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0121: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [2024-08-12 11:22:56,313] [INFO] [comm.py:637:init_distributed] cdb=None
g0123: [2024-08-12 11:22:56,313] [INFO] [comm.py:637:init_distributed] cdb=None
g0123: [2024-08-12 11:22:56,314] [INFO] [comm.py:637:init_distributed] cdb=None
g0123: [2024-08-12 11:22:56,314] [INFO] [comm.py:637:init_distributed] cdb=None
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0115: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0115: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0115: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0115: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0115: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0115: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0115: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0115: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0115: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cutlass_opscutlass_ops  ........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0120: 
g0120: quantizerquantizer  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0120: 
g0120: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0115: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0115: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0115: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0115: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0115: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0115: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0115: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0115: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0115: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0115: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0115: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0115: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0115: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0115: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0115: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... ragged_ops[93m[NO][0m
g0121:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0115: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0115: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0115: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0115: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0115: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0115: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0115: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0115: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0115: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0115: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0115: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0115: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0115: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0115: --------------------------------------------------
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0115: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0115: --------------------------------------------------
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0115: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0115: --------------------------------------------------
g0115: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0115: --------------------------------------------------
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0115: DeepSpeed general environment info:
g0115: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0115: torch version .................... 2.0.1+cu118
g0115: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0115: deepspeed info ................... 0.12.4, unknown, unknown
g0115: torch cuda version ............... 11.8
g0115: torch hip version ................ None
g0115: nvcc version ..................... 11.8
g0115: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0115: shared memory (/dev/shm) size .... 188.13 GB
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0120: DeepSpeed general environment info:
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0120: torch version .................... 2.0.1+cu118
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0120: DeepSpeed general environment info:
g0115: DeepSpeed general environment info:
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0115: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0120: torch version .................... 2.0.1+cu118
g0115: torch version .................... 2.0.1+cu118
g0115: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0115: deepspeed info ................... 0.12.4, unknown, unknown
g0115: torch cuda version ............... 11.8
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0115: torch hip version ................ None
g0115: nvcc version ..................... 11.8
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0115: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0115: shared memory (/dev/shm) size .... 188.13 GB
g0115: DeepSpeed general environment info:
g0120: DeepSpeed general environment info:
g0115: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0115: torch version .................... 2.0.1+cu118
g0115: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0115: deepspeed info ................... 0.12.4, unknown, unknown
g0115: torch cuda version ............... 11.8
g0115: torch hip version ................ None
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0120: torch version .................... 2.0.1+cu118
g0115: nvcc version ..................... 11.8
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0115: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0115: shared memory (/dev/shm) size .... 188.13 GB
g0115: DeepSpeed general environment info:
g0115: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0115: torch version .................... 2.0.1+cu118
g0120: DeepSpeed general environment info:
g0115: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0115: deepspeed info ................... 0.12.4, unknown, unknown
g0115: torch cuda version ............... 11.8
g0115: torch hip version ................ None
g0115: nvcc version ..................... 11.8
g0115: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0115: shared memory (/dev/shm) size .... 188.13 GB
g0120: torch version .................... 2.0.1+cu118
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0113: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0113: 
g0113: 
g0113: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0113: 
g0113: 
g0113: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0113: 
g0113: 
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: 
g0113: 
g0113: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0113: 
g0113: 
g0113: JIT compiled ops requires ninjaJIT compiled ops requires ninjaJIT compiled ops requires ninja
g0113: 
g0113: 
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0113: ninjaninjaninjaninja   .................................... ..................  .................. [92m[OKAY][0m[92m[OKAY][0m [92m[OKAY][0m
g0113: [92m[OKAY][0m
g0113: 
g0113: 
g0113: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0113: --------------------------------------------------
g0113: 
g0113: 
g0113: op nameop name op nameop name ................  ................................................    installedinstalledinstalledinstalled    ........    compatiblecompatiblecompatiblecompatible
g0113: 
g0113: 
g0113: 
g0113: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
g0113: 
g0113: 
g0113: 
g0115: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0115: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0115: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0115: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0124: --------------------------------------------------
g0124: DeepSpeed C++/CUDA extension op report
g0124: --------------------------------------------------
g0124: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0124:       runtime if needed. Op compatibility means that your system
g0124:       meet the required dependencies to JIT install the op.
g0124: --------------------------------------------------
g0124: --------------------------------------------------JIT compiled ops requires ninja
g0124: 
g0124: DeepSpeed C++/CUDA extension op report
g0124: --------------------------------------------------
g0124: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0124:       runtime if needed. Op compatibility means that your system
g0124:       meet the required dependencies to JIT install the op.
g0124: --------------------------------------------------
g0124: JIT compiled ops requires ninja
g0124: --------------------------------------------------
g0124: DeepSpeed C++/CUDA extension op report
g0124: --------------------------------------------------
g0124: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0124:       runtime if needed. Op compatibility means that your system
g0124:       meet the required dependencies to JIT install the op.
g0124: --------------------------------------------------
g0124: JIT compiled ops requires ninja
g0124: --------------------------------------------------
g0124: DeepSpeed C++/CUDA extension op report
g0124: --------------------------------------------------
g0124: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0124:       runtime if needed. Op compatibility means that your system
g0124:       meet the required dependencies to JIT install the op.
g0124: --------------------------------------------------
g0124: JIT compiled ops requires ninja
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: async_iocpu_adagrad  ...........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0113: 
g0113: cpu_lion ............... fused_adam[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0113: ...... [92m[OKAY][0m
g0113: cpu_adam [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH...............
g0113:  [92m[YES][0mevoformer_attn  ...............  [92m[OKAY][0m[93m[NO][0m
g0113:  ....... cpu_adagrad[93m[NO][0m 
g0113: ............ [92m[YES][0m fused_lamb......  .............[92m[OKAY][0m [92m[YES][0m
g0113:  ...... cpu_lion[92m[OKAY][0m 
g0113: ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0113: [92m[OKAY][0m
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: ninjaninja  ....................................  [92m[OKAY][0m[92m[OKAY][0m
g0124: 
g0124: ----------------------------------------------------------------------------------------------------
g0124: 
g0124: op name op nameninja................  ................  ..................installedinstalled   ....[92m[OKAY][0m  
g0124: compatiblecompatible
g0124: 
g0124: ----------------------------------------------------------------------------------------------------
g0124: --------------------------------------------------
g0124: 
g0124: op name ................ installed .. compatible
g0124: --------------------------------------------------
g0124: ninja .................. [92m[OKAY][0m
g0124: --------------------------------------------------
g0124: op name ................ installed .. compatible
g0124: --------------------------------------------------
g0113: inference_core_opsinference_core_opsinference_core_ops   ...............   [92m[YES][0m[92m[YES][0m[92m[YES][0m   ..................   [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0113: 
g0113: 
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0121: [2024-08-12 11:22:56,481] [INFO] [comm.py:637:init_distributed] cdb=None
g0115: [2024-08-12 11:22:56,481] [INFO] [comm.py:637:init_distributed] cdb=None
g0115: [2024-08-12 11:22:56,481] [INFO] [comm.py:637:init_distributed] cdb=None
g0121: [2024-08-12 11:22:56,481] [INFO] [comm.py:637:init_distributed] cdb=None
g0121: [2024-08-12 11:22:56,481] [INFO] [comm.py:637:init_distributed] cdb=None
g0120: [2024-08-12 11:22:56,482] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0115: [2024-08-12 11:22:56,482] [INFO] [comm.py:637:init_distributed] cdb=None
g0120: [2024-08-12 11:22:56,482] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0120: [2024-08-12 11:22:56,483] [INFO] [comm.py:637:init_distributed] cdb=None
g0120: [2024-08-12 11:22:56,483] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0115: [2024-08-12 11:22:56,486] [INFO] [comm.py:637:init_distributed] cdb=None
g0115: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0115: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0115: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [2024-08-12 11:22:56,487] [INFO] [comm.py:637:init_distributed] cdb=None
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: DeepSpeed general environment info:
g0113: deepspeed wheel compiled w.torch install path  .....................  torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch'] 
g0113: .... 188.13 GBtorch version
g0113:  .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0115: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0115: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0115: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0115: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0115: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0124: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0124: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0124: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0124: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0124: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0124: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0124: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0124: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0124: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0124: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0124: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: async_iofused_lion  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0124: 
g0124: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0124: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0124: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0124: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0124: async_io fused_lamb...............  .............[92m[YES][0m  ......[92m[YES][0m  [92m[OKAY][0m......
g0124:  [92m[OKAY][0m
g0124: fused_adam ............. [92m[YES][0m ......fused_lion  [92m[OKAY][0m
g0124: ............. [92m[YES][0mcpu_adam  .....................  [92m[YES][0m [92m[OKAY][0m......
g0124:  [92m[OKAY][0m
g0124: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0124: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0124: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0124: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0124: inference_core_ops inference_core_ops..... [92m[YES][0m ......  .....[92m[OKAY][0m 
g0124: [92m[YES][0m ...... [92m[OKAY][0m
g0124: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0124: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0124: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0124: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0124: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0124: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0124: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0124: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0124: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0124: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0124: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0124: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0124: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0124: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0124: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0124: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0124: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0124: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0124: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0124: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0124: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0124: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0124: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0124: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0124: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0124: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0124: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0124: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0124: --------------------------------------------------
g0124: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0124: --------------------------------------------------
g0124: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0124: --------------------------------------------------
g0124: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0124: --------------------------------------------------
g0124: DeepSpeed general environment info:
g0124: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0124: torch version .................... 2.0.1+cu118
g0124: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0124: deepspeed info ................... 0.12.4, unknown, unknown
g0124: torch cuda version ............... 11.8
g0124: torch hip version ................ None
g0124: nvcc version ..................... 11.8
g0124: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0124: shared memory (/dev/shm) size .... 188.13 GB
g0124: DeepSpeed general environment info:
g0124: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0124: torch version .................... 2.0.1+cu118
g0124: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0124: deepspeed info ................... 0.12.4, unknown, unknown
g0124: torch cuda version ............... 11.8
g0124: torch hip version ................ None
g0124: nvcc version ..................... 11.8
g0124: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0124: shared memory (/dev/shm) size .... 188.13 GB
g0124: DeepSpeed general environment info:
g0124: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0124: torch version .................... 2.0.1+cu118
g0124: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0124: deepspeed info ................... 0.12.4, unknown, unknown
g0124: torch cuda version ............... 11.8
g0124: torch hip version ................ None
g0124: nvcc version ..................... 11.8
g0124: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0124: shared memory (/dev/shm) size .... 188.13 GB
g0124: DeepSpeed general environment info:
g0124: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0124: torch version .................... 2.0.1+cu118
g0124: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0124: deepspeed info ................... 0.12.4, unknown, unknown
g0124: torch cuda version ............... 11.8
g0124: torch hip version ................ None
g0124: nvcc version ..................... 11.8
g0124: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0124: shared memory (/dev/shm) size .... 188.13 GB
g0119: --------------------------------------------------
g0119: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0119: 
g0119: DeepSpeed C++/CUDA extension op report
g0119: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0119: 
g0119: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0119:       runtime if needed. Op compatibility means that your system
g0119:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0119:       runtime if needed. Op compatibility means that your system
g0119:       meet the required dependencies to JIT install the op.
g0119: 
g0119: 
g0119: DeepSpeed C++/CUDA extension op report----------------------------------------------------------------------------------------------------
g0119: 
g0119: 
g0119: JIT compiled ops requires ninjaJIT compiled ops requires ninja--------------------------------------------------
g0119: 
g0119: 
g0119: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0119:       runtime if needed. Op compatibility means that your system
g0119:       meet the required dependencies to JIT install the op.
g0119: --------------------------------------------------
g0119: JIT compiled ops requires ninja
g0119: --------------------------------------------------
g0119: DeepSpeed C++/CUDA extension op report
g0119: --------------------------------------------------
g0119: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0119:       runtime if needed. Op compatibility means that your system
g0119:       meet the required dependencies to JIT install the op.
g0119: --------------------------------------------------
g0119: JIT compiled ops requires ninja
g0119: ninjaninjaninjaninja   .................................... ..................   [92m[OKAY][0m..................[92m[OKAY][0m[92m[OKAY][0m
g0119:  
g0119: 
g0119: [92m[OKAY][0m----------------------------------------------------------------------------------------------------
g0119: --------------------------------------------------
g0119: 
g0119: 
g0119: --------------------------------------------------op nameop nameop name
g0119:    ................................op name................    installedinstalled................installed    ......installed    compatiblecompatiblecompatible..
g0119: 
g0119: 
g0119:  --------------------------------------------------compatible----------------------------------------------------------------------------------------------------
g0119: 
g0119: 
g0119: 
g0119: --------------------------------------------------
g0113: [2024-08-12 11:22:56,617] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [2024-08-12 11:22:56,617] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [2024-08-12 11:22:56,617] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [2024-08-12 11:22:56,618] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0124: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0124: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0124: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0124: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0119: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0119: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0119: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0119: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0119: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0119: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0119: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0119: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0119: cpu_adagrad ............ [92m[YES][0m ......async_io [92m[OKAY][0m 
g0119: ............... [92m[YES][0mcpu_lion  .....................  [92m[OKAY][0m[92m[YES][0m
g0119:  ...... [92m[OKAY][0m
g0119: fused_adam ............. [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0119: [92m[OKAY][0m
g0119: evoformer_attn ......... cpu_adam[93m[NO][0m  ......................  [92m[YES][0m[93m[NO][0m 
g0119: ...... [92m[OKAY][0mfused_lamb
g0119:  ............. [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0m[92m[YES][0m
g0119:  ...... [92m[OKAY][0m
g0119: cpu_lionfused_lion  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0119: 
g0119: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0119: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0119: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0119: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0119: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0119: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0119: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0119: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0119: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0119: inference_core_ops ..... inference_core_ops[92m[YES][0m  ...........  [92m[OKAY][0m[92m[YES][0m
g0119:  ...... [92m[OKAY][0m
g0119: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0119: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0119: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0119: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0119: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0119: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0119: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0119: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0119: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0119: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0119: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0119: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0119: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0119: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0119: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0119: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0119: ragged_ops [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible.............
g0119:  [92m[YES][0m sparse_attn......  ............[92m[OKAY][0m 
g0119: [93m[NO][0m ....... random_ltd[93m[NO][0m 
g0119: ............. [92m[YES][0m ...... [92m[OKAY][0m
g0119: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0119: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0119: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0119: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0119: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0119: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0119: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0119: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0119: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0119: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0119: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0119: stochastic_transformer . [92m[YES][0m spatial_inference......  ......[92m[OKAY][0m 
g0119: [92m[YES][0m ...... [92m[OKAY][0m
g0119: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0119: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0119: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0119: --------------------------------------------------
g0119: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0119: --------------------------------------------------
g0119: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0119: --------------------------------------------------
g0119: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0119: --------------------------------------------------
g0124: [2024-08-12 11:22:56,715] [INFO] [comm.py:637:init_distributed] cdb=None
g0119: DeepSpeed general environment info:
g0119: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0119: torch version .................... 2.0.1+cu118
g0119: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0119: deepspeed info ................... 0.12.4, unknown, unknown
g0119: torch cuda version ............... 11.8
g0119: torch hip version ................ None
g0119: nvcc version ..................... 11.8
g0119: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0119: shared memory (/dev/shm) size .... 188.13 GB
g0119: DeepSpeed general environment info:
g0119: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0119: torch version .................... 2.0.1+cu118
g0119: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0119: deepspeed info ................... 0.12.4, unknown, unknown
g0119: torch cuda version ............... 11.8
g0119: torch hip version ................ None
g0119: nvcc version ..................... 11.8
g0119: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0119: shared memory (/dev/shm) size .... 188.13 GB
g0119: DeepSpeed general environment info:
g0119: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0119: torch version .................... 2.0.1+cu118
g0119: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0119: deepspeed info ................... 0.12.4, unknown, unknown
g0119: torch cuda version ............... 11.8
g0119: torch hip version ................ None
g0119: nvcc version ..................... 11.8
g0119: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0119: shared memory (/dev/shm) size .... 188.13 GB
g0119: DeepSpeed general environment info:
g0119: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0119: torch version .................... 2.0.1+cu118
g0119: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0119: deepspeed info ................... 0.12.4, unknown, unknown
g0119: torch cuda version ............... 11.8
g0119: torch hip version ................ None
g0119: nvcc version ..................... 11.8
g0119: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0119: shared memory (/dev/shm) size .... 188.13 GB
g0124: [2024-08-12 11:22:56,720] [INFO] [comm.py:637:init_distributed] cdb=None
g0124: [2024-08-12 11:22:56,721] [INFO] [comm.py:637:init_distributed] cdb=None
g0124: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0124: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0124: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0124: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0124: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0124: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0119: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0119: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0119: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0119: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0119: [2024-08-12 11:22:56,845] [INFO] [comm.py:637:init_distributed] cdb=None
g0119: [2024-08-12 11:22:56,846] [INFO] [comm.py:637:init_distributed] cdb=None
g0119: [2024-08-12 11:22:56,847] [INFO] [comm.py:637:init_distributed] cdb=None
g0119: [2024-08-12 11:22:56,847] [INFO] [comm.py:637:init_distributed] cdb=None
g0119: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0119: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0119: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0119: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0119: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0119: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0119: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0119: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0124: > setting tensorboard ...
g0124: [2024-08-12 11:22:57,150] [INFO] [comm.py:637:init_distributed] cdb=None
g0124: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0124: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: > initialized tensor model parallel with size 1
g0108: > initialized pipeline model parallel with size 8
g0108: > setting random seeds to 1234 ...
g0108: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0108: > compiling dataset index builder ...
g0108: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0108: make: Nothing to be done for 'default'.
g0108: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0108: >>> done with dataset index builder. Compilation time: 0.078 seconds
g0108: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0108: > compiling and loading fused kernels ...
g0108: Detected CUDA files, patching ldflags
g0108: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0108: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0108: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0108: ninja: no work to do.
g0108: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0108: Detected CUDA files, patching ldflags
g0108: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0108: Building extension module scaled_masked_softmax_cuda...
g0108: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0108: ninja: no work to do.
g0108: Loading extension module scaled_masked_softmax_cuda...
g0108: Detected CUDA files, patching ldflags
g0108: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0108: Building extension module scaled_softmax_cuda...
g0108: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0108: ninja: no work to do.
g0108: Loading extension module scaled_softmax_cuda...
g0108: >>> done with compiling and loading fused kernels. Compilation time: 8.059 seconds
g0108: time to initialize megatron (seconds): 23.076
g0108: [after megatron is initialized] datetime: 2024-08-12 11:23:07 
g0108: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0120: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0115: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0123: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0113: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0119: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0121: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0124: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0120: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0120: wandb:  $ pip install wandb --upgrade
g0120: wandb: Tracking run with wandb version 0.17.5
g0120: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_112309-wewjp6mp
g0120: wandb: Run `wandb offline` to turn off syncing.
g0120: wandb: Syncing run g0120.abci.local
g0120: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0120: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/wewjp6mp
g0124: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0124: wandb:  $ pip install wandb --upgrade
g0124: wandb: Tracking run with wandb version 0.17.5
g0124: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_112309-k4qgems6
g0124: wandb: Run `wandb offline` to turn off syncing.
g0124: wandb: Syncing run g0124.abci.local
g0124: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0124: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/k4qgems6
g0119: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0119: wandb:  $ pip install wandb --upgrade
g0119: wandb: Tracking run with wandb version 0.17.5
g0119: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_112309-bu1nq8qh
g0119: wandb: Run `wandb offline` to turn off syncing.
g0108: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0108: wandb:  $ pip install wandb --upgrade
g0108: wandb: Tracking run with wandb version 0.17.5
g0108: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_112309-j3ooarzb
g0108: wandb: Run `wandb offline` to turn off syncing.
g0119: wandb: Syncing run g0119.abci.local
g0119: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0119: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/bu1nq8qh
g0108: wandb: Syncing run g0108.abci.local
g0108: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0108: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/j3ooarzb
g0115: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0115: wandb:  $ pip install wandb --upgrade
g0115: wandb: Tracking run with wandb version 0.17.5
g0115: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_112309-1713kljo
g0115: wandb: Run `wandb offline` to turn off syncing.
g0115: wandb: Syncing run g0115.abci.local
g0115: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0115: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/1713kljo
g0121: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0121: wandb:  $ pip install wandb --upgrade
g0121: wandb: Tracking run with wandb version 0.17.5
g0121: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_112309-m9d4vnej
g0121: wandb: Run `wandb offline` to turn off syncing.
g0123: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0123: wandb:  $ pip install wandb --upgrade
g0123: wandb: Tracking run with wandb version 0.17.5
g0123: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_112309-0qtp36al
g0123: wandb: Run `wandb offline` to turn off syncing.
g0121: wandb: Syncing run g0121.abci.local
g0121: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0121: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/m9d4vnej
g0123: wandb: Syncing run g0123.abci.local
g0123: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0123: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/0qtp36al
g0113: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0113: wandb:  $ pip install wandb --upgrade
g0113: wandb: Tracking run with wandb version 0.17.5
g0113: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_112309-iwte2v5x
g0113: wandb: Run `wandb offline` to turn off syncing.
g0113: wandb: Syncing run g0113.abci.local
g0113: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0113: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/iwte2v5x
g0108: building GPT model ...
g0108: [2024-08-12 11:23:10,422] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0108: [2024-08-12 11:23:10,423] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0108: [2024-08-12 11:23:10,423] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 53.7 GB, percent = 14.3%
g0108: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0108: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0108: [2024-08-12 11:23:10,929] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0108: stage=0 layers=5
g0108:      0: _to_float16
g0108:      1: EmbeddingPipe
g0108:      2: ParallelTransformerLayerPipe
g0108:      3: ParallelTransformerLayerPipe
g0108:      4: ParallelTransformerLayerPipe
g0108: stage=1 layers=3
g0108:      5: ParallelTransformerLayerPipe
g0108:      6: ParallelTransformerLayerPipe
g0108:      7: ParallelTransformerLayerPipe
g0108: stage=2 layers=3
g0108:      8: ParallelTransformerLayerPipe
g0108:      9: ParallelTransformerLayerPipe
g0108:     10: ParallelTransformerLayerPipe
g0108: stage=3 layers=3
g0108:     11: ParallelTransformerLayerPipe
g0108:     12: ParallelTransformerLayerPipe
g0108:     13: ParallelTransformerLayerPipe
g0108: stage=4 layers=3
g0108:     14: ParallelTransformerLayerPipe
g0108:     15: ParallelTransformerLayerPipe
g0108:     16: ParallelTransformerLayerPipe
g0108: stage=5 layers=3
g0108:     17: ParallelTransformerLayerPipe
g0108:     18: ParallelTransformerLayerPipe
g0108:     19: ParallelTransformerLayerPipe
g0108: stage=6 layers=3
g0108:     20: ParallelTransformerLayerPipe
g0108:     21: ParallelTransformerLayerPipe
g0108:     22: ParallelTransformerLayerPipe
g0108: stage=7 layers=3
g0108:     23: ParallelTransformerLayerPipe
g0108:     24: MixedFusedRMSNorm
g0108:     25: LMHeadPipe
g0108:   loss: CrossEntropy
g0123:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0120:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0115:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0121:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0124:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0113:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0119:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0123: [2024-08-12 11:23:11,501] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0123: [2024-08-12 11:23:11,501] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0123: [2024-08-12 11:23:11,501] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0123: [2024-08-12 11:23:11,502] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 11:23:11,527] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0108: [2024-08-12 11:23:11,529] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0108: [2024-08-12 11:23:11,530] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 53.76 GB, percent = 14.3%
g0108:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0108: setting training iterations to 52000
g0108: > learning rate decay style: cosine
g0108: DeepSpeed is enabled.
g0108: [2024-08-12 11:23:11,532] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0115: [2024-08-12 11:23:11,533] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0115: [2024-08-12 11:23:11,533] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0115: [2024-08-12 11:23:11,533] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0115: [2024-08-12 11:23:11,533] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 11:23:11,539] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 11:23:11,539] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 11:23:11,539] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 11:23:11,539] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 11:23:11,556] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 11:23:11,556] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 11:23:11,556] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 11:23:11,556] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0124: [2024-08-12 11:23:11,559] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0124: [2024-08-12 11:23:11,559] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0124: [2024-08-12 11:23:11,559] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0124: [2024-08-12 11:23:11,559] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0120: [2024-08-12 11:23:11,565] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0120: [2024-08-12 11:23:11,565] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0120: [2024-08-12 11:23:11,566] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0120: [2024-08-12 11:23:11,566] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0119: [2024-08-12 11:23:11,589] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0119: [2024-08-12 11:23:11,589] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0119: [2024-08-12 11:23:11,590] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0119: [2024-08-12 11:23:11,590] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 11:23:11,748] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0108: [2024-08-12 11:23:11,749] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0108: [2024-08-12 11:23:11,749] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0108: [2024-08-12 11:23:11,749] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0108: [2024-08-12 11:23:11,750] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0108: [2024-08-12 11:23:11,763] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 11:23:11,763] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 11:23:11,764] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0108: [2024-08-12 11:23:11,765] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0108: [2024-08-12 11:23:11,765] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 11:23:11,765] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f41b40638d0>
g0108: [2024-08-12 11:23:11,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: [2024-08-12 11:23:11,765] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0108: [2024-08-12 11:23:11,766] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0108:     "partition_activations": false, 
g0108:     "contiguous_memory_optimization": false, 
g0108:     "cpu_checkpointing": false, 
g0108:     "number_checkpoints": null, 
g0108:     "synchronize_checkpoint_boundary": false, 
g0108:     "profile": false
g0108: }
g0108: [2024-08-12 11:23:11,766] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0108: [2024-08-12 11:23:11,766] [INFO] [config.py:983:print]   amp_enabled .................. False
g0108: [2024-08-12 11:23:11,766] [INFO] [config.py:983:print]   amp_params ................... False
g0108: [2024-08-12 11:23:11,767] [INFO] [config.py:983:print]   autotuning_config ............ {
g0108:     "enabled": false, 
g0108:     "start_step": null, 
g0108:     "end_step": null, 
g0108:     "metric_path": null, 
g0108:     "arg_mappings": null, 
g0108:     "metric": "throughput", 
g0108:     "model_info": null, 
g0108:     "results_dir": "autotuning_results", 
g0108:     "exps_dir": "autotuning_exps", 
g0108:     "overwrite": true, 
g0108:     "fast": true, 
g0108:     "start_profile_step": 3, 
g0108:     "end_profile_step": 5, 
g0108:     "tuner_type": "gridsearch", 
g0108:     "tuner_early_stopping": 5, 
g0108:     "tuner_num_trials": 50, 
g0108:     "model_info_path": null, 
g0108:     "mp_size": 1, 
g0108:     "max_train_batch_size": null, 
g0108:     "min_train_batch_size": 1, 
g0108:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0108:     "min_train_micro_batch_size_per_gpu": 1, 
g0108:     "num_tuning_micro_batch_sizes": 3
g0108: }
g0108: [2024-08-12 11:23:11,767] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0108: [2024-08-12 11:23:11,767] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0108: [2024-08-12 11:23:11,767] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0108: [2024-08-12 11:23:11,767] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0108: [2024-08-12 11:23:11,767] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f4180bceb10>
g0108: [2024-08-12 11:23:11,767] [INFO] [config.py:983:print]   communication_data_type ...... None
g0108: [2024-08-12 11:23:11,768] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0108: [2024-08-12 11:23:11,768] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0108: [2024-08-12 11:23:11,768] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0108: [2024-08-12 11:23:11,768] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0108: [2024-08-12 11:23:11,768] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0108: [2024-08-12 11:23:11,768] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0108: [2024-08-12 11:23:11,768] [INFO] [config.py:983:print]   disable_allgather ............ False
g0108: [2024-08-12 11:23:11,768] [INFO] [config.py:983:print]   dump_state ................... False
g0108: [2024-08-12 11:23:11,769] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0108: [2024-08-12 11:23:11,769] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0108: [2024-08-12 11:23:11,769] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0108: [2024-08-12 11:23:11,769] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0108: [2024-08-12 11:23:11,769] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0108: [2024-08-12 11:23:11,769] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0108: [2024-08-12 11:23:11,769] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0108: [2024-08-12 11:23:11,769] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0108: [2024-08-12 11:23:11,769] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0108: [2024-08-12 11:23:11,770] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0108: [2024-08-12 11:23:11,770] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0108:     "enabled": false, 
g0108:     "recompute_fwd_factor": 0.0, 
g0108:     "profile_step": 1, 
g0108:     "module_depth": -1, 
g0108:     "top_modules": 1, 
g0108:     "detailed": true, 
g0108:     "output_file": null
g0108: }
g0108: [2024-08-12 11:23:11,770] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0108: [2024-08-12 11:23:11,770] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0108: [2024-08-12 11:23:11,770] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0108: [2024-08-12 11:23:11,770] [INFO] [config.py:983:print]   global_rank .................. 0
g0108: [2024-08-12 11:23:11,770] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0108: [2024-08-12 11:23:11,770] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0108: [2024-08-12 11:23:11,770] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0108: [2024-08-12 11:23:11,771] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0108: [2024-08-12 11:23:11,771] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0108: [2024-08-12 11:23:11,771] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0108: [2024-08-12 11:23:11,771] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0108: [2024-08-12 11:23:11,771] [INFO] [config.py:983:print]   loss_scale ................... 0
g0108: [2024-08-12 11:23:11,771] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0108: [2024-08-12 11:23:11,771] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0108: [2024-08-12 11:23:11,771] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0108: [2024-08-12 11:23:11,772] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0108: [2024-08-12 11:23:11,772] [INFO] [config.py:983:print]   nebula_config ................ {
g0108:     "enabled": false, 
g0108:     "persistent_storage_path": null, 
g0108:     "persistent_time_interval": 100, 
g0108:     "num_of_version_in_retention": 2, 
g0108:     "enable_nebula_load": true, 
g0108:     "load_path": null
g0108: }
g0108: [2024-08-12 11:23:11,772] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0108: [2024-08-12 11:23:11,772] [INFO] [config.py:983:print]   optimizer_name ............... None
g0108: [2024-08-12 11:23:11,772] [INFO] [config.py:983:print]   optimizer_params ............. None
g0108: [2024-08-12 11:23:11,772] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0108: [2024-08-12 11:23:11,772] [INFO] [config.py:983:print]   pld_enabled .................. False
g0108: [2024-08-12 11:23:11,773] [INFO] [config.py:983:print]   pld_params ................... False
g0108: [2024-08-12 11:23:11,773] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0108: [2024-08-12 11:23:11,773] [INFO] [config.py:983:print]   scheduler_name ............... None
g0108: [2024-08-12 11:23:11,773] [INFO] [config.py:983:print]   scheduler_params ............. None
g0108: [2024-08-12 11:23:11,773] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0108: [2024-08-12 11:23:11,773] [INFO] [config.py:983:print]   sparse_attention ............. None
g0108: [2024-08-12 11:23:11,773] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0108: [2024-08-12 11:23:11,773] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0108: [2024-08-12 11:23:11,773] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0108: [2024-08-12 11:23:11,774] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0108: [2024-08-12 11:23:11,774] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0108: [2024-08-12 11:23:11,774] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0108: [2024-08-12 11:23:11,774] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0108: [2024-08-12 11:23:11,774] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0108: [2024-08-12 11:23:11,774] [INFO] [config.py:983:print]   world_size ................... 4
g0108: [2024-08-12 11:23:11,774] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0108: [2024-08-12 11:23:11,774] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0108: [2024-08-12 11:23:11,775] [INFO] [config.py:983:print]   zero_enabled ................. False
g0108: [2024-08-12 11:23:11,775] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0108: [2024-08-12 11:23:11,775] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0108: [2024-08-12 11:23:11,775] [INFO] [config.py:969:print_user_config]   json = {
g0108:     "train_batch_size": 128, 
g0108:     "train_micro_batch_size_per_gpu": 1, 
g0108:     "steps_per_print": 10, 
g0108:     "zero_optimization": {
g0108:         "stage": 0
g0108:     }, 
g0108:     "gradient_clipping": 1.0, 
g0108:     "prescale_gradients": true, 
g0108:     "fp16": {
g0108:         "enabled": true, 
g0108:         "loss_scale": 0, 
g0108:         "loss_scale_window": 500, 
g0108:         "hysteresis": 2, 
g0108:         "min_loss_scale": 1, 
g0108:         "initial_scale_power": 11
g0108:     }, 
g0108:     "wall_clock_breakdown": false
g0108: }
g0108: [2024-08-12 11:23:11,775] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0108: [2024-08-12 11:23:11,775] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 11:23:12,487] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0120: [2024-08-12 11:23:12,488] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0115: [2024-08-12 11:23:12,488] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0123: [2024-08-12 11:23:12,488] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0121: [2024-08-12 11:23:12,489] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0124: [2024-08-12 11:23:12,489] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0119: [2024-08-12 11:23:12,489] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0113: [2024-08-12 11:23:12,489] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0120: [2024-08-12 11:23:13,180] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 11:23:13,180] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 11:23:13,180] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0119: [2024-08-12 11:23:13,180] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 11:23:13,180] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0124: [2024-08-12 11:23:13,180] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0124: [2024-08-12 11:23:13,180] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0124: [2024-08-12 11:23:13,180] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0119: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0115: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0115: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0115: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0120: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0119: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0120: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0124: [2024-08-12 11:23:13,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0119: [2024-08-12 11:23:13,182] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 11:23:13,182] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 11:23:13,182] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 11:23:13,182] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 11:23:13,182] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0120: [2024-08-12 11:23:13,182] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0115: [2024-08-12 11:23:13,182] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 11:23:13,183] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 11:23:13,193] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 11:23:13,197] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0119: [2024-08-12 11:23:18,281] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0119: [2024-08-12 11:23:18,281] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0119: [2024-08-12 11:23:18,283] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0119: [2024-08-12 11:23:18,283] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0119: [2024-08-12 11:23:18,290] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_03_model_states.pt...
g0119: [2024-08-12 11:23:18,290] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_03_model_states.pt...
g0119: [2024-08-12 11:23:18,291] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_03_model_states.pt...
g0119: [2024-08-12 11:23:18,291] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_03_model_states.pt...
g0108: [2024-08-12 11:23:18,331] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 11:23:18,331] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 11:23:18,331] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 11:23:18,331] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 11:23:18,338] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 11:23:18,339] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 11:23:18,340] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 11:23:18,339] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt...
g0124: [2024-08-12 11:23:18,341] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0124: [2024-08-12 11:23:18,341] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0124: [2024-08-12 11:23:18,341] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0124: [2024-08-12 11:23:18,341] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0124: [2024-08-12 11:23:18,350] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_07_model_states.pt...
g0124: [2024-08-12 11:23:18,350] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_07_model_states.pt...
g0124: [2024-08-12 11:23:18,350] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_07_model_states.pt...
g0124: [2024-08-12 11:23:18,350] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_07_model_states.pt...
g0113: [2024-08-12 11:23:18,405] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 11:23:18,405] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 11:23:18,405] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 11:23:18,405] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 11:23:18,413] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_01_model_states.pt...
g0113: [2024-08-12 11:23:18,413] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_01_model_states.pt...
g0113: [2024-08-12 11:23:18,413] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_01_model_states.pt...
g0113: [2024-08-12 11:23:18,414] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_01_model_states.pt...
g0123: [2024-08-12 11:23:18,441] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 11:23:18,441] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 11:23:18,441] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 11:23:18,441] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 11:23:18,448] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_06_model_states.pt...
g0123: [2024-08-12 11:23:18,448] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_06_model_states.pt...
g0123: [2024-08-12 11:23:18,449] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_06_model_states.pt...
g0123: [2024-08-12 11:23:18,449] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_06_model_states.pt...
g0120: [2024-08-12 11:23:18,521] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 11:23:18,522] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 11:23:18,522] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 11:23:18,522] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0115: [2024-08-12 11:23:18,524] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0115: [2024-08-12 11:23:18,524] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0115: [2024-08-12 11:23:18,524] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0115: [2024-08-12 11:23:18,524] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 11:23:18,529] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_04_model_states.pt...
g0120: [2024-08-12 11:23:18,530] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_04_model_states.pt...
g0120: [2024-08-12 11:23:18,530] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_04_model_states.pt...
g0120: [2024-08-12 11:23:18,530] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_04_model_states.pt...
g0115: [2024-08-12 11:23:18,531] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_02_model_states.pt...
g0115: [2024-08-12 11:23:18,532] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_02_model_states.pt...
g0115: [2024-08-12 11:23:18,532] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_02_model_states.pt...
g0115: [2024-08-12 11:23:18,532] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_02_model_states.pt...
g0121: [2024-08-12 11:23:19,527] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 11:23:19,527] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 11:23:19,527] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 11:23:19,527] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 11:23:19,536] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_05_model_states.pt...
g0121: [2024-08-12 11:23:19,536] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_05_model_states.pt...
g0121: [2024-08-12 11:23:19,536] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_05_model_states.pt...
g0121: [2024-08-12 11:23:19,536] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_05_model_states.pt...
g0124: [2024-08-12 11:23:20,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_07_model_states.pt.
g0124: [2024-08-12 11:23:20,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_07_model_states.pt.
g0124: [2024-08-12 11:23:20,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_07_model_states.pt.
g0124: [2024-08-12 11:23:20,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_07_model_states.pt.
g0124: [2024-08-12 11:23:20,009] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,010] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,010] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,010] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,038] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_01_model_states.pt.
g0113: [2024-08-12 11:23:20,038] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_01_model_states.pt.
g0113: [2024-08-12 11:23:20,039] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,039] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_01_model_states.pt.
g0113: [2024-08-12 11:23:20,040] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,040] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,041] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_01_model_states.pt.
g0113: [2024-08-12 11:23:20,043] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt...
g0119: [2024-08-12 11:23:20,087] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_03_model_states.pt.
g0119: [2024-08-12 11:23:20,089] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt...
g0119: [2024-08-12 11:23:20,089] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_03_model_states.pt.
g0119: [2024-08-12 11:23:20,089] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_03_model_states.pt.
g0119: [2024-08-12 11:23:20,090] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_03_model_states.pt.
g0119: [2024-08-12 11:23:20,090] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt...
g0119: [2024-08-12 11:23:20,090] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt...
g0119: [2024-08-12 11:23:20,091] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,174] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_06_model_states.pt.
g0123: [2024-08-12 11:23:20,174] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_06_model_states.pt.
g0123: [2024-08-12 11:23:20,175] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_06_model_states.pt.
g0123: [2024-08-12 11:23:20,175] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,175] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,175] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_06_model_states.pt.
g0123: [2024-08-12 11:23:20,176] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,176] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,239] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,240] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,240] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,240] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,240] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,240] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,240] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,241] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,247] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_04_model_states.pt.
g0120: [2024-08-12 11:23:20,248] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_04_model_states.pt.
g0120: [2024-08-12 11:23:20,248] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_04_model_states.pt.
g0120: [2024-08-12 11:23:20,248] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,248] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_04_model_states.pt.
g0120: [2024-08-12 11:23:20,249] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,249] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,249] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,275] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,275] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,276] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,276] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_23-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,283] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,284] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,284] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,284] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,284] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,285] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,285] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,285] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,290] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,297] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,297] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,298] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,313] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,313] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,313] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,314] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,314] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,314] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,314] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,314] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,314] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,314] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,314] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,314] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_24-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,315] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,315] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,315] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,316] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,318] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,318] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,321] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,321] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,334] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,335] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,343] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,344] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,375] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,375] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,375] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,376] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,376] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,376] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,376] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,376] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,409] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,409] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,412] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,412] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_20-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,423] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,426] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,431] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,433] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,456] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,456] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,456] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,457] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,457] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,457] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,457] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,457] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,461] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_02_model_states.pt.
g0115: [2024-08-12 11:23:20,461] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_02_model_states.pt.
g0115: [2024-08-12 11:23:20,462] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_02_model_states.pt.
g0115: [2024-08-12 11:23:20,462] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_02_model_states.pt.
g0115: [2024-08-12 11:23:20,462] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,462] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,462] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,462] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,489] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,489] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,489] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,492] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_14-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,497] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,497] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,498] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,498] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,498] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,498] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,498] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,498] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,504] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,505] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,508] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,511] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt...
g0124: [2024-08-12 11:23:20,545] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,545] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,549] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt.
g0124: [2024-08-12 11:23:20,549] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_25-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,556] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,557] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,557] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,557] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,557] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,557] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,557] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,558] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,589] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,589] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,592] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,592] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_06-model_00-model_states.pt.
g0119: [2024-08-12 11:23:20,597] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt.
g0119: [2024-08-12 11:23:20,598] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt...
g0119: [2024-08-12 11:23:20,598] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt.
g0119: [2024-08-12 11:23:20,598] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt.
g0119: [2024-08-12 11:23:20,598] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt.
g0119: [2024-08-12 11:23:20,599] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt...
g0119: [2024-08-12 11:23:20,599] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt...
g0119: [2024-08-12 11:23:20,599] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,606] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,606] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,613] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,614] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt...
g0119: [2024-08-12 11:23:20,631] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt.
g0119: [2024-08-12 11:23:20,631] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt.
g0119: [2024-08-12 11:23:20,633] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt.
g0119: [2024-08-12 11:23:20,634] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_11-model_00-model_states.pt.
g0119: [2024-08-12 11:23:20,648] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt...
g0119: [2024-08-12 11:23:20,648] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt...
g0119: [2024-08-12 11:23:20,654] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt...
g0119: [2024-08-12 11:23:20,654] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,727] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,728] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,728] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,728] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,728] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,729] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,729] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,729] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,734] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,734] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,734] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,734] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,734] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,734] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,735] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,735] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,742] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,742] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,742] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,743] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,743] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,743] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,743] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,743] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,761] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,761] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,764] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,764] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_08-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,765] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,765] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,768] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,768] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_21-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,774] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,774] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,778] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,778] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,779] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,782] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,783] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,783] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,783] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt.
g0120: [2024-08-12 11:23:20,784] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,787] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,788] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,789] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,795] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,798] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt...
g0120: [2024-08-12 11:23:20,798] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,886] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,886] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,886] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,886] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,887] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 11:23:20,918] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,918] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,922] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 11:23:20,922] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_07-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,989] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,990] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,990] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,990] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,990] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt.
g0123: [2024-08-12 11:23:20,990] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,991] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt...
g0123: [2024-08-12 11:23:20,991] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,993] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,993] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,993] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,994] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt.
g0115: [2024-08-12 11:23:20,994] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,994] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,994] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt...
g0115: [2024-08-12 11:23:20,994] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 11:23:21,011] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt.
g0120: [2024-08-12 11:23:21,011] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt.
g0120: [2024-08-12 11:23:21,011] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt...
g0120: [2024-08-12 11:23:21,011] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt...
g0120: [2024-08-12 11:23:21,016] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt.
g0120: [2024-08-12 11:23:21,016] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt.
g0120: [2024-08-12 11:23:21,016] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt...
g0120: [2024-08-12 11:23:21,016] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 11:23:21,022] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt.
g0123: [2024-08-12 11:23:21,022] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt.
g0123: [2024-08-12 11:23:21,023] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt.
g0123: [2024-08-12 11:23:21,023] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_22-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,025] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,025] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,027] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,027] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_09-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt...
g0115: [2024-08-12 11:23:21,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 11:23:21,042] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt.
g0120: [2024-08-12 11:23:21,042] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,047] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt...
g0115: [2024-08-12 11:23:21,047] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 11:23:21,059] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt.
g0120: [2024-08-12 11:23:21,059] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_16-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,289] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,290] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,290] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,290] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,290] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt...
g0115: [2024-08-12 11:23:21,290] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt...
g0115: [2024-08-12 11:23:21,290] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt...
g0115: [2024-08-12 11:23:21,290] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt...
g0115: [2024-08-12 11:23:21,321] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,321] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,324] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt.
g0115: [2024-08-12 11:23:21,324] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_10-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,372] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,372] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,372] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,372] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,372] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,372] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,373] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,373] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,405] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,405] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,405] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,406] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_12-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,420] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,420] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,426] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,426] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 11:23:21,509] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_05_model_states.pt.
g0121: [2024-08-12 11:23:21,509] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_05_model_states.pt.
g0121: [2024-08-12 11:23:21,509] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_05_model_states.pt.
g0121: [2024-08-12 11:23:21,509] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt...
g0121: [2024-08-12 11:23:21,509] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt...
g0121: [2024-08-12 11:23:21,510] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt...
g0121: [2024-08-12 11:23:21,511] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_05_model_states.pt.
g0121: [2024-08-12 11:23:21,512] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,690] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,690] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,691] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,691] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,691] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,691] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,691] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,692] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt...
g0119: [2024-08-12 11:23:21,723] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,723] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,725] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt.
g0119: [2024-08-12 11:23:21,725] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 11:23:21,824] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt.
g0121: [2024-08-12 11:23:21,824] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt.
g0121: [2024-08-12 11:23:21,824] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt.
g0121: [2024-08-12 11:23:21,825] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt.
g0121: [2024-08-12 11:23:21,825] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt...
g0121: [2024-08-12 11:23:21,825] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt...
g0121: [2024-08-12 11:23:21,825] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt...
g0121: [2024-08-12 11:23:21,826] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt...
g0121: [2024-08-12 11:23:21,858] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt.
g0121: [2024-08-12 11:23:21,858] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt.
g0121: [2024-08-12 11:23:21,858] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt.
g0121: [2024-08-12 11:23:21,860] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_17-model_00-model_states.pt.
g0121: [2024-08-12 11:23:21,875] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt...
g0121: [2024-08-12 11:23:21,879] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt...
g0121: [2024-08-12 11:23:21,879] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt...
g0121: [2024-08-12 11:23:21,881] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,078] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,078] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,078] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,079] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,079] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,079] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,079] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,079] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,109] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,109] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,109] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,111] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_18-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,125] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,130] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,130] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,130] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,377] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,377] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,377] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,377] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,377] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,377] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,377] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,377] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt...
g0121: [2024-08-12 11:23:22,409] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,412] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,412] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt.
g0121: [2024-08-12 11:23:22,413] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_19-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,302] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 11:23:24,302] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 11:23:24,302] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 11:23:24,302] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 11:23:24,303] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,303] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,303] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,304] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,624] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,624] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,624] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,625] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,625] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,625] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,625] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,626] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,669] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,669] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,675] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,675] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,687] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,695] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,698] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,704] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,948] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,948] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,948] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,949] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,949] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,949] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,949] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,950] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,981] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,981] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,984] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,985] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 11:23:24,994] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 11:23:24,999] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,002] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,008] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,426] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,427] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,427] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,427] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,427] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,427] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,428] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,428] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,457] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,457] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,459] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,463] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,470] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,474] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,479] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,486] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,708] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,708] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,708] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,708] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,708] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,709] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,709] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,709] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 11:23:25,739] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,739] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,742] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 11:23:25,742] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step46000/layer_04-model_00-model_states.pt.
g0108:  > overriding learning rate value to 0.0002
g0108:  > overriding minimum learning rate value to 2e-06
g0108:  > overriding warmup iterations value to 0
g0108:  > overriding warmup tokens value to 3000000000
g0108:  > overriding total number of iterations value to 6656000
g0108:  > overriding decay tokens value to 300000000000
g0108:  > overriding learning rate decay style value to cosine
g0108:  > overriding start weight decay value to 0.1
g0108:  > overriding end weight decay value to 0.1
g0108:  > overriding total number of weight decay iterations value to 6656000
g0108:  > overriding weight decay incr style value to constant
g0108:  checkpoint version 3.0
g0108:   successfully loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase at iteration 46000
g0124: (min, max) time across ranks (ms):
g0124:     load-checkpoint ................................: (14078.12, 14079.44)
g0108: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-12 11:23:27 
g0108: > building train, validation, and test datasets ...
g0108:  > datasets target sizes (minimum size):
g0108:     train:      6656000
g0108:     validation: 678400
g0108:     test:       12800
g0108: > building train, validation, and test datasets for GPT ...
g0108: Single data path provided for train, valid & test
g0108:  > building dataset index ...
g0108:     reading sizes...
g0108:     reading pointers...
g0108:     reading document index...
g0108:     creating numpy buffer of mmap...
g0108:     creating memory view of numpy buffer...
g0108:  > finished creating indexed dataset in 0.087803 seconds
g0108:     number of documents: 250886
g0108:  > dataset split:
g0108:     train:
g0108:      document indices in [0, 238091) total of 238091 documents
g0108:     validation:
g0108:      document indices in [238091, 250635) total of 12544 documents
g0108:     test:
g0108:      document indices in [250635, 250886) total of 251 documents
g0108:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/1b748295524dd12ee996beae300eb264_doc_idx.npy
g0108:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/1b748295524dd12ee996beae300eb264_sample_idx.npy
g0108:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/1b748295524dd12ee996beae300eb264_shuffle_idx.npy
g0108:     loaded indexed file in 0.124 seconds
g0108:     total number of samples: 6675496
g0108:     total number of epochs: 14
g0108:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3fb31b5e2ebe09b3ddb60248542435ee_doc_idx.npy
g0108:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3fb31b5e2ebe09b3ddb60248542435ee_sample_idx.npy
g0108:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/3fb31b5e2ebe09b3ddb60248542435ee_shuffle_idx.npy
g0108:     loaded indexed file in 0.027 seconds
g0108:     total number of samples: 692334
g0108:     total number of epochs: 27
g0108:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/b4dcabb9403349ef9dc0eded6edcbcbf_doc_idx.npy
g0108:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/b4dcabb9403349ef9dc0eded6edcbcbf_sample_idx.npy
g0108:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/b4dcabb9403349ef9dc0eded6edcbcbf_shuffle_idx.npy
g0108:     loaded indexed file in 0.157 seconds
g0108:     total number of samples: 12809
g0108:     total number of epochs: 34
g0108: > finished creating GPT datasets ...
g0108: [after dataloaders are built] datetime: 2024-08-12 11:23:28 
g0108: done with setup ...
g0108: training ...
g0124: (min, max) time across ranks (ms):
g0124:     model-and-optimizer-setup ......................: (16995.55, 17003.47)
g0124:     train/valid/test-data-iterators-setup ..........: (1538.07, 1542.67)
g0108: [before the start of training step] datetime: 2024-08-12 11:23:28 
g0115: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46002
g0115: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0115: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46002
g0115: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46002
g0120: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46002
g0113: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46002
g0119: Grad overflow on iteration 46002
g0123: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0123: Grad overflow on iteration 46002
g0115: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0123: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0120: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0108: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46002
g0108: Grad overflow on iteration 46002
g0113: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0121: Grad overflow on iteration 46002
g0121: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0120: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46002
g0120: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46002
g0120: Grad overflow on iteration 46002
g0123: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0120: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0119: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0115: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46002
g0119: Grad overflow on iteration 46002
g0119: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0120: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0119: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46002
g0119: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46002
g0120: Grad overflow on iteration 46002
g0119: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0123: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0108: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0123: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46002
g0113: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46002
g0113: Grad overflow on iteration 46002
g0121: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0123: Grad overflow on iteration 46002
g0108: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0121: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46002
g0123: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0121: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0113: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46002
g0108: Grad overflow on iteration 46002
g0121: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0121: Grad overflow on iteration 46002
g0124: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46002
g0113: Grad overflow on iteration 46002
g0124: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0124: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46002
g0108: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0124: Grad overflow on iteration 46002
g0124: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0124: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46002
g0124: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0124: [2024-08-12 11:23:53,440] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46002
g0115: [2024-08-12 11:23:53,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0123: [2024-08-12 11:23:53,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0119: [2024-08-12 11:23:53,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0121: [2024-08-12 11:23:53,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0120: [2024-08-12 11:23:53,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0113: [2024-08-12 11:23:53,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0124: [2024-08-12 11:23:53,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0108: [2024-08-12 11:23:53,441] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1024.0 to 512.0
g0108: [2024-08-12 11:23:53,442] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1024.0, reducing to 512.0
g0123: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46005
g0123: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46005
g0123: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0123: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0121: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46005
g0121: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0121: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46005
g0121: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46005
g0113: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46005
g0113: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0113: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0113: Grad overflow on iteration 46005
g0115: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0115: Grad overflow on iteration 46005
g0115: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46005
g0113: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0115: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0115: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0115: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46005
g0108: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0123: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46005
g0123: Grad overflow on iteration 46005
g0119: Grad overflow on iteration 46005
g0123: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0119: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46005
g0119: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0108: Grad overflow on iteration 46005
g0120: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46005
g0113: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0120: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46005
g0119: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0119: Grad overflow on iteration 46005
g0115: Grad overflow on iteration 46005
g0120: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0113: Grad overflow on iteration 46005
g0108: Grad overflow on iteration 46005
g0119: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0108: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0113: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0120: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46005
g0120: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0108: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0108: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46005
g0124: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0124: Grad overflow on iteration 46005
g0124: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0124: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46005
g0124: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0124: [2024-08-12 11:24:06,112] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46005
g0124: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0123: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46005
g0119: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46005
g0113: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0115: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0121: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46005
g0108: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46005
g0120: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46005
g0123: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0119: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0121: [2024-08-12 11:24:06,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0108: [2024-08-12 11:24:06,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0120: [2024-08-12 11:24:06,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0124: [2024-08-12 11:24:06,113] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46005
g0108: [2024-08-12 11:24:06,114] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 512.0, reducing to 256.0
g0124: [2024-08-12 11:24:06,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 512.0 to 256.0
g0108: [2024-08-12 11:24:23,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=46010, skipped=75, lr=[0.00019954562940391118, 0.00019954562940391118], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46010 loss: 1.4927 iter time (s): 5.434 samples/sec: 23.557
g0124:  iteration    46010/   52000 | consumed samples:      5889280 | consumed tokens:  12061245440 | elapsed time per iteration (ms): 5468.7 | learning rate: 1.995E-04 | global batch size:   128 | loss scale: 256.0 | grad norm: 0.255 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.406 | tokens per gpu per second (tgs): 1497.982 | TFLOPs: 12.05 |
g0121: [Rank 20] (after 46010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5434.0 | max reserved: 5434.0
g0120: [Rank 16] (after 46010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6308.0 | max reserved: 6308.0
g0108: [Rank 0] (after 46010 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 10882.0 | max reserved: 10882.0
g0124: [Rank 28] (after 46010 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0119: [Rank 12] (after 46010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7202.0 | max reserved: 7202.0
g0123: [Rank 24] (after 46010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 5054.0 | max reserved: 5054.0
g0113: [Rank 4] (after 46010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 8990.0 | max reserved: 8990.0
g0115: [Rank 8] (after 46010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8096.0 | max reserved: 8096.0
g0113: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46012
g0113: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46012
g0113: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0113: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0113: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46012
g0113: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46012
g0108: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46012
g0123: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0119: Grad overflow on iteration 46012
g0108: Grad overflow on iteration 46012
g0121: Grad overflow on iteration 46012
g0108: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46012
g0121: Grad overflow on iteration 46012
g0115: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0115: Grad overflow on iteration 46012
g0108: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46012
g0119: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46012
g0124: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0123: Grad overflow on iteration 46012
g0108: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0115: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0119: Grad overflow on iteration 46012
g0123: Grad overflow on iteration 46012
g0119: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0123: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46012
g0123: Grad overflow on iteration 46012
g0121: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0120: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0121: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46012
g0108: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46012
g0120: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0121: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0121: Grad overflow on iteration 46012
g0115: Grad overflow on iteration 46012
g0121: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0115: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0124: Grad overflow on iteration 46012
g0115: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46012
g0120: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0120: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0124: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46012
g0124: Grad overflow on iteration 46012
g0119: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0119: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46012
g0119: Grad overflow on iteration 46012
g0108: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0124: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0115: Grad overflow on iteration 46012
g0108: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0124: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0115: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0124: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46012
g0115: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0119: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0123: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0123: Grad overflow on iteration 46012
g0115: Grad overflow on iteration 46012
g0115: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0115: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0119: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0123: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0123: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0124: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0120: [2024-08-12 11:24:37,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 256.0 to 128.0
g0108: [2024-08-12 11:24:37,878] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 256.0, reducing to 128.0
g0123: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46013
g0123: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0123: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46013
g0123: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46013
g0120: Grad overflow on iteration 46013
g0119: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0119: Grad overflow on iteration 46013
g0120: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46013
g0120: Grad overflow on iteration 46013
g0120: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0119: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0121: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0115: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46013
g0115: Grad overflow on iteration 46013
g0123: Grad overflow on iteration 46013
g0119: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46013
g0123: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0113: Grad overflow on iteration 46013
g0119: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0120: Grad overflow on iteration 46013
g0113: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46013
g0115: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0120: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0115: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0119: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46013
g0115: Grad overflow on iteration 46013
g0123: [2024-08-12 11:24:42,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0119: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0115: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46013
g0124: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46013
g0124: Grad overflow on iteration 46013
g0113: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0119: [2024-08-12 11:24:42,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0113: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46013
g0121: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0113: Grad overflow on iteration 46013
g0121: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0113: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0108: Grad overflow on iteration 46013
g0113: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46013
g0113: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0120: Grad overflow on iteration 46013
g0120: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0115: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0115: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:24:42,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0120: [2024-08-12 11:24:42,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0115: Grad overflow on iteration 46013
g0115: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0121: Grad overflow on iteration 46013
g0115: [2024-08-12 11:24:42,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0108: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46013
g0108: Grad overflow on iteration 46013
g0124: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46013
g0108: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0124: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0108: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46013
g0124: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:24:42,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0108: [2024-08-12 11:24:42,155] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46013
g0108: Grad overflow on iteration 46013
g0108: [2024-08-12 11:24:42,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0124: [2024-08-12 11:24:42,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0121: [2024-08-12 11:24:42,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0108: [2024-08-12 11:24:42,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0108: [2024-08-12 11:24:42,156] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 128.0, reducing to 64.0
g0124: [2024-08-12 11:24:42,156] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 128.0 to 64.0
g0113: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46017
g0113: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0123: Grad overflow on iteration 46017
g0123: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0113: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46017
g0113: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0113: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46017
g0123: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46017
g0123: Grad overflow on iteration 46017
g0115: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0123: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46017
g0123: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46017
g0123: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0115: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46017
g0115: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46017
g0115: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0115: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46017
g0115: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0120: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0120: Grad overflow on iteration 46017
g0108: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46017
g0120: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0120: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46017
g0108: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46017
g0108: Grad overflow on iteration 46017
g0120: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0108: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0108: Grad overflow on iteration 46017
g0108: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0120: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0108: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0108: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46017
g0108: Grad overflow on iteration 46017
g0120: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46017
g0120: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0119: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46017
g0119: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46017
g0119: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0119: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0119: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46017
g0119: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0119: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46017
g0124: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46017
g0124: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46017
g0124: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0121: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0124: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46017
g0124: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0121: Grad overflow on iteration 46017
g0121: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46017
g0121: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46017
g0121: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0121: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0121: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0121: [2024-08-12 11:24:59,590] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46017
g0120: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0124: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46017
g0115: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0113: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0119: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0108: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0121: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0123: [2024-08-12 11:24:59,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0108: [2024-08-12 11:24:59,591] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 64.0, reducing to 32.0
g0124: [2024-08-12 11:24:59,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 64.0 to 32.0
g0123: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46018
g0123: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0113: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46018
g0113: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46018
g0113: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0113: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46018
g0113: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0123: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46018
g0113: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0123: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46018
g0113: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46018
g0119: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46018
g0119: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0119: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46018
g0123: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0120: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46018
g0115: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46018
g0119: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0123: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0108: Grad overflow on iteration 46018
g0115: Grad overflow on iteration 46018
g0113: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0108: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46018
g0119: Grad overflow on iteration 46018
g0120: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0115: Grad overflow on iteration 46018
g0121: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46018
g0119: Grad overflow on iteration 46018
g0121: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0123: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0119: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0115: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0115: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46018
g0108: Grad overflow on iteration 46018
g0115: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0115: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0115: Grad overflow on iteration 46018
g0115: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0124: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46018
g0120: Grad overflow on iteration 46018
g0124: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46018
g0120: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0115: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0124: Grad overflow on iteration 46018
g0121: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0121: Grad overflow on iteration 46018
g0121: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0124: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46018
g0124: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0124: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46018
g0120: Grad overflow on iteration 46018
g0121: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0124: Grad overflow on iteration 46018
g0120: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0124: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0120: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0121: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0119: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0124: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0108: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46018
g0108: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0108: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0108: [2024-08-12 11:25:04,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46018
g0108: [2024-08-12 11:25:04,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0108: [2024-08-12 11:25:04,005] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32.0, reducing to 16.0
g0115: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46019
g0115: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0115: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46019
g0115: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46019
g0115: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46019
g0123: Grad overflow on iteration 46019
g0115: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0115: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0123: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0123: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46019
g0123: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46019
g0123: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46019
g0113: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46019
g0123: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0108: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0123: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0108: Grad overflow on iteration 46019
g0113: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0120: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46019
g0113: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46019
g0113: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46019
g0113: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0120: Grad overflow on iteration 46019
g0119: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0121: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0119: Grad overflow on iteration 46019
g0121: Grad overflow on iteration 46019
g0119: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46019
g0119: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46019
g0120: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0120: Grad overflow on iteration 46019
g0120: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46019
g0121: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0120: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46019
g0120: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0120: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0121: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0121: Grad overflow on iteration 46019
g0119: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0121: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46019
g0119: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0121: Grad overflow on iteration 46019
g0121: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0121: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0121: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0121: Grad overflow on iteration 46019
g0113: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0123: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0124: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46019
g0120: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0124: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46019
g0124: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0124: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46019
g0124: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0124: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46019
g0124: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0108: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46019
g0108: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46019
g0108: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46019
g0108: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0108: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0121: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0124: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0108: [2024-08-12 11:25:08,220] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0108: [2024-08-12 11:25:08,221] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16.0, reducing to 8.0
g0108: [2024-08-12 11:25:08,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=46020, skipped=80, lr=[0.00019954544549022598, 0.00019954544549022598], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46020 loss: nan iter time (s): 4.291 samples/sec: 29.830
g0124:  iteration    46020/   52000 | consumed samples:      5890560 | consumed tokens:  12063866880 | elapsed time per iteration (ms): 4458.7 | learning rate: 1.995E-04 | global batch size:   128 | loss scale: 8.0 | grad norm: 0.255 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.708 | tokens per gpu per second (tgs): 1837.324 | TFLOPs: 14.79 |
g0113: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46021
g0113: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46021
g0113: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0115: Grad overflow on iteration 46021
g0113: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46021
g0113: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0115: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46021
g0115: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46021
g0115: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0113: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0115: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0115: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0115: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46021
g0123: Grad overflow on iteration 46021
g0123: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46021
g0123: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0123: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46021
g0123: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0123: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0115: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0120: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46021
g0120: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46021
g0121: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46021
g0120: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0108: Grad overflow on iteration 46021
g0121: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46021
g0120: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46021
g0120: Grad overflow on iteration 46021
g0123: Grad overflow on iteration 46021
g0123: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0120: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0119: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0119: Grad overflow on iteration 46021
g0120: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46021
g0120: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0121: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0121: Grad overflow on iteration 46021
g0119: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0121: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0121: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46021
g0119: Grad overflow on iteration 46021
g0119: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0113: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0119: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46021
g0121: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0113: Grad overflow on iteration 46021
g0113: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0124: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46021
g0108: Grad overflow on iteration 46021
g0119: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46021
g0108: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0119: Grad overflow on iteration 46021
g0124: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0119: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0108: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0124: [2024-08-12 11:25:16,544] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46021
g0124: Grad overflow on iteration 46021
g0108: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0124: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0108: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0124: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46021
g0124: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0124: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0108: [2024-08-12 11:25:16,545] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8.0, reducing to 4.0
g0119: [2024-08-12 11:25:16,545] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0123: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46024
g0119: Grad overflow on iteration 46024
g0119: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0113: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0113: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46024
g0113: Grad overflow on iteration 46024
g0123: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0113: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46024
g0108: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46024
g0113: Grad overflow on iteration 46024
g0119: Grad overflow on iteration 46024
g0115: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0119: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0115: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46024
g0120: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0108: Grad overflow on iteration 46024
g0119: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46024
g0121: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46024
g0120: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46024
g0120: Grad overflow on iteration 46024
g0108: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46024
g0120: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0119: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46024
g0119: Grad overflow on iteration 46024
g0123: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46024
g0123: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0108: Grad overflow on iteration 46024
g0121: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0115: Grad overflow on iteration 46024
g0123: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: Grad overflow on iteration 46024
g0113: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46024
g0120: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0108: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0115: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0121: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46024
g0121: Grad overflow on iteration 46024
g0113: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0121: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0121: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0121: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0121: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46024
g0120: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46024
g0121: Grad overflow on iteration 46024
g0108: Grad overflow on iteration 46024
g0120: Grad overflow on iteration 46024
g0115: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0108: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0124: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0115: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0124: Grad overflow on iteration 46024
g0108: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0124: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46024
g0124: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0124: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0124: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0124: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46024
g0113: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0119: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0121: [2024-08-12 11:25:29,538] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0108: [2024-08-12 11:25:29,539] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4.0, reducing to 2.0
g0124: [2024-08-12 11:25:29,539] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0123: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46026
g0123: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0123: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46026
g0113: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46026
g0121: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46026
g0121: Grad overflow on iteration 46026
g0123: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46026
g0123: Grad overflow on iteration 46026
g0115: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0120: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0113: Grad overflow on iteration 46026
g0120: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0123: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0115: Grad overflow on iteration 46026
g0113: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46026
g0115: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46026
g0108: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46026
g0120: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0119: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46026
g0123: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0115: Grad overflow on iteration 46026
g0115: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0121: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46026
g0120: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0108: Grad overflow on iteration 46026
g0115: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46026
g0120: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0119: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0115: Grad overflow on iteration 46026
g0119: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0115: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0113: Grad overflow on iteration 46026
g0124: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0115: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0124: Grad overflow on iteration 46026
g0108: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46026
g0108: Grad overflow on iteration 46026
g0119: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46026
g0124: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0119: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0121: Grad overflow on iteration 46026
g0113: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:25:38,126] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0124: Grad overflow on iteration 46026
g0121: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0108: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0113: Grad overflow on iteration 46026
g0119: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0124: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0121: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46026
g0113: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46026
g0121: Grad overflow on iteration 46026
g0113: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0113: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0121: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0124: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0121: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0124: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46026
g0124: Grad overflow on iteration 46026
g0124: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0108: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46026
g0108: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0108: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0108: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0124: [2024-08-12 11:25:38,127] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0108: [2024-08-12 11:25:38,127] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2.0, reducing to 1.0
g0108: [2024-08-12 11:25:51,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=46030, skipped=83, lr=[0.00019954510383773689, 0.00019954510383773689], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46030 loss: 1.7686 iter time (s): 4.265 samples/sec: 30.010
g0124:  iteration    46030/   52000 | consumed samples:      5891840 | consumed tokens:  12066488320 | elapsed time per iteration (ms): 4298.1 | learning rate: 1.995E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 2.132 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.780 | tokens per gpu per second (tgs): 1905.947 | TFLOPs: 15.34 |
g0108: [2024-08-12 11:26:33,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=46040, skipped=83, lr=[0.00019954484094099345, 0.00019954484094099345], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46040 loss: 2.7198 iter time (s): 4.228 samples/sec: 30.276
g0124:  iteration    46040/   52000 | consumed samples:      5893120 | consumed tokens:  12069109760 | elapsed time per iteration (ms): 4261.5 | learning rate: 1.995E-04 | global batch size:   128 | lm loss: 2.102883E+00 | loss scale: 1.0 | grad norm: 2.913 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.037 | tokens per gpu per second (tgs): 1922.340 | TFLOPs: 15.47 |
g0108: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46047
g0108: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46047
g0115: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46047
g0120: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46047
g0113: Grad overflow on iteration 46047
g0123: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46047
g0113: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46047
g0113: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0120: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46047
g0120: Grad overflow on iteration 46047
g0113: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0123: Grad overflow on iteration 46047
g0113: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46047
g0123: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46047
g0120: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0120: Grad overflow on iteration 46047
g0123: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0115: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0121: Grad overflow on iteration 46047
g0123: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46047
g0121: Grad overflow on iteration 46047
g0124: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46047
g0124: Grad overflow on iteration 46047
g0121: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0113: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0115: Grad overflow on iteration 46047
g0119: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0120: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0115: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0115: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46047
g0121: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0120: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0121: Grad overflow on iteration 46047
g0121: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0123: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0123: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46047
g0123: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0119: Grad overflow on iteration 46047
g0123: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0119: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46047
g0115: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0119: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46047
g0108: Grad overflow on iteration 46047
g0124: Grad overflow on iteration 46047
g0108: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46047
g0120: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0119: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0120: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0124: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: Grad overflow on iteration 46047
g0119: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46047
g0108: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0124: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0124: Grad overflow on iteration 46047
g0124: [2024-08-12 11:27:08,390] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46047
g0124: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0119: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0119: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0124: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0124: [2024-08-12 11:27:08,391] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: [2024-08-12 11:27:08,391] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1.0, reducing to 1
g0108: [2024-08-12 11:27:16,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=46050, skipped=84, lr=[0.00019954457796847946, 0.00019954457796847946], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46050 loss: 3.9202 iter time (s): 4.254 samples/sec: 30.092
g0124:  iteration    46050/   52000 | consumed samples:      5894400 | consumed tokens:  12071731200 | elapsed time per iteration (ms): 4286.6 | learning rate: 1.995E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 3.507 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.861 | tokens per gpu per second (tgs): 1911.073 | TFLOPs: 15.38 |
g0108: [2024-08-12 11:27:59,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=46060, skipped=84, lr=[0.0001995443149201951, 0.0001995443149201951], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46060 loss: 5.8242 iter time (s): 4.228 samples/sec: 30.273
g0124:  iteration    46060/   52000 | consumed samples:      5895680 | consumed tokens:  12074352640 | elapsed time per iteration (ms): 4261.5 | learning rate: 1.995E-04 | global batch size:   128 | lm loss: 4.596178E+00 | loss scale: 1.0 | grad norm: 4.565 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.036 | tokens per gpu per second (tgs): 1922.315 | TFLOPs: 15.47 |
g0108: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46062
g0108: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46062
g0108: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46062
g0108: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46062
g0120: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46062
g0120: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46062
g0120: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46062
g0120: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46062
g0115: Grad overflow on iteration 46062
g0113: Grad overflow on iteration 46062
g0121: Grad overflow on iteration 46062
g0113: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46062
g0121: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46062
g0113: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46062
g0113: Grad overflow on iteration 46062
g0124: Grad overflow on iteration 46062
g0123: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46062
g0123: Grad overflow on iteration 46062
g0121: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46062
g0123: Grad overflow on iteration 46062
g0124: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46062
g0123: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46062
g0113: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46062
g0124: Grad overflow on iteration 46062
g0115: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46062
g0115: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46062
g0115: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46062
g0123: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46062
g0119: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46062
g0120: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46062
g0124: [2024-08-12 11:28:12,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46062
g0115: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46062
g0124: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:12,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:12,043] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0119: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46065
g0115: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46065
g0119: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46065
g0119: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46065
g0113: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46065
g0113: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46065
g0123: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46065
g0120: Grad overflow on iteration 46065
g0121: Grad overflow on iteration 46065
g0120: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46065
g0115: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46065
g0119: Grad overflow on iteration 46065
g0115: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46065
g0113: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46065
g0115: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46065
g0108: Grad overflow on iteration 46065
g0108: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46065
g0115: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46065
g0124: Grad overflow on iteration 46065
g0123: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46065
g0113: Grad overflow on iteration 46065
g0113: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46065
g0113: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46065
g0108: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46065
g0124: Grad overflow on iteration 46065
g0108: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46065
g0120: Grad overflow on iteration 46065
g0108: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46065
g0124: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46065
g0124: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46065
g0121: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46065
g0121: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46065
g0121: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:24,922] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:24,923] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:28:24,923] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:24,923] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46068
g0120: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46068
g0120: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46068
g0115: Grad overflow on iteration 46068
g0119: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46068
g0121: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46068
g0115: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46068
g0119: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46068
g0119: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46068
g0119: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46068
g0108: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46068
g0121: Grad overflow on iteration 46068
g0108: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46068
g0113: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46068
g0113: Grad overflow on iteration 46068
g0123: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46068
g0113: Grad overflow on iteration 46068
g0115: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46068
g0121: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46068
g0123: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46068
g0124: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46068
g0123: Grad overflow on iteration 46068
g0121: Grad overflow on iteration 46068
g0123: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46068
g0121: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46068
g0123: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46068
g0120: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46068
g0123: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46068
g0124: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46068
g0124: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:37,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46068
g0119: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46068
g0124: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46068
g0108: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:37,512] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:37,512] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0120: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46069
g0108: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46069
g0120: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46069
g0120: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46069
g0121: Grad overflow on iteration 46069
g0115: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46069
g0120: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46069
g0119: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46069
g0113: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46069
g0108: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46069
g0108: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46069
g0108: Grad overflow on iteration 46069
g0113: Grad overflow on iteration 46069
g0124: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46069
g0115: Grad overflow on iteration 46069
g0124: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46069
g0120: [2024-08-12 11:28:41,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46069
g0119: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46069
g0108: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46069
g0108: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46069
g0121: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46069
g0121: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46069
g0113: Grad overflow on iteration 46069
g0113: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:41,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46069
g0119: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:41,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46069
g0123: Grad overflow on iteration 46069
g0121: [2024-08-12 11:28:41,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46069
g0123: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:41,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46069
g0119: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46069
g0119: Grad overflow on iteration 46069
g0123: [2024-08-12 11:28:41,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:41,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:41,850] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46069
g0124: Grad overflow on iteration 46069
g0124: [2024-08-12 11:28:41,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:41,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:41,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:41,851] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:28:41,851] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:41,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=46070, skipped=88, lr=[0.00019954410442701306, 0.00019954410442701306], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46070 loss: nan iter time (s): 4.222 samples/sec: 30.316
g0124:  iteration    46070/   52000 | consumed samples:      5896960 | consumed tokens:  12076974080 | elapsed time per iteration (ms): 4255.2 | learning rate: 1.995E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 4.798 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.081 | tokens per gpu per second (tgs): 1925.181 | TFLOPs: 15.49 |
g0123: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46071
g0115: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46071
g0123: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46071
g0115: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46071
g0115: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46071
g0115: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46071
g0120: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46071
g0115: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46071
g0120: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46071
g0108: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46071
g0123: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46071
g0108: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46071
g0108: Grad overflow on iteration 46071
g0119: Grad overflow on iteration 46071
g0113: Grad overflow on iteration 46071
g0123: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46071
g0123: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46071
g0113: Grad overflow on iteration 46071
g0120: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46071
g0119: Grad overflow on iteration 46071
g0108: Grad overflow on iteration 46071
g0124: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46071
g0113: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46071
g0121: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46071
g0124: Grad overflow on iteration 46071
g0121: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46071
g0124: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46071
g0124: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46071
g0124: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46071
g0113: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46071
g0119: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46071
g0113: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46071
g0119: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:50,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:50,344] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:50,344] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:28:50,344] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46072
g0113: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46072
g0113: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46072
g0113: Grad overflow on iteration 46072
g0108: Grad overflow on iteration 46072
g0113: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46072
g0115: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46072
g0113: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46072
g0120: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46072
g0120: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46072
g0108: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46072
g0120: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46072
g0115: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46072
g0120: Grad overflow on iteration 46072
g0113: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46072
g0123: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46072
g0115: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46072
g0123: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46072
g0108: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46072
g0108: Grad overflow on iteration 46072
g0115: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46072
g0121: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46072
g0115: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46072
g0121: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46072
g0121: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46072
g0108: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46072
g0119: Grad overflow on iteration 46072
g0120: [2024-08-12 11:28:54,360] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46072
g0124: Grad overflow on iteration 46072
g0119: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46072
g0119: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46072
g0119: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:54,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46072
g0108: [2024-08-12 11:28:54,360] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:54,360] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:28:54,360] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46073
g0120: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46073
g0120: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46073
g0120: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46073
g0108: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46073
g0120: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46073
g0108: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46073
g0115: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46073
g0108: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46073
g0124: Grad overflow on iteration 46073
g0115: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46073
g0115: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46073
g0121: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46073
g0113: Grad overflow on iteration 46073
g0113: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46073
g0121: Grad overflow on iteration 46073
g0124: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46073
g0121: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46073
g0115: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46073
g0121: Grad overflow on iteration 46073
g0124: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46073
g0124: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46073
g0113: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46073
g0123: Grad overflow on iteration 46073
g0121: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46073
g0113: Grad overflow on iteration 46073
g0124: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46073
g0119: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46073
g0123: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46073
g0119: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46073
g0119: [2024-08-12 11:28:58,728] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46073
g0119: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46073
g0108: [2024-08-12 11:28:58,729] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:28:58,729] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0120: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46074
g0120: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46074
g0120: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46074
g0120: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46074
g0108: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46074
g0108: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46074
g0108: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46074
g0120: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46074
g0113: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46074
g0119: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46074
g0121: Grad overflow on iteration 46074
g0121: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46074
g0115: Grad overflow on iteration 46074
g0121: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46074
g0115: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46074
g0121: Grad overflow on iteration 46074
g0123: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46074
g0113: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46074
g0115: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46074
g0121: Grad overflow on iteration 46074
g0115: Grad overflow on iteration 46074
g0121: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46074
g0124: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46074
g0123: Grad overflow on iteration 46074
g0124: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46074
g0115: Grad overflow on iteration 46074
g0115: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46074
g0113: Grad overflow on iteration 46074
g0119: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46074
g0124: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:03,006] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46074
g0108: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46074
g0108: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46074
g0123: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:03,007] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0115: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:03,007] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46075
g0115: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46075
g0120: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46075
g0120: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46075
g0120: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46075
g0119: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46075
g0119: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46075
g0119: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46075
g0119: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46075
g0108: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46075
g0115: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46075
g0119: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46075
g0121: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46075
g0120: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46075
g0120: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46075
g0113: Grad overflow on iteration 46075
g0121: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46075
g0120: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46075
g0120: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46075
g0121: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46075
g0113: Grad overflow on iteration 46075
g0121: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46075
g0113: Grad overflow on iteration 46075
g0113: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46075
g0108: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46075
g0108: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46075
g0123: Grad overflow on iteration 46075
g0108: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46075
g0123: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46075
g0124: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46075
g0124: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46075
g0124: [2024-08-12 11:29:07,212] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46075
g0108: [2024-08-12 11:29:07,213] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:07,213] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46076
g0108: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46076
g0120: Grad overflow on iteration 46076
g0120: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46076
g0108: Grad overflow on iteration 46076
g0108: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46076
g0120: Grad overflow on iteration 46076
g0119: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46076
g0119: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46076
g0119: Grad overflow on iteration 46076
g0115: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46076
g0119: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46076
g0123: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46076
g0119: Grad overflow on iteration 46076
g0119: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46076
g0113: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46076
g0119: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46076
g0113: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46076
g0113: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46076
g0123: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46076
g0123: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46076
g0123: Grad overflow on iteration 46076
g0108: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46076
g0108: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46076
g0108: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46076
g0123: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46076
g0108: [2024-08-12 11:29:11,554] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0123: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46076
g0121: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46076
g0121: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46076
g0124: [2024-08-12 11:29:11,553] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46076
g0121: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46076
g0124: Grad overflow on iteration 46076
g0115: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:11,554] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46078
g0113: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46078
g0113: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46078
g0123: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46078
g0113: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46078
g0113: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46078
g0120: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46078
g0121: Grad overflow on iteration 46078
g0121: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46078
g0120: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46078
g0121: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46078
g0121: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46078
g0121: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46078
g0108: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:19,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46078
g0113: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46078
g0119: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46078
g0115: Grad overflow on iteration 46078
g0124: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46078
g0120: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46078
g0119: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46078
g0115: Grad overflow on iteration 46078
g0119: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46078
g0108: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46078
g0115: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46078
g0108: Grad overflow on iteration 46078
g0108: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46078
g0119: Grad overflow on iteration 46078
g0119: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46078
g0124: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46078
g0124: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:19,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46078
g0108: Grad overflow on iteration 46078
g0124: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46078
g0124: [2024-08-12 11:29:19,949] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:19,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:19,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:19,950] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:19,950] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46079
g0108: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46079
g0108: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46079
g0123: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46079
g0123: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46079
g0121: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46079
g0115: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46079
g0123: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46079
g0123: Grad overflow on iteration 46079
g0113: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46079
g0115: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46079
g0115: Grad overflow on iteration 46079
g0113: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46079
g0115: Grad overflow on iteration 46079
g0113: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46079
g0115: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46079
g0123: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46079
g0120: Grad overflow on iteration 46079
g0121: Grad overflow on iteration 46079
g0121: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46079
g0119: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46079
g0119: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46079
g0113: Grad overflow on iteration 46079
g0113: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46079
g0119: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46079
g0124: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46079
g0120: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46079
g0124: Grad overflow on iteration 46079
g0124: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46079
g0120: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46079
g0119: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46079
g0119: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46079
g0119: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:24,088] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:24,089] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:29:24,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=46080, skipped=96, lr=[0.00019954384124234255, 0.00019954384124234255], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46080 loss: nan iter time (s): 4.191 samples/sec: 30.541
g0124:  iteration    46080/   52000 | consumed samples:      5898240 | consumed tokens:  12079595520 | elapsed time per iteration (ms): 4224.0 | learning rate: 1.995E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 5.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.303 | tokens per gpu per second (tgs): 1939.385 | TFLOPs: 15.61 |
g0108: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46080
g0115: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46080
g0123: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46080
g0115: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46080
g0115: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46080
g0123: Grad overflow on iteration 46080
g0123: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46080
g0123: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46080
g0113: Grad overflow on iteration 46080
g0121: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46080
g0119: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46080
g0115: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46080
g0113: Grad overflow on iteration 46080
g0119: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46080
g0115: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46080
g0120: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46080
g0121: Grad overflow on iteration 46080
g0113: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46080
g0113: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46080
g0124: Grad overflow on iteration 46080
g0119: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46080
g0124: Grad overflow on iteration 46080
g0120: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46080
g0113: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:28,447] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46080
g0113: Grad overflow on iteration 46080
g0124: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46080
g0108: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46080
g0120: Grad overflow on iteration 46080
g0108: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46080
g0119: Grad overflow on iteration 46080
g0120: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46080
g0108: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46080
g0119: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:28,448] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0120: [2024-08-12 11:29:28,448] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46081
g0115: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46081
g0108: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46081
g0115: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46081
g0115: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46081
g0115: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46081
g0119: Grad overflow on iteration 46081
g0119: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46081
g0113: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46081
g0120: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46081
g0123: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46081
g0124: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46081
g0121: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46081
g0124: Grad overflow on iteration 46081
g0119: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46081
g0121: Grad overflow on iteration 46081
g0119: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46081
g0123: Grad overflow on iteration 46081
g0119: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46081
g0121: Grad overflow on iteration 46081
g0113: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46081
g0113: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46081
g0124: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46081
g0124: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46081
g0124: Grad overflow on iteration 46081
g0123: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46081
g0123: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46081
g0124: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46081
g0124: Grad overflow on iteration 46081
g0108: Grad overflow on iteration 46081
g0120: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46081
g0113: [2024-08-12 11:29:32,781] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46081
g0108: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:32,782] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:29:32,782] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46082
g0115: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46082
g0119: [2024-08-12 11:29:37,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46082
g0119: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46082
g0119: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46082
g0115: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46082
g0108: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46082
g0108: Grad overflow on iteration 46082
g0119: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:37,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46082
g0113: [2024-08-12 11:29:37,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46082
g0121: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46082
g0115: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46082
g0108: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46082
g0113: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46082
g0115: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46082
g0121: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46082
g0113: Grad overflow on iteration 46082
g0121: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46082
g0120: Grad overflow on iteration 46082
g0121: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46082
g0120: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46082
g0123: Grad overflow on iteration 46082
g0119: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46082
g0123: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46082
g0120: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46082
g0121: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46082
g0123: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46082
g0123: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46082
g0123: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46082
g0124: Grad overflow on iteration 46082
g0121: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46082
g0124: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46082
g0108: [2024-08-12 11:29:37,014] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:37,014] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:29:37,015] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46083
g0113: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46083
g0115: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46083
g0115: Grad overflow on iteration 46083
g0119: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46083
g0115: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46083
g0113: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46083
g0113: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46083
g0108: Grad overflow on iteration 46083
g0120: Grad overflow on iteration 46083
g0123: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46083
g0120: Grad overflow on iteration 46083
g0123: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46083
g0123: Grad overflow on iteration 46083
g0119: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46083
g0108: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46083
g0121: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46083
g0115: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46083
g0121: Grad overflow on iteration 46083
g0115: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46083
g0123: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46083
g0124: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46083
g0119: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46083
g0119: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46083
g0121: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46083
g0121: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46083
g0119: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46083
g0115: Grad overflow on iteration 46083
g0121: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:41,430] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46083
g0124: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46083
g0124: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46083
g0124: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46083
g0108: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:41,431] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:29:41,431] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46084
g0120: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46084
g0121: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46084
g0121: Grad overflow on iteration 46084
g0119: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46084
g0120: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46084
g0121: Grad overflow on iteration 46084
g0123: Grad overflow on iteration 46084
g0108: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46084
g0123: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46084
g0123: Grad overflow on iteration 46084
g0119: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46084
g0113: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46084
g0115: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46084
g0120: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46084
g0121: Grad overflow on iteration 46084
g0119: Grad overflow on iteration 46084
g0120: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46084
g0113: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46084
g0123: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46084
g0123: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46084
g0119: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:45,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46084
g0124: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46084
g0113: Grad overflow on iteration 46084
g0123: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46084
g0120: Grad overflow on iteration 46084
g0120: [2024-08-12 11:29:45,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46084
g0108: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46084
g0121: [2024-08-12 11:29:45,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46084
g0115: Grad overflow on iteration 46084
g0124: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:45,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46084
g0115: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46084
g0115: [2024-08-12 11:29:45,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:45,499] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:45,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:45,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:45,500] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:45,500] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0119: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46085
g0119: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46085
g0119: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46085
g0108: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46085
g0108: Grad overflow on iteration 46085
g0121: Grad overflow on iteration 46085
g0119: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46085
g0115: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46085
g0121: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46085
g0123: Grad overflow on iteration 46085
g0120: Grad overflow on iteration 46085
g0113: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46085
g0113: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46085
g0121: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46085
g0121: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46085
g0120: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46085
g0120: Grad overflow on iteration 46085
g0108: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46085
g0119: Grad overflow on iteration 46085
g0123: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46085
g0123: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46085
g0121: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46085
g0124: Grad overflow on iteration 46085
g0113: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46085
g0113: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46085
g0113: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46085
g0123: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46085
g0113: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46085
g0124: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46085
g0120: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46085
g0108: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46085
g0124: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:49,830] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46085
g0124: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:49,831] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:49,831] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46086
g0121: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46086
g0121: Grad overflow on iteration 46086
g0115: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46086
g0120: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46086
g0121: Grad overflow on iteration 46086
g0123: Grad overflow on iteration 46086
g0121: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46086
g0123: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46086
g0123: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46086
g0115: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46086
g0119: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46086
g0113: Grad overflow on iteration 46086
g0119: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46086
g0113: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46086
g0123: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46086
g0124: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46086
g0120: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46086
g0120: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46086
g0120: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46086
g0108: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:54,271] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46086
g0124: Grad overflow on iteration 46086
g0115: Grad overflow on iteration 46086
g0115: [2024-08-12 11:29:54,271] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46086
g0113: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46086
g0108: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46086
g0108: [2024-08-12 11:29:54,271] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46086
g0119: [2024-08-12 11:29:54,271] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46086
g0113: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46086
g0113: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:54,271] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:54,271] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46086
g0113: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:54,271] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:54,270] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46086
g0108: [2024-08-12 11:29:54,271] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46086
g0108: [2024-08-12 11:29:54,271] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:54,271] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46087
g0115: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46087
g0119: Grad overflow on iteration 46087
g0115: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46087
g0115: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46087
g0115: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46087
g0108: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46087
g0123: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46087
g0115: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46087
g0120: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46087
g0119: Grad overflow on iteration 46087
g0108: Grad overflow on iteration 46087
g0121: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46087
g0108: Grad overflow on iteration 46087
g0121: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46087
g0119: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46087
g0123: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46087
g0121: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46087
g0121: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46087
g0120: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46087
g0113: Grad overflow on iteration 46087
g0123: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46087
g0113: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:29:58,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:29:58,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46087
g0120: Grad overflow on iteration 46087
g0113: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:29:58,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46087
g0120: Grad overflow on iteration 46087
g0113: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46087
g0113: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46087
g0124: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46087
g0124: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46087
g0124: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46087
g0124: [2024-08-12 11:29:58,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:58,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46087
g0108: [2024-08-12 11:29:58,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:58,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:29:58,559] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0120: [2024-08-12 11:29:58,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:29:58,559] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46088
g0113: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46088
g0108: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46088
g0113: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46088
g0108: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46088
g0115: Grad overflow on iteration 46088
g0119: Grad overflow on iteration 46088
g0113: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46088
g0115: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46088
g0119: Grad overflow on iteration 46088
g0108: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46088
g0123: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46088
g0115: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46088
g0120: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46088
g0121: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46088
g0123: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46088
g0121: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46088
g0120: Grad overflow on iteration 46088
g0121: Grad overflow on iteration 46088
g0120: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46088
g0120: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46088
g0124: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46088
g0120: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46088
g0124: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46088
g0108: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46088
g0124: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46088
g0124: [2024-08-12 11:30:02,960] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46088
g0124: Grad overflow on iteration 46088
g0124: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46088
g0119: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46088
g0124: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46088
g0119: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:02,961] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46088
g0108: [2024-08-12 11:30:02,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46089
g0121: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46089
g0119: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46089
g0115: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46089
g0120: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46089
g0115: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46089
g0120: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46089
g0113: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46089
g0121: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46089
g0113: Grad overflow on iteration 46089
g0123: Grad overflow on iteration 46089
g0108: Grad overflow on iteration 46089
g0121: Grad overflow on iteration 46089
g0119: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46089
g0113: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46089
g0119: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46089
g0123: Grad overflow on iteration 46089
g0120: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46089
g0115: Grad overflow on iteration 46089
g0113: Grad overflow on iteration 46089
g0115: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46089
g0121: Grad overflow on iteration 46089
g0115: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46089
g0115: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46089
g0124: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46089
g0123: Grad overflow on iteration 46089
g0124: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46089
g0123: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46089
g0113: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46089
g0113: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46089
g0123: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46089
g0108: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46089
g0108: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:07,031] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:30:07,031] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:07,032] [INFO] [logging.py:96:log_dist] [Rank 0] step=46090, skipped=106, lr=[0.00019954384124234255, 0.00019954384124234255], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46090 loss: nan iter time (s): 4.261 samples/sec: 30.040
g0124:  iteration    46090/   52000 | consumed samples:      5899520 | consumed tokens:  12082216960 | elapsed time per iteration (ms): 4294.2 | learning rate: 1.995E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 5.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.808 | tokens per gpu per second (tgs): 1907.700 | TFLOPs: 15.35 |
g0108: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46090
g0120: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46090
g0119: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46090
g0120: Grad overflow on iteration 46090
g0124: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46090
g0120: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46090
g0124: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46090
g0121: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46090
g0121: Grad overflow on iteration 46090
g0115: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46090
g0115: Grad overflow on iteration 46090
g0123: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46090
g0120: Grad overflow on iteration 46090
g0113: Grad overflow on iteration 46090
g0115: Grad overflow on iteration 46090
g0119: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46090
g0121: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46090
g0123: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46090
g0123: Grad overflow on iteration 46090
g0115: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46090
g0123: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46090
g0113: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46090
g0119: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46090
g0119: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46090
g0121: Grad overflow on iteration 46090
g0121: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46090
g0124: Grad overflow on iteration 46090
g0113: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46090
g0121: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46090
g0124: Grad overflow on iteration 46090
g0113: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46090
g0108: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46090
g0108: [2024-08-12 11:30:11,359] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:11,359] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46091
g0108: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46091
g0121: Grad overflow on iteration 46091
g0113: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46091
g0121: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46091
g0115: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46091
g0120: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46091
g0115: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46091
g0120: Grad overflow on iteration 46091
g0120: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46091
g0119: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46091
g0120: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46091
g0123: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46091
g0123: Grad overflow on iteration 46091
g0120: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46091
g0119: Grad overflow on iteration 46091
g0120: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46091
g0119: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46091
g0123: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46091
g0119: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46091
g0108: Grad overflow on iteration 46091
g0123: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46091
g0113: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46091
g0113: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46091
g0123: Grad overflow on iteration 46091
g0113: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46091
g0124: Grad overflow on iteration 46091
g0119: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46091
g0124: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46091
g0124: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:15,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46091
g0124: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46091
g0124: [2024-08-12 11:30:15,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:15,574] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46091
g0108: [2024-08-12 11:30:15,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:15,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:15,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:15,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:15,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:15,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:15,575] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:15,575] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0113: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46092
g0123: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46092
g0113: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46092
g0113: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46092
g0119: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46092
g0120: Grad overflow on iteration 46092
g0123: Grad overflow on iteration 46092
g0121: Grad overflow on iteration 46092
g0113: Grad overflow on iteration 46092
g0119: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46092
g0123: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46092
g0113: Grad overflow on iteration 46092
g0121: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46092
g0121: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46092
g0115: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46092
g0119: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46092
g0119: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46092
g0123: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46092
g0119: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46092
g0115: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46092
g0121: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46092
g0124: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46092
g0120: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46092
g0115: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46092
g0124: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46092
g0119: Grad overflow on iteration 46092
g0124: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46092
g0121: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46092
g0108: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46092
g0124: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46092
g0120: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:19,833] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46092
g0108: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46092
g0120: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:19,834] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:19,835] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:19,835] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46093
g0119: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46093
g0120: [2024-08-12 11:30:24,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46093
g0123: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46093
g0115: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46093
g0115: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46093
g0108: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46093
g0119: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:24,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46093
g0123: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46093
g0119: Grad overflow on iteration 46093
g0123: Grad overflow on iteration 46093
g0121: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46093
g0108: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46093
g0113: Grad overflow on iteration 46093
g0121: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46093
g0121: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46093
g0115: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:24,012] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46093
g0113: Grad overflow on iteration 46093
g0123: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46093
g0119: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46093
g0120: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46093
g0123: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46093
g0113: Grad overflow on iteration 46093
g0121: Grad overflow on iteration 46093
g0120: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46093
g0115: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46093
g0120: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46093
g0113: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46093
g0123: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46093
g0113: Grad overflow on iteration 46093
g0113: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46093
g0124: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46093
g0124: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:24,013] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:24,013] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0120: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46094
g0115: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46094
g0108: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46094
g0119: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46094
g0115: Grad overflow on iteration 46094
g0124: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46094
g0115: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46094
g0124: Grad overflow on iteration 46094
g0115: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46094
g0119: Grad overflow on iteration 46094
g0120: Grad overflow on iteration 46094
g0115: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46094
g0120: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46094
g0120: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46094
g0108: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46094
g0123: Grad overflow on iteration 46094
g0108: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46094
g0108: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46094
g0115: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46094
g0124: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46094
g0119: Grad overflow on iteration 46094
g0124: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46094
g0124: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46094
g0121: Grad overflow on iteration 46094
g0108: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46094
g0124: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46094
g0123: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46094
g0123: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:28,342] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46094
g0113: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46094
g0121: Grad overflow on iteration 46094
g0113: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46094
g0113: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46094
g0113: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:28,343] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:28,343] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46095
g0113: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46095
g0123: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46095
g0121: Grad overflow on iteration 46095
g0123: Grad overflow on iteration 46095
g0113: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46095
g0121: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46095
g0121: Grad overflow on iteration 46095
g0115: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46095
g0121: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46095
g0124: Grad overflow on iteration 46095
g0123: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46095
g0115: Grad overflow on iteration 46095
g0119: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46095
g0115: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46095
g0124: Grad overflow on iteration 46095
g0124: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46095
g0120: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46095
g0124: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46095
g0123: Grad overflow on iteration 46095
g0115: Grad overflow on iteration 46095
g0113: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46095
g0113: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46095
g0120: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46095
g0120: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46095
g0108: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46095
g0119: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46095
g0123: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46095
g0119: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46095
g0119: [2024-08-12 11:30:32,724] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46095
g0121: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46095
g0120: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46095
g0123: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:32,725] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:32,726] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46096
g0108: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46096
g0113: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46096
g0113: Grad overflow on iteration 46096
g0119: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46096
g0113: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46096
g0115: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46096
g0113: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46096
g0113: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46096
g0123: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46096
g0113: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46096
g0115: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46096
g0115: Grad overflow on iteration 46096
g0115: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46096
g0121: Grad overflow on iteration 46096
g0119: Grad overflow on iteration 46096
g0121: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46096
g0123: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46096
g0115: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46096
g0123: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46096
g0108: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46096
g0108: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46096
g0108: Grad overflow on iteration 46096
g0108: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46096
g0121: Grad overflow on iteration 46096
g0124: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46096
g0124: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46096
g0124: Grad overflow on iteration 46096
g0121: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46096
g0119: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46096
g0120: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46096
g0108: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46096
g0124: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:37,001] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:37,002] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46097
g0119: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46097
g0119: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46097
g0119: Grad overflow on iteration 46097
g0120: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46097
g0113: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46097
g0113: Grad overflow on iteration 46097
g0119: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46097
g0121: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46097
g0124: Grad overflow on iteration 46097
g0121: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46097
g0120: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46097
g0124: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46097
g0120: Grad overflow on iteration 46097
g0120: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46097
g0120: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46097
g0123: Grad overflow on iteration 46097
g0119: Grad overflow on iteration 46097
g0123: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46097
g0113: Grad overflow on iteration 46097
g0124: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46097
g0108: Grad overflow on iteration 46097
g0121: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46097
g0115: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46097
g0108: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46097
g0123: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46097
g0113: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46097
g0113: Grad overflow on iteration 46097
g0123: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46097
g0113: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46097
g0115: Grad overflow on iteration 46097
g0113: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46097
g0115: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:41,226] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:30:41,226] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46098
g0121: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46098
g0121: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46098
g0121: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46098
g0121: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:45,394] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46098
g0113: Grad overflow on iteration 46098
g0108: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:45,394] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:45,394] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46098
g0113: Grad overflow on iteration 46098
g0124: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46098
g0113: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46098
g0113: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46098
g0113: [2024-08-12 11:30:45,394] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:45,394] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46098
g0119: [2024-08-12 11:30:45,394] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46098
g0124: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46098
g0119: Grad overflow on iteration 46098
g0108: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:45,394] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46098
g0120: [2024-08-12 11:30:45,394] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46098
g0113: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46098
g0120: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46098
g0108: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46098
g0119: Grad overflow on iteration 46098
g0115: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46098
g0123: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46098
g0119: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46098
g0115: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46098
g0115: Grad overflow on iteration 46098
g0123: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46098
g0119: Grad overflow on iteration 46098
g0124: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46098
g0115: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46098
g0115: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46098
g0119: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:45,395] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0123: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46098
g0123: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:45,395] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46099
g0113: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46099
g0108: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46099
g0108: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46099
g0123: Grad overflow on iteration 46099
g0123: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46099
g0108: Grad overflow on iteration 46099
g0108: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46099
g0124: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46099
g0123: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46099
g0124: Grad overflow on iteration 46099
g0123: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46099
g0120: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46099
g0123: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46099
g0123: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46099
g0115: Grad overflow on iteration 46099
g0121: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46099
g0120: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46099
g0113: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46099
g0120: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46099
g0113: Grad overflow on iteration 46099
g0115: Grad overflow on iteration 46099
g0113: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46099
g0124: Grad overflow on iteration 46099
g0115: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46099
g0115: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46099
g0120: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46099
g0119: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46099
g0119: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:49,648] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46099
g0119: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46099
g0119: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46099
g0115: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46099
g0108: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:49,649] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0120: [2024-08-12 11:30:49,649] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:49,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=46100, skipped=116, lr=[0.00019954384124234255, 0.00019954384124234255], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46100 loss: nan iter time (s): 4.229 samples/sec: 30.269
g0124:  iteration    46100/   52000 | consumed samples:      5900800 | consumed tokens:  12084838400 | elapsed time per iteration (ms): 4261.6 | learning rate: 1.995E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 5.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.036 | tokens per gpu per second (tgs): 1922.285 | TFLOPs: 15.47 |
g0108: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46100
g0108: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46100
g0119: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46100
g0113: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46100
g0119: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46100
g0113: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46100
g0123: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46100
g0113: Grad overflow on iteration 46100
g0113: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46100
g0119: Grad overflow on iteration 46100
g0120: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46100
g0123: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46100
g0120: Grad overflow on iteration 46100
g0123: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46100
g0113: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46100
g0115: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46100
g0115: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46100
g0123: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46100
g0120: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46100
g0120: Grad overflow on iteration 46100
g0124: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:53,939] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46100
g0120: Grad overflow on iteration 46100
g0124: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46100
g0123: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46100
g0124: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46100
g0121: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46100
g0123: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46100
g0123: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46100
g0115: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46100
g0115: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46100
g0115: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46100
g0124: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46100
g0108: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:53,940] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:53,941] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0113: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46101
g0113: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46101
g0113: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46101
g0113: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46101
g0119: Grad overflow on iteration 46101
g0121: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46101
g0108: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46101
g0108: Grad overflow on iteration 46101
g0115: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46101
g0124: Grad overflow on iteration 46101
g0113: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46101
g0120: Grad overflow on iteration 46101
g0120: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46101
g0108: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46101
g0121: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46101
g0115: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46101
g0124: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46101
g0120: Grad overflow on iteration 46101
g0124: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46101
g0123: Grad overflow on iteration 46101
g0115: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46101
g0121: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46101
g0121: [2024-08-12 11:30:57,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:30:57,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46101
g0113: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46101
g0124: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46101
g0108: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46101
g0120: Grad overflow on iteration 46101
g0108: [2024-08-12 11:30:57,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46101
g0124: [2024-08-12 11:30:57,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46101
g0124: [2024-08-12 11:30:57,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46101
g0123: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46101
g0119: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:30:57,952] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:30:57,951] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46101
g0108: [2024-08-12 11:30:57,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46102
g0115: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46102
g0115: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46102
g0115: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46102
g0113: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46102
g0113: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46102
g0113: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46102
g0120: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46102
g0119: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46102
g0119: Grad overflow on iteration 46102
g0113: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46102
g0121: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46102
g0115: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46102
g0108: Grad overflow on iteration 46102
g0119: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46102
g0123: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46102
g0108: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46102
g0108: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46102
g0108: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46102
g0108: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46102
g0120: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46102
g0120: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46102
g0120: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46102
g0119: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46102
g0123: Grad overflow on iteration 46102
g0121: Grad overflow on iteration 46102
g0124: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46102
g0123: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46102
g0124: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46102
g0124: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46102
g0124: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46102
g0124: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46102
g0124: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:02,318] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:02,319] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0113: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46103
g0113: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46103
g0113: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46103
g0113: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46103
g0113: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46103
g0123: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46103
g0120: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46103
g0123: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46103
g0123: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46103
g0121: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46103
g0108: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46103
g0120: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46103
g0120: Grad overflow on iteration 46103
g0121: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46103
g0124: Grad overflow on iteration 46103
g0120: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46103
g0121: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46103
g0120: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46103
g0124: Grad overflow on iteration 46103
g0121: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46103
g0124: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46103
g0121: Grad overflow on iteration 46103
g0115: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46103
g0119: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46103
g0115: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46103
g0115: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46103
g0108: Grad overflow on iteration 46103
g0108: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46103
g0108: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46103
g0108: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46103
g0108: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46103
g0119: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46103
g0119: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:06,511] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0121: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:06,511] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46104
g0108: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46104
g0108: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46104
g0113: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46104
g0113: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46104
g0113: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46104
g0113: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46104
g0119: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46104
g0120: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46104
g0120: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46104
g0120: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46104
g0119: Grad overflow on iteration 46104
g0119: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46104
g0119: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46104
g0123: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46104
g0123: Grad overflow on iteration 46104
g0115: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46104
g0119: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46104
g0115: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46104
g0124: Grad overflow on iteration 46104
g0108: Grad overflow on iteration 46104
g0124: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46104
g0108: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46104
g0108: Grad overflow on iteration 46104
g0123: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46104
g0121: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46104
g0124: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46104
g0124: Grad overflow on iteration 46104
g0124: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46104
g0121: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46104
g0115: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46104
g0108: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46104
g0108: [2024-08-12 11:31:10,825] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0123: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46105
g0108: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46105
g0121: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46105
g0121: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46105
g0121: Grad overflow on iteration 46105
g0115: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46105
g0115: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46105
g0115: Grad overflow on iteration 46105
g0119: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46105
g0115: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46105
g0113: Grad overflow on iteration 46105
g0119: Grad overflow on iteration 46105
g0120: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46105
g0120: Grad overflow on iteration 46105
g0119: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46105
g0123: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46105
g0115: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46105
g0123: Grad overflow on iteration 46105
g0121: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46105
g0123: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46105
g0113: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46105
g0124: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46105
g0124: Grad overflow on iteration 46105
g0108: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46105
g0120: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46105
g0119: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46105
g0113: Grad overflow on iteration 46105
g0119: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46105
g0123: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46105
g0120: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46105
g0108: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46105
g0113: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:15,114] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:15,114] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:31:15,115] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46105
g0124: [2024-08-12 11:31:15,115] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46106
g0108: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46106
g0108: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46106
g0113: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46106
g0115: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46106
g0115: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46106
g0115: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46106
g0124: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46106
g0113: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46106
g0113: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46106
g0115: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46106
g0113: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46106
g0115: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46106
g0123: Grad overflow on iteration 46106
g0121: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46106
g0123: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46106
g0124: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46106
g0123: Grad overflow on iteration 46106
g0121: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46106
g0124: Grad overflow on iteration 46106
g0119: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46106
g0124: Grad overflow on iteration 46106
g0120: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46106
g0123: Grad overflow on iteration 46106
g0119: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46106
g0115: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46106
g0119: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46106
g0123: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46106
g0123: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46106
g0120: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46106
g0108: Grad overflow on iteration 46106
g0108: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46106
g0120: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:19,459] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:19,460] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:31:19,460] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:23,614] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46107
g0108: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46107
g0108: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:23,614] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46107
g0119: [2024-08-12 11:31:23,614] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46107
g0121: Grad overflow on iteration 46107
g0113: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:23,614] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46107
g0113: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46107
g0113: Grad overflow on iteration 46107
g0113: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46107
g0113: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:23,614] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46107
g0124: Grad overflow on iteration 46107
g0124: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46107
g0123: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46107
g0115: Grad overflow on iteration 46107
g0124: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46107
g0124: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46107
g0119: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46107
g0123: Grad overflow on iteration 46107
g0124: Grad overflow on iteration 46107
g0120: [2024-08-12 11:31:23,614] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46107
g0113: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46107
g0119: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46107
g0121: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46107
g0115: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46107
g0123: Grad overflow on iteration 46107
g0124: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46107
g0120: Grad overflow on iteration 46107
g0124: Grad overflow on iteration 46107
g0123: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46107
g0115: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46107
g0108: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46107
g0108: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46107
g0108: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:23,615] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0113: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:23,615] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46108
g0124: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46108
g0120: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46108
g0123: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46108
g0123: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46108
g0120: Grad overflow on iteration 46108
g0119: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46108
g0120: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46108
g0121: Grad overflow on iteration 46108
g0113: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46108
g0108: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46108
g0115: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46108
g0115: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46108
g0120: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46108
g0120: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46108
g0115: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46108
g0121: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46108
g0113: Grad overflow on iteration 46108
g0123: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46108
g0124: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46108
g0124: Grad overflow on iteration 46108
g0108: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46108
g0119: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46108
g0108: Grad overflow on iteration 46108
g0119: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:28,004] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46108
g0123: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46108
g0113: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46108
g0119: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46108
g0119: Grad overflow on iteration 46108
g0124: Grad overflow on iteration 46108
g0119: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46108
g0120: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:28,005] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46108
g0108: [2024-08-12 11:31:28,005] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46109
g0119: Grad overflow on iteration 46109
g0119: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46109
g0119: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46109
g0119: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46109
g0121: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46109
g0115: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46109
g0119: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46109
g0108: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46109
g0119: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46109
g0113: Grad overflow on iteration 46109
g0108: Grad overflow on iteration 46109
g0113: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46109
g0113: Grad overflow on iteration 46109
g0124: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46109
g0108: Grad overflow on iteration 46109
g0113: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46109
g0113: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46109
g0115: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46109
g0108: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46109
g0121: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46109
g0120: Grad overflow on iteration 46109
g0120: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46109
g0115: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46109
g0121: Grad overflow on iteration 46109
g0123: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46109
g0121: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46109
g0123: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46109
g0121: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46109
g0120: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46109
g0120: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46109
g0108: Grad overflow on iteration 46109
g0120: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:32,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:32,454] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:32,454] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:31:32,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=46110, skipped=126, lr=[0.00019954384124234255, 0.00019954384124234255], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46110 loss: nan iter time (s): 4.247 samples/sec: 30.137
g0124:  iteration    46110/   52000 | consumed samples:      5902080 | consumed tokens:  12087459840 | elapsed time per iteration (ms): 4280.8 | learning rate: 1.995E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 5.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.901 | tokens per gpu per second (tgs): 1913.654 | TFLOPs: 15.40 |
g0108: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46110
g0124: Grad overflow on iteration 46110
g0119: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46110
g0123: Grad overflow on iteration 46110
g0120: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46110
g0113: Grad overflow on iteration 46110
g0113: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46110
g0121: Grad overflow on iteration 46110
g0113: Grad overflow on iteration 46110
g0113: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46110
g0119: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46110
g0119: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46110
g0124: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46110
g0108: Grad overflow on iteration 46110
g0124: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46110
g0120: Grad overflow on iteration 46110
g0119: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46110
g0119: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46110
g0115: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46110
g0115: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46110
g0123: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46110
g0121: Grad overflow on iteration 46110
g0115: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46110
g0124: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46110
g0113: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46110
g0113: Grad overflow on iteration 46110
g0124: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46110
g0113: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:36,687] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46110
g0123: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46110
g0113: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46110
g0121: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46110
g0120: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46110
g0121: [2024-08-12 11:31:36,688] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:36,688] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:31:36,689] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46111
g0113: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46111
g0113: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46111
g0113: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46111
g0124: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46111
g0113: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46111
g0113: Grad overflow on iteration 46111
g0124: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46111
g0124: Grad overflow on iteration 46111
g0115: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46111
g0121: Grad overflow on iteration 46111
g0121: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46111
g0121: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46111
g0121: Grad overflow on iteration 46111
g0115: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46111
g0121: Grad overflow on iteration 46111
g0121: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46111
g0119: Grad overflow on iteration 46111
g0113: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46111
g0123: Grad overflow on iteration 46111
g0124: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46111
g0120: Grad overflow on iteration 46111
g0121: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46111
g0119: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46111
g0119: Grad overflow on iteration 46111
g0108: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46111
g0124: Grad overflow on iteration 46111
g0120: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46111
g0120: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:40,961] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46111
g0120: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46111
g0123: Grad overflow on iteration 46111
g0123: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46111
g0108: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:40,962] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:31:40,962] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:45,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46112
g0119: [2024-08-12 11:31:45,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46112
g0113: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46112
g0113: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46112
g0108: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46112
g0121: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:45,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46112
g0115: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46112
g0121: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46112
g0113: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46112
g0123: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46112
g0108: Grad overflow on iteration 46112
g0115: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46112
g0119: Grad overflow on iteration 46112
g0113: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:45,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46112
g0120: Grad overflow on iteration 46112
g0120: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46112
g0120: Grad overflow on iteration 46112
g0121: Grad overflow on iteration 46112
g0115: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46112
g0115: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46112
g0124: Grad overflow on iteration 46112
g0115: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46112
g0124: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46112
g0123: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46112
g0115: Grad overflow on iteration 46112
g0124: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:45,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46112
g0120: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46112
g0119: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46112
g0120: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46112
g0108: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46112
g0121: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46112
g0121: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46112
g0121: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:45,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:45,332] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46113
g0108: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46113
g0108: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46113
g0113: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46113
g0113: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46113
g0113: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46113
g0123: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46113
g0123: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46113
g0123: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46113
g0115: Grad overflow on iteration 46113
g0120: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46113
g0124: Grad overflow on iteration 46113
g0119: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46113
g0123: Grad overflow on iteration 46113
g0124: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46113
g0119: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46113
g0121: Grad overflow on iteration 46113
g0124: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46113
g0119: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46113
g0121: Grad overflow on iteration 46113
g0121: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46113
g0121: Grad overflow on iteration 46113
g0119: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46113
g0120: Grad overflow on iteration 46113
g0115: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46113
g0120: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46113
g0121: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46113
g0120: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46113
g0124: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46113
g0115: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46113
g0113: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46113
g0119: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46113
g0108: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:49,576] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:31:49,576] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46114
g0119: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46114
g0119: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46114
g0119: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46114
g0108: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46114
g0115: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46114
g0108: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46114
g0113: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46114
g0121: Grad overflow on iteration 46114
g0120: Grad overflow on iteration 46114
g0121: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46114
g0123: Grad overflow on iteration 46114
g0123: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46114
g0123: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46114
g0123: Grad overflow on iteration 46114
g0124: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46114
g0124: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46114
g0124: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46114
g0123: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46114
g0121: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46114
g0124: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46114
g0123: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46114
g0113: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46114
g0123: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46114
g0113: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46114
g0113: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46114
g0120: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46114
g0120: Grad overflow on iteration 46114
g0124: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46114
g0113: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46114
g0115: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46114
g0121: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46114
g0108: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:53,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:31:53,992] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0121: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46116
g0108: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46116
g0119: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46116
g0115: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46116
g0120: Grad overflow on iteration 46116
g0113: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46116
g0121: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46116
g0121: Grad overflow on iteration 46116
g0119: Grad overflow on iteration 46116
g0113: Grad overflow on iteration 46116
g0121: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46116
g0120: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46116
g0120: Grad overflow on iteration 46116
g0113: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46116
g0108: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46116
g0123: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46116
g0121: Grad overflow on iteration 46116
g0108: Grad overflow on iteration 46116
g0121: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46116
g0119: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46116
g0120: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46116
g0120: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46116
g0115: Grad overflow on iteration 46116
g0113: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46116
g0108: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46116
g0123: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:02,747] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46116
g0115: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46116
g0124: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46116
g0115: Grad overflow on iteration 46116
g0123: Grad overflow on iteration 46116
g0108: [2024-08-12 11:32:02,748] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46116
g0124: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:02,748] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46116
g0124: [2024-08-12 11:32:02,749] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46117
g0108: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46117
g0108: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46117
g0108: Grad overflow on iteration 46117
g0121: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46117
g0121: Grad overflow on iteration 46117
g0115: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46117
g0121: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46117
g0123: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46117
g0123: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46117
g0121: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46117
g0121: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46117
g0120: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46117
g0121: Grad overflow on iteration 46117
g0120: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46117
g0124: Grad overflow on iteration 46117
g0115: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46117
g0113: Grad overflow on iteration 46117
g0119: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46117
g0124: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46117
g0124: Grad overflow on iteration 46117
g0119: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46117
g0124: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46117
g0124: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46117
g0120: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46117
g0120: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46117
g0108: Grad overflow on iteration 46117
g0123: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46117
g0108: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46117
g0123: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46117
g0123: Grad overflow on iteration 46117
g0108: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46117
g0108: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:06,755] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:06,755] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:06,755] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0123: [2024-08-12 11:32:06,754] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:06,755] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46118
g0113: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46118
g0113: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46118
g0113: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46118
g0115: Grad overflow on iteration 46118
g0108: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46118
g0113: Grad overflow on iteration 46118
g0121: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46118
g0120: Grad overflow on iteration 46118
g0115: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46118
g0115: Grad overflow on iteration 46118
g0123: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46118
g0108: Grad overflow on iteration 46118
g0121: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46118
g0108: Grad overflow on iteration 46118
g0119: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46118
g0121: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46118
g0124: Grad overflow on iteration 46118
g0119: Grad overflow on iteration 46118
g0115: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46118
g0123: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46118
g0115: Grad overflow on iteration 46118
g0124: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46118
g0120: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46118
g0115: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46118
g0108: Grad overflow on iteration 46118
g0120: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46118
g0108: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46118
g0123: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46118
g0108: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46118
g0123: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46118
g0119: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46118
g0119: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:11,116] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0119: [2024-08-12 11:32:11,116] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46119
g0108: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46119
g0123: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46119
g0108: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46119
g0121: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46119
g0121: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46119
g0121: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46119
g0108: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46119
g0123: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46119
g0119: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46119
g0113: Grad overflow on iteration 46119
g0119: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46119
g0115: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46119
g0123: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46119
g0113: Grad overflow on iteration 46119
g0108: Grad overflow on iteration 46119
g0113: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46119
g0115: Grad overflow on iteration 46119
g0113: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46119
g0123: Grad overflow on iteration 46119
g0115: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46119
g0120: Grad overflow on iteration 46119
g0119: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:15,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46119
g0121: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46119
g0120: Grad overflow on iteration 46119
g0119: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46119
g0120: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46119
g0113: [2024-08-12 11:32:15,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46119
g0120: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46119
g0124: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46119
g0120: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:15,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:15,325] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46119
g0108: [2024-08-12 11:32:15,326] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:15,326] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:32:15,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=46120, skipped=135, lr=[0.00019954284044957564, 0.00019954284044957564], mom=[(0.9, 0.95), (0.9, 0.95)]
g0124: [2024-08-12 11:32:15,328] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46119
g0124: [2024-08-12 11:32:15,330] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: steps: 46120 loss: nan iter time (s): 4.254 samples/sec: 30.086
g0124:  iteration    46120/   52000 | consumed samples:      5903360 | consumed tokens:  12090081280 | elapsed time per iteration (ms): 4287.6 | learning rate: 1.995E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 4.940 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.854 | tokens per gpu per second (tgs): 1910.638 | TFLOPs: 15.38 |
g0113: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46120
g0113: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46120
g0108: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46120
g0121: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46120
g0121: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46120
g0120: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46120
g0123: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46120
g0108: Grad overflow on iteration 46120
g0120: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46120
g0115: Grad overflow on iteration 46120
g0108: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46120
g0120: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46120
g0120: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46120
g0119: Grad overflow on iteration 46120
g0119: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46120
g0119: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46120
g0123: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46120
g0121: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46120
g0121: Grad overflow on iteration 46120
g0123: Grad overflow on iteration 46120
g0113: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46120
g0120: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46120
g0123: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46120
g0119: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46120
g0113: Grad overflow on iteration 46120
g0119: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46120
g0115: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46120
g0115: Grad overflow on iteration 46120
g0113: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46120
g0115: Grad overflow on iteration 46120
g0115: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46120
g0124: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46120
g0123: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:19,613] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:19,614] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:19,614] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:19,614] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0120: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46121
g0120: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46121
g0115: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46121
g0113: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46121
g0119: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46121
g0115: Grad overflow on iteration 46121
g0120: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46121
g0113: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46121
g0119: Grad overflow on iteration 46121
g0113: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46121
g0120: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46121
g0120: Grad overflow on iteration 46121
g0119: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46121
g0124: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46121
g0119: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46121
g0115: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46121
g0123: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46121
g0115: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46121
g0120: Grad overflow on iteration 46121
g0124: Grad overflow on iteration 46121
g0124: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46121
g0124: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46121
g0119: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46121
g0113: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46121
g0124: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46121
g0124: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46121
g0108: Grad overflow on iteration 46121
g0121: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46121
g0113: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46121
g0121: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46121
g0119: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46121
g0119: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46121
g0108: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:23,791] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0120: [2024-08-12 11:32:23,791] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:23,792] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46122
g0124: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46122
g0108: Grad overflow on iteration 46122
g0121: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46122
g0123: Grad overflow on iteration 46122
g0123: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46122
g0121: Grad overflow on iteration 46122
g0123: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46122
g0123: Grad overflow on iteration 46122
g0121: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46122
g0124: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46122
g0113: Grad overflow on iteration 46122
g0113: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46122
g0113: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46122
g0113: Grad overflow on iteration 46122
g0115: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46122
g0123: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46122
g0119: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46122
g0120: Grad overflow on iteration 46122
g0119: Grad overflow on iteration 46122
g0115: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46122
g0124: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46122
g0123: Grad overflow on iteration 46122
g0124: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46122
g0123: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46122
g0115: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46122
g0121: Grad overflow on iteration 46122
g0121: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46122
g0121: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46122
g0108: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46122
g0120: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46122
g0119: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46122
g0120: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:28,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:28,050] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0119: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46123
g0115: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46123
g0123: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46123
g0121: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46123
g0119: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46123
g0121: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46123
g0121: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46123
g0121: Grad overflow on iteration 46123
g0115: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46123
g0108: Grad overflow on iteration 46123
g0123: Grad overflow on iteration 46123
g0119: Grad overflow on iteration 46123
g0123: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46123
g0124: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46123
g0113: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46123
g0123: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46123
g0108: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46123
g0123: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46123
g0120: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46123
g0113: Grad overflow on iteration 46123
g0113: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46123
g0113: Grad overflow on iteration 46123
g0123: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46123
g0119: Grad overflow on iteration 46123
g0124: Grad overflow on iteration 46123
g0121: [2024-08-12 11:32:32,183] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46123
g0124: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46123
g0108: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46123
g0113: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46123
g0113: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46123
g0120: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46123
g0108: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:32,183] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46123
g0124: [2024-08-12 11:32:32,182] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:32,183] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:32,183] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46124
g0108: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46124
g0108: Grad overflow on iteration 46124
g0120: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46124
g0120: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46124
g0123: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46124
g0123: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46124
g0121: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46124
g0119: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46124
g0120: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46124
g0120: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46124
g0120: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46124
g0115: Grad overflow on iteration 46124
g0123: Grad overflow on iteration 46124
g0121: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46124
g0123: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46124
g0121: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46124
g0115: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46124
g0120: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46124
g0115: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46124
g0121: Grad overflow on iteration 46124
g0123: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46124
g0119: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46124
g0113: Grad overflow on iteration 46124
g0113: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46124
g0119: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46124
g0119: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46124
g0124: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46124
g0124: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46124
g0124: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46124
g0124: [2024-08-12 11:32:37,057] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46124
g0108: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46124
g0108: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:37,056] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:37,057] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:32:37,057] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46125
g0108: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46125
g0108: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46125
g0108: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46125
g0113: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46125
g0120: Grad overflow on iteration 46125
g0121: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46125
g0121: Grad overflow on iteration 46125
g0120: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46125
g0115: Grad overflow on iteration 46125
g0115: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46125
g0108: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46125
g0119: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46125
g0120: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46125
g0113: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46125
g0124: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46125
g0124: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46125
g0121: Grad overflow on iteration 46125
g0119: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46125
g0120: Grad overflow on iteration 46125
g0115: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46125
g0121: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46125
g0113: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46125
g0121: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46125
g0120: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46125
g0115: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46125
g0123: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46125
g0123: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46125
g0124: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:41,297] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46125
g0108: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46125
g0123: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:41,297] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:41,296] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46125
g0108: Grad overflow on iteration 46125
g0108: [2024-08-12 11:32:41,297] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:32:41,297] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:41,297] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46126
g0108: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46126
g0108: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:45,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46126
g0119: Grad overflow on iteration 46126
g0119: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46126
g0119: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46126
g0115: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46126
g0115: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46126
g0119: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46126
g0120: [2024-08-12 11:32:45,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:45,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46126
g0115: Grad overflow on iteration 46126
g0121: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46126
g0119: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46126
g0119: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:45,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46126
g0121: Grad overflow on iteration 46126
g0120: Grad overflow on iteration 46126
g0123: [2024-08-12 11:32:45,431] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46126
g0113: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46126
g0115: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46126
g0120: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46126
g0120: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46126
g0121: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46126
g0113: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46126
g0121: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46126
g0113: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46126
g0113: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46126
g0124: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46126
g0123: Grad overflow on iteration 46126
g0113: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46126
g0123: Grad overflow on iteration 46126
g0124: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46126
g0123: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46126
g0108: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:45,432] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0121: [2024-08-12 11:32:45,432] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:45,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46127
g0113: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46127
g0119: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46127
g0119: Grad overflow on iteration 46127
g0113: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46127
g0113: Grad overflow on iteration 46127
g0115: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46127
g0123: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46127
g0121: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46127
g0120: Grad overflow on iteration 46127
g0124: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46127
g0123: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46127
g0108: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46127
g0124: Grad overflow on iteration 46127
g0119: Grad overflow on iteration 46127
g0119: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46127
g0115: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46127
g0119: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46127
g0123: Grad overflow on iteration 46127
g0108: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46127
g0115: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46127
g0124: Grad overflow on iteration 46127
g0123: Grad overflow on iteration 46127
g0121: Grad overflow on iteration 46127
g0123: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46127
g0124: Grad overflow on iteration 46127
g0120: Grad overflow on iteration 46127
g0124: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46127
g0120: Grad overflow on iteration 46127
g0108: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46127
g0120: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46127
g0120: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:49,603] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46127
g0108: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:49,604] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0121: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:49,604] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46128
g0115: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46128
g0115: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46128
g0115: Grad overflow on iteration 46128
g0120: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46128
g0115: Grad overflow on iteration 46128
g0119: Grad overflow on iteration 46128
g0115: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46128
g0113: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46128
g0120: Grad overflow on iteration 46128
g0119: Grad overflow on iteration 46128
g0120: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46128
g0119: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46128
g0121: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46128
g0120: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46128
g0123: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46128
g0113: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46128
g0120: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46128
g0119: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46128
g0123: Grad overflow on iteration 46128
g0119: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46128
g0108: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46128
g0121: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46128
g0108: Grad overflow on iteration 46128
g0124: Grad overflow on iteration 46128
g0124: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46128
g0123: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46128
g0123: Grad overflow on iteration 46128
g0113: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46128
g0123: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46128
g0108: Grad overflow on iteration 46128
g0108: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46128
g0108: [2024-08-12 11:32:53,713] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:53,714] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:53,714] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:53,714] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:32:53,714] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46129
g0121: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46129
g0119: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46129
g0120: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46129
g0108: Grad overflow on iteration 46129
g0113: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46129
g0119: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46129
g0121: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46129
g0121: Grad overflow on iteration 46129
g0119: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46129
g0119: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46129
g0120: Grad overflow on iteration 46129
g0113: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46129
g0115: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46129
g0113: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46129
g0119: Grad overflow on iteration 46129
g0119: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46129
g0123: Grad overflow on iteration 46129
g0119: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46129
g0124: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46129
g0124: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46129
g0123: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46129
g0115: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46129
g0124: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46129
g0123: Grad overflow on iteration 46129
g0115: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46129
g0123: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46129
g0108: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46129
g0120: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46129
g0108: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46129
g0108: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46129
g0123: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46129
g0120: [2024-08-12 11:32:57,916] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:32:57,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:32:57,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:57,917] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:32:57,917] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 11:32:57,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=46130, skipped=145, lr=[0.00019954284044957564, 0.00019954284044957564], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 46130 loss: nan iter time (s): 4.225 samples/sec: 30.294
g0124:  iteration    46130/   52000 | consumed samples:      5904640 | consumed tokens:  12092702720 | elapsed time per iteration (ms): 4258.4 | learning rate: 1.995E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 4.940 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.058 | tokens per gpu per second (tgs): 1923.723 | TFLOPs: 15.48 |
g0119: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46130
g0119: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46130
g0119: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46130
g0119: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46130
g0113: Grad overflow on iteration 46130
g0113: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46130
g0113: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46130
g0113: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46130
g0108: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46130
g0124: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46130
g0124: Grad overflow on iteration 46130
g0120: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46130
g0108: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46130
g0121: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46130
g0120: Grad overflow on iteration 46130
g0115: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46130
g0108: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46130
g0120: Grad overflow on iteration 46130
g0119: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46130
g0120: Grad overflow on iteration 46130
g0124: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46130
g0123: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46130
g0108: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46130
g0123: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46130
g0108: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46130
g0115: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46130
g0115: Grad overflow on iteration 46130
g0121: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46130
g0121: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46130
g0123: Grad overflow on iteration 46130
g0115: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46130
g0119: Grad overflow on iteration 46130
g0123: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:02,241] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:02,242] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:33:02,242] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:33:02,242] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46131
g0119: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46131
g0119: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46131
g0119: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46131
g0124: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46131
g0121: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46131
g0113: Grad overflow on iteration 46131
g0115: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46131
g0113: Grad overflow on iteration 46131
g0120: Grad overflow on iteration 46131
g0120: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46131
g0119: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46131
g0121: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46131
g0120: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46131
g0108: Grad overflow on iteration 46131
g0113: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46131
g0120: Grad overflow on iteration 46131
g0124: Grad overflow on iteration 46131
g0120: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46131
g0120: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46131
g0115: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46131
g0120: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46131
g0124: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46131
g0124: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46131
g0115: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46131
g0121: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46131
g0121: Grad overflow on iteration 46131
g0121: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46131
g0123: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46131
g0108: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46131
g0123: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46131
g0123: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:06,433] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46131
g0108: [2024-08-12 11:33:06,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:06,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:06,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:06,434] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:06,434] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0115: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46132
g0113: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46132
g0113: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46132
g0113: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46132
g0108: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46132
g0113: Grad overflow on iteration 46132
g0115: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46132
g0113: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46132
g0119: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46132
g0113: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: Grad overflow on iteration 46132
g0108: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46132
g0121: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46132
g0123: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46132
g0119: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46132
g0108: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: Grad overflow on iteration 46132
g0119: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46132
g0115: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46132
g0120: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46132
g0124: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46132
g0123: Grad overflow on iteration 46132
g0115: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46132
g0123: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46132
g0115: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46132
g0123: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46132
g0120: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46132
g0121: Grad overflow on iteration 46132
g0120: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46132
g0124: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46132
g0119: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46132
g0124: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46132
g0119: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46132
g0124: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:10,705] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 46132
g0108: [2024-08-12 11:33:10,706] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0120: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:10,706] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46133
g0119: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46133
g0119: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46133
g0124: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46133
g0113: Grad overflow on iteration 46133
g0119: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46133
g0115: Grad overflow on iteration 46133
g0121: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46133
g0115: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46133
g0119: Grad overflow on iteration 46133
g0119: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46133
g0115: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46133
g0115: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46133
g0115: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46133
g0121: Grad overflow on iteration 46133
g0123: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46133
g0113: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46133
g0120: Grad overflow on iteration 46133
g0120: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0119: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46133
g0123: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46133
g0120: Grad overflow on iteration 46133
g0120: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46133
g0121: Grad overflow on iteration 46133
g0108: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46133
g0124: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46133
g0120: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46133
g0108: Grad overflow on iteration 46133
g0124: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46133
g0108: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46133
g0113: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46133
g0108: [2024-08-12 11:33:14,778] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46133
g0124: Grad overflow on iteration 46133
g0108: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:14,779] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:14,779] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0115: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46134
g0115: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46134
g0115: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46134
g0115: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46134
g0119: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46134
g0119: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: Grad overflow on iteration 46134
g0119: Grad overflow on iteration 46134
g0113: Grad overflow on iteration 46134
g0113: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46134
g0113: Grad overflow on iteration 46134
g0113: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 46134
g0113: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46134
g0113: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0115: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 46134
g0119: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 46134
g0113: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0115: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 46134
g0108: Grad overflow on iteration 46134
g0113: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: Grad overflow on iteration 46134
g0120: Grad overflow on iteration 46134
g0120: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 46134
g0123: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: Grad overflow on iteration 46134
g0121: Grad overflow on iteration 46134
g0123: Grad overflow on iteration 46134
g0119: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46134
g0124: Grad overflow on iteration 46134
g0123: Grad overflow on iteration 46134
g0120: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46134
g0108: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0119: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 46134
g0120: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46134
g0123: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0124: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 46134
g0108: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 46134
g0124: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 46134
g0124: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 11:33:18,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: Grad overflow on iteration 46134
g0121: [2024-08-12 11:33:18,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 11:33:18,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 11:33:18,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0124: [2024-08-12 11:33:18,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 11:33:18,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0124: [2024-08-12 11:33:18,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
