
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0069
    HostName g0069
    Port 2222
    StrictHostKeyChecking no

Host g0086
    HostName g0086
    Port 2222
    StrictHostKeyChecking no

Host g0087
    HostName g0087
    Port 2222
    StrictHostKeyChecking no

Host g0088
    HostName g0088
    Port 2222
    StrictHostKeyChecking no

Host g0090
    HostName g0090
    Port 2222
    StrictHostKeyChecking no

Host g0091
    HostName g0091
    Port 2222
    StrictHostKeyChecking no

Host g0092
    HostName g0092
    Port 2222
    StrictHostKeyChecking no

Host g0093
    HostName g0093
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42771611
g0069 slots=4
g0086 slots=4
g0087 slots=4
g0088 slots=4
g0090 slots=4
g0091 slots=4
g0092 slots=4
g0093 slots=4

[2024-08-02 18:10:45,237] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 18:10:51,076] [INFO] [runner.py:463:main] Using IP address of 10.1.3.1 for node g0069
[2024-08-02 18:10:51,078] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0069,g0086,g0087,g0088,g0090,g0091,g0092,g0093
[2024-08-02 18:10:51,078] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0069,g0086,g0087,g0088,g0090,g0091,g0092,g0093 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDA2OSI6IFswLCAxLCAyLCAzXSwgImcwMDg2IjogWzAsIDEsIDIsIDNdLCAiZzAwODciOiBbMCwgMSwgMiwgM10sICJnMDA4OCI6IFswLCAxLCAyLCAzXSwgImcwMDkwIjogWzAsIDEsIDIsIDNdLCAiZzAwOTEiOiBbMCwgMSwgMiwgM10sICJnMDA5MiI6IFswLCAxLCAyLCAzXSwgImcwMDkzIjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.3.1 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '2621440000000' --train-samples '1280000000' --train-data-exact-num-epochs '1' --lr '2.0e-4' --min-lr '1.0e-5' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True' --wandb_tag 'other_gpu'
g0069: [2024-08-02 18:10:54,578] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0069: [2024-08-02 18:10:56,817] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0069: [2024-08-02 18:10:56,817] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0069': [0, 1, 2, 3], 'g0086': [0, 1, 2, 3], 'g0087': [0, 1, 2, 3], 'g0088': [0, 1, 2, 3], 'g0090': [0, 1, 2, 3], 'g0091': [0, 1, 2, 3], 'g0092': [0, 1, 2, 3], 'g0093': [0, 1, 2, 3]}
g0069: [2024-08-02 18:10:56,817] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0069: [2024-08-02 18:10:56,817] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0069': [0, 1, 2, 3], 'g0086': [4, 5, 6, 7], 'g0087': [8, 9, 10, 11], 'g0088': [12, 13, 14, 15], 'g0090': [16, 17, 18, 19], 'g0091': [20, 21, 22, 23], 'g0092': [24, 25, 26, 27], 'g0093': [28, 29, 30, 31]})
g0069: [2024-08-02 18:10:56,817] [INFO] [launch.py:163:main] dist_world_size=32
g0069: [2024-08-02 18:10:56,817] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0069: [2024-08-02 18:10:59,946] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0069: [2024-08-02 18:10:59,946] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0069: [2024-08-02 18:10:59,947] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0069: [2024-08-02 18:11:00,215] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0087: [2024-08-02 18:11:00,972] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0086: [2024-08-02 18:11:01,060] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0093: [2024-08-02 18:11:01,648] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0091: [2024-08-02 18:11:01,704] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0088: [2024-08-02 18:11:01,748] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0092: [2024-08-02 18:11:02,115] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0090: [2024-08-02 18:11:02,691] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0069: --------------------------------------------------
g0069: DeepSpeed C++/CUDA extension op report
g0069: --------------------------------------------------
g0069: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0069:       runtime if needed. Op compatibility means that your system
g0069:       meet the required dependencies to JIT install the op.
g0069: --------------------------------------------------
g0069: JIT compiled ops requires ninja
g0069: --------------------------------------------------
g0069: DeepSpeed C++/CUDA extension op report
g0069: --------------------------------------------------
g0069: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0069:       runtime if needed. Op compatibility means that your system
g0069:       meet the required dependencies to JIT install the op.
g0069: --------------------------------------------------
g0069: JIT compiled ops requires ninja
g0069: --------------------------------------------------
g0069: DeepSpeed C++/CUDA extension op report
g0069: --------------------------------------------------
g0069: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0069:       runtime if needed. Op compatibility means that your system
g0069:       meet the required dependencies to JIT install the op.
g0069: --------------------------------------------------
g0069: JIT compiled ops requires ninja
g0069: --------------------------------------------------
g0069: DeepSpeed C++/CUDA extension op report
g0069: --------------------------------------------------
g0069: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0069:       runtime if needed. Op compatibility means that your system
g0069:       meet the required dependencies to JIT install the op.
g0069: --------------------------------------------------
g0069: JIT compiled ops requires ninja
g0069: ninjaninjaninja  .................. .................. .................. [92m[OKAY][0m [92m[OKAY][0m[92m[OKAY][0mninja
g0069: 
g0069: 
g0069:  ----------------------------------------------------------------------------------------------------..................--------------------------------------------------
g0069: 
g0069:  
g0069: op name[92m[OKAY][0mop name op name 
g0069: ................ ................ ................--------------------------------------------------  installed
g0069: installedinstalled  ..op name..    compatible..................compatible
g0069:  
g0069:  compatible----------------------------------------------------------------------------------------------------installed
g0069: 
g0069:  
g0069: ..-------------------------------------------------- 
g0069: compatible
g0069: --------------------------------------------------
g0069: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0069: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0069: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0069: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0069: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0069: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0069: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0069: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0069: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0069: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0069: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0069: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0069: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0069: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0069: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0069: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0069: async_io[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0069:  ...............evoformer_attn  [92m[YES][0m.........  ......[93m[NO][0m  [92m[OKAY][0m.......
g0069:  [93m[NO][0m
g0069: fused_lamb .............fused_adam  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0069:  [92m[OKAY][0m
g0069: cpu_adam ............... [92m[YES][0mfused_lion  ...................  [92m[OKAY][0m[92m[YES][0m
g0069:  ...... cpu_adagrad[92m[OKAY][0m 
g0069: ............ [92m[YES][0m ...... [92m[OKAY][0m
g0069: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0069: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0069: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0069: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0069: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0069: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0069: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0069: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0069: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0069: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: cutlass_opscutlass_ops  ........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0069: 
g0069: quantizerquantizer  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0069: 
g0069: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0069: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0069: ragged_device_ops ...... ragged_device_ops[92m[YES][0m  ............  [92m[OKAY][0m[92m[YES][0m
g0069:  ...... [92m[OKAY][0m
g0069: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0069: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0069: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0069: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0069: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0069: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0069: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0mragged_ops
g0069:  ............. random_ltd[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0069: ...... [92m[OKAY][0m
g0069: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0069: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0069: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0069: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0069: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0069: 
g0069: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0069: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0069: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0069: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0069: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0069: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0069: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0069: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0069: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0069: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0069: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0069: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0069: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0069: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0069: --------------------------------------------------
g0069: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0069: --------------------------------------------------
g0069: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0069: --------------------------------------------------
g0069: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0069: --------------------------------------------------
g0069: DeepSpeed general environment info:
g0069: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0069: torch version .................... 2.0.1+cu118
g0069: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0069: deepspeed info ................... 0.12.4, unknown, unknown
g0069: torch cuda version ............... 11.8
g0069: torch hip version ................ None
g0069: nvcc version ..................... 11.8
g0069: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0069: shared memory (/dev/shm) size .... 188.13 GB
g0069: DeepSpeed general environment info:
g0069: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0069: torch version .................... 2.0.1+cu118
g0069: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0069: deepspeed info ................... 0.12.4, unknown, unknown
g0069: torch cuda version ............... 11.8
g0069: torch hip version ................ None
g0069: nvcc version ..................... 11.8
g0069: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0069: shared memory (/dev/shm) size .... 188.13 GB
g0069: DeepSpeed general environment info:
g0069: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0069: torch version .................... 2.0.1+cu118
g0069: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0069: deepspeed info ................... 0.12.4, unknown, unknown
g0069: torch cuda version ............... 11.8
g0069: torch hip version ................ None
g0069: nvcc version ..................... 11.8
g0069: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0069: shared memory (/dev/shm) size .... 188.13 GB
g0069: DeepSpeed general environment info:
g0069: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0069: torch version .................... 2.0.1+cu118
g0069: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0069: deepspeed info ................... 0.12.4, unknown, unknown
g0069: torch cuda version ............... 11.8
g0069: torch hip version ................ None
g0069: nvcc version ..................... 11.8
g0069: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0069: shared memory (/dev/shm) size .... 188.13 GB
g0069: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0069: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0069: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0069: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0069: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0069: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0069: using torch.float32 for parameters ...
g0069: ------------------------ arguments ------------------------
g0069:   accumulate_allreduce_grads_in_fp32 .............. False
g0069:   adam_beta1 ...................................... 0.9
g0069:   adam_beta2 ...................................... 0.95
g0069:   adam_eps ........................................ 1e-08
g0069:   add_bias_linear ................................. False
g0069:   add_position_embedding .......................... False
g0069:   adlr_autoresume ................................. False
g0069:   adlr_autoresume_interval ........................ 1000
g0069:   aml_data_download_path .......................... None
g0069:   apply_layernorm_1p .............................. False
g0069:   apply_query_key_layer_scaling ................... False
g0069:   apply_residual_connection_post_layernorm ........ False
g0069:   async_tensor_model_parallel_allreduce ........... False
g0069:   attention_dropout ............................... 0.0
g0069:   attention_softmax_in_fp32 ....................... False
g0069:   barrier_with_L1_time ............................ True
g0069:   bert_binary_head ................................ True
g0069:   bert_embedder_type .............................. megatron
g0069:   bert_load ....................................... None
g0069:   bf16 ............................................ False
g0069:   bias_dropout_fusion ............................. True
g0069:   bias_gelu_fusion ................................ False
g0069:   biencoder_projection_dim ........................ 0
g0069:   biencoder_shared_query_context_model ............ False
g0069:   block_data_path ................................. None
g0069:   checkpoint_activations .......................... False
g0069:   checkpoint_in_cpu ............................... False
g0069:   checkpoint_num_layers ........................... 1
g0069:   classes_fraction ................................ 1.0
g0069:   clip_grad ....................................... 1.0
g0069:   compression_training ............................ False
g0069:   consumed_train_samples .......................... 0
g0069:   consumed_train_tokens ........................... 0
g0069:   consumed_valid_samples .......................... 0
g0069:   contigious_checkpointing ........................ False
g0069:   cpu_optimizer ................................... False
g0069:   cpu_torch_adam .................................. False
g0069:   create_moe_param_group .......................... False
g0069:   curriculum_learning_legacy ...................... False
g0069:   data_cache_path ................................. None
g0069:   data_efficiency_curriculum_learning ............. False
g0069:   data_impl ....................................... mmap
g0069:   data_parallel_random_init ....................... False
g0069:   data_parallel_size .............................. 4
g0069:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document']
g0069:   data_per_class_fraction ......................... 1.0
g0069:   data_sharding ................................... True
g0069:   dataloader_type ................................. single
g0069:   DDP_impl ........................................ local
g0069:   decoder_num_layers .............................. None
g0069:   decoder_seq_length .............................. None
g0069:   deepscale ....................................... False
g0069:   deepscale_config ................................ None
g0069:   deepspeed ....................................... True
g0069:   deepspeed_activation_checkpointing .............. False
g0069:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0069:   deepspeed_mpi ................................... False
g0069:   dino_bottleneck_size ............................ 256
g0069:   dino_freeze_last_layer .......................... 1
g0069:   dino_head_hidden_size ........................... 2048
g0069:   dino_local_crops_number ......................... 10
g0069:   dino_local_img_size ............................. 96
g0069:   dino_norm_last_layer ............................ False
g0069:   dino_teacher_temp ............................... 0.07
g0069:   dino_warmup_teacher_temp ........................ 0.04
g0069:   dino_warmup_teacher_temp_epochs ................. 30
g0069:   distribute_checkpointed_activations ............. False
g0069:   distribute_saved_activations .................... False
g0069:   distributed_backend ............................. nccl
g0069:   distributed_timeout_minutes ..................... 10
g0069:   ds_fused_adam ................................... False
g0069:   ds_inference .................................... False
g0069:   ds_pipeline_enabled ............................. True
g0069:   ds_sequence_parallel_size ....................... 1
g0069:   embedding_path .................................. None
g0069:   embedding_weights_in_fp32 ....................... False
g0069:   empty_unused_memory_level ....................... 0
g0069:   enable_expert_tensor_parallelism ................ False
g0069:   encoder_num_layers .............................. 22
g0069:   encoder_seq_length .............................. 2048
g0069:   end_weight_decay ................................ 0.1
g0069:   eod_mask_loss ................................... False
g0069:   eval_interval ................................... 1000
g0069:   eval_iters ...................................... 100
g0069:   evidence_data_path .............................. None
g0069:   exit_duration_in_mins ........................... 30000000
g0069:   exit_interval ................................... None
g0069:   exit_on_missing_checkpoint ...................... False
g0069:   exit_signal_handler ............................. False
g0069:   expert_interval ................................. 2
g0069:   ffn_hidden_size ................................. 5632
g0069:   finetune ........................................ False
g0069:   force_ds_sequence_parallel ...................... False
g0069:   fp16 ............................................ False
g0069:   fp16_lm_cross_entropy ........................... False
g0069:   fp32_residual_connection ........................ False
g0069:   fp8_amax_compute_algo ........................... most_recent
g0069:   fp8_amax_history_len ............................ 1
g0069:   fp8_e4m3 ........................................ False
g0069:   fp8_hybrid ...................................... False
g0069:   fp8_interval .................................... 1
g0069:   fp8_margin ...................................... 0
g0069:   fp8_wgrad ....................................... True
g0069:   global_batch_size ............................... 128
g0069:   gradient_accumulation_fusion .................... True
g0069:   head_lr_mult .................................... 1.0
g0069:   hidden_dropout .................................. 0.0
g0069:   hidden_size ..................................... 2048
g0069:   hidden_size_teacher ............................. None
g0069:   hysteresis ...................................... 2
g0069:   ict_head_size ................................... None
g0069:   ict_load ........................................ None
g0069:   img_h ........................................... 224
g0069:   img_w ........................................... 224
g0069:   indexer_batch_size .............................. 128
g0069:   indexer_log_interval ............................ 1000
g0069:   inference ....................................... False
g0069:   inference_batch_times_seqlen_threshold .......... 512
g0069:   init_method_std ................................. 0.013
g0069:   init_method_xavier_uniform ...................... False
g0069:   initial_loss_scale .............................. 4294967296
g0069:   iter_per_epoch .................................. 1250
g0069:   kd .............................................. False
g0069:   kd_alpha_ce ..................................... 1
g0069:   kd_beta_ce ...................................... 1
g0069:   kd_temp ......................................... 1.0
g0069:   kv_channels ..................................... 128
g0069:   layernorm_epsilon ............................... 1e-05
g0069:   lazy_mpu_init ................................... None
g0069:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069:   load_teacher .................................... None
g0069:   local_rank ...................................... 0
g0069:   log_batch_size_to_tensorboard ................... True
g0069:   log_interval .................................... 10
g0069:   log_learning_rate_to_tensorboard ................ True
g0069:   log_loss_scale_to_tensorboard ................... True
g0069:   log_memory_to_tensorboard ....................... False
g0069:   log_num_zeros_in_grad ........................... False
g0069:   log_optimizer_states_to_tensorboard ............. True
g0069:   log_params_norm ................................. False
g0069:   log_timers_to_tensorboard ....................... True
g0069:   log_validation_ppl_to_tensorboard ............... True
g0069:   log_world_size_to_tensorboard ................... False
g0069:   loss_scale ...................................... None
g0069:   loss_scale_window ............................... 1000
g0069:   lr .............................................. 0.0002
g0069:   lr_decay_iters .................................. None
g0069:   lr_decay_samples ................................ None
g0069:   lr_decay_style .................................. cosine
g0069:   lr_decay_tokens ................................. 300000000000
g0069:   lr_warmup_fraction .............................. None
g0069:   lr_warmup_iters ................................. 0
g0069:   lr_warmup_samples ............................... 0
g0069:   lr_warmup_tokens ................................ 3000000000
g0069:   make_vocab_size_divisible_by .................... 128
g0069:   mask_factor ..................................... 1.0
g0069:   mask_prob ....................................... 0.15
g0069:   mask_type ....................................... random
g0069:   masked_softmax_fusion ........................... True
g0069:   max_position_embeddings ......................... 2048
g0069:   max_tokens_to_oom ............................... 12000
g0069:   mem_efficient_ln ................................ True
g0069:   memory_centric_tiled_linear ..................... False
g0069:   merge_file ...................................... None
g0069:   micro_batch_size ................................ 1
g0069:   min_loss_scale .................................. 1.0
g0069:   min_lr .......................................... 1e-05
g0069:   mlp_type ........................................ standard
g0069:   mmap_warmup ..................................... False
g0069:   moe_eval_capacity_factor ........................ 1.0
g0069:   moe_expert_parallel_size ........................ 1
g0069:   moe_loss_coeff .................................. 0.1
g0069:   moe_min_capacity ................................ 4
g0069:   moe_token_dropping .............................. True
g0069:   moe_train_capacity_factor ....................... 1.0
g0069:   mos ............................................. False
g0069:   no_load_lr_state ................................ False
g0069:   no_load_optim ................................... None
g0069:   no_load_rng ..................................... None
g0069:   no_persist_layer_norm ........................... False
g0069:   no_pipeline_parallel ............................ False
g0069:   no_save_optim ................................... None
g0069:   no_save_rng ..................................... None
g0069:   normalization ................................... rmsnorm
g0069:   num_attention_heads ............................. 16
g0069:   num_attention_heads_teacher ..................... None
g0069:   num_channels .................................... 3
g0069:   num_classes ..................................... 1000
g0069:   num_experts ..................................... [1]
g0069:   num_experts_switch .............................. None
g0069:   num_experts_teacher ............................. [1]
g0069:   num_key_value_heads ............................. 4
g0069:   num_layers ...................................... 22
g0069:   num_layers_per_virtual_pipeline_stage ........... None
g0069:   num_layers_teacher .............................. None
g0069:   num_workers ..................................... 0
g0069:   onnx_safe ....................................... None
g0069:   openai_gelu ..................................... False
g0069:   optimizer ....................................... adam
g0069:   output_bert_embeddings .......................... False
g0069:   overlap_p2p_comm ................................ False
g0069:   override_opt_param_scheduler .................... True
g0069:   params_dtype .................................... torch.float32
g0069:   partition_activations ........................... False
g0069:   patch_dim ....................................... 16
g0069:   perform_initialization .......................... True
g0069:   pipeline_model_parallel_size .................... 8
g0069:   pipeline_model_parallel_split_rank .............. None
g0069:   profile_backward ................................ False
g0069:   query_in_block_prob ............................. 0.1
g0069:   rampup_batch_size ............................... None
g0069:   random_ltd ...................................... False
g0069:   rank ............................................ 0
g0069:   recompute_granularity ........................... None
g0069:   recompute_method ................................ None
g0069:   recompute_num_layers ............................ 1
g0069:   remote_device ................................... none
g0069:   repeated_dataloader ............................. False
g0069:   reset_attention_mask ............................ False
g0069:   reset_iteration ................................. False
g0069:   reset_position_ids .............................. False
g0069:   retriever_report_topk_accuracies ................ []
g0069:   retriever_score_scaling ......................... False
g0069:   retriever_seq_length ............................ 256
g0069:   retro_add_retriever ............................. False
g0069:   retro_cyclic_train_iters ........................ None
g0069:   retro_encoder_attention_dropout ................. 0.1
g0069:   retro_encoder_hidden_dropout .................... 0.1
g0069:   retro_encoder_layers ............................ 2
g0069:   retro_num_neighbors ............................. 2
g0069:   retro_num_retrieved_chunks ...................... 2
g0069:   retro_return_doc_ids ............................ False
g0069:   retro_workdir ................................... None
g0069:   return_data_index ............................... False
g0069:   rotary_percent .................................. 1.0
g0069:   sample_rate ..................................... 1.0
g0069:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069:   save_interval ................................... 1000
g0069:   scatter_gather_tensors_in_pipeline .............. True
g0069:   scattered_embeddings ............................ False
g0069:   seed ............................................ 1234
g0069:   seq_length ...................................... 2048
g0069:   sequence_parallel ............................... False
g0069:   sgd_momentum .................................... 0.9
g0069:   short_seq_prob .................................. 0.1
g0069:   skip_train ...................................... False
g0069:   split ........................................... 949,50,1
g0069:   split_transformers .............................. False
g0069:   squared_relu .................................... False
g0069:   standalone_embedding_stage ...................... False
g0069:   start_weight_decay .............................. 0.1
g0069:   swiglu .......................................... True
g0069:   swin_backbone_type .............................. tiny
g0069:   synchronize_each_layer .......................... False
g0069:   tensor_model_parallel_size ...................... 1
g0069:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True
g0069:   tensorboard_log_interval ........................ 1
g0069:   tensorboard_queue_size .......................... 1
g0069:   test_data_path .................................. None
g0069:   tf32 ............................................ False
g0069:   tile_factor ..................................... 1
g0069:   timing_log_level ................................ 0
g0069:   timing_log_option ............................... minmax
g0069:   titles_data_path ................................ None
g0069:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
g0069:   tokenizer_type .................................. SentencePieceTokenizer
g0069:   topk ............................................ 1
g0069:   train_data_exact_num_epochs ..................... 1
g0069:   train_data_path ................................. None
g0069:   train_desc_path ................................. None
g0069:   train_doc_idx_path .............................. None
g0069:   train_idx_path .................................. None
g0069:   train_iters ..................................... None
g0069:   train_sample_idx_path ........................... None
g0069:   train_samples ................................... 1280000000
g0069:   train_shuffle_idx_path .......................... None
g0069:   train_tokens .................................... 2621440000000
g0069:   transformer_impl ................................ local
g0069:   transformer_pipeline_model_parallel_size ........ 8
g0069:   universal_checkpoint ............................ False
g0069:   untie_embeddings_and_output_weights ............. True
g0069:   use_checkpoint_args ............................. False
g0069:   use_checkpoint_opt_param_scheduler .............. False
g0069:   use_contiguous_buffers_in_local_ddp ............. True
g0069:   use_cpu_initialization .......................... None
g0069:   use_dataset_only ................................ False
g0069:   use_distributed_optimizer ....................... False
g0069:   use_flash_attn .................................. False
g0069:   use_flash_attn_triton ........................... False
g0069:   use_flash_attn_v1 ............................... False
g0069:   use_flash_attn_v2 ............................... False
g0069:   use_one_sent_docs ............................... False
g0069:   use_pin_memory .................................. False
g0069:   use_ring_exchange_p2p ........................... False
g0069:   use_rotary_position_embeddings .................. True
g0069:   use_tutel ....................................... False
g0069:   use_wandb ....................................... True
g0069:   valid_data_path ................................. None
g0069:   variable_seq_lengths ............................ False
g0069:   virtual_pipeline_model_parallel_size ............ None
g0069:   vision_backbone_type ............................ vit
g0069:   vision_pretraining .............................. False
g0069:   vision_pretraining_type ......................... classify
g0069:   vocab_extra_ids ................................. 0
g0069:   vocab_file ...................................... None
g0069:   vocab_size ...................................... None
g0069:   wandb_entity .................................... yohei-kobashi
g0069:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True
g0069:   wandb_project ................................... encrypted_data_LLM
g0069:   wandb_tag ....................................... other_gpu
g0069:   weight_decay .................................... 0.1
g0069:   weight_decay_incr_style ......................... constant
g0069:   world_size ...................................... 32
g0069:   zero_allgather_bucket_size ...................... 0.0
g0069:   zero_contigious_gradients ....................... False
g0069:   zero_reduce_bucket_size ......................... 0.0
g0069:   zero_reduce_scatter ............................. False
g0069:   zero_stage ...................................... 0
g0069: -------------------- end of arguments ---------------------
g0069: setting number of micro-batches to constant 32
g0069: > building SentencePieceTokenizer tokenizer ...
g0069: [2024-08-02 18:11:04,705] [INFO] [comm.py:637:init_distributed] cdb=None
g0069: [2024-08-02 18:11:04,706] [INFO] [comm.py:637:init_distributed] cdb=None
g0069:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0069: > initializing torch distributed ...
g0069: [2024-08-02 18:11:04,711] [INFO] [comm.py:637:init_distributed] cdb=None
g0069: [2024-08-02 18:11:04,711] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0069: [2024-08-02 18:11:04,712] [INFO] [comm.py:637:init_distributed] cdb=None
g0069: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0069: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0069: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0069: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0069: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0069: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0087: [2024-08-02 18:11:04,816] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0087: [2024-08-02 18:11:04,816] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0069': [0, 1, 2, 3], 'g0086': [0, 1, 2, 3], 'g0087': [0, 1, 2, 3], 'g0088': [0, 1, 2, 3], 'g0090': [0, 1, 2, 3], 'g0091': [0, 1, 2, 3], 'g0092': [0, 1, 2, 3], 'g0093': [0, 1, 2, 3]}
g0087: [2024-08-02 18:11:04,816] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0087: [2024-08-02 18:11:04,816] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0069': [0, 1, 2, 3], 'g0086': [4, 5, 6, 7], 'g0087': [8, 9, 10, 11], 'g0088': [12, 13, 14, 15], 'g0090': [16, 17, 18, 19], 'g0091': [20, 21, 22, 23], 'g0092': [24, 25, 26, 27], 'g0093': [28, 29, 30, 31]})
g0087: [2024-08-02 18:11:04,816] [INFO] [launch.py:163:main] dist_world_size=32
g0087: [2024-08-02 18:11:04,816] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0086: [2024-08-02 18:11:05,033] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0086: [2024-08-02 18:11:05,033] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0069': [0, 1, 2, 3], 'g0086': [0, 1, 2, 3], 'g0087': [0, 1, 2, 3], 'g0088': [0, 1, 2, 3], 'g0090': [0, 1, 2, 3], 'g0091': [0, 1, 2, 3], 'g0092': [0, 1, 2, 3], 'g0093': [0, 1, 2, 3]}
g0086: [2024-08-02 18:11:05,033] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0086: [2024-08-02 18:11:05,033] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0069': [0, 1, 2, 3], 'g0086': [4, 5, 6, 7], 'g0087': [8, 9, 10, 11], 'g0088': [12, 13, 14, 15], 'g0090': [16, 17, 18, 19], 'g0091': [20, 21, 22, 23], 'g0092': [24, 25, 26, 27], 'g0093': [28, 29, 30, 31]})
g0086: [2024-08-02 18:11:05,033] [INFO] [launch.py:163:main] dist_world_size=32
g0086: [2024-08-02 18:11:05,033] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0093: [2024-08-02 18:11:05,611] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0093: [2024-08-02 18:11:05,611] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0069': [0, 1, 2, 3], 'g0086': [0, 1, 2, 3], 'g0087': [0, 1, 2, 3], 'g0088': [0, 1, 2, 3], 'g0090': [0, 1, 2, 3], 'g0091': [0, 1, 2, 3], 'g0092': [0, 1, 2, 3], 'g0093': [0, 1, 2, 3]}
g0093: [2024-08-02 18:11:05,611] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0093: [2024-08-02 18:11:05,611] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0069': [0, 1, 2, 3], 'g0086': [4, 5, 6, 7], 'g0087': [8, 9, 10, 11], 'g0088': [12, 13, 14, 15], 'g0090': [16, 17, 18, 19], 'g0091': [20, 21, 22, 23], 'g0092': [24, 25, 26, 27], 'g0093': [28, 29, 30, 31]})
g0093: [2024-08-02 18:11:05,611] [INFO] [launch.py:163:main] dist_world_size=32
g0093: [2024-08-02 18:11:05,611] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0091: [2024-08-02 18:11:05,649] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0091: [2024-08-02 18:11:05,649] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0069': [0, 1, 2, 3], 'g0086': [0, 1, 2, 3], 'g0087': [0, 1, 2, 3], 'g0088': [0, 1, 2, 3], 'g0090': [0, 1, 2, 3], 'g0091': [0, 1, 2, 3], 'g0092': [0, 1, 2, 3], 'g0093': [0, 1, 2, 3]}
g0091: [2024-08-02 18:11:05,649] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0091: [2024-08-02 18:11:05,649] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0069': [0, 1, 2, 3], 'g0086': [4, 5, 6, 7], 'g0087': [8, 9, 10, 11], 'g0088': [12, 13, 14, 15], 'g0090': [16, 17, 18, 19], 'g0091': [20, 21, 22, 23], 'g0092': [24, 25, 26, 27], 'g0093': [28, 29, 30, 31]})
g0091: [2024-08-02 18:11:05,649] [INFO] [launch.py:163:main] dist_world_size=32
g0091: [2024-08-02 18:11:05,649] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0069: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0069: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: [2024-08-02 18:11:05,728] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0088: [2024-08-02 18:11:05,729] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0069': [0, 1, 2, 3], 'g0086': [0, 1, 2, 3], 'g0087': [0, 1, 2, 3], 'g0088': [0, 1, 2, 3], 'g0090': [0, 1, 2, 3], 'g0091': [0, 1, 2, 3], 'g0092': [0, 1, 2, 3], 'g0093': [0, 1, 2, 3]}
g0088: [2024-08-02 18:11:05,729] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0088: [2024-08-02 18:11:05,729] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0069': [0, 1, 2, 3], 'g0086': [4, 5, 6, 7], 'g0087': [8, 9, 10, 11], 'g0088': [12, 13, 14, 15], 'g0090': [16, 17, 18, 19], 'g0091': [20, 21, 22, 23], 'g0092': [24, 25, 26, 27], 'g0093': [28, 29, 30, 31]})
g0088: [2024-08-02 18:11:05,729] [INFO] [launch.py:163:main] dist_world_size=32
g0088: [2024-08-02 18:11:05,729] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0092: [2024-08-02 18:11:06,061] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0092: [2024-08-02 18:11:06,061] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0069': [0, 1, 2, 3], 'g0086': [0, 1, 2, 3], 'g0087': [0, 1, 2, 3], 'g0088': [0, 1, 2, 3], 'g0090': [0, 1, 2, 3], 'g0091': [0, 1, 2, 3], 'g0092': [0, 1, 2, 3], 'g0093': [0, 1, 2, 3]}
g0092: [2024-08-02 18:11:06,061] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0092: [2024-08-02 18:11:06,062] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0069': [0, 1, 2, 3], 'g0086': [4, 5, 6, 7], 'g0087': [8, 9, 10, 11], 'g0088': [12, 13, 14, 15], 'g0090': [16, 17, 18, 19], 'g0091': [20, 21, 22, 23], 'g0092': [24, 25, 26, 27], 'g0093': [28, 29, 30, 31]})
g0092: [2024-08-02 18:11:06,062] [INFO] [launch.py:163:main] dist_world_size=32
g0092: [2024-08-02 18:11:06,062] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0090: [2024-08-02 18:11:06,651] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0090: [2024-08-02 18:11:06,651] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0069': [0, 1, 2, 3], 'g0086': [0, 1, 2, 3], 'g0087': [0, 1, 2, 3], 'g0088': [0, 1, 2, 3], 'g0090': [0, 1, 2, 3], 'g0091': [0, 1, 2, 3], 'g0092': [0, 1, 2, 3], 'g0093': [0, 1, 2, 3]}
g0090: [2024-08-02 18:11:06,651] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0090: [2024-08-02 18:11:06,651] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0069': [0, 1, 2, 3], 'g0086': [4, 5, 6, 7], 'g0087': [8, 9, 10, 11], 'g0088': [12, 13, 14, 15], 'g0090': [16, 17, 18, 19], 'g0091': [20, 21, 22, 23], 'g0092': [24, 25, 26, 27], 'g0093': [28, 29, 30, 31]})
g0090: [2024-08-02 18:11:06,651] [INFO] [launch.py:163:main] dist_world_size=32
g0090: [2024-08-02 18:11:06,651] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0087: [2024-08-02 18:11:07,939] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0087: [2024-08-02 18:11:07,939] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0087: [2024-08-02 18:11:07,939] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0087: [2024-08-02 18:11:08,100] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0086: [2024-08-02 18:11:08,195] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0086: [2024-08-02 18:11:08,195] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0086: [2024-08-02 18:11:08,198] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0086: [2024-08-02 18:11:08,203] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0093: [2024-08-02 18:11:08,713] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0093: [2024-08-02 18:11:08,713] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0093: [2024-08-02 18:11:08,713] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0091: [2024-08-02 18:11:08,772] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0091: [2024-08-02 18:11:08,772] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0091: [2024-08-02 18:11:08,773] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0093: [2024-08-02 18:11:08,844] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0088: [2024-08-02 18:11:08,900] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0088: [2024-08-02 18:11:08,900] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0088: [2024-08-02 18:11:08,900] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0091: [2024-08-02 18:11:09,002] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0088: [2024-08-02 18:11:09,041] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0092: [2024-08-02 18:11:09,173] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0092: [2024-08-02 18:11:09,173] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0092: [2024-08-02 18:11:09,191] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0092: [2024-08-02 18:11:09,368] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0090: [2024-08-02 18:11:09,762] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0090: [2024-08-02 18:11:09,762] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0090: [2024-08-02 18:11:09,810] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0090: [2024-08-02 18:11:10,054] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0087: --------------------------------------------------
g0087: DeepSpeed C++/CUDA extension op report
g0087: --------------------------------------------------
g0087: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0087:       runtime if needed. Op compatibility means that your system
g0087:       meet the required dependencies to JIT install the op.
g0087: --------------------------------------------------
g0087: JIT compiled ops requires ninja
g0087: --------------------------------------------------
g0087: DeepSpeed C++/CUDA extension op report
g0087: --------------------------------------------------
g0087: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0087:       runtime if needed. Op compatibility means that your system
g0087:       meet the required dependencies to JIT install the op.
g0087: --------------------------------------------------
g0087: JIT compiled ops requires ninja
g0087: --------------------------------------------------
g0087: DeepSpeed C++/CUDA extension op report
g0087: --------------------------------------------------
g0087: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0087:       runtime if needed. Op compatibility means that your system
g0087:       meet the required dependencies to JIT install the op.
g0087: --------------------------------------------------
g0087: JIT compiled ops requires ninja
g0087: --------------------------------------------------
g0087: DeepSpeed C++/CUDA extension op report
g0087: --------------------------------------------------
g0087: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0087:       runtime if needed. Op compatibility means that your system
g0087:       meet the required dependencies to JIT install the op.
g0087: --------------------------------------------------
g0087: JIT compiled ops requires ninja
g0087: ninjaninjaninja ninja ..................  .................. ..................[92m[OKAY][0m .................. 
g0087: [92m[OKAY][0m [92m[OKAY][0m
g0087: [92m[OKAY][0m--------------------------------------------------
g0087: 
g0087: 
g0087: ----------------------------------------------------------------------------------------------------
g0087: op name--------------------------------------------------
g0087:  op name
g0087: ................op name  op name ................installed ................  ................ installed.. installed  installed ..compatible .. 
g0087: .. compatible --------------------------------------------------compatible
g0087: compatible
g0087: 
g0087: 
g0087: ----------------------------------------------------------------------------------------------------
g0087: --------------------------------------------------
g0087: 
g0087: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0087: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0087: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0087: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0087: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0087: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0087: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: async_io ............... [92m[YES][0m ...... [92m[OKAY][0masync_io
g0087:  ............... [92m[YES][0m ......fused_adam  [92m[OKAY][0m.............
g0087:  [92m[YES][0m ...... [92m[OKAY][0m
g0087: fused_adam ............. cpu_adam[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0087: ...... [92m[OKAY][0m
g0087: cpu_adam ...............cpu_adagrad  [92m[YES][0m............  ......[92m[YES][0m  [92m[OKAY][0m......
g0087:  [92m[OKAY][0m
g0087: cpu_adagrad cpu_lion............  ...............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m async_io[92m[OKAY][0m
g0087: 
g0087:  ...............cpu_lion  [92m[YES][0m...............  ......[92m[YES][0m  [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m......
g0087:  
g0087: [92m[OKAY][0mevoformer_attn
g0087:  ......... [93m[NO][0mfused_adam  ....... .............[93m[NO][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0087: [92m[YES][0m
g0087:  ......evoformer_attnfused_lamb   [92m[OKAY][0m......................
g0087:   [93m[NO][0m[92m[YES][0m  cpu_adam.............   ...............[93m[NO][0m[92m[OKAY][0m 
g0087: 
g0087: [92m[YES][0m ......fused_lamb  [92m[OKAY][0m.............
g0087:  [92m[YES][0m cpu_adagrad......fused_lion   ............[92m[OKAY][0m............. 
g0087:  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0087: 
g0087: fused_lioncpu_lion  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0087: 
g0087: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0087: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0087: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0087: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0087: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0087: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0087: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0087: quantizer .............. [92m[YES][0mcutlass_ops cutlass_ops......  ............ [92m[OKAY][0m ............
g0087: [92m[YES][0m ......  [92m[OKAY][0m[92m[YES][0m
g0087:  ...... [92m[OKAY][0mquantizer 
g0087: .............. [92m[YES][0mquantizer ......  ..............[92m[OKAY][0m 
g0087: [92m[YES][0m ...... [92m[OKAY][0m
g0087: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0087: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0087: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0087: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0087: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0087: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0087: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0087: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0087: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0087: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0087: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0087: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0087: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0087: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0087: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0087: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0087: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0087: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0087: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0087: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0087: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0087: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0087: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0087: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0087: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0087: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0087: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0087: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0087: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0087: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0087: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0087: --------------------------------------------------
g0087: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0087: --------------------------------------------------
g0087: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0087: --------------------------------------------------
g0087: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0087: --------------------------------------------------
g0087: DeepSpeed general environment info:
g0087: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0087: torch version .................... 2.0.1+cu118
g0087: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0087: deepspeed info ................... 0.12.4, unknown, unknown
g0087: torch cuda version ............... 11.8
g0087: torch hip version ................ None
g0087: nvcc version ..................... 11.8
g0087: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0087: shared memory (/dev/shm) size .... 188.13 GB
g0087: DeepSpeed general environment info:
g0087: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0087: torch version .................... 2.0.1+cu118
g0087: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0087: deepspeed info ................... 0.12.4, unknown, unknown
g0087: torch cuda version ............... 11.8
g0087: torch hip version ................ None
g0087: nvcc version ..................... 11.8
g0087: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0087: shared memory (/dev/shm) size .... 188.13 GB
g0087: DeepSpeed general environment info:
g0087: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0087: torch version .................... 2.0.1+cu118
g0087: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0087: deepspeed info ................... 0.12.4, unknown, unknown
g0087: torch cuda version ............... 11.8
g0087: torch hip version ................ None
g0087: nvcc version ..................... 11.8
g0087: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0087: shared memory (/dev/shm) size .... 188.13 GB
g0087: DeepSpeed general environment info:
g0087: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0087: torch version .................... 2.0.1+cu118
g0087: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0087: deepspeed info ................... 0.12.4, unknown, unknown
g0087: torch cuda version ............... 11.8
g0087: torch hip version ................ None
g0087: nvcc version ..................... 11.8
g0087: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0087: shared memory (/dev/shm) size .... 188.13 GB
g0087: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0087: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0087: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0087: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0087: [2024-08-02 18:11:12,473] [INFO] [comm.py:637:init_distributed] cdb=None
g0087: [2024-08-02 18:11:12,473] [INFO] [comm.py:637:init_distributed] cdb=None
g0087: [2024-08-02 18:11:12,473] [INFO] [comm.py:637:init_distributed] cdb=None
g0087: [2024-08-02 18:11:12,474] [INFO] [comm.py:637:init_distributed] cdb=None
g0087: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0087: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0087: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0087: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0087: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0087: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0087: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0087: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0086: ----------------------------------------------------------------------------------------------------
g0086: DeepSpeed C++/CUDA extension op report
g0086: --------------------------------------------------
g0086: DeepSpeed C++/CUDA extension op report
g0086: --------------------------------------------------
g0086: --------------------------------------------------
g0086: DeepSpeed C++/CUDA extension op report
g0086: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0086:       runtime if needed. Op compatibility means that your system
g0086:       meet the required dependencies to JIT install the op.
g0086: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0086:       runtime if needed. Op compatibility means that your system
g0086:       meet the required dependencies to JIT install the op.
g0086: --------------------------------------------------
g0086: --------------------------------------------------
g0086: 
g0086: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0086:       runtime if needed. Op compatibility means that your system
g0086:       meet the required dependencies to JIT install the op.
g0086: JIT compiled ops requires ninja
g0086: JIT compiled ops requires ninja
g0086: --------------------------------------------------
g0086: 
g0086: JIT compiled ops requires ninja
g0086: --------------------------------------------------
g0086: DeepSpeed C++/CUDA extension op report
g0086: --------------------------------------------------
g0086: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0086:       runtime if needed. Op compatibility means that your system
g0086:       meet the required dependencies to JIT install the op.
g0086: --------------------------------------------------
g0086: JIT compiled ops requires ninja
g0086: ninjaninja ninja..................  ninja.................. .................. [92m[OKAY][0m  [92m[OKAY][0m
g0086: [92m[OKAY][0m..................
g0086: 
g0086:  ----------------------------------------------------------------------------------------------------[92m[OKAY][0m--------------------------------------------------
g0086: 
g0086: 
g0086: 
g0086: op nameop nameop name--------------------------------------------------   
g0086: ................................................  op name installedinstalled installed  ................ .... ..  installedcompatible compatible 
g0086: compatible
g0086: ..
g0086: -------------------------------------------------- --------------------------------------------------
g0086: --------------------------------------------------
g0086: compatible
g0086: 
g0086: --------------------------------------------------
g0086: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0086: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0086: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0086: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0086: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0086: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0086: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0086: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0086: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0086: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0086: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0086: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0086: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0086: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0086: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0086: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0086: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0086: evoformer_attn .........async_io [93m[NO][0m  ......................  [92m[YES][0m[93m[NO][0m 
g0086: ...... [92m[OKAY][0mfused_lamb
g0086:  ............. [92m[YES][0m ...... fused_adam[92m[OKAY][0m
g0086:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: fused_lion cpu_adam.............  ...............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0086: [92m[OKAY][0m
g0086: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0086: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0086: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0086: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0086: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0086: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0086: 
g0086: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0086: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0086: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0086: cutlass_ops quantizer............  ..............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0086: [92m[OKAY][0m
g0086: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0086: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0086: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0086: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0086: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0086: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0086: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0086: random_ltd .............sparse_attn  [92m[YES][0m............  ......[93m[NO][0m  [92m[OKAY][0m.......
g0086:  [93m[NO][0m
g0086: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0086: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0086: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0086: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0086: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0086: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0086: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0086: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0086: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0086: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0086: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0086: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0086: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0086: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0086: transformerspatial_inference  ..................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0086: 
g0086: stochastic_transformer .transformer  [92m[YES][0m............  ......[92m[YES][0m  [92m[OKAY][0m......
g0086:  [92m[OKAY][0m
g0086: spatial_inference stochastic_transformer......  .[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0086: [92m[OKAY][0m
g0086: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0086: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0086: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0086: --------------------------------------------------
g0086: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0086: --------------------------------------------------
g0086: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0086: --------------------------------------------------
g0086: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0086: --------------------------------------------------
g0086: DeepSpeed general environment info:
g0086: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0086: torch version .................... 2.0.1+cu118
g0086: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0086: deepspeed info ................... 0.12.4, unknown, unknown
g0086: torch cuda version ............... 11.8
g0086: torch hip version ................ None
g0086: nvcc version ..................... 11.8
g0086: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0086: shared memory (/dev/shm) size .... 188.13 GB
g0086: DeepSpeed general environment info:
g0086: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0086: torch version .................... 2.0.1+cu118
g0086: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0086: deepspeed info ................... 0.12.4, unknown, unknown
g0086: torch cuda version ............... 11.8
g0086: torch hip version ................ None
g0086: nvcc version ..................... 11.8
g0086: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0086: shared memory (/dev/shm) size .... 188.13 GB
g0086: DeepSpeed general environment info:
g0086: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0086: torch version .................... 2.0.1+cu118
g0086: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0086: deepspeed info ................... 0.12.4, unknown, unknown
g0086: torch cuda version ............... 11.8
g0086: torch hip version ................ None
g0086: nvcc version ..................... 11.8
g0086: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0086: shared memory (/dev/shm) size .... 188.13 GB
g0086: DeepSpeed general environment info:
g0086: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0086: torch version .................... 2.0.1+cu118
g0086: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0086: deepspeed info ................... 0.12.4, unknown, unknown
g0086: torch cuda version ............... 11.8
g0086: torch hip version ................ None
g0086: nvcc version ..................... 11.8
g0086: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0086: shared memory (/dev/shm) size .... 188.13 GB
g0086: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0086: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0086: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0086: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0086: [2024-08-02 18:11:12,824] [INFO] [comm.py:637:init_distributed] cdb=None
g0086: [2024-08-02 18:11:12,824] [INFO] [comm.py:637:init_distributed] cdb=None
g0086: [2024-08-02 18:11:12,825] [INFO] [comm.py:637:init_distributed] cdb=None
g0086: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0086: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0086: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0086: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0086: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0086: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0086: [2024-08-02 18:11:12,835] [INFO] [comm.py:637:init_distributed] cdb=None
g0086: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0086: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0093: --------------------------------------------------
g0093: DeepSpeed C++/CUDA extension op report
g0093: --------------------------------------------------
g0093: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0093:       runtime if needed. Op compatibility means that your system
g0093:       meet the required dependencies to JIT install the op.
g0093: --------------------------------------------------
g0093: JIT compiled ops requires ninja
g0093: --------------------------------------------------
g0093: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0093: 
g0093: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0093: 
g0093: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0093:       runtime if needed. Op compatibility means that your system
g0093:       meet the required dependencies to JIT install the op.--------------------------------------------------
g0093: 
g0093: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0093:       runtime if needed. Op compatibility means that your system
g0093:       meet the required dependencies to JIT install the op.
g0093: 
g0093: JIT compiled ops requires ninja--------------------------------------------------
g0093: 
g0093: JIT compiled ops requires ninja
g0093: --------------------------------------------------
g0093: DeepSpeed C++/CUDA extension op report
g0093: --------------------------------------------------
g0093: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0093:       runtime if needed. Op compatibility means that your system
g0093:       meet the required dependencies to JIT install the op.
g0093: --------------------------------------------------
g0093: JIT compiled ops requires ninja
g0093: ninjaninjaninjaninja    .................................... ....................................[92m[OKAY][0m  
g0093:  [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0093: --------------------------------------------------
g0093: 
g0093: --------------------------------------------------
g0093: 
g0093: ----------------------------------------------------------------------------------------------------op nameop name
g0093:  
g0093:  ................op nameop name................    installed................installed ................  installed installed.. ..  .. compatible compatible..
g0093: compatible
g0093:  
g0093: --------------------------------------------------compatible--------------------------------------------------
g0093: 
g0093: 
g0093: ----------------------------------------------------------------------------------------------------
g0093: 
g0093: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0093: fused_adam async_io.............  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m......
g0093:  async_io[92m[OKAY][0m 
g0093: cpu_adam...............  ...............[92m[YES][0m  [92m[YES][0m......  ......fused_adam[92m[OKAY][0m  [92m[OKAY][0m
g0093: .............
g0093:  [92m[YES][0m cpu_adagrad......  ............fused_adam[92m[OKAY][0m  
g0093: [92m[YES][0m.............  ......[92m[YES][0m cpu_adam [92m[OKAY][0m ......
g0093: ...............  [92m[OKAY][0m[92m[YES][0m
g0093: cpu_lion  ..................... cpu_adam [92m[OKAY][0m [92m[YES][0m
g0093: ...............  ......[92m[YES][0m cpu_adagrad [92m[OKAY][0m ......
g0093:  ............[92m[OKAY][0m 
g0093: [92m[YES][0m ...... cpu_adagrad[92m[OKAY][0m 
g0093: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH............
g0093:  cpu_lion[92m[YES][0m evoformer_attn ............... ...... ......... [92m[YES][0m [92m[OKAY][0m [93m[NO][0m
g0093: ......  .......[92m[OKAY][0mcpu_lion 
g0093:  [93m[NO][0m...............
g0093:  [92m[YES][0m fused_lamb......  .............[92m[OKAY][0m 
g0093: [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0093: ...... [92m[OKAY][0mevoformer_attn
g0093:  ......... [93m[NO][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH.......
g0093:  [93m[NO][0m
g0093: evoformer_attn fused_lion.........async_io  fused_lamb[93m[NO][0m.............   ....................[92m[YES][0m    [92m[YES][0m[93m[NO][0m...... 
g0093: ............... ......[92m[OKAY][0mfused_lamb 
g0093:   [92m[OKAY][0m.............
g0093:  [92m[YES][0m[92m[YES][0m ......  [92m[OKAY][0mfused_lion......
g0093:  .............  [92m[YES][0m ......[92m[OKAY][0m fused_lion[92m[OKAY][0m 
g0093: 
g0093: ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0093: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0093: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0093: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0093: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0093: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0093: 
g0093: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0093: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0093: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0093: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0093: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0093: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0093: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0093: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0093: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0093: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0093: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0093: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0093: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0093: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0093: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0093: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0093: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0093: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0093: sparse_attn ............ [93m[NO][0m[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0 
g0093: ....... [93m[NO][0m
g0093: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0093: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0093: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0093: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0093: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0093: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0093: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0093: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0093: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0093: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0093: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0093: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0093: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0093: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0093: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0093: --------------------------------------------------
g0093: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0093: --------------------------------------------------
g0093: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0093: --------------------------------------------------
g0093: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0093: --------------------------------------------------
g0093: DeepSpeed general environment info:
g0093: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0093: torch version .................... 2.0.1+cu118
g0093: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0093: deepspeed info ................... 0.12.4, unknown, unknown
g0093: torch cuda version ............... 11.8
g0093: torch hip version ................ None
g0093: nvcc version ..................... 11.8
g0093: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0093: shared memory (/dev/shm) size .... 188.13 GB
g0093: DeepSpeed general environment info:
g0093: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0093: torch version .................... 2.0.1+cu118
g0093: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0093: deepspeed info ................... 0.12.4, unknown, unknown
g0093: torch cuda version ............... 11.8
g0093: torch hip version ................ None
g0093: nvcc version ..................... 11.8
g0093: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0093: shared memory (/dev/shm) size .... 188.13 GB
g0093: DeepSpeed general environment info:
g0093: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0093: torch version .................... 2.0.1+cu118
g0093: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0093: deepspeed info ................... 0.12.4, unknown, unknown
g0093: torch cuda version ............... 11.8
g0093: torch hip version ................ None
g0093: nvcc version ..................... 11.8
g0093: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0093: shared memory (/dev/shm) size .... 188.13 GB
g0093: DeepSpeed general environment info:
g0093: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0093: torch version .................... 2.0.1+cu118
g0093: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0093: deepspeed info ................... 0.12.4, unknown, unknown
g0093: torch cuda version ............... 11.8
g0093: torch hip version ................ None
g0093: nvcc version ..................... 11.8
g0093: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0093: shared memory (/dev/shm) size .... 188.13 GB
g0091: --------------------------------------------------
g0091: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0091: --------------------------------------------------
g0091: 
g0091: DeepSpeed C++/CUDA extension op reportNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0091:       runtime if needed. Op compatibility means that your system
g0091:       meet the required dependencies to JIT install the op.
g0091: 
g0091: ----------------------------------------------------------------------------------------------------
g0091: 
g0091: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0091:       runtime if needed. Op compatibility means that your system
g0091:       meet the required dependencies to JIT install the op.JIT compiled ops requires ninja
g0091: 
g0091: --------------------------------------------------
g0091: JIT compiled ops requires ninja
g0091: --------------------------------------------------
g0091: DeepSpeed C++/CUDA extension op report
g0091: --------------------------------------------------
g0091: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0091:       runtime if needed. Op compatibility means that your system
g0091:       meet the required dependencies to JIT install the op.
g0091: --------------------------------------------------
g0091: --------------------------------------------------JIT compiled ops requires ninja
g0091: 
g0091: DeepSpeed C++/CUDA extension op report
g0091: --------------------------------------------------
g0091: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0091:       runtime if needed. Op compatibility means that your system
g0091:       meet the required dependencies to JIT install the op.
g0091: --------------------------------------------------
g0091: JIT compiled ops requires ninja
g0091: ninjaninjaninja  ninja ......................................................    ..................[92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m
g0091: 
g0091: 
g0091:  [92m[OKAY][0m----------------------------------------------------------------------------------------------------
g0091: --------------------------------------------------
g0091: 
g0091: 
g0091: --------------------------------------------------op nameop name
g0091: op name   ................op name................................   installed ................installed installed  .. installed.. ..  compatible compatible..
g0091: compatible
g0091:  
g0091: --------------------------------------------------compatible--------------------------------------------------
g0091: --------------------------------------------------
g0091: 
g0091: 
g0091: --------------------------------------------------
g0093: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0093: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0093: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0093: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0091: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0091: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0091: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0091: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0091: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0091: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0091: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0091: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: cpu_adam ............... [92m[YES][0m ......async_io [92m[OKAY][0m 
g0091: ............... [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0m[92m[YES][0m 
g0091: ...... [92m[OKAY][0m
g0091: cpu_lionfused_adam  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0091: 
g0091: cpu_adam ............... [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0091: ...... evoformer_attn[92m[OKAY][0m 
g0091: ......... [93m[NO][0mcpu_adagrad  ...................  [93m[NO][0m[92m[YES][0m
g0091:  ...... [92m[OKAY][0mfused_lamb
g0091:  ............. cpu_lion[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0091: ...... [92m[OKAY][0m
g0091: fused_lion [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH.............
g0091:  [92m[YES][0m evoformer_attnasync_io......  .........  [92m[OKAY][0m...............
g0091: [93m[NO][0m  [92m[YES][0m.......  ......[93m[NO][0m 
g0091: [92m[OKAY][0m
g0091: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0mfused_adam
g0091:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: fused_lion .............cpu_adam  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m......
g0091:  [92m[OKAY][0m
g0091: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0091: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0091: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0091: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0091: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0093: [2024-08-02 18:11:13,178] [INFO] [comm.py:637:init_distributed] cdb=None
g0093: [2024-08-02 18:11:13,178] [INFO] [comm.py:637:init_distributed] cdb=None
g0093: [2024-08-02 18:11:13,178] [INFO] [comm.py:637:init_distributed] cdb=None
g0091: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0091: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0091: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0091: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0093: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0093: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0093: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0093: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0093: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0093: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0091: cutlass_ops ............ [92m[YES][0m ......cutlass_ops [92m[OKAY][0m 
g0091: ............ [92m[YES][0m ......quantizer  [92m[OKAY][0m..............
g0091:  [92m[YES][0m ...... quantizer[92m[OKAY][0m 
g0091: .............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0091: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0091: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0091: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0091: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0091: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0091: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0091: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0091: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0091: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0091: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0091: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0091: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0091: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0091: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0091: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0091: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0091: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0091: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0091: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0091: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0091: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0091: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0091: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0091: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0091: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0091: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0091: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0091: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0091: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0091: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0091: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0091: --------------------------------------------------
g0091: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0091: --------------------------------------------------
g0091: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0091: --------------------------------------------------
g0091: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0091: --------------------------------------------------
g0091: DeepSpeed general environment info:
g0091: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0091: torch version .................... 2.0.1+cu118
g0091: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0091: deepspeed info ................... 0.12.4, unknown, unknown
g0091: torch cuda version ............... 11.8
g0091: torch hip version ................ None
g0091: nvcc version ..................... 11.8
g0091: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0091: shared memory (/dev/shm) size .... 188.13 GB
g0091: DeepSpeed general environment info:
g0091: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0091: torch version .................... 2.0.1+cu118
g0091: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0091: deepspeed info ................... 0.12.4, unknown, unknown
g0091: torch cuda version ............... 11.8
g0091: torch hip version ................ None
g0091: nvcc version ..................... 11.8
g0091: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0091: shared memory (/dev/shm) size .... 188.13 GB
g0091: DeepSpeed general environment info:
g0091: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0091: torch version .................... 2.0.1+cu118
g0091: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0091: deepspeed info ................... 0.12.4, unknown, unknown
g0091: torch cuda version ............... 11.8
g0091: torch hip version ................ None
g0091: nvcc version ..................... 11.8
g0091: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0091: shared memory (/dev/shm) size .... 188.13 GB
g0091: DeepSpeed general environment info:
g0091: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0091: torch version .................... 2.0.1+cu118
g0091: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0091: deepspeed info ................... 0.12.4, unknown, unknown
g0091: torch cuda version ............... 11.8
g0091: torch hip version ................ None
g0091: nvcc version ..................... 11.8
g0091: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0091: shared memory (/dev/shm) size .... 188.13 GB
g0088: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0088: 
g0088: 
g0088: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0088: 
g0088: 
g0088: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0088: 
g0088: 
g0088: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0088:       runtime if needed. Op compatibility means that your system
g0088:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0088:       runtime if needed. Op compatibility means that your system
g0088:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0088:       runtime if needed. Op compatibility means that your system
g0088:       meet the required dependencies to JIT install the op.
g0088: 
g0088: 
g0088: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0088: 
g0088: 
g0088: JIT compiled ops requires ninjaJIT compiled ops requires ninjaJIT compiled ops requires ninja
g0088: 
g0088: 
g0088: --------------------------------------------------
g0088: DeepSpeed C++/CUDA extension op report
g0088: --------------------------------------------------
g0088: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0088:       runtime if needed. Op compatibility means that your system
g0088:       meet the required dependencies to JIT install the op.
g0088: --------------------------------------------------
g0088: JIT compiled ops requires ninja
g0088: ninjaninjaninja   ninja......................................................    [92m[OKAY][0m[92m[OKAY][0m..................[92m[OKAY][0m
g0088: 
g0088:  
g0088: [92m[OKAY][0m----------------------------------------------------------------------------------------------------
g0088: --------------------------------------------------
g0088: 
g0088: 
g0088: --------------------------------------------------op nameop name
g0088: op name   ................................op name................    installedinstalled................installed    ....installed   ..compatible..compatible 
g0088:  
g0088: compatiblecompatible--------------------------------------------------
g0088: --------------------------------------------------
g0088: 
g0088: 
g0088: ----------------------------------------------------------------------------------------------------
g0088: 
g0091: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0091: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0091: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0091: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0088: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0088: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0088: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0088: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0088: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0088: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0088: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: fused_lion ............. async_io[92m[YES][0m  .....................  [92m[OKAY][0m[92m[YES][0m
g0088:  ...... [92m[OKAY][0m
g0088: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0088: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0088: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0088: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0088: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0088: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0088: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0088: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0088: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0088: async_io ............... [92m[YES][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......
g0088:  [92m[OKAY][0mevoformer_attn
g0088:  ......... [93m[NO][0m ....... [93m[NO][0mfused_adam
g0088:  ............. [92m[YES][0mfused_lamb  ...................  [92m[OKAY][0m[92m[YES][0m
g0088:  ...... [92m[OKAY][0mcpu_adam
g0088:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0088: fused_lion .............cpu_adagrad  [92m[YES][0m............  ......[92m[YES][0m  [92m[OKAY][0m......
g0088:  [92m[OKAY][0m
g0088: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0088: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0088: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0088: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0088: inference_core_ops ..... [92m[YES][0m ...... inference_core_ops[92m[OKAY][0m 
g0088: ..... [92m[YES][0m ...... [92m[OKAY][0m
g0088: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0088: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0088: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0088: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0088: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0088: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0088: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0088: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0088: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0088: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0088: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0088: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0088: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0088: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0088: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0088: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0088: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0088: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0088: sparse_attn ............ [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0[93m[NO][0m
g0088:  ....... [93m[NO][0m
g0088: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0088: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0091: [2024-08-02 18:11:13,342] [INFO] [comm.py:637:init_distributed] cdb=None
g0091: [2024-08-02 18:11:13,342] [INFO] [comm.py:637:init_distributed] cdb=None
g0091: [2024-08-02 18:11:13,343] [INFO] [comm.py:637:init_distributed] cdb=None
g0091: [2024-08-02 18:11:13,344] [INFO] [comm.py:637:init_distributed] cdb=None
g0088: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0088: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0088: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0088: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0088: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0088: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0088: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0088: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0088: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0088: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0088: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0088: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0091: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0091: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0091: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0088: --------------------------------------------------
g0091: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0091: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0088: --------------------------------------------------
g0091: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0088: --------------------------------------------------
g0088: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0088: --------------------------------------------------
g0091: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0091: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: DeepSpeed general environment info:
g0088: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0088: torch version .................... 2.0.1+cu118
g0088: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0088: deepspeed info ................... 0.12.4, unknown, unknown
g0088: torch cuda version ............... 11.8
g0088: torch hip version ................ None
g0088: nvcc version ..................... 11.8
g0088: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0088: shared memory (/dev/shm) size .... 188.13 GB
g0088: DeepSpeed general environment info:
g0088: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0088: torch version .................... 2.0.1+cu118
g0088: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0088: deepspeed info ................... 0.12.4, unknown, unknown
g0088: torch cuda version ............... 11.8
g0088: torch hip version ................ None
g0088: nvcc version ..................... 11.8
g0088: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0088: shared memory (/dev/shm) size .... 188.13 GB
g0088: DeepSpeed general environment info:
g0088: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0088: torch version .................... 2.0.1+cu118
g0088: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0088: deepspeed info ................... 0.12.4, unknown, unknown
g0088: torch cuda version ............... 11.8
g0088: torch hip version ................ None
g0088: nvcc version ..................... 11.8
g0088: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0088: shared memory (/dev/shm) size .... 188.13 GB
g0088: DeepSpeed general environment info:
g0088: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0088: torch version .................... 2.0.1+cu118
g0088: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0088: deepspeed info ................... 0.12.4, unknown, unknown
g0088: torch cuda version ............... 11.8
g0088: torch hip version ................ None
g0088: nvcc version ..................... 11.8
g0088: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0088: shared memory (/dev/shm) size .... 188.13 GB
g0088: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0088: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0088: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0088: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0088: [2024-08-02 18:11:13,483] [INFO] [comm.py:637:init_distributed] cdb=None
g0088: [2024-08-02 18:11:13,484] [INFO] [comm.py:637:init_distributed] cdb=None
g0088: [2024-08-02 18:11:13,484] [INFO] [comm.py:637:init_distributed] cdb=None
g0088: [2024-08-02 18:11:13,484] [INFO] [comm.py:637:init_distributed] cdb=None
g0088: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0088: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0092: ----------------------------------------------------------------------------------------------------
g0092: 
g0092: --------------------------------------------------DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0092: 
g0092: 
g0092: --------------------------------------------------DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0092: 
g0092: 
g0092: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0092:       runtime if needed. Op compatibility means that your system
g0092:       meet the required dependencies to JIT install the op.--------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0092:       runtime if needed. Op compatibility means that your system
g0092:       meet the required dependencies to JIT install the op.
g0092: 
g0092: 
g0092: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0092:       runtime if needed. Op compatibility means that your system
g0092:       meet the required dependencies to JIT install the op.--------------------------------------------------
g0092: 
g0092: 
g0092: JIT compiled ops requires ninja--------------------------------------------------JIT compiled ops requires ninja
g0092: 
g0092: 
g0092: JIT compiled ops requires ninja
g0092: --------------------------------------------------
g0092: DeepSpeed C++/CUDA extension op report
g0092: --------------------------------------------------
g0092: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0092:       runtime if needed. Op compatibility means that your system
g0092:       meet the required dependencies to JIT install the op.
g0092: --------------------------------------------------
g0092: JIT compiled ops requires ninja
g0092: ninjaninjaninja   ......................................................ninja   [92m[OKAY][0m[92m[OKAY][0m [92m[OKAY][0m
g0092: 
g0092: ..................
g0092:  ------------------------------------------------------------------------------------------------------------------------------------------------------[92m[OKAY][0m
g0092: 
g0092: 
g0092: 
g0092: op nameop nameop name --------------------------------------------------  ................
g0092: ................ ................ installed op nameinstalled installed  .. ....................    installedcompatiblecompatiblecompatible 
g0092: 
g0092: 
g0092: ..-------------------------------------------------- ----------------------------------------------------------------------------------------------------
g0092: compatible
g0092: 
g0092: 
g0092: --------------------------------------------------
g0093: > setting tensorboard ...
g0093: [2024-08-02 18:11:13,576] [INFO] [comm.py:637:init_distributed] cdb=None
g0093: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0093: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0092: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0092: fused_adamasync_io .............  ...............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0092: [92m[OKAY][0m
g0092: cpu_adam ............... [92m[YES][0m fused_adam......  .............[92m[OKAY][0m 
g0092: [92m[YES][0m ...... cpu_adagrad[92m[OKAY][0m 
g0092: ............ async_io[92m[YES][0mcpu_adam   .....................  [92m[OKAY][0m[92m[YES][0m...............
g0092:   ......[92m[YES][0m  [92m[OKAY][0mcpu_lion......
g0092:   ...............[92m[OKAY][0m 
g0092: cpu_adagrad[92m[YES][0m  ..................  [92m[YES][0m[92m[OKAY][0m 
g0092: fused_adam......  .............[92m[OKAY][0m 
g0092: [92m[YES][0m ......cpu_lion  [92m[OKAY][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH...............
g0092: async_io
g0092:  [92m[YES][0m evoformer_attn cpu_adam............... ......  ......... ...............[92m[YES][0m [92m[OKAY][0m  [93m[NO][0m
g0092: [92m[YES][0m......   .............[92m[OKAY][0m  [93m[NO][0m[92m[OKAY][0m
g0092: 
g0092: 
g0092: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHfused_lamb
g0092:  cpu_adagrad............. evoformer_attn fused_adam............ [92m[YES][0m  ......... .............[92m[YES][0m ...... [93m[NO][0m  [92m[YES][0m[92m[OKAY][0m ...... 
g0092: ....... ...... [92m[OKAY][0m [93m[NO][0m
g0092: [92m[OKAY][0m
g0092: 
g0092: cpu_lionfused_lamb cpu_adam ...............fused_lion.............    ...............[92m[YES][0m.............[92m[YES][0m   [92m[YES][0m ...... ...... [92m[YES][0m...... [92m[OKAY][0m  [92m[OKAY][0m
g0092: ......[92m[OKAY][0m
g0092:  
g0092: [92m[OKAY][0m
g0092: cpu_adagrad ............ [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH fused_lion
g0092: ......  .............evoformer_attn[92m[OKAY][0m  
g0092: [92m[YES][0m.........  ......cpu_lion[93m[NO][0m   [92m[OKAY][0m......................
g0092:   [92m[YES][0m[93m[NO][0m 
g0092: ...... [92m[OKAY][0mfused_lamb
g0092:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0092: evoformer_attn ......... fused_lion[93m[NO][0m  ....................  [92m[YES][0m[93m[NO][0m 
g0092: ...... [92m[OKAY][0m
g0092: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0092: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0092: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0092: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0092: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0092: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0092: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0092: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0092: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0092: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0092: ragged_device_ops ...... ragged_device_ops[92m[YES][0m  ............  [92m[YES][0m[92m[OKAY][0m 
g0092: ...... [92m[OKAY][0m
g0092: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0092: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0092: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0092: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0092: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0092: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0092: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0092: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0092: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0092: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0092: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0092: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0092: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0092: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0092: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0092: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0092: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0092: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0092: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0092: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0092: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0092: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0092: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0092: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0092: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0092: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0092: --------------------------------------------------
g0092: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0092: --------------------------------------------------
g0092: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0092: --------------------------------------------------
g0092: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0092: --------------------------------------------------
g0092: DeepSpeed general environment info:
g0092: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0092: torch version .................... 2.0.1+cu118
g0092: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0092: deepspeed info ................... 0.12.4, unknown, unknown
g0092: torch cuda version ............... 11.8
g0092: torch hip version ................ None
g0092: nvcc version ..................... 11.8
g0092: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0092: shared memory (/dev/shm) size .... 188.13 GB
g0092: DeepSpeed general environment info:
g0092: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0092: torch version .................... 2.0.1+cu118
g0092: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0092: deepspeed info ................... 0.12.4, unknown, unknown
g0092: torch cuda version ............... 11.8
g0092: torch hip version ................ None
g0092: nvcc version ..................... 11.8
g0092: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0092: shared memory (/dev/shm) size .... 188.13 GB
g0092: DeepSpeed general environment info:
g0092: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0092: torch version .................... 2.0.1+cu118
g0092: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0092: deepspeed info ................... 0.12.4, unknown, unknown
g0092: torch cuda version ............... 11.8
g0092: torch hip version ................ None
g0092: nvcc version ..................... 11.8
g0092: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0092: shared memory (/dev/shm) size .... 188.13 GB
g0092: DeepSpeed general environment info:
g0092: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0092: torch version .................... 2.0.1+cu118
g0092: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0092: deepspeed info ................... 0.12.4, unknown, unknown
g0092: torch cuda version ............... 11.8
g0092: torch hip version ................ None
g0092: nvcc version ..................... 11.8
g0092: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0092: shared memory (/dev/shm) size .... 188.13 GB
g0092: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0092: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0092: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0092: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0092: [2024-08-02 18:11:13,796] [INFO] [comm.py:637:init_distributed] cdb=None
g0092: [2024-08-02 18:11:13,797] [INFO] [comm.py:637:init_distributed] cdb=None
g0092: [2024-08-02 18:11:13,797] [INFO] [comm.py:637:init_distributed] cdb=None
g0092: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0092: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0092: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0092: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0092: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0092: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0092: [2024-08-02 18:11:13,805] [INFO] [comm.py:637:init_distributed] cdb=None
g0092: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0092: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0090: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0090: 
g0090: 
g0090: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0090: 
g0090: 
g0090: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0090: 
g0090: 
g0090: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0090:       runtime if needed. Op compatibility means that your system
g0090:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0090:       runtime if needed. Op compatibility means that your system
g0090:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0090:       runtime if needed. Op compatibility means that your system
g0090:       meet the required dependencies to JIT install the op.
g0090: 
g0090: 
g0090: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0090: 
g0090: 
g0090: JIT compiled ops requires ninjaJIT compiled ops requires ninjaJIT compiled ops requires ninja
g0090: 
g0090: 
g0090: --------------------------------------------------
g0090: DeepSpeed C++/CUDA extension op report
g0090: --------------------------------------------------
g0090: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0090:       runtime if needed. Op compatibility means that your system
g0090:       meet the required dependencies to JIT install the op.
g0090: --------------------------------------------------
g0090: JIT compiled ops requires ninja
g0090: ninjaninjaninja ninja  ......................................................    [92m[OKAY][0m..................[92m[OKAY][0m[92m[OKAY][0m
g0090:  
g0090: 
g0090: [92m[OKAY][0m--------------------------------------------------
g0090: --------------------------------------------------
g0090: --------------------------------------------------
g0090: 
g0090: --------------------------------------------------op nameop name
g0090:  op name ................ op name................ ................  installed ................installed installed  .. installed .... compatible  ..
g0090: compatible compatible--------------------------------------------------
g0090: compatible
g0090: 
g0090: 
g0090: ----------------------------------------------------------------------------------------------------
g0090: --------------------------------------------------
g0090: 
g0090: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0090: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0090: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0090: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0090: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0090: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0090: fused_lamb ............. [92m[YES][0m async_io......  [92m[OKAY][0m
g0090: ............... [92m[YES][0m ......async_io [92m[OKAY][0m 
g0090: ............... fused_lion[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m ......
g0090:  [92m[OKAY][0m
g0090: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: fused_adam .............cpu_adam  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m......
g0090:  [92m[OKAY][0m
g0090: cpu_adagrad ............ [92m[YES][0mcpu_adamasync_io  ......  ..............................[92m[OKAY][0m  [92m[YES][0m
g0090: [92m[YES][0m  ............ cpu_lion [92m[OKAY][0m [92m[OKAY][0m
g0090: ...............
g0090:  [92m[YES][0m ......cpu_adagrad  [92m[OKAY][0m............fused_adam
g0090:   [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0090:  [92m[OKAY][0m
g0090: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0090: cpu_lioncpu_adam evoformer_attn ............... ............... ......... [92m[YES][0m [92m[YES][0m [93m[NO][0m ...... ...... ....... [92m[OKAY][0m [92m[OKAY][0m
g0090: [93m[NO][0m
g0090: 
g0090: cpu_adagrad fused_lamb............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0090: [92m[OKAY][0m
g0090: cpu_lion ............... [92m[YES][0m ...... fused_lion[92m[OKAY][0m 
g0090: ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0090: evoformer_attn [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......... 
g0090: [93m[NO][0m .......evoformer_attn  [93m[NO][0m.........
g0090:  [93m[NO][0m fused_lamb.......  .............[93m[NO][0m 
g0090: [92m[YES][0m ...... [92m[OKAY][0m
g0090: fused_lamb ............. [92m[YES][0m ...... fused_lion[92m[OKAY][0m 
g0090: ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0090: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0090: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0090: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0090: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0090: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0090: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0090: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0090: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0090: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0090: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0090: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0090: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0090: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0090: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0090: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0090: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0090: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0090: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0090: ragged_ops [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible.............
g0090:  [92m[YES][0m sparse_attn......  [92m[OKAY][0m............
g0090:  [93m[NO][0m ....... [93m[NO][0m
g0090: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0090: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0090: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0090: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0090: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0090: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0090: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0090: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0090: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0090: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0090: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0090: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0090: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0090: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0090: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0090: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0090: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0090: --------------------------------------------------
g0090: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0090: --------------------------------------------------
g0090: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0090: --------------------------------------------------
g0090: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0090: --------------------------------------------------
g0090: DeepSpeed general environment info:
g0090: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0090: torch version .................... 2.0.1+cu118
g0090: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0090: deepspeed info ................... 0.12.4, unknown, unknown
g0090: torch cuda version ............... 11.8
g0090: torch hip version ................ None
g0090: nvcc version ..................... 11.8
g0090: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0090: shared memory (/dev/shm) size .... 188.13 GB
g0090: DeepSpeed general environment info:
g0090: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0090: torch version .................... 2.0.1+cu118
g0090: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0090: deepspeed info ................... 0.12.4, unknown, unknown
g0090: torch cuda version ............... 11.8
g0090: torch hip version ................ None
g0090: nvcc version ..................... 11.8
g0090: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0090: shared memory (/dev/shm) size .... 188.13 GB
g0090: DeepSpeed general environment info:
g0090: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0090: torch version .................... 2.0.1+cu118
g0090: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0090: deepspeed info ................... 0.12.4, unknown, unknown
g0090: torch cuda version ............... 11.8
g0090: torch hip version ................ None
g0090: nvcc version ..................... 11.8
g0090: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0090: shared memory (/dev/shm) size .... 188.13 GB
g0090: DeepSpeed general environment info:
g0090: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0090: torch version .................... 2.0.1+cu118
g0090: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0090: deepspeed info ................... 0.12.4, unknown, unknown
g0090: torch cuda version ............... 11.8
g0090: torch hip version ................ None
g0090: nvcc version ..................... 11.8
g0090: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0090: shared memory (/dev/shm) size .... 188.13 GB
g0090: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0090: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0090: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0090: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0090: [2024-08-02 18:11:14,504] [INFO] [comm.py:637:init_distributed] cdb=None
g0090: [2024-08-02 18:11:14,504] [INFO] [comm.py:637:init_distributed] cdb=None
g0090: [2024-08-02 18:11:14,506] [INFO] [comm.py:637:init_distributed] cdb=None
g0090: [2024-08-02 18:11:14,507] [INFO] [comm.py:637:init_distributed] cdb=None
g0090: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0090: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0090: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0090: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0090: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0090: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0090: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0090: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0069: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0069-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0069: > initialized tensor model parallel with size 1
g0069: > initialized pipeline model parallel with size 8
g0069: > setting random seeds to 1234 ...
g0069: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0069: > compiling dataset index builder ...
g0069: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0069: make: Nothing to be done for 'default'.
g0069: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0069: >>> done with dataset index builder. Compilation time: 0.079 seconds
g0069: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0069: > compiling and loading fused kernels ...
g0069: Detected CUDA files, patching ldflags
g0069: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0069: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0069: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0069: ninja: no work to do.
g0069: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0069: Detected CUDA files, patching ldflags
g0069: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0069: Building extension module scaled_masked_softmax_cuda...
g0069: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0069: ninja: no work to do.
g0069: Loading extension module scaled_masked_softmax_cuda...
g0069: Detected CUDA files, patching ldflags
g0069: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0069: Building extension module scaled_softmax_cuda...
g0069: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0069: ninja: no work to do.
g0069: Loading extension module scaled_softmax_cuda...
g0069: >>> done with compiling and loading fused kernels. Compilation time: 7.484 seconds
g0069: time to initialize megatron (seconds): 22.008
g0069: [after megatron is initialized] datetime: 2024-08-02 18:11:24 
g0069: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0091: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0086: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0092: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0093: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0090: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0087: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0088: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0069: wandb: Tracking run with wandb version 0.17.5
g0069: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181126-ukxdujp0
g0069: wandb: Run `wandb offline` to turn off syncing.
g0069: wandb: Syncing run g0069.abci.local
g0069: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0069: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/ukxdujp0
g0086: wandb: Tracking run with wandb version 0.17.5
g0086: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181126-ie02y5g5
g0086: wandb: Run `wandb offline` to turn off syncing.
g0086: wandb: Syncing run g0086.abci.local
g0086: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0086: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/ie02y5g5
g0093: wandb: Tracking run with wandb version 0.17.5
g0093: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181126-8nxim40y
g0093: wandb: Run `wandb offline` to turn off syncing.
g0088: wandb: Tracking run with wandb version 0.17.5
g0088: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181126-du9f8a04
g0088: wandb: Run `wandb offline` to turn off syncing.
g0093: wandb: Syncing run g0093.abci.local
g0093: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0093: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/8nxim40y
g0088: wandb: Syncing run g0088.abci.local
g0088: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0088: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/du9f8a04
g0091: wandb: Tracking run with wandb version 0.17.5
g0091: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181126-rbqtymm6
g0091: wandb: Run `wandb offline` to turn off syncing.
g0091: wandb: Syncing run g0091.abci.local
g0091: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0091: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/rbqtymm6
g0087: wandb: Tracking run with wandb version 0.17.5
g0087: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181126-60ikoe7x
g0087: wandb: Run `wandb offline` to turn off syncing.
g0087: wandb: Syncing run g0087.abci.local
g0087: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0087: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/60ikoe7x
g0092: wandb: Tracking run with wandb version 0.17.5
g0092: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181126-twe8yyyb
g0092: wandb: Run `wandb offline` to turn off syncing.
g0092: wandb: Syncing run g0092.abci.local
g0092: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0092: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/twe8yyyb
g0090: wandb: Tracking run with wandb version 0.17.5
g0090: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_181126-fgodz22v
g0090: wandb: Run `wandb offline` to turn off syncing.
g0090: wandb: Syncing run g0090.abci.local
g0090: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0090: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/fgodz22v
g0069: building GPT model ...
g0069: [2024-08-02 18:11:27,246] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0069: [2024-08-02 18:11:27,247] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0069: [2024-08-02 18:11:27,247] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 36.89 GB, percent = 9.8%
g0069: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0069: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0069: [2024-08-02 18:11:27,769] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0069: stage=0 layers=5
g0069:      0: _to_float16
g0069:      1: EmbeddingPipe
g0069:      2: ParallelTransformerLayerPipe
g0069:      3: ParallelTransformerLayerPipe
g0069:      4: ParallelTransformerLayerPipe
g0069: stage=1 layers=3
g0069:      5: ParallelTransformerLayerPipe
g0069:      6: ParallelTransformerLayerPipe
g0069:      7: ParallelTransformerLayerPipe
g0069: stage=2 layers=3
g0069:      8: ParallelTransformerLayerPipe
g0069:      9: ParallelTransformerLayerPipe
g0069:     10: ParallelTransformerLayerPipe
g0069: stage=3 layers=3
g0069:     11: ParallelTransformerLayerPipe
g0069:     12: ParallelTransformerLayerPipe
g0069:     13: ParallelTransformerLayerPipe
g0069: stage=4 layers=3
g0069:     14: ParallelTransformerLayerPipe
g0069:     15: ParallelTransformerLayerPipe
g0069:     16: ParallelTransformerLayerPipe
g0069: stage=5 layers=3
g0069:     17: ParallelTransformerLayerPipe
g0069:     18: ParallelTransformerLayerPipe
g0069:     19: ParallelTransformerLayerPipe
g0069: stage=6 layers=3
g0069:     20: ParallelTransformerLayerPipe
g0069:     21: ParallelTransformerLayerPipe
g0069:     22: ParallelTransformerLayerPipe
g0069: stage=7 layers=3
g0069:     23: ParallelTransformerLayerPipe
g0069:     24: MixedFusedRMSNorm
g0069:     25: LMHeadPipe
g0069:   loss: CrossEntropy
g0091:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0090:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0087:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0086:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0093:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0092:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0088:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0069: [2024-08-02 18:11:28,381] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0069: [2024-08-02 18:11:28,382] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0069: [2024-08-02 18:11:28,382] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 36.95 GB, percent = 9.8%
g0069:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0069: setting training iterations to 10000000
g0069: > learning rate decay style: cosine
g0069: DeepSpeed is enabled.
g0069: [2024-08-02 18:11:28,384] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0091: [2024-08-02 18:11:28,386] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0091: [2024-08-02 18:11:28,387] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0091: [2024-08-02 18:11:28,387] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0091: [2024-08-02 18:11:28,387] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0090: [2024-08-02 18:11:28,398] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0090: [2024-08-02 18:11:28,398] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0090: [2024-08-02 18:11:28,398] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0090: [2024-08-02 18:11:28,398] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0086: [2024-08-02 18:11:28,415] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0086: [2024-08-02 18:11:28,415] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0086: [2024-08-02 18:11:28,415] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0086: [2024-08-02 18:11:28,415] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0093: [2024-08-02 18:11:28,420] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0093: [2024-08-02 18:11:28,420] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0093: [2024-08-02 18:11:28,420] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0092: [2024-08-02 18:11:28,420] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0092: [2024-08-02 18:11:28,420] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0092: [2024-08-02 18:11:28,420] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0093: [2024-08-02 18:11:28,421] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0092: [2024-08-02 18:11:28,421] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0087: [2024-08-02 18:11:28,422] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0087: [2024-08-02 18:11:28,422] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0087: [2024-08-02 18:11:28,422] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0087: [2024-08-02 18:11:28,422] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0088: [2024-08-02 18:11:28,432] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0088: [2024-08-02 18:11:28,432] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0088: [2024-08-02 18:11:28,432] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0088: [2024-08-02 18:11:28,432] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0069: [2024-08-02 18:11:28,598] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0069: [2024-08-02 18:11:28,599] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0069: [2024-08-02 18:11:28,599] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0069: [2024-08-02 18:11:28,600] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0069: [2024-08-02 18:11:28,600] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0069: [2024-08-02 18:11:28,633] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0069: [2024-08-02 18:11:28,633] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0069: [2024-08-02 18:11:28,634] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0069: [2024-08-02 18:11:28,634] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0069: [2024-08-02 18:11:28,634] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0069: [2024-08-02 18:11:28,634] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f4008140890>
g0069: [2024-08-02 18:11:28,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: [2024-08-02 18:11:28,635] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0069: [2024-08-02 18:11:28,635] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0069:     "partition_activations": false, 
g0069:     "contiguous_memory_optimization": false, 
g0069:     "cpu_checkpointing": false, 
g0069:     "number_checkpoints": null, 
g0069:     "synchronize_checkpoint_boundary": false, 
g0069:     "profile": false
g0069: }
g0069: [2024-08-02 18:11:28,635] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0069: [2024-08-02 18:11:28,636] [INFO] [config.py:983:print]   amp_enabled .................. False
g0069: [2024-08-02 18:11:28,636] [INFO] [config.py:983:print]   amp_params ................... False
g0069: [2024-08-02 18:11:28,636] [INFO] [config.py:983:print]   autotuning_config ............ {
g0069:     "enabled": false, 
g0069:     "start_step": null, 
g0069:     "end_step": null, 
g0069:     "metric_path": null, 
g0069:     "arg_mappings": null, 
g0069:     "metric": "throughput", 
g0069:     "model_info": null, 
g0069:     "results_dir": "autotuning_results", 
g0069:     "exps_dir": "autotuning_exps", 
g0069:     "overwrite": true, 
g0069:     "fast": true, 
g0069:     "start_profile_step": 3, 
g0069:     "end_profile_step": 5, 
g0069:     "tuner_type": "gridsearch", 
g0069:     "tuner_early_stopping": 5, 
g0069:     "tuner_num_trials": 50, 
g0069:     "model_info_path": null, 
g0069:     "mp_size": 1, 
g0069:     "max_train_batch_size": null, 
g0069:     "min_train_batch_size": 1, 
g0069:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0069:     "min_train_micro_batch_size_per_gpu": 1, 
g0069:     "num_tuning_micro_batch_sizes": 3
g0069: }
g0069: [2024-08-02 18:11:28,636] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0069: [2024-08-02 18:11:28,637] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0069: [2024-08-02 18:11:28,637] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0069: [2024-08-02 18:11:28,637] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0069: [2024-08-02 18:11:28,637] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f3fd146b690>
g0069: [2024-08-02 18:11:28,637] [INFO] [config.py:983:print]   communication_data_type ...... None
g0069: [2024-08-02 18:11:28,638] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0069: [2024-08-02 18:11:28,638] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0069: [2024-08-02 18:11:28,638] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0069: [2024-08-02 18:11:28,638] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0069: [2024-08-02 18:11:28,638] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0069: [2024-08-02 18:11:28,638] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0069: [2024-08-02 18:11:28,639] [INFO] [config.py:983:print]   disable_allgather ............ False
g0069: [2024-08-02 18:11:28,639] [INFO] [config.py:983:print]   dump_state ................... False
g0069: [2024-08-02 18:11:28,639] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0069: [2024-08-02 18:11:28,639] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0069: [2024-08-02 18:11:28,639] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0069: [2024-08-02 18:11:28,639] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0069: [2024-08-02 18:11:28,640] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0069: [2024-08-02 18:11:28,640] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0069: [2024-08-02 18:11:28,640] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0069: [2024-08-02 18:11:28,640] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0069: [2024-08-02 18:11:28,640] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0069: [2024-08-02 18:11:28,640] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0069: [2024-08-02 18:11:28,641] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0069:     "enabled": false, 
g0069:     "recompute_fwd_factor": 0.0, 
g0069:     "profile_step": 1, 
g0069:     "module_depth": -1, 
g0069:     "top_modules": 1, 
g0069:     "detailed": true, 
g0069:     "output_file": null
g0069: }
g0069: [2024-08-02 18:11:28,641] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0069: [2024-08-02 18:11:28,641] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0069: [2024-08-02 18:11:28,641] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0069: [2024-08-02 18:11:28,641] [INFO] [config.py:983:print]   global_rank .................. 0
g0069: [2024-08-02 18:11:28,642] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0069: [2024-08-02 18:11:28,642] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0069: [2024-08-02 18:11:28,642] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0069: [2024-08-02 18:11:28,642] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0069: [2024-08-02 18:11:28,642] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0069: [2024-08-02 18:11:28,642] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0069: [2024-08-02 18:11:28,643] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0069: [2024-08-02 18:11:28,643] [INFO] [config.py:983:print]   loss_scale ................... 0
g0069: [2024-08-02 18:11:28,643] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0069: [2024-08-02 18:11:28,643] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0069: [2024-08-02 18:11:28,643] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0069: [2024-08-02 18:11:28,643] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0069: [2024-08-02 18:11:28,644] [INFO] [config.py:983:print]   nebula_config ................ {
g0069:     "enabled": false, 
g0069:     "persistent_storage_path": null, 
g0069:     "persistent_time_interval": 100, 
g0069:     "num_of_version_in_retention": 2, 
g0069:     "enable_nebula_load": true, 
g0069:     "load_path": null
g0069: }
g0069: [2024-08-02 18:11:28,644] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0069: [2024-08-02 18:11:28,644] [INFO] [config.py:983:print]   optimizer_name ............... None
g0069: [2024-08-02 18:11:28,644] [INFO] [config.py:983:print]   optimizer_params ............. None
g0069: [2024-08-02 18:11:28,645] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0069: [2024-08-02 18:11:28,645] [INFO] [config.py:983:print]   pld_enabled .................. False
g0069: [2024-08-02 18:11:28,645] [INFO] [config.py:983:print]   pld_params ................... False
g0069: [2024-08-02 18:11:28,645] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0069: [2024-08-02 18:11:28,645] [INFO] [config.py:983:print]   scheduler_name ............... None
g0069: [2024-08-02 18:11:28,645] [INFO] [config.py:983:print]   scheduler_params ............. None
g0069: [2024-08-02 18:11:28,645] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0069: [2024-08-02 18:11:28,646] [INFO] [config.py:983:print]   sparse_attention ............. None
g0069: [2024-08-02 18:11:28,646] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0069: [2024-08-02 18:11:28,646] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0069: [2024-08-02 18:11:28,646] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0069: [2024-08-02 18:11:28,646] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0069: [2024-08-02 18:11:28,646] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0069: [2024-08-02 18:11:28,647] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0069: [2024-08-02 18:11:28,647] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0069: [2024-08-02 18:11:28,647] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0069: [2024-08-02 18:11:28,647] [INFO] [config.py:983:print]   world_size ................... 4
g0069: [2024-08-02 18:11:28,647] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0069: [2024-08-02 18:11:28,647] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0069: [2024-08-02 18:11:28,648] [INFO] [config.py:983:print]   zero_enabled ................. False
g0069: [2024-08-02 18:11:28,648] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0069: [2024-08-02 18:11:28,648] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0069: [2024-08-02 18:11:28,648] [INFO] [config.py:969:print_user_config]   json = {
g0069:     "train_batch_size": 128, 
g0069:     "train_micro_batch_size_per_gpu": 1, 
g0069:     "steps_per_print": 10, 
g0069:     "zero_optimization": {
g0069:         "stage": 0
g0069:     }, 
g0069:     "gradient_clipping": 1.0, 
g0069:     "prescale_gradients": true, 
g0069:     "fp16": {
g0069:         "enabled": true, 
g0069:         "loss_scale": 0, 
g0069:         "loss_scale_window": 500, 
g0069:         "hysteresis": 2, 
g0069:         "min_loss_scale": 1, 
g0069:         "initial_scale_power": 11
g0069:     }, 
g0069:     "wall_clock_breakdown": false
g0069: }
g0069: [2024-08-02 18:11:28,648] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0069: [2024-08-02 18:11:28,649] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0069: [2024-08-02 18:11:29,353] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0090: [2024-08-02 18:11:29,354] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0092: [2024-08-02 18:11:29,354] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0088: [2024-08-02 18:11:29,355] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0091: [2024-08-02 18:11:29,355] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0093: [2024-08-02 18:11:29,355] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0086: [2024-08-02 18:11:29,355] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0087: [2024-08-02 18:11:29,355] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0093: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0093: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0069: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0087: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0069: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0069: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0087: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0069: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0087: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0088: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0090: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0092: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0090: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0092: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0086: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0090: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0090: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0088: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0092: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0086: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0088: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0092: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0093: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0086: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0093: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0091: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0091: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0086: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0091: [2024-08-02 18:11:30,052] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0088: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0069: WARNING: could not find the metadata file /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase 
g0069:     will not load any checkpoints and will start from random
g0087: [2024-08-02 18:11:30,053] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0091: [2024-08-02 18:11:30,055] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0093: (min, max) time across ranks (ms):
g0093:     load-checkpoint ................................: (1.73, 5.01)
g0069: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-02 18:11:30 
g0069: > building train, validation, and test datasets ...
g0069:  > datasets target sizes (minimum size):
g0069:     train:      1280000000
g0069:     validation: 128012800
g0069:     test:       12800
g0069: > building train, validation, and test datasets for GPT ...
g0069: Single data path provided for train, valid & test
g0069:  > building dataset index ...
g0069:     reading sizes...
g0069:     reading pointers...
g0069:     reading document index...
g0069:     creating numpy buffer of mmap...
g0069:     creating memory view of numpy buffer...
g0069:  > finished creating indexed dataset in 0.024580 seconds
g0069:     number of documents: 2237032
g0069:  > dataset split:
g0069:     train:
g0069:      document indices in [0, 2122943) total of 2122943 documents
g0069:     validation:
g0069:      document indices in [2122943, 2234795) total of 111852 documents
g0069:     test:
g0069:      document indices in [2234795, 2237032) total of 2237 documents
g0069:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0069:  > only one epoch required, setting separate_last_epoch to False
g0069:  > elasped time to build and save doc-idx mapping (seconds): 0.082629
g0069:     using:
g0069:      number of documents:       2122943
g0069:      number of epochs:          1
g0069:      sequence length:           2048
g0069:      total number of samples:   10738038
g0069:  > elasped time to build and save sample-idx mapping (seconds): 0.430238
g0069:  > building shuffle index with split [0, 10738038) and [10738038, 10738038) ...
g0069:  > elasped time to build and save shuffle-idx mapping (seconds): 0.351920
g0069:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_doc_idx.npy
g0069:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_sample_idx.npy
g0069:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_shuffle_idx.npy
g0069:     loaded indexed file in 0.088 seconds
g0069:     total number of samples: 10738039
g0069:     total number of epochs: 1
g0069:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0069:  > last epoch number of samples (184620) is smaller than 80% of number of samples per epoch (563119), setting separate_last_epoch to True
g0069:  > elasped time to build and save doc-idx mapping (seconds): 1.548373
g0069:     using:
g0069:      number of documents:       111852
g0069:      number of epochs:          228
g0069:      sequence length:           2048
g0069:      total number of samples:   128391299
g0069:  > elasped time to build and save sample-idx mapping (seconds): 2.443902
g0069:  > building shuffle index with split [0, 127828180) and [127828180, 128391299) ...
g0069:  > elasped time to build and save shuffle-idx mapping (seconds): 7.645546
g0069:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_doc_idx.npy
g0069:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_sample_idx.npy
g0069:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_shuffle_idx.npy
g0069:     loaded indexed file in 0.018 seconds
g0069:     total number of samples: 128391300
g0069:     total number of epochs: 228
g0069:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0069:  > last epoch number of samples (5572) is smaller than 80% of number of samples per epoch (7228), setting separate_last_epoch to True
g0069:  > elasped time to build and save doc-idx mapping (seconds): 0.001766
g0069:     using:
g0069:      number of documents:       2237
g0069:      number of epochs:          2
g0069:      sequence length:           2048
g0069:      total number of samples:   14457
g0069:  > elasped time to build and save sample-idx mapping (seconds): 0.001810
g0069:  > building shuffle index with split [0, 7228) and [7228, 14457) ...
g0069:  > elasped time to build and save shuffle-idx mapping (seconds): 0.001973
g0069:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_doc_idx.npy
g0069:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_sample_idx.npy
g0069:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_shuffle_idx.npy
g0069:     loaded indexed file in 0.007 seconds
g0069:     total number of samples: 14458
g0069:     total number of epochs: 2
g0069: > finished creating GPT datasets ...
g0069: [after dataloaders are built] datetime: 2024-08-02 18:11:44 
g0069: done with setup ...
g0069: training ...
g0093: (min, max) time across ranks (ms):
g0093:     model-and-optimizer-setup ......................: (2978.45, 2986.84)
g0093:     train/valid/test-data-iterators-setup ..........: (14746.00, 14761.02)
g0069: [before the start of training step] datetime: 2024-08-02 18:11:44 
g0069: [2024-08-02 18:12:36,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.5728640000000002e-07, 1.5728640000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 10 loss: 10.5456 iter time (s): 5.105 samples/sec: 25.073
g0093:  iteration       10/10000000 | consumed samples:         1280 | consumed tokens:      2621440 | elapsed time per iteration (ms): 5141.3 | learning rate: 1.573E-07 | global batch size:   128 | lm loss: 1.055321E+01 | loss scale: 2048.0 | grad norm: 7.723 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.896 | tokens per gpu per second (tgs): 1593.359 | TFLOPs: 12.82 |
g0091: [Rank 20] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5410.0 | max reserved: 5410.0
g0086: [Rank 4] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 8986.0 | max reserved: 8986.0
g0093: [Rank 28] (after 10 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0092: [Rank 24] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 4662.0 | max reserved: 4662.0
g0069: [Rank 0] (after 10 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 11650.0 | max reserved: 11650.0
g0087: [Rank 8] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8092.0 | max reserved: 8092.0
g0088: [Rank 12] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7198.0 | max reserved: 7198.0
g0090: [Rank 16] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6304.0 | max reserved: 6304.0
g0069: [2024-08-02 18:13:20,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[3.3204906666666666e-07, 3.3204906666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 20 loss: 10.4413 iter time (s): 4.251 samples/sec: 30.114
g0093:  iteration       20/10000000 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 4416.5 | learning rate: 3.320E-07 | global batch size:   128 | lm loss: 1.049026E+01 | loss scale: 2048.0 | grad norm: 8.565 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.982 | tokens per gpu per second (tgs): 1854.871 | TFLOPs: 14.93 |
g0069: [2024-08-02 18:14:00,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[5.068117333333334e-07, 5.068117333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 30 loss: 10.1883 iter time (s): 3.950 samples/sec: 32.404
g0093:  iteration       30/10000000 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 3984.1 | learning rate: 5.068E-07 | global batch size:   128 | lm loss: 1.031588E+01 | loss scale: 2048.0 | grad norm: 11.524 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.127 | tokens per gpu per second (tgs): 2056.150 | TFLOPs: 16.55 |
g0069: [2024-08-02 18:14:40,771] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[6.815744000000001e-07, 6.815744000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 40 loss: 9.7750 iter time (s): 4.007 samples/sec: 31.941
g0093:  iteration       40/10000000 | consumed samples:         5120 | consumed tokens:     10485760 | elapsed time per iteration (ms): 4040.6 | learning rate: 6.816E-07 | global batch size:   128 | lm loss: 9.960600E+00 | loss scale: 2048.0 | grad norm: 7.314 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.678 | tokens per gpu per second (tgs): 2027.417 | TFLOPs: 16.31 |
g0069: [2024-08-02 18:15:21,471] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[8.563370666666667e-07, 8.563370666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 50 loss: 9.5010 iter time (s): 4.037 samples/sec: 31.704
g0093:  iteration       50/10000000 | consumed samples:         6400 | consumed tokens:     13107200 | elapsed time per iteration (ms): 4070.0 | learning rate: 8.563E-07 | global batch size:   128 | lm loss: 9.613245E+00 | loss scale: 2048.0 | grad norm: 4.014 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.449 | tokens per gpu per second (tgs): 2012.768 | TFLOPs: 16.20 |
g0069: [2024-08-02 18:16:02,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.0310997333333332e-06, 1.0310997333333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 60 loss: 9.3304 iter time (s): 4.115 samples/sec: 31.104
g0093:  iteration       60/10000000 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 4148.3 | learning rate: 1.031E-06 | global batch size:   128 | lm loss: 9.396758E+00 | loss scale: 2048.0 | grad norm: 2.404 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.856 | tokens per gpu per second (tgs): 1974.788 | TFLOPs: 15.89 |
g0069: [2024-08-02 18:16:42,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.2058624000000002e-06, 1.2058624000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 70 loss: 9.2303 iter time (s): 3.885 samples/sec: 32.947
g0093:  iteration       70/10000000 | consumed samples:         8960 | consumed tokens:     18350080 | elapsed time per iteration (ms): 3918.0 | learning rate: 1.206E-06 | global batch size:   128 | lm loss: 9.270943E+00 | loss scale: 2048.0 | grad norm: 2.132 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.670 | tokens per gpu per second (tgs): 2090.874 | TFLOPs: 16.83 |
g0069: [2024-08-02 18:17:22,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.3806250666666669e-06, 1.3806250666666669e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 80 loss: 9.1487 iter time (s): 4.050 samples/sec: 31.607
g0093:  iteration       80/10000000 | consumed samples:        10240 | consumed tokens:     20971520 | elapsed time per iteration (ms): 4083.4 | learning rate: 1.381E-06 | global batch size:   128 | lm loss: 9.189739E+00 | loss scale: 2048.0 | grad norm: 2.052 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.347 | tokens per gpu per second (tgs): 2006.185 | TFLOPs: 16.14 |
g0069: [2024-08-02 18:18:03,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.5553877333333333e-06, 1.5553877333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 90 loss: 9.0839 iter time (s): 4.060 samples/sec: 31.527
g0093:  iteration       90/10000000 | consumed samples:        11520 | consumed tokens:     23592960 | elapsed time per iteration (ms): 4092.7 | learning rate: 1.555E-06 | global batch size:   128 | lm loss: 9.109593E+00 | loss scale: 2048.0 | grad norm: 1.922 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.275 | tokens per gpu per second (tgs): 2001.620 | TFLOPs: 16.11 |
g0069: [2024-08-02 18:18:43,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1.7301504e-06, 1.7301504e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 100 loss: 9.0077 iter time (s): 3.962 samples/sec: 32.305
g0093:  iteration      100/10000000 | consumed samples:        12800 | consumed tokens:     26214400 | elapsed time per iteration (ms): 3994.9 | learning rate: 1.730E-06 | global batch size:   128 | lm loss: 9.042211E+00 | loss scale: 2048.0 | grad norm: 1.899 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.041 | tokens per gpu per second (tgs): 2050.624 | TFLOPs: 16.50 |
g0069: [2024-08-02 18:19:24,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1.9049130666666667e-06, 1.9049130666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 110 loss: 8.9553 iter time (s): 3.993 samples/sec: 32.059
g0093:  iteration      110/10000000 | consumed samples:        14080 | consumed tokens:     28835840 | elapsed time per iteration (ms): 4025.1 | learning rate: 1.905E-06 | global batch size:   128 | lm loss: 8.983637E+00 | loss scale: 2048.0 | grad norm: 1.876 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.800 | tokens per gpu per second (tgs): 2035.210 | TFLOPs: 16.38 |
g0069: [2024-08-02 18:20:04,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[2.0796757333333334e-06, 2.0796757333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 120 loss: 8.8822 iter time (s): 4.022 samples/sec: 31.827
g0093:  iteration      120/10000000 | consumed samples:        15360 | consumed tokens:     31457280 | elapsed time per iteration (ms): 4054.4 | learning rate: 2.080E-06 | global batch size:   128 | lm loss: 8.913886E+00 | loss scale: 2048.0 | grad norm: 1.841 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.571 | tokens per gpu per second (tgs): 2020.539 | TFLOPs: 16.26 |
g0069: [2024-08-02 18:20:45,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[2.2544384e-06, 2.2544384e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 130 loss: 8.8180 iter time (s): 4.009 samples/sec: 31.930
g0093:  iteration      130/10000000 | consumed samples:        16640 | consumed tokens:     34078720 | elapsed time per iteration (ms): 4042.1 | learning rate: 2.254E-06 | global batch size:   128 | lm loss: 8.850151E+00 | loss scale: 2048.0 | grad norm: 1.834 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.666 | tokens per gpu per second (tgs): 2026.645 | TFLOPs: 16.31 |
g0069: [2024-08-02 18:21:26,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[2.429201066666667e-06, 2.429201066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 140 loss: 8.7475 iter time (s): 4.096 samples/sec: 31.249
g0093:  iteration      140/10000000 | consumed samples:        17920 | consumed tokens:     36700160 | elapsed time per iteration (ms): 4128.7 | learning rate: 2.429E-06 | global batch size:   128 | lm loss: 8.776087E+00 | loss scale: 2048.0 | grad norm: 1.810 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.002 | tokens per gpu per second (tgs): 1984.155 | TFLOPs: 15.97 |
g0069: [2024-08-02 18:22:06,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[2.6039637333333333e-06, 2.6039637333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 150 loss: 8.6502 iter time (s): 4.029 samples/sec: 31.768
g0093:  iteration      150/10000000 | consumed samples:        19200 | consumed tokens:     39321600 | elapsed time per iteration (ms): 4062.1 | learning rate: 2.604E-06 | global batch size:   128 | lm loss: 8.696147E+00 | loss scale: 2048.0 | grad norm: 1.814 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.511 | tokens per gpu per second (tgs): 2016.696 | TFLOPs: 16.23 |
g0069: [2024-08-02 18:22:48,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[2.7787264000000002e-06, 2.7787264000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 160 loss: 8.5748 iter time (s): 4.078 samples/sec: 31.390
g0093:  iteration      160/10000000 | consumed samples:        20480 | consumed tokens:     41943040 | elapsed time per iteration (ms): 4110.3 | learning rate: 2.779E-06 | global batch size:   128 | lm loss: 8.614978E+00 | loss scale: 2048.0 | grad norm: 1.812 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.141 | tokens per gpu per second (tgs): 1993.033 | TFLOPs: 16.04 |
g0069: [2024-08-02 18:23:32,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[2.953489066666667e-06, 2.953489066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 170 loss: 8.4802 iter time (s): 4.430 samples/sec: 28.895
g0093:  iteration      170/10000000 | consumed samples:        21760 | consumed tokens:     44564480 | elapsed time per iteration (ms): 4462.5 | learning rate: 2.953E-06 | global batch size:   128 | lm loss: 8.526006E+00 | loss scale: 2048.0 | grad norm: 1.771 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.684 | tokens per gpu per second (tgs): 1835.760 | TFLOPs: 14.77 |
g0069: [2024-08-02 18:24:15,761] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[3.128251733333333e-06, 3.128251733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 180 loss: 8.3891 iter time (s): 4.273 samples/sec: 29.954
g0093:  iteration      180/10000000 | consumed samples:        23040 | consumed tokens:     47185920 | elapsed time per iteration (ms): 4306.3 | learning rate: 3.128E-06 | global batch size:   128 | lm loss: 8.430720E+00 | loss scale: 2048.0 | grad norm: 1.825 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.724 | tokens per gpu per second (tgs): 1902.340 | TFLOPs: 15.31 |
g0069: [2024-08-02 18:24:58,357] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[3.3030144e-06, 3.3030144e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 190 loss: 8.2825 iter time (s): 4.226 samples/sec: 30.287
g0093:  iteration      190/10000000 | consumed samples:        24320 | consumed tokens:     49807360 | elapsed time per iteration (ms): 4259.6 | learning rate: 3.303E-06 | global batch size:   128 | lm loss: 8.332027E+00 | loss scale: 2048.0 | grad norm: 1.698 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.050 | tokens per gpu per second (tgs): 1923.187 | TFLOPs: 15.48 |
g0069: [2024-08-02 18:25:39,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[3.477777066666667e-06, 3.477777066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 200 loss: 8.1756 iter time (s): 4.087 samples/sec: 31.317
g0093:  iteration      200/10000000 | consumed samples:        25600 | consumed tokens:     52428800 | elapsed time per iteration (ms): 4119.9 | learning rate: 3.478E-06 | global batch size:   128 | lm loss: 8.225684E+00 | loss scale: 2048.0 | grad norm: 1.665 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.068 | tokens per gpu per second (tgs): 1988.379 | TFLOPs: 16.00 |
g0069: [2024-08-02 18:26:20,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[3.6525397333333335e-06, 3.6525397333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 210 loss: 8.0789 iter time (s): 4.078 samples/sec: 31.389
g0093:  iteration      210/10000000 | consumed samples:        26880 | consumed tokens:     55050240 | elapsed time per iteration (ms): 4111.3 | learning rate: 3.653E-06 | global batch size:   128 | lm loss: 8.119110E+00 | loss scale: 2048.0 | grad norm: 1.667 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.134 | tokens per gpu per second (tgs): 1992.576 | TFLOPs: 16.03 |
g0069: [2024-08-02 18:27:02,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[3.8273024e-06, 3.8273024e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 220 loss: 7.9520 iter time (s): 4.129 samples/sec: 30.997
g0093:  iteration      220/10000000 | consumed samples:        28160 | consumed tokens:     57671680 | elapsed time per iteration (ms): 4162.1 | learning rate: 3.827E-06 | global batch size:   128 | lm loss: 8.007580E+00 | loss scale: 2048.0 | grad norm: 1.613 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.754 | tokens per gpu per second (tgs): 1968.236 | TFLOPs: 15.84 |
g0069: [2024-08-02 18:27:42,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[4.002065066666667e-06, 4.002065066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 230 loss: 7.8402 iter time (s): 4.013 samples/sec: 31.892
g0093:  iteration      230/10000000 | consumed samples:        29440 | consumed tokens:     60293120 | elapsed time per iteration (ms): 4046.0 | learning rate: 4.002E-06 | global batch size:   128 | lm loss: 7.894682E+00 | loss scale: 2048.0 | grad norm: 1.538 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.636 | tokens per gpu per second (tgs): 2024.727 | TFLOPs: 16.29 |
g0069: [2024-08-02 18:28:22,295] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[4.176827733333334e-06, 4.176827733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 240 loss: 7.7181 iter time (s): 3.922 samples/sec: 32.636
g0093:  iteration      240/10000000 | consumed samples:        30720 | consumed tokens:     62914560 | elapsed time per iteration (ms): 3954.5 | learning rate: 4.177E-06 | global batch size:   128 | lm loss: 7.777229E+00 | loss scale: 2048.0 | grad norm: 1.509 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.368 | tokens per gpu per second (tgs): 2071.556 | TFLOPs: 16.67 |
g0069: [2024-08-02 18:29:04,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[4.351590400000001e-06, 4.351590400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 250 loss: 7.6102 iter time (s): 4.207 samples/sec: 30.428
g0093:  iteration      250/10000000 | consumed samples:        32000 | consumed tokens:     65536000 | elapsed time per iteration (ms): 4239.4 | learning rate: 4.352E-06 | global batch size:   128 | lm loss: 7.661590E+00 | loss scale: 2048.0 | grad norm: 1.429 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.193 | tokens per gpu per second (tgs): 1932.345 | TFLOPs: 15.55 |
g0069: [2024-08-02 18:29:45,224] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[4.526353066666667e-06, 4.526353066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 260 loss: 7.4932 iter time (s): 4.020 samples/sec: 31.844
g0093:  iteration      260/10000000 | consumed samples:        33280 | consumed tokens:     68157440 | elapsed time per iteration (ms): 4053.4 | learning rate: 4.526E-06 | global batch size:   128 | lm loss: 7.544064E+00 | loss scale: 2048.0 | grad norm: 1.371 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.578 | tokens per gpu per second (tgs): 2021.015 | TFLOPs: 16.26 |
g0069: [2024-08-02 18:30:26,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[4.701115733333334e-06, 4.701115733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 270 loss: 7.3702 iter time (s): 4.121 samples/sec: 31.064
g0093:  iteration      270/10000000 | consumed samples:        34560 | consumed tokens:     70778880 | elapsed time per iteration (ms): 4153.4 | learning rate: 4.701E-06 | global batch size:   128 | lm loss: 7.427383E+00 | loss scale: 2048.0 | grad norm: 1.319 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.818 | tokens per gpu per second (tgs): 1972.337 | TFLOPs: 15.87 |
g0069: [2024-08-02 18:31:07,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[4.875878400000001e-06, 4.875878400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 280 loss: 7.2713 iter time (s): 4.018 samples/sec: 31.855
g0093:  iteration      280/10000000 | consumed samples:        35840 | consumed tokens:     73400320 | elapsed time per iteration (ms): 4050.7 | learning rate: 4.876E-06 | global batch size:   128 | lm loss: 7.316685E+00 | loss scale: 2048.0 | grad norm: 1.275 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.600 | tokens per gpu per second (tgs): 2022.369 | TFLOPs: 16.27 |
g0069: [2024-08-02 18:31:47,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[5.050641066666667e-06, 5.050641066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 290 loss: 7.1650 iter time (s): 3.998 samples/sec: 32.017
g0093:  iteration      290/10000000 | consumed samples:        37120 | consumed tokens:     76021760 | elapsed time per iteration (ms): 4030.6 | learning rate: 5.051E-06 | global batch size:   128 | lm loss: 7.209647E+00 | loss scale: 2048.0 | grad norm: 1.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.757 | tokens per gpu per second (tgs): 2032.450 | TFLOPs: 16.36 |
g0069: [2024-08-02 18:32:28,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[5.225403733333334e-06, 5.225403733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 300 loss: 7.0709 iter time (s): 4.084 samples/sec: 31.343
g0093:  iteration      300/10000000 | consumed samples:        38400 | consumed tokens:     78643200 | elapsed time per iteration (ms): 4116.8 | learning rate: 5.225E-06 | global batch size:   128 | lm loss: 7.112807E+00 | loss scale: 2048.0 | grad norm: 1.115 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.092 | tokens per gpu per second (tgs): 1989.913 | TFLOPs: 16.01 |
g0069: [2024-08-02 18:33:09,998] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[5.4001664e-06, 5.4001664e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 310 loss: 6.9593 iter time (s): 4.093 samples/sec: 31.274
g0093:  iteration      310/10000000 | consumed samples:        39680 | consumed tokens:     81264640 | elapsed time per iteration (ms): 4125.9 | learning rate: 5.400E-06 | global batch size:   128 | lm loss: 7.006969E+00 | loss scale: 2048.0 | grad norm: 1.068 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.023 | tokens per gpu per second (tgs): 1985.503 | TFLOPs: 15.98 |
g0069: [2024-08-02 18:33:52,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[5.574929066666667e-06, 5.574929066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 320 loss: 6.8832 iter time (s): 4.183 samples/sec: 30.602
g0093:  iteration      320/10000000 | consumed samples:        40960 | consumed tokens:     83886080 | elapsed time per iteration (ms): 4215.5 | learning rate: 5.575E-06 | global batch size:   128 | lm loss: 6.918217E+00 | loss scale: 2048.0 | grad norm: 0.992 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.364 | tokens per gpu per second (tgs): 1943.311 | TFLOPs: 15.64 |
g0069: [2024-08-02 18:34:31,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[5.7496917333333335e-06, 5.7496917333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 330 loss: 6.8080 iter time (s): 3.935 samples/sec: 32.530
g0093:  iteration      330/10000000 | consumed samples:        42240 | consumed tokens:     86507520 | elapsed time per iteration (ms): 3967.3 | learning rate: 5.750E-06 | global batch size:   128 | lm loss: 6.834657E+00 | loss scale: 2048.0 | grad norm: 0.869 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.264 | tokens per gpu per second (tgs): 2064.896 | TFLOPs: 16.62 |
g0069: [2024-08-02 18:35:14,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[5.9244543999999995e-06, 5.9244543999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 340 loss: 6.7251 iter time (s): 4.203 samples/sec: 30.452
g0093:  iteration      340/10000000 | consumed samples:        43520 | consumed tokens:     89128960 | elapsed time per iteration (ms): 4239.2 | learning rate: 5.924E-06 | global batch size:   128 | lm loss: 6.758298E+00 | loss scale: 2048.0 | grad norm: 0.809 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.195 | tokens per gpu per second (tgs): 1932.456 | TFLOPs: 15.55 |
g0069: [2024-08-02 18:35:56,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[6.0992170666666664e-06, 6.0992170666666664e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 350 loss: 6.6623 iter time (s): 4.146 samples/sec: 30.875
g0093:  iteration      350/10000000 | consumed samples:        44800 | consumed tokens:     91750400 | elapsed time per iteration (ms): 4178.4 | learning rate: 6.099E-06 | global batch size:   128 | lm loss: 6.697206E+00 | loss scale: 2048.0 | grad norm: 1.031 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.634 | tokens per gpu per second (tgs): 1960.545 | TFLOPs: 15.78 |
g0069: [2024-08-02 18:36:37,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[6.273979733333333e-06, 6.273979733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 360 loss: 6.6200 iter time (s): 4.165 samples/sec: 30.734
g0093:  iteration      360/10000000 | consumed samples:        46080 | consumed tokens:     94371840 | elapsed time per iteration (ms): 4197.9 | learning rate: 6.274E-06 | global batch size:   128 | lm loss: 6.642068E+00 | loss scale: 2048.0 | grad norm: 0.855 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.491 | tokens per gpu per second (tgs): 1951.451 | TFLOPs: 15.70 |
g0069: [2024-08-02 18:37:18,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[6.4487424e-06, 6.4487424e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 370 loss: 6.5619 iter time (s): 4.021 samples/sec: 31.837
g0093:  iteration      370/10000000 | consumed samples:        47360 | consumed tokens:     96993280 | elapsed time per iteration (ms): 4053.6 | learning rate: 6.449E-06 | global batch size:   128 | lm loss: 6.588490E+00 | loss scale: 2048.0 | grad norm: 0.651 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.577 | tokens per gpu per second (tgs): 2020.910 | TFLOPs: 16.26 |
g0069: [2024-08-02 18:38:00,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[6.623505066666667e-06, 6.623505066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 380 loss: 6.5221 iter time (s): 4.214 samples/sec: 30.377
g0093:  iteration      380/10000000 | consumed samples:        48640 | consumed tokens:     99614720 | elapsed time per iteration (ms): 4246.3 | learning rate: 6.624E-06 | global batch size:   128 | lm loss: 6.549710E+00 | loss scale: 2048.0 | grad norm: 0.623 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.144 | tokens per gpu per second (tgs): 1929.190 | TFLOPs: 15.52 |
g0069: [2024-08-02 18:38:42,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[6.798267733333334e-06, 6.798267733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 390 loss: 6.4865 iter time (s): 4.101 samples/sec: 31.213
g0093:  iteration      390/10000000 | consumed samples:        49920 | consumed tokens:    102236160 | elapsed time per iteration (ms): 4133.4 | learning rate: 6.798E-06 | global batch size:   128 | lm loss: 6.510976E+00 | loss scale: 2048.0 | grad norm: 0.674 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.967 | tokens per gpu per second (tgs): 1981.909 | TFLOPs: 15.95 |
g0069: [2024-08-02 18:39:25,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[6.973030400000001e-06, 6.973030400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 400 loss: 6.4498 iter time (s): 4.323 samples/sec: 29.611
g0093:  iteration      400/10000000 | consumed samples:        51200 | consumed tokens:    104857600 | elapsed time per iteration (ms): 4355.3 | learning rate: 6.973E-06 | global batch size:   128 | lm loss: 6.473724E+00 | loss scale: 2048.0 | grad norm: 0.577 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.390 | tokens per gpu per second (tgs): 1880.932 | TFLOPs: 15.14 |
g0069: [2024-08-02 18:40:08,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[7.147793066666666e-06, 7.147793066666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 410 loss: 6.4397 iter time (s): 4.200 samples/sec: 30.477
g0093:  iteration      410/10000000 | consumed samples:        52480 | consumed tokens:    107479040 | elapsed time per iteration (ms): 4232.5 | learning rate: 7.148E-06 | global batch size:   128 | lm loss: 6.450391E+00 | loss scale: 2048.0 | grad norm: 0.566 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.242 | tokens per gpu per second (tgs): 1935.505 | TFLOPs: 15.58 |
g0069: [2024-08-02 18:40:48,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[7.322555733333333e-06, 7.322555733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 420 loss: 6.4356 iter time (s): 4.044 samples/sec: 31.651
g0093:  iteration      420/10000000 | consumed samples:        53760 | consumed tokens:    110100480 | elapsed time per iteration (ms): 4077.2 | learning rate: 7.323E-06 | global batch size:   128 | lm loss: 6.427063E+00 | loss scale: 2048.0 | grad norm: 0.578 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.394 | tokens per gpu per second (tgs): 2009.228 | TFLOPs: 16.17 |
g0069: [2024-08-02 18:41:33,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[7.4973184e-06, 7.4973184e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 430 loss: 6.3948 iter time (s): 4.465 samples/sec: 28.666
g0093:  iteration      430/10000000 | consumed samples:        55040 | consumed tokens:    112721920 | elapsed time per iteration (ms): 4498.2 | learning rate: 7.497E-06 | global batch size:   128 | lm loss: 6.409281E+00 | loss scale: 2048.0 | grad norm: 0.580 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.456 | tokens per gpu per second (tgs): 1821.185 | TFLOPs: 14.66 |
g0069: [2024-08-02 18:42:16,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[7.672081066666667e-06, 7.672081066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 440 loss: 6.3868 iter time (s): 4.186 samples/sec: 30.581
g0093:  iteration      440/10000000 | consumed samples:        56320 | consumed tokens:    115343360 | elapsed time per iteration (ms): 4218.6 | learning rate: 7.672E-06 | global batch size:   128 | lm loss: 6.393583E+00 | loss scale: 2048.0 | grad norm: 0.561 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.342 | tokens per gpu per second (tgs): 1941.883 | TFLOPs: 15.63 |
g0069: [2024-08-02 18:42:58,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[7.846843733333333e-06, 7.846843733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 450 loss: 6.3613 iter time (s): 4.236 samples/sec: 30.214
g0093:  iteration      450/10000000 | consumed samples:        57600 | consumed tokens:    117964800 | elapsed time per iteration (ms): 4269.4 | learning rate: 7.847E-06 | global batch size:   128 | lm loss: 6.371356E+00 | loss scale: 2048.0 | grad norm: 0.556 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.981 | tokens per gpu per second (tgs): 1918.784 | TFLOPs: 15.44 |
g0069: [2024-08-02 18:43:39,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[8.0216064e-06, 8.0216064e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 460 loss: 6.3588 iter time (s): 4.072 samples/sec: 31.436
g0093:  iteration      460/10000000 | consumed samples:        58880 | consumed tokens:    120586240 | elapsed time per iteration (ms): 4104.5 | learning rate: 8.022E-06 | global batch size:   128 | lm loss: 6.365994E+00 | loss scale: 2048.0 | grad norm: 0.720 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.185 | tokens per gpu per second (tgs): 1995.865 | TFLOPs: 16.06 |
g0069: [2024-08-02 18:44:22,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[8.196369066666667e-06, 8.196369066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 470 loss: 6.3528 iter time (s): 4.180 samples/sec: 30.623
g0093:  iteration      470/10000000 | consumed samples:        60160 | consumed tokens:    123207680 | elapsed time per iteration (ms): 4213.0 | learning rate: 8.196E-06 | global batch size:   128 | lm loss: 6.356299E+00 | loss scale: 2048.0 | grad norm: 0.538 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.382 | tokens per gpu per second (tgs): 1944.470 | TFLOPs: 15.65 |
g0069: [2024-08-02 18:45:03,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[8.371131733333335e-06, 8.371131733333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 480 loss: 6.3461 iter time (s): 4.092 samples/sec: 31.279
g0093:  iteration      480/10000000 | consumed samples:        61440 | consumed tokens:    125829120 | elapsed time per iteration (ms): 4125.1 | learning rate: 8.371E-06 | global batch size:   128 | lm loss: 6.348703E+00 | loss scale: 2048.0 | grad norm: 0.668 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.029 | tokens per gpu per second (tgs): 1985.883 | TFLOPs: 15.98 |
g0069: [2024-08-02 18:45:44,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[8.5458944e-06, 8.5458944e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 490 loss: 6.3402 iter time (s): 4.112 samples/sec: 31.130
g0093:  iteration      490/10000000 | consumed samples:        62720 | consumed tokens:    128450560 | elapsed time per iteration (ms): 4144.7 | learning rate: 8.546E-06 | global batch size:   128 | lm loss: 6.341891E+00 | loss scale: 2048.0 | grad norm: 0.729 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.883 | tokens per gpu per second (tgs): 1976.504 | TFLOPs: 15.91 |
g0069: [2024-08-02 18:46:26,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[8.720657066666667e-06, 8.720657066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 500 loss: 6.3362 iter time (s): 4.143 samples/sec: 30.894
g0093:  iteration      500/10000000 | consumed samples:        64000 | consumed tokens:    131072000 | elapsed time per iteration (ms): 4176.5 | learning rate: 8.721E-06 | global batch size:   128 | lm loss: 6.335664E+00 | loss scale: 2048.0 | grad norm: 0.609 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.648 | tokens per gpu per second (tgs): 1961.469 | TFLOPs: 15.78 |
g0088: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0069: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0088: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0090: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0088: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0088: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0090: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0091: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0090: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0092: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0087: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0092: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0092: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0092: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0091: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0093: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0093: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0092: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0090: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0087: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0091: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0086: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0091: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0086: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0086: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0093: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0069: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0069: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0069: [2024-08-02 18:46:30,649] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0069: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0087: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0093: [2024-08-02 18:46:30,650] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0069: [2024-08-02 18:47:08,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[8.895419733333333e-06, 8.895419733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 510 loss: 6.3324 iter time (s): 4.141 samples/sec: 30.913
g0093:  iteration      510/10000000 | consumed samples:        65280 | consumed tokens:    133693440 | elapsed time per iteration (ms): 4174.4 | learning rate: 8.895E-06 | global batch size:   128 | lm loss: 6.324387E+00 | loss scale: 4096.0 | grad norm: 0.749 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.663 | tokens per gpu per second (tgs): 1962.441 | TFLOPs: 15.79 |
g0069: [2024-08-02 18:47:50,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[9.0701824e-06, 9.0701824e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 520 loss: 6.3271 iter time (s): 4.152 samples/sec: 30.832
g0093:  iteration      520/10000000 | consumed samples:        66560 | consumed tokens:    136314880 | elapsed time per iteration (ms): 4185.1 | learning rate: 9.070E-06 | global batch size:   128 | lm loss: 6.326398E+00 | loss scale: 4096.0 | grad norm: 0.691 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.584 | tokens per gpu per second (tgs): 1957.400 | TFLOPs: 15.75 |
g0069: [2024-08-02 18:48:32,295] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[9.244945066666667e-06, 9.244945066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 530 loss: 6.3058 iter time (s): 4.191 samples/sec: 30.544
g0093:  iteration      530/10000000 | consumed samples:        67840 | consumed tokens:    138936320 | elapsed time per iteration (ms): 4223.5 | learning rate: 9.245E-06 | global batch size:   128 | lm loss: 6.315108E+00 | loss scale: 4096.0 | grad norm: 0.620 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.306 | tokens per gpu per second (tgs): 1939.611 | TFLOPs: 15.61 |
g0069: [2024-08-02 18:49:13,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[9.419707733333334e-06, 9.419707733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 540 loss: 6.3194 iter time (s): 4.070 samples/sec: 31.447
g0093:  iteration      540/10000000 | consumed samples:        69120 | consumed tokens:    141557760 | elapsed time per iteration (ms): 4103.1 | learning rate: 9.420E-06 | global batch size:   128 | lm loss: 6.318205E+00 | loss scale: 4096.0 | grad norm: 1.423 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.196 | tokens per gpu per second (tgs): 1996.556 | TFLOPs: 16.07 |
g0069: [2024-08-02 18:49:53,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[9.5944704e-06, 9.5944704e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 550 loss: 6.3078 iter time (s): 3.938 samples/sec: 32.503
g0093:  iteration      550/10000000 | consumed samples:        70400 | consumed tokens:    144179200 | elapsed time per iteration (ms): 3971.0 | learning rate: 9.594E-06 | global batch size:   128 | lm loss: 6.310885E+00 | loss scale: 4096.0 | grad norm: 0.642 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.233 | tokens per gpu per second (tgs): 2062.932 | TFLOPs: 16.60 |
g0069: [2024-08-02 18:50:33,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[9.769233066666668e-06, 9.769233066666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 560 loss: 6.2993 iter time (s): 4.012 samples/sec: 31.908
g0093:  iteration      560/10000000 | consumed samples:        71680 | consumed tokens:    146800640 | elapsed time per iteration (ms): 4045.2 | learning rate: 9.769E-06 | global batch size:   128 | lm loss: 6.308733E+00 | loss scale: 4096.0 | grad norm: 0.662 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.642 | tokens per gpu per second (tgs): 2025.105 | TFLOPs: 16.30 |
g0069: [2024-08-02 18:51:15,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[9.943995733333334e-06, 9.943995733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 570 loss: 6.2978 iter time (s): 4.175 samples/sec: 30.655
g0093:  iteration      570/10000000 | consumed samples:        72960 | consumed tokens:    149422080 | elapsed time per iteration (ms): 4208.2 | learning rate: 9.944E-06 | global batch size:   128 | lm loss: 6.301753E+00 | loss scale: 4096.0 | grad norm: 0.656 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.417 | tokens per gpu per second (tgs): 1946.679 | TFLOPs: 15.67 |
g0069: [2024-08-02 18:51:58,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[1.01187584e-05, 1.01187584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 580 loss: 6.2944 iter time (s): 4.240 samples/sec: 30.192
g0093:  iteration      580/10000000 | consumed samples:        74240 | consumed tokens:    152043520 | elapsed time per iteration (ms): 4271.9 | learning rate: 1.012E-05 | global batch size:   128 | lm loss: 6.296613E+00 | loss scale: 4096.0 | grad norm: 0.650 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.963 | tokens per gpu per second (tgs): 1917.626 | TFLOPs: 15.43 |
g0069: [2024-08-02 18:52:41,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[1.0293521066666666e-05, 1.0293521066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 590 loss: 6.2988 iter time (s): 4.322 samples/sec: 29.616
g0093:  iteration      590/10000000 | consumed samples:        75520 | consumed tokens:    154664960 | elapsed time per iteration (ms): 4355.0 | learning rate: 1.029E-05 | global batch size:   128 | lm loss: 6.295020E+00 | loss scale: 4096.0 | grad norm: 0.686 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.392 | tokens per gpu per second (tgs): 1881.062 | TFLOPs: 15.14 |
g0069: [2024-08-02 18:53:23,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[1.0468283733333334e-05, 1.0468283733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 600 loss: 6.2889 iter time (s): 4.097 samples/sec: 31.242
g0093:  iteration      600/10000000 | consumed samples:        76800 | consumed tokens:    157286400 | elapsed time per iteration (ms): 4130.3 | learning rate: 1.047E-05 | global batch size:   128 | lm loss: 6.292960E+00 | loss scale: 4096.0 | grad norm: 1.369 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.991 | tokens per gpu per second (tgs): 1983.410 | TFLOPs: 15.96 |
g0069: [2024-08-02 18:54:03,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[1.06430464e-05, 1.06430464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 610 loss: 6.3044 iter time (s): 3.970 samples/sec: 32.238
g0093:  iteration      610/10000000 | consumed samples:        78080 | consumed tokens:    159907840 | elapsed time per iteration (ms): 4003.5 | learning rate: 1.064E-05 | global batch size:   128 | lm loss: 6.293594E+00 | loss scale: 4096.0 | grad norm: 0.860 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.972 | tokens per gpu per second (tgs): 2046.221 | TFLOPs: 16.47 |
g0069: [2024-08-02 18:54:46,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[1.0817809066666668e-05, 1.0817809066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 620 loss: 6.2923 iter time (s): 4.256 samples/sec: 30.074
g0093:  iteration      620/10000000 | consumed samples:        79360 | consumed tokens:    162529280 | elapsed time per iteration (ms): 4288.9 | learning rate: 1.082E-05 | global batch size:   128 | lm loss: 6.287854E+00 | loss scale: 4096.0 | grad norm: 0.720 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.844 | tokens per gpu per second (tgs): 1910.031 | TFLOPs: 15.37 |
g0069: [2024-08-02 18:55:28,858] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[1.0992571733333332e-05, 1.0992571733333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 630 loss: 6.2838 iter time (s): 4.246 samples/sec: 30.144
g0093:  iteration      630/10000000 | consumed samples:        80640 | consumed tokens:    165150720 | elapsed time per iteration (ms): 4279.2 | learning rate: 1.099E-05 | global batch size:   128 | lm loss: 6.277835E+00 | loss scale: 4096.0 | grad norm: 1.026 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.912 | tokens per gpu per second (tgs): 1914.393 | TFLOPs: 15.41 |
g0069: [2024-08-02 18:56:10,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[1.11673344e-05, 1.11673344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 640 loss: 6.2692 iter time (s): 4.159 samples/sec: 30.775
g0093:  iteration      640/10000000 | consumed samples:        81920 | consumed tokens:    167772160 | elapsed time per iteration (ms): 4192.7 | learning rate: 1.117E-05 | global batch size:   128 | lm loss: 6.275071E+00 | loss scale: 4096.0 | grad norm: 0.711 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.529 | tokens per gpu per second (tgs): 1953.867 | TFLOPs: 15.72 |
g0069: [2024-08-02 18:56:52,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[1.1342097066666666e-05, 1.1342097066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 650 loss: 6.2751 iter time (s): 4.185 samples/sec: 30.587
g0093:  iteration      650/10000000 | consumed samples:        83200 | consumed tokens:    170393600 | elapsed time per iteration (ms): 4217.9 | learning rate: 1.134E-05 | global batch size:   128 | lm loss: 6.272303E+00 | loss scale: 4096.0 | grad norm: 0.815 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.347 | tokens per gpu per second (tgs): 1942.193 | TFLOPs: 15.63 |
g0069: [2024-08-02 18:57:36,514] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[1.1516859733333334e-05, 1.1516859733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 660 loss: 6.2747 iter time (s): 4.322 samples/sec: 29.616
g0093:  iteration      660/10000000 | consumed samples:        84480 | consumed tokens:    173015040 | elapsed time per iteration (ms): 4354.7 | learning rate: 1.152E-05 | global batch size:   128 | lm loss: 6.270369E+00 | loss scale: 4096.0 | grad norm: 0.721 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.394 | tokens per gpu per second (tgs): 1881.187 | TFLOPs: 15.14 |
g0069: [2024-08-02 18:58:20,775] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[1.16916224e-05, 1.16916224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 670 loss: 6.2659 iter time (s): 4.394 samples/sec: 29.134
g0093:  iteration      670/10000000 | consumed samples:        85760 | consumed tokens:    175636480 | elapsed time per iteration (ms): 4426.1 | learning rate: 1.169E-05 | global batch size:   128 | lm loss: 6.267376E+00 | loss scale: 4096.0 | grad norm: 1.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.920 | tokens per gpu per second (tgs): 1850.855 | TFLOPs: 14.89 |
g0069: [2024-08-02 18:59:04,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[1.1866385066666668e-05, 1.1866385066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 680 loss: 6.2719 iter time (s): 4.386 samples/sec: 29.183
g0093:  iteration      680/10000000 | consumed samples:        87040 | consumed tokens:    178257920 | elapsed time per iteration (ms): 4419.1 | learning rate: 1.187E-05 | global batch size:   128 | lm loss: 6.261387E+00 | loss scale: 4096.0 | grad norm: 0.933 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.965 | tokens per gpu per second (tgs): 1853.757 | TFLOPs: 14.92 |
g0069: [2024-08-02 18:59:50,010] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[1.2041147733333334e-05, 1.2041147733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 690 loss: 6.2532 iter time (s): 4.472 samples/sec: 28.624
g0093:  iteration      690/10000000 | consumed samples:        88320 | consumed tokens:    180879360 | elapsed time per iteration (ms): 4504.3 | learning rate: 1.204E-05 | global batch size:   128 | lm loss: 6.254750E+00 | loss scale: 4096.0 | grad norm: 1.117 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.418 | tokens per gpu per second (tgs): 1818.720 | TFLOPs: 14.64 |
g0069: [2024-08-02 19:00:34,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[1.22159104e-05, 1.22159104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 700 loss: 6.2520 iter time (s): 4.379 samples/sec: 29.231
g0093:  iteration      700/10000000 | consumed samples:        89600 | consumed tokens:    183500800 | elapsed time per iteration (ms): 4411.8 | learning rate: 1.222E-05 | global batch size:   128 | lm loss: 6.255553E+00 | loss scale: 4096.0 | grad norm: 2.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.013 | tokens per gpu per second (tgs): 1856.844 | TFLOPs: 14.94 |
g0069: [2024-08-02 19:01:18,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[1.2390673066666668e-05, 1.2390673066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 710 loss: 6.2367 iter time (s): 4.451 samples/sec: 28.756
g0093:  iteration      710/10000000 | consumed samples:        90880 | consumed tokens:    186122240 | elapsed time per iteration (ms): 4485.4 | learning rate: 1.239E-05 | global batch size:   128 | lm loss: 6.249882E+00 | loss scale: 4096.0 | grad norm: 1.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.537 | tokens per gpu per second (tgs): 1826.351 | TFLOPs: 14.70 |
g0069: [2024-08-02 19:02:02,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[1.2565435733333334e-05, 1.2565435733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 720 loss: 6.2542 iter time (s): 4.270 samples/sec: 29.977
g0093:  iteration      720/10000000 | consumed samples:        92160 | consumed tokens:    188743680 | elapsed time per iteration (ms): 4302.6 | learning rate: 1.257E-05 | global batch size:   128 | lm loss: 6.242689E+00 | loss scale: 4096.0 | grad norm: 0.875 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.749 | tokens per gpu per second (tgs): 1903.954 | TFLOPs: 15.32 |
g0069: [2024-08-02 19:02:47,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[1.2740198400000001e-05, 1.2740198400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 730 loss: 6.2221 iter time (s): 4.523 samples/sec: 28.301
g0093:  iteration      730/10000000 | consumed samples:        93440 | consumed tokens:    191365120 | elapsed time per iteration (ms): 4555.8 | learning rate: 1.274E-05 | global batch size:   128 | lm loss: 6.235986E+00 | loss scale: 4096.0 | grad norm: 1.081 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.096 | tokens per gpu per second (tgs): 1798.153 | TFLOPs: 14.47 |
g0069: [2024-08-02 19:03:30,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[1.2914961066666667e-05, 1.2914961066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 740 loss: 6.2222 iter time (s): 4.277 samples/sec: 29.927
g0093:  iteration      740/10000000 | consumed samples:        94720 | consumed tokens:    193986560 | elapsed time per iteration (ms): 4309.6 | learning rate: 1.291E-05 | global batch size:   128 | lm loss: 6.232875E+00 | loss scale: 4096.0 | grad norm: 1.327 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.701 | tokens per gpu per second (tgs): 1900.879 | TFLOPs: 15.30 |
g0069: [2024-08-02 19:04:15,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[1.3089723733333335e-05, 1.3089723733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 750 loss: 6.2223 iter time (s): 4.467 samples/sec: 28.654
g0093:  iteration      750/10000000 | consumed samples:        96000 | consumed tokens:    196608000 | elapsed time per iteration (ms): 4499.9 | learning rate: 1.309E-05 | global batch size:   128 | lm loss: 6.226485E+00 | loss scale: 4096.0 | grad norm: 1.627 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.445 | tokens per gpu per second (tgs): 1820.480 | TFLOPs: 14.65 |
g0069: [2024-08-02 19:04:58,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[1.3264486400000001e-05, 1.3264486400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 760 loss: 6.2247 iter time (s): 4.261 samples/sec: 30.042
g0093:  iteration      760/10000000 | consumed samples:        97280 | consumed tokens:    199229440 | elapsed time per iteration (ms): 4294.1 | learning rate: 1.326E-05 | global batch size:   128 | lm loss: 6.223143E+00 | loss scale: 4096.0 | grad norm: 1.423 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.809 | tokens per gpu per second (tgs): 1907.744 | TFLOPs: 15.35 |
g0069: [2024-08-02 19:05:42,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[1.3439249066666669e-05, 1.3439249066666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 770 loss: 6.1983 iter time (s): 4.375 samples/sec: 29.260
g0093:  iteration      770/10000000 | consumed samples:        98560 | consumed tokens:    201850880 | elapsed time per iteration (ms): 4407.8 | learning rate: 1.344E-05 | global batch size:   128 | lm loss: 6.210806E+00 | loss scale: 4096.0 | grad norm: 1.118 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.039 | tokens per gpu per second (tgs): 1858.508 | TFLOPs: 14.96 |
g0069: [2024-08-02 19:06:28,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[1.3614011733333333e-05, 1.3614011733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 780 loss: 6.1850 iter time (s): 4.597 samples/sec: 27.846
g0093:  iteration      780/10000000 | consumed samples:        99840 | consumed tokens:    204472320 | elapsed time per iteration (ms): 4629.6 | learning rate: 1.361E-05 | global batch size:   128 | lm loss: 6.206109E+00 | loss scale: 4096.0 | grad norm: 1.328 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.648 | tokens per gpu per second (tgs): 1769.489 | TFLOPs: 14.24 |
g0069: [2024-08-02 19:07:12,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[1.37887744e-05, 1.37887744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 790 loss: 6.1950 iter time (s): 4.305 samples/sec: 29.730
g0093:  iteration      790/10000000 | consumed samples:       101120 | consumed tokens:    207093760 | elapsed time per iteration (ms): 4338.5 | learning rate: 1.379E-05 | global batch size:   128 | lm loss: 6.202274E+00 | loss scale: 4096.0 | grad norm: 1.353 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.503 | tokens per gpu per second (tgs): 1888.213 | TFLOPs: 15.19 |
g0069: [2024-08-02 19:07:55,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[1.3963537066666667e-05, 1.3963537066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 800 loss: 6.1841 iter time (s): 4.249 samples/sec: 30.126
g0093:  iteration      800/10000000 | consumed samples:       102400 | consumed tokens:    209715200 | elapsed time per iteration (ms): 4282.0 | learning rate: 1.396E-05 | global batch size:   128 | lm loss: 6.193269E+00 | loss scale: 4096.0 | grad norm: 1.318 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.893 | tokens per gpu per second (tgs): 1913.122 | TFLOPs: 15.40 |
g0069: [2024-08-02 19:08:38,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[1.4138299733333333e-05, 1.4138299733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 810 loss: 6.1896 iter time (s): 4.288 samples/sec: 29.848
g0093:  iteration      810/10000000 | consumed samples:       103680 | consumed tokens:    212336640 | elapsed time per iteration (ms): 4321.8 | learning rate: 1.414E-05 | global batch size:   128 | lm loss: 6.178479E+00 | loss scale: 4096.0 | grad norm: 2.075 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.618 | tokens per gpu per second (tgs): 1895.520 | TFLOPs: 15.25 |
g0069: [2024-08-02 19:09:20,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[1.43130624e-05, 1.43130624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 820 loss: 6.1648 iter time (s): 4.185 samples/sec: 30.585
g0093:  iteration      820/10000000 | consumed samples:       104960 | consumed tokens:    214958080 | elapsed time per iteration (ms): 4218.3 | learning rate: 1.431E-05 | global batch size:   128 | lm loss: 6.174192E+00 | loss scale: 4096.0 | grad norm: 1.841 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.344 | tokens per gpu per second (tgs): 1942.031 | TFLOPs: 15.63 |
g0069: [2024-08-02 19:10:03,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[1.4487825066666667e-05, 1.4487825066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 830 loss: 6.1603 iter time (s): 4.294 samples/sec: 29.806
g0093:  iteration      830/10000000 | consumed samples:       106240 | consumed tokens:    217579520 | elapsed time per iteration (ms): 4328.1 | learning rate: 1.449E-05 | global batch size:   128 | lm loss: 6.166578E+00 | loss scale: 4096.0 | grad norm: 1.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.574 | tokens per gpu per second (tgs): 1892.728 | TFLOPs: 15.23 |
g0069: [2024-08-02 19:10:46,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[1.4662587733333333e-05, 1.4662587733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 840 loss: 6.1621 iter time (s): 4.225 samples/sec: 30.294
g0093:  iteration      840/10000000 | consumed samples:       107520 | consumed tokens:    220200960 | elapsed time per iteration (ms): 4258.5 | learning rate: 1.466E-05 | global batch size:   128 | lm loss: 6.152998E+00 | loss scale: 4096.0 | grad norm: 1.978 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.057 | tokens per gpu per second (tgs): 1923.663 | TFLOPs: 15.48 |
g0069: [2024-08-02 19:11:29,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=0, lr=[1.4837350400000001e-05, 1.4837350400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 850 loss: 6.1271 iter time (s): 4.306 samples/sec: 29.725
g0093:  iteration      850/10000000 | consumed samples:       108800 | consumed tokens:    222822400 | elapsed time per iteration (ms): 4339.0 | learning rate: 1.484E-05 | global batch size:   128 | lm loss: 6.147176E+00 | loss scale: 4096.0 | grad norm: 1.735 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.500 | tokens per gpu per second (tgs): 1887.999 | TFLOPs: 15.19 |
g0069: [2024-08-02 19:12:11,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[1.5012113066666667e-05, 1.5012113066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 860 loss: 6.1214 iter time (s): 4.114 samples/sec: 31.112
g0093:  iteration      860/10000000 | consumed samples:       110080 | consumed tokens:    225443840 | elapsed time per iteration (ms): 4147.2 | learning rate: 1.501E-05 | global batch size:   128 | lm loss: 6.135307E+00 | loss scale: 4096.0 | grad norm: 1.597 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.864 | tokens per gpu per second (tgs): 1975.306 | TFLOPs: 15.90 |
g0069: [2024-08-02 19:12:52,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[1.5186875733333335e-05, 1.5186875733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 870 loss: 6.1281 iter time (s): 4.070 samples/sec: 31.450
g0093:  iteration      870/10000000 | consumed samples:       111360 | consumed tokens:    228065280 | elapsed time per iteration (ms): 4102.8 | learning rate: 1.519E-05 | global batch size:   128 | lm loss: 6.128831E+00 | loss scale: 4096.0 | grad norm: 1.790 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.198 | tokens per gpu per second (tgs): 1996.689 | TFLOPs: 16.07 |
g0069: [2024-08-02 19:13:34,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[1.5361638400000003e-05, 1.5361638400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 880 loss: 6.1145 iter time (s): 4.142 samples/sec: 30.904
g0093:  iteration      880/10000000 | consumed samples:       112640 | consumed tokens:    230686720 | elapsed time per iteration (ms): 4177.1 | learning rate: 1.536E-05 | global batch size:   128 | lm loss: 6.108136E+00 | loss scale: 4096.0 | grad norm: 2.938 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.643 | tokens per gpu per second (tgs): 1961.163 | TFLOPs: 15.78 |
g0069: [2024-08-02 19:14:15,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[1.553640106666667e-05, 1.553640106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 890 loss: 6.0958 iter time (s): 4.066 samples/sec: 31.477
g0093:  iteration      890/10000000 | consumed samples:       113920 | consumed tokens:    233308160 | elapsed time per iteration (ms): 4099.2 | learning rate: 1.554E-05 | global batch size:   128 | lm loss: 6.099521E+00 | loss scale: 4096.0 | grad norm: 2.042 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.226 | tokens per gpu per second (tgs): 1998.450 | TFLOPs: 16.08 |
g0069: [2024-08-02 19:14:57,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[1.5711163733333335e-05, 1.5711163733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 900 loss: 6.0679 iter time (s): 4.254 samples/sec: 30.089
g0093:  iteration      900/10000000 | consumed samples:       115200 | consumed tokens:    235929600 | elapsed time per iteration (ms): 4287.0 | learning rate: 1.571E-05 | global batch size:   128 | lm loss: 6.082410E+00 | loss scale: 4096.0 | grad norm: 2.116 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.858 | tokens per gpu per second (tgs): 1910.892 | TFLOPs: 15.38 |
g0069: [2024-08-02 19:15:39,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[1.58859264e-05, 1.58859264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 910 loss: 6.0508 iter time (s): 4.111 samples/sec: 31.135
g0093:  iteration      910/10000000 | consumed samples:       116480 | consumed tokens:    238551040 | elapsed time per iteration (ms): 4143.9 | learning rate: 1.589E-05 | global batch size:   128 | lm loss: 6.063065E+00 | loss scale: 4096.0 | grad norm: 2.355 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.889 | tokens per gpu per second (tgs): 1976.875 | TFLOPs: 15.91 |
g0069: [2024-08-02 19:16:23,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[1.6060689066666667e-05, 1.6060689066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 920 loss: 5.9970 iter time (s): 4.351 samples/sec: 29.416
g0093:  iteration      920/10000000 | consumed samples:       117760 | consumed tokens:    241172480 | elapsed time per iteration (ms): 4385.1 | learning rate: 1.606E-05 | global batch size:   128 | lm loss: 6.040224E+00 | loss scale: 4096.0 | grad norm: 1.983 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.189 | tokens per gpu per second (tgs): 1868.127 | TFLOPs: 15.03 |
g0069: [2024-08-02 19:17:06,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[1.6235451733333336e-05, 1.6235451733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 930 loss: 6.0099 iter time (s): 4.241 samples/sec: 30.178
g0093:  iteration      930/10000000 | consumed samples:       119040 | consumed tokens:    243793920 | elapsed time per iteration (ms): 4291.2 | learning rate: 1.624E-05 | global batch size:   128 | lm loss: 6.031047E+00 | loss scale: 4096.0 | grad norm: 2.273 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.829 | tokens per gpu per second (tgs): 1909.037 | TFLOPs: 15.36 |
g0069: [2024-08-02 19:17:46,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[1.6410214400000002e-05, 1.6410214400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 940 loss: 6.0188 iter time (s): 4.015 samples/sec: 31.883
g0093:  iteration      940/10000000 | consumed samples:       120320 | consumed tokens:    246415360 | elapsed time per iteration (ms): 4053.1 | learning rate: 1.641E-05 | global batch size:   128 | lm loss: 6.015512E+00 | loss scale: 4096.0 | grad norm: 2.722 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.580 | tokens per gpu per second (tgs): 2021.145 | TFLOPs: 16.26 |
g0069: [2024-08-02 19:18:28,418] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[1.6584977066666665e-05, 1.6584977066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 950 loss: 5.9425 iter time (s): 4.137 samples/sec: 30.940
g0093:  iteration      950/10000000 | consumed samples:       121600 | consumed tokens:    249036800 | elapsed time per iteration (ms): 4170.9 | learning rate: 1.658E-05 | global batch size:   128 | lm loss: 5.990530E+00 | loss scale: 4096.0 | grad norm: 1.919 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.689 | tokens per gpu per second (tgs): 1964.066 | TFLOPs: 15.81 |
g0069: [2024-08-02 19:19:11,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[1.6759739733333334e-05, 1.6759739733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 960 loss: 5.9107 iter time (s): 4.270 samples/sec: 29.974
g0093:  iteration      960/10000000 | consumed samples:       122880 | consumed tokens:    251658240 | elapsed time per iteration (ms): 4303.2 | learning rate: 1.676E-05 | global batch size:   128 | lm loss: 5.955711E+00 | loss scale: 4096.0 | grad norm: 2.041 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.746 | tokens per gpu per second (tgs): 1903.718 | TFLOPs: 15.32 |
g0069: [2024-08-02 19:19:53,998] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[1.69345024e-05, 1.69345024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 970 loss: 5.9052 iter time (s): 4.222 samples/sec: 30.318
g0093:  iteration      970/10000000 | consumed samples:       124160 | consumed tokens:    254279680 | elapsed time per iteration (ms): 4254.8 | learning rate: 1.693E-05 | global batch size:   128 | lm loss: 5.948372E+00 | loss scale: 4096.0 | grad norm: 2.569 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.084 | tokens per gpu per second (tgs): 1925.345 | TFLOPs: 15.49 |
g0069: [2024-08-02 19:20:35,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[1.7109265066666667e-05, 1.7109265066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 980 loss: 5.9100 iter time (s): 4.164 samples/sec: 30.738
g0093:  iteration      980/10000000 | consumed samples:       125440 | consumed tokens:    256901120 | elapsed time per iteration (ms): 4196.8 | learning rate: 1.711E-05 | global batch size:   128 | lm loss: 5.913386E+00 | loss scale: 4096.0 | grad norm: 2.471 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.500 | tokens per gpu per second (tgs): 1951.969 | TFLOPs: 15.71 |
g0069: [2024-08-02 19:21:15,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=0, lr=[1.7284027733333333e-05, 1.7284027733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 990 loss: 5.9111 iter time (s): 3.927 samples/sec: 32.591
g0093:  iteration      990/10000000 | consumed samples:       126720 | consumed tokens:    259522560 | elapsed time per iteration (ms): 3960.1 | learning rate: 1.728E-05 | global batch size:   128 | lm loss: 5.891257E+00 | loss scale: 4096.0 | grad norm: 2.728 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.323 | tokens per gpu per second (tgs): 2068.651 | TFLOPs: 16.65 |
g0069: [2024-08-02 19:21:56,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.7458790400000002e-05, 1.7458790400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1000 loss: 5.8615 iter time (s): 4.014 samples/sec: 31.889
g0093:  iteration     1000/10000000 | consumed samples:       128000 | consumed tokens:    262144000 | elapsed time per iteration (ms): 4047.1 | learning rate: 1.746E-05 | global batch size:   128 | lm loss: 5.876964E+00 | loss scale: 4096.0 | grad norm: 2.426 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.627 | tokens per gpu per second (tgs): 2024.150 | TFLOPs: 16.29 |
g0093: ------------------------------------------------------------------------------------------------
g0093:  validation loss at iteration 1000 | lm loss value: 5.854791E+00 | lm loss PPL: 3.489020E+02 | 
g0093: ------------------------------------------------------------------------------------------------
g0069: saving checkpoint at iteration    1000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: [2024-08-02 19:28:41,921] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
g0069: [2024-08-02 19:28:41,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0069: [2024-08-02 19:28:41,930] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0093: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0093: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0086: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0086: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0069: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0086: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0093: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0087: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0087: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0087: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0091: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0091: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0091: [2024-08-02 19:28:41,931] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0088: [2024-08-02 19:28:41,934] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0088: [2024-08-02 19:28:41,934] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0088: [2024-08-02 19:28:41,934] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0090: [2024-08-02 19:28:41,934] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0090: [2024-08-02 19:28:41,934] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0090: [2024-08-02 19:28:41,934] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0092: [2024-08-02 19:28:41,936] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0092: [2024-08-02 19:28:41,936] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0092: [2024-08-02 19:28:41,936] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0093: [2024-08-02 19:28:41,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_23-model_00-model_states.pt...
g0087: [2024-08-02 19:28:41,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_08-model_00-model_states.pt...
g0088: [2024-08-02 19:28:41,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_11-model_00-model_states.pt...
g0090: [2024-08-02 19:28:41,966] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_14-model_00-model_states.pt...
g0086: [2024-08-02 19:28:41,970] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_05-model_00-model_states.pt...
g0091: [2024-08-02 19:28:41,969] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_17-model_00-model_states.pt...
g0092: [2024-08-02 19:28:41,974] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_20-model_00-model_states.pt...
g0069: [2024-08-02 19:28:41,979] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_01-model_00-model_states.pt...
g0087: [2024-08-02 19:28:42,074] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_08-model_00-model_states.pt.
g0090: [2024-08-02 19:28:42,079] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_14-model_00-model_states.pt.
g0088: [2024-08-02 19:28:42,081] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_11-model_00-model_states.pt.
g0086: [2024-08-02 19:28:42,084] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_05-model_00-model_states.pt.
g0091: [2024-08-02 19:28:42,087] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_17-model_00-model_states.pt.
g0087: [2024-08-02 19:28:42,110] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_09-model_00-model_states.pt...
g0092: [2024-08-02 19:28:42,112] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_20-model_00-model_states.pt.
g0090: [2024-08-02 19:28:42,113] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_15-model_00-model_states.pt...
g0088: [2024-08-02 19:28:42,115] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_12-model_00-model_states.pt...
g0086: [2024-08-02 19:28:42,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_06-model_00-model_states.pt...
g0091: [2024-08-02 19:28:42,127] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_18-model_00-model_states.pt...
g0092: [2024-08-02 19:28:42,152] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_21-model_00-model_states.pt...
g0069: [2024-08-02 19:28:42,171] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_01-model_00-model_states.pt.
g0069: [2024-08-02 19:28:42,201] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_02-model_00-model_states.pt...
g0088: [2024-08-02 19:28:42,249] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_12-model_00-model_states.pt.
g0091: [2024-08-02 19:28:42,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_18-model_00-model_states.pt.
g0087: [2024-08-02 19:28:42,257] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_09-model_00-model_states.pt.
g0086: [2024-08-02 19:28:42,260] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_06-model_00-model_states.pt.
g0088: [2024-08-02 19:28:42,282] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_13-model_00-model_states.pt...
g0093: [2024-08-02 19:28:42,286] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_23-model_00-model_states.pt.
g0093: [2024-08-02 19:28:42,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_24-model_00-model_states.pt...
g0087: [2024-08-02 19:28:42,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_10-model_00-model_states.pt...
g0093: [2024-08-02 19:28:42,289] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_24-model_00-model_states.pt.
g0091: [2024-08-02 19:28:42,292] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_19-model_00-model_states.pt...
g0086: [2024-08-02 19:28:42,298] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_07-model_00-model_states.pt...
g0093: [2024-08-02 19:28:42,342] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_25-model_00-model_states.pt...
g0091: [2024-08-02 19:28:42,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_19-model_00-model_states.pt.
g0091: [2024-08-02 19:28:42,409] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_05_model_states.pt...
g0069: [2024-08-02 19:28:42,411] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_02-model_00-model_states.pt.
g0087: [2024-08-02 19:28:42,416] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_10-model_00-model_states.pt.
g0087: [2024-08-02 19:28:42,417] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_02_model_states.pt...
g0086: [2024-08-02 19:28:42,420] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_07-model_00-model_states.pt.
g0086: [2024-08-02 19:28:42,422] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_01_model_states.pt...
g0069: [2024-08-02 19:28:42,444] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_03-model_00-model_states.pt...
g0088: [2024-08-02 19:28:42,465] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_13-model_00-model_states.pt.
g0088: [2024-08-02 19:28:42,467] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_03_model_states.pt...
g0093: [2024-08-02 19:28:42,471] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_25-model_00-model_states.pt.
g0093: [2024-08-02 19:28:42,472] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_07_model_states.pt...
g0069: [2024-08-02 19:28:42,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_03-model_00-model_states.pt.
g0069: [2024-08-02 19:28:42,589] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_04-model_00-model_states.pt...
g0090: [2024-08-02 19:28:42,686] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_15-model_00-model_states.pt.
g0090: [2024-08-02 19:28:42,719] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_16-model_00-model_states.pt...
g0069: [2024-08-02 19:28:42,735] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_04-model_00-model_states.pt.
g0069: [2024-08-02 19:28:42,737] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt
g0069: [2024-08-02 19:28:42,737] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt...
g0090: [2024-08-02 19:28:42,812] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_16-model_00-model_states.pt.
g0090: [2024-08-02 19:28:42,814] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_04_model_states.pt...
g0092: [2024-08-02 19:28:42,900] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_21-model_00-model_states.pt.
g0092: [2024-08-02 19:28:42,938] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_22-model_00-model_states.pt...
g0092: [2024-08-02 19:28:43,059] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_22-model_00-model_states.pt.
g0092: [2024-08-02 19:28:43,061] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_06_model_states.pt...
g0093: [2024-08-02 19:28:44,504] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_07_model_states.pt.
g0093: [2024-08-02 19:28:44,504] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0088: [2024-08-02 19:28:44,720] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_03_model_states.pt.
g0088: [2024-08-02 19:28:44,720] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0087: [2024-08-02 19:28:44,743] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_02_model_states.pt.
g0087: [2024-08-02 19:28:44,744] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0086: [2024-08-02 19:28:44,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_01_model_states.pt.
g0086: [2024-08-02 19:28:44,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0091: [2024-08-02 19:28:45,565] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_05_model_states.pt.
g0091: [2024-08-02 19:28:45,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0069: [2024-08-02 19:28:46,132] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt.
g0069: [2024-08-02 19:28:46,133] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0090: [2024-08-02 19:28:46,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_04_model_states.pt.
g0090: [2024-08-02 19:28:46,276] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0092: [2024-08-02 19:28:46,332] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_06_model_states.pt.
g0092: [2024-08-02 19:28:46,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0069:   successfully saved checkpoint at iteration    1000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: Checkpoint Save GB: 22.521, GB/Sec: 5.09, Latency(second): 4.424
g0093: (min, max) time across ranks (ms):
g0093:     save-checkpoint ................................: (4423.89, 4423.99)
g0088: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0088: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0088: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0090: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0088: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0086: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 19:28:50,677] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0090: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 19:28:50,677] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0090: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0092: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0088: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0087: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0090: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0091: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0087: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0091: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0087: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0091: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0091: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0087: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0087: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0092: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0092: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0093: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0092: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0069: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0093: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0093: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 19:28:50,677] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0069: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0069: [2024-08-02 19:28:50,676] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0092: [2024-08-02 19:28:50,677] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0091: [2024-08-02 19:28:50,677] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0093: [2024-08-02 19:28:50,677] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0069: [2024-08-02 19:29:29,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=0, lr=[1.7633553066666668e-05, 1.7633553066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1010 loss: 5.8180 iter time (s): 4.289 samples/sec: 29.843
g0093:  iteration     1010/10000000 | consumed samples:       129280 | consumed tokens:    264765440 | elapsed time per iteration (ms): 45348.8 | learning rate: 1.763E-05 | global batch size:   128 | lm loss: 5.835882E+00 | loss scale: 8192.0 | grad norm: 3.062 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.823 | tokens per gpu per second (tgs): 180.644 | TFLOPs: 1.45 |
g0069: [2024-08-02 19:30:12,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=0, lr=[1.7808315733333334e-05, 1.7808315733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1020 loss: 5.8021 iter time (s): 4.247 samples/sec: 30.140
g0093:  iteration     1020/10000000 | consumed samples:       130560 | consumed tokens:    267386880 | elapsed time per iteration (ms): 4280.2 | learning rate: 1.781E-05 | global batch size:   128 | lm loss: 5.808223E+00 | loss scale: 8192.0 | grad norm: 3.526 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.905 | tokens per gpu per second (tgs): 1913.921 | TFLOPs: 15.40 |
g0069: [2024-08-02 19:30:57,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=0, lr=[1.79830784e-05, 1.79830784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1030 loss: 5.8070 iter time (s): 4.466 samples/sec: 28.659
g0093:  iteration     1030/10000000 | consumed samples:       131840 | consumed tokens:    270008320 | elapsed time per iteration (ms): 4500.6 | learning rate: 1.798E-05 | global batch size:   128 | lm loss: 5.800903E+00 | loss scale: 8192.0 | grad norm: 3.018 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.441 | tokens per gpu per second (tgs): 1820.199 | TFLOPs: 14.65 |
g0069: [2024-08-02 19:31:40,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=0, lr=[1.8157841066666666e-05, 1.8157841066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1040 loss: 5.7458 iter time (s): 4.256 samples/sec: 30.076
g0093:  iteration     1040/10000000 | consumed samples:       133120 | consumed tokens:    272629760 | elapsed time per iteration (ms): 4288.8 | learning rate: 1.816E-05 | global batch size:   128 | lm loss: 5.744671E+00 | loss scale: 8192.0 | grad norm: 2.941 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.845 | tokens per gpu per second (tgs): 1910.081 | TFLOPs: 15.37 |
g0069: [2024-08-02 19:32:22,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=0, lr=[1.8332603733333336e-05, 1.8332603733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1050 loss: 5.7037 iter time (s): 4.163 samples/sec: 30.746
g0093:  iteration     1050/10000000 | consumed samples:       134400 | consumed tokens:    275251200 | elapsed time per iteration (ms): 4196.0 | learning rate: 1.833E-05 | global batch size:   128 | lm loss: 5.716946E+00 | loss scale: 8192.0 | grad norm: 3.308 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.505 | tokens per gpu per second (tgs): 1952.349 | TFLOPs: 15.71 |
g0069: [2024-08-02 19:33:06,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=0, lr=[1.8507366400000002e-05, 1.8507366400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1060 loss: 5.7005 iter time (s): 4.372 samples/sec: 29.278
g0093:  iteration     1060/10000000 | consumed samples:       135680 | consumed tokens:    277872640 | elapsed time per iteration (ms): 4404.5 | learning rate: 1.851E-05 | global batch size:   128 | lm loss: 5.686316E+00 | loss scale: 8192.0 | grad norm: 2.109 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.061 | tokens per gpu per second (tgs): 1859.926 | TFLOPs: 14.97 |
g0069: [2024-08-02 19:33:49,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=0, lr=[1.8682129066666668e-05, 1.8682129066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1070 loss: 5.6246 iter time (s): 4.280 samples/sec: 29.907
g0093:  iteration     1070/10000000 | consumed samples:       136960 | consumed tokens:    280494080 | elapsed time per iteration (ms): 4312.3 | learning rate: 1.868E-05 | global batch size:   128 | lm loss: 5.660053E+00 | loss scale: 8192.0 | grad norm: 4.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.682 | tokens per gpu per second (tgs): 1899.663 | TFLOPs: 15.29 |
g0069: [2024-08-02 19:34:30,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=0, lr=[1.8856891733333334e-05, 1.8856891733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1080 loss: 5.6610 iter time (s): 4.055 samples/sec: 31.565
g0093:  iteration     1080/10000000 | consumed samples:       138240 | consumed tokens:    283115520 | elapsed time per iteration (ms): 4087.6 | learning rate: 1.886E-05 | global batch size:   128 | lm loss: 5.625326E+00 | loss scale: 8192.0 | grad norm: 3.691 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.314 | tokens per gpu per second (tgs): 2004.095 | TFLOPs: 16.13 |
g0069: [2024-08-02 19:35:12,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=0, lr=[1.9031654400000003e-05, 1.9031654400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1090 loss: 5.5876 iter time (s): 4.173 samples/sec: 30.675
g0093:  iteration     1090/10000000 | consumed samples:       139520 | consumed tokens:    285736960 | elapsed time per iteration (ms): 4205.7 | learning rate: 1.903E-05 | global batch size:   128 | lm loss: 5.604610E+00 | loss scale: 8192.0 | grad norm: 3.904 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.435 | tokens per gpu per second (tgs): 1947.826 | TFLOPs: 15.67 |
g0069: [2024-08-02 19:35:55,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=0, lr=[1.920641706666667e-05, 1.920641706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1100 loss: 5.5336 iter time (s): 4.277 samples/sec: 29.924
g0093:  iteration     1100/10000000 | consumed samples:       140800 | consumed tokens:    288358400 | elapsed time per iteration (ms): 4309.7 | learning rate: 1.921E-05 | global batch size:   128 | lm loss: 5.563997E+00 | loss scale: 8192.0 | grad norm: 4.111 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.700 | tokens per gpu per second (tgs): 1900.828 | TFLOPs: 15.30 |
g0069: [2024-08-02 19:36:35,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=0, lr=[1.9381179733333332e-05, 1.9381179733333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1110 loss: 5.4831 iter time (s): 3.950 samples/sec: 32.406
g0093:  iteration     1110/10000000 | consumed samples:       142080 | consumed tokens:    290979840 | elapsed time per iteration (ms): 3982.7 | learning rate: 1.938E-05 | global batch size:   128 | lm loss: 5.531190E+00 | loss scale: 8192.0 | grad norm: 3.470 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.139 | tokens per gpu per second (tgs): 2056.918 | TFLOPs: 16.55 |
g0069: [2024-08-02 19:37:18,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=0, lr=[1.95559424e-05, 1.95559424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1120 loss: 5.4672 iter time (s): 4.276 samples/sec: 29.932
g0093:  iteration     1120/10000000 | consumed samples:       143360 | consumed tokens:    293601280 | elapsed time per iteration (ms): 4310.5 | learning rate: 1.956E-05 | global batch size:   128 | lm loss: 5.500079E+00 | loss scale: 8192.0 | grad norm: 4.123 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.695 | tokens per gpu per second (tgs): 1900.468 | TFLOPs: 15.29 |
g0069: [2024-08-02 19:38:00,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=0, lr=[1.9730705066666668e-05, 1.9730705066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1130 loss: 5.4100 iter time (s): 4.232 samples/sec: 30.246
g0093:  iteration     1130/10000000 | consumed samples:       144640 | consumed tokens:    296222720 | elapsed time per iteration (ms): 4264.4 | learning rate: 1.973E-05 | global batch size:   128 | lm loss: 5.445453E+00 | loss scale: 8192.0 | grad norm: 4.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.016 | tokens per gpu per second (tgs): 1920.999 | TFLOPs: 15.46 |
g0069: [2024-08-02 19:38:45,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=0, lr=[1.9905467733333334e-05, 1.9905467733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1140 loss: 5.4596 iter time (s): 4.377 samples/sec: 29.246
g0093:  iteration     1140/10000000 | consumed samples:       145920 | consumed tokens:    298844160 | elapsed time per iteration (ms): 4410.5 | learning rate: 1.991E-05 | global batch size:   128 | lm loss: 5.432791E+00 | loss scale: 8192.0 | grad norm: 3.924 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.022 | tokens per gpu per second (tgs): 1857.386 | TFLOPs: 14.95 |
g0069: [2024-08-02 19:39:28,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=0, lr=[2.00802304e-05, 2.00802304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1150 loss: 5.3520 iter time (s): 4.321 samples/sec: 29.622
g0093:  iteration     1150/10000000 | consumed samples:       147200 | consumed tokens:    301465600 | elapsed time per iteration (ms): 4354.3 | learning rate: 2.008E-05 | global batch size:   128 | lm loss: 5.388351E+00 | loss scale: 8192.0 | grad norm: 4.782 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.396 | tokens per gpu per second (tgs): 1881.347 | TFLOPs: 15.14 |
g0069: [2024-08-02 19:40:09,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=0, lr=[2.0254993066666666e-05, 2.0254993066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1160 loss: 5.2478 iter time (s): 4.089 samples/sec: 31.305
g0093:  iteration     1160/10000000 | consumed samples:       148480 | consumed tokens:    304087040 | elapsed time per iteration (ms): 4121.4 | learning rate: 2.025E-05 | global batch size:   128 | lm loss: 5.312748E+00 | loss scale: 8192.0 | grad norm: 3.894 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.058 | tokens per gpu per second (tgs): 1987.686 | TFLOPs: 16.00 |
g0069: [2024-08-02 19:40:53,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=0, lr=[2.0429755733333335e-05, 2.0429755733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1170 loss: 5.2541 iter time (s): 4.310 samples/sec: 29.701
g0093:  iteration     1170/10000000 | consumed samples:       149760 | consumed tokens:    306708480 | elapsed time per iteration (ms): 4342.7 | learning rate: 2.043E-05 | global batch size:   128 | lm loss: 5.291869E+00 | loss scale: 8192.0 | grad norm: 4.028 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.475 | tokens per gpu per second (tgs): 1886.375 | TFLOPs: 15.18 |
g0069: [2024-08-02 19:41:35,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=0, lr=[2.06045184e-05, 2.06045184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1180 loss: 5.2644 iter time (s): 4.205 samples/sec: 30.439
g0093:  iteration     1180/10000000 | consumed samples:       151040 | consumed tokens:    309329920 | elapsed time per iteration (ms): 4237.8 | learning rate: 2.060E-05 | global batch size:   128 | lm loss: 5.273051E+00 | loss scale: 8192.0 | grad norm: 4.637 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.204 | tokens per gpu per second (tgs): 1933.080 | TFLOPs: 15.56 |
g0069: [2024-08-02 19:42:17,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=0, lr=[2.0779281066666667e-05, 2.0779281066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1190 loss: 5.2516 iter time (s): 4.143 samples/sec: 30.898
g0093:  iteration     1190/10000000 | consumed samples:       152320 | consumed tokens:    311951360 | elapsed time per iteration (ms): 4175.1 | learning rate: 2.078E-05 | global batch size:   128 | lm loss: 5.253890E+00 | loss scale: 8192.0 | grad norm: 5.031 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.658 | tokens per gpu per second (tgs): 1962.127 | TFLOPs: 15.79 |
g0069: [2024-08-02 19:42:57,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=0, lr=[2.0954043733333333e-05, 2.0954043733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1200 loss: 5.1978 iter time (s): 3.988 samples/sec: 32.100
g0093:  iteration     1200/10000000 | consumed samples:       153600 | consumed tokens:    314572800 | elapsed time per iteration (ms): 4020.7 | learning rate: 2.095E-05 | global batch size:   128 | lm loss: 5.176303E+00 | loss scale: 8192.0 | grad norm: 3.678 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.836 | tokens per gpu per second (tgs): 2037.481 | TFLOPs: 16.40 |
g0069: [2024-08-02 19:43:39,728] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=0, lr=[2.1128806400000003e-05, 2.1128806400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1210 loss: 5.0841 iter time (s): 4.181 samples/sec: 30.617
g0093:  iteration     1210/10000000 | consumed samples:       154880 | consumed tokens:    317194240 | elapsed time per iteration (ms): 4214.7 | learning rate: 2.113E-05 | global batch size:   128 | lm loss: 5.162991E+00 | loss scale: 8192.0 | grad norm: 4.497 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.370 | tokens per gpu per second (tgs): 1943.691 | TFLOPs: 15.64 |
g0069: [2024-08-02 19:44:23,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=0, lr=[2.130356906666667e-05, 2.130356906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1220 loss: 5.0946 iter time (s): 4.312 samples/sec: 29.687
g0093:  iteration     1220/10000000 | consumed samples:       156160 | consumed tokens:    319815680 | elapsed time per iteration (ms): 4344.5 | learning rate: 2.130E-05 | global batch size:   128 | lm loss: 5.112037E+00 | loss scale: 8192.0 | grad norm: 3.875 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.463 | tokens per gpu per second (tgs): 1885.624 | TFLOPs: 15.17 |
g0069: [2024-08-02 19:45:04,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=0, lr=[2.1478331733333335e-05, 2.1478331733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1230 loss: 5.0552 iter time (s): 4.104 samples/sec: 31.191
g0093:  iteration     1230/10000000 | consumed samples:       157440 | consumed tokens:    322437120 | elapsed time per iteration (ms): 4136.3 | learning rate: 2.148E-05 | global batch size:   128 | lm loss: 5.082613E+00 | loss scale: 8192.0 | grad norm: 5.128 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.945 | tokens per gpu per second (tgs): 1980.505 | TFLOPs: 15.94 |
g0069: [2024-08-02 19:45:47,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=0, lr=[2.16530944e-05, 2.16530944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1240 loss: 5.0413 iter time (s): 4.255 samples/sec: 30.082
g0093:  iteration     1240/10000000 | consumed samples:       158720 | consumed tokens:    325058560 | elapsed time per iteration (ms): 4287.7 | learning rate: 2.165E-05 | global batch size:   128 | lm loss: 5.039335E+00 | loss scale: 8192.0 | grad norm: 4.444 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.853 | tokens per gpu per second (tgs): 1910.581 | TFLOPs: 15.37 |
g0069: [2024-08-02 19:46:30,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=0, lr=[2.1827857066666667e-05, 2.1827857066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1250 loss: 4.9929 iter time (s): 4.252 samples/sec: 30.105
g0093:  iteration     1250/10000000 | consumed samples:       160000 | consumed tokens:    327680000 | elapsed time per iteration (ms): 4284.6 | learning rate: 2.183E-05 | global batch size:   128 | lm loss: 5.017829E+00 | loss scale: 8192.0 | grad norm: 4.594 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.875 | tokens per gpu per second (tgs): 1911.976 | TFLOPs: 15.39 |
g0069: [2024-08-02 19:47:11,558] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=0, lr=[2.2002619733333337e-05, 2.2002619733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1260 loss: 4.9751 iter time (s): 4.093 samples/sec: 31.270
g0093:  iteration     1260/10000000 | consumed samples:       161280 | consumed tokens:    330301440 | elapsed time per iteration (ms): 4132.0 | learning rate: 2.200E-05 | global batch size:   128 | lm loss: 4.981617E+00 | loss scale: 8192.0 | grad norm: 4.826 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.978 | tokens per gpu per second (tgs): 1982.569 | TFLOPs: 15.95 |
g0069: [2024-08-02 19:47:50,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=0, lr=[2.2177382400000003e-05, 2.2177382400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1270 loss: 4.9482 iter time (s): 3.878 samples/sec: 33.006
g0093:  iteration     1270/10000000 | consumed samples:       162560 | consumed tokens:    332922880 | elapsed time per iteration (ms): 3910.5 | learning rate: 2.218E-05 | global batch size:   128 | lm loss: 4.929499E+00 | loss scale: 8192.0 | grad norm: 5.881 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.732 | tokens per gpu per second (tgs): 2094.854 | TFLOPs: 16.86 |
g0069: [2024-08-02 19:48:31,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=0, lr=[2.235214506666667e-05, 2.235214506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1280 loss: 4.8420 iter time (s): 4.082 samples/sec: 31.355
g0093:  iteration     1280/10000000 | consumed samples:       163840 | consumed tokens:    335544320 | elapsed time per iteration (ms): 4115.2 | learning rate: 2.235E-05 | global batch size:   128 | lm loss: 4.905445E+00 | loss scale: 8192.0 | grad norm: 4.872 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.104 | tokens per gpu per second (tgs): 1990.651 | TFLOPs: 16.02 |
g0069: [2024-08-02 19:49:16,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=0, lr=[2.2526907733333335e-05, 2.2526907733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1290 loss: 4.8267 iter time (s): 4.460 samples/sec: 28.700
g0093:  iteration     1290/10000000 | consumed samples:       165120 | consumed tokens:    338165760 | elapsed time per iteration (ms): 4492.8 | learning rate: 2.253E-05 | global batch size:   128 | lm loss: 4.861446E+00 | loss scale: 8192.0 | grad norm: 4.311 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.490 | tokens per gpu per second (tgs): 1823.375 | TFLOPs: 14.67 |
g0069: [2024-08-02 19:49:59,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=0, lr=[2.2701670400000004e-05, 2.2701670400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1300 loss: 4.8594 iter time (s): 4.272 samples/sec: 29.963
g0093:  iteration     1300/10000000 | consumed samples:       166400 | consumed tokens:    340787200 | elapsed time per iteration (ms): 4307.5 | learning rate: 2.270E-05 | global batch size:   128 | lm loss: 4.821962E+00 | loss scale: 8192.0 | grad norm: 4.739 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.715 | tokens per gpu per second (tgs): 1901.779 | TFLOPs: 15.30 |
g0069: [2024-08-02 19:50:42,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=0, lr=[2.287643306666667e-05, 2.287643306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1310 loss: 4.7293 iter time (s): 4.224 samples/sec: 30.301
g0093:  iteration     1310/10000000 | consumed samples:       167680 | consumed tokens:    343408640 | elapsed time per iteration (ms): 4256.9 | learning rate: 2.288E-05 | global batch size:   128 | lm loss: 4.808334E+00 | loss scale: 8192.0 | grad norm: 4.470 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.069 | tokens per gpu per second (tgs): 1924.392 | TFLOPs: 15.49 |
g0069: [2024-08-02 19:51:24,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=0, lr=[2.3051195733333336e-05, 2.3051195733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1320 loss: 4.7773 iter time (s): 4.133 samples/sec: 30.968
g0093:  iteration     1320/10000000 | consumed samples:       168960 | consumed tokens:    346030080 | elapsed time per iteration (ms): 4166.1 | learning rate: 2.305E-05 | global batch size:   128 | lm loss: 4.750488E+00 | loss scale: 8192.0 | grad norm: 4.074 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.724 | tokens per gpu per second (tgs): 1966.347 | TFLOPs: 15.82 |
g0069: [2024-08-02 19:52:04,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=0, lr=[2.3225958400000002e-05, 2.3225958400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1330 loss: 4.6734 iter time (s): 4.029 samples/sec: 31.767
g0093:  iteration     1330/10000000 | consumed samples:       170240 | consumed tokens:    348651520 | elapsed time per iteration (ms): 4062.2 | learning rate: 2.323E-05 | global batch size:   128 | lm loss: 4.730462E+00 | loss scale: 8192.0 | grad norm: 4.669 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.510 | tokens per gpu per second (tgs): 2016.656 | TFLOPs: 16.23 |
g0069: [2024-08-02 19:52:46,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=0, lr=[2.340072106666667e-05, 2.340072106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1340 loss: 4.7074 iter time (s): 4.167 samples/sec: 30.714
g0093:  iteration     1340/10000000 | consumed samples:       171520 | consumed tokens:    351272960 | elapsed time per iteration (ms): 4200.3 | learning rate: 2.340E-05 | global batch size:   128 | lm loss: 4.678531E+00 | loss scale: 8192.0 | grad norm: 4.453 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.474 | tokens per gpu per second (tgs): 1950.356 | TFLOPs: 15.69 |
g0069: [2024-08-02 19:53:28,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=0, lr=[2.3575483733333338e-05, 2.3575483733333338e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1350 loss: 4.5809 iter time (s): 4.189 samples/sec: 30.554
g0093:  iteration     1350/10000000 | consumed samples:       172800 | consumed tokens:    353894400 | elapsed time per iteration (ms): 4221.8 | learning rate: 2.358E-05 | global batch size:   128 | lm loss: 4.650529E+00 | loss scale: 8192.0 | grad norm: 4.926 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.319 | tokens per gpu per second (tgs): 1940.421 | TFLOPs: 15.61 |
g0069: [2024-08-02 19:54:11,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=0, lr=[2.3750246399999997e-05, 2.3750246399999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1360 loss: 4.6196 iter time (s): 4.241 samples/sec: 30.185
g0093:  iteration     1360/10000000 | consumed samples:       174080 | consumed tokens:    356515840 | elapsed time per iteration (ms): 4274.8 | learning rate: 2.375E-05 | global batch size:   128 | lm loss: 4.626729E+00 | loss scale: 8192.0 | grad norm: 4.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.943 | tokens per gpu per second (tgs): 1916.329 | TFLOPs: 15.42 |
g0069: [2024-08-02 19:54:52,717] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=0, lr=[2.3925009066666667e-05, 2.3925009066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1370 loss: 4.6335 iter time (s): 4.072 samples/sec: 31.433
g0093:  iteration     1370/10000000 | consumed samples:       175360 | consumed tokens:    359137280 | elapsed time per iteration (ms): 4105.2 | learning rate: 2.393E-05 | global batch size:   128 | lm loss: 4.585455E+00 | loss scale: 8192.0 | grad norm: 4.549 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.180 | tokens per gpu per second (tgs): 1995.508 | TFLOPs: 16.06 |
g0069: [2024-08-02 19:55:34,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=0, lr=[2.4099771733333333e-05, 2.4099771733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1380 loss: 4.5472 iter time (s): 4.172 samples/sec: 30.683
g0093:  iteration     1380/10000000 | consumed samples:       176640 | consumed tokens:    361758720 | elapsed time per iteration (ms): 4205.2 | learning rate: 2.410E-05 | global batch size:   128 | lm loss: 4.569575E+00 | loss scale: 8192.0 | grad norm: 4.798 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.438 | tokens per gpu per second (tgs): 1948.055 | TFLOPs: 15.68 |
g0069: [2024-08-02 19:56:17,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=0, lr=[2.42745344e-05, 2.42745344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1390 loss: 4.4733 iter time (s): 4.212 samples/sec: 30.392
g0093:  iteration     1390/10000000 | consumed samples:       177920 | consumed tokens:    364380160 | elapsed time per iteration (ms): 4244.2 | learning rate: 2.427E-05 | global batch size:   128 | lm loss: 4.521965E+00 | loss scale: 8192.0 | grad norm: 4.127 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.159 | tokens per gpu per second (tgs): 1930.181 | TFLOPs: 15.53 |
g0069: [2024-08-02 19:56:59,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=0, lr=[2.4449297066666665e-05, 2.4449297066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1400 loss: 4.4668 iter time (s): 4.213 samples/sec: 30.384
g0093:  iteration     1400/10000000 | consumed samples:       179200 | consumed tokens:    367001600 | elapsed time per iteration (ms): 4245.1 | learning rate: 2.445E-05 | global batch size:   128 | lm loss: 4.492884E+00 | loss scale: 8192.0 | grad norm: 6.238 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.152 | tokens per gpu per second (tgs): 1929.749 | TFLOPs: 15.53 |
g0069: [2024-08-02 19:57:40,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=0, lr=[2.4624059733333334e-05, 2.4624059733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1410 loss: 4.5002 iter time (s): 4.041 samples/sec: 31.678
g0093:  iteration     1410/10000000 | consumed samples:       180480 | consumed tokens:    369623040 | elapsed time per iteration (ms): 4073.0 | learning rate: 2.462E-05 | global batch size:   128 | lm loss: 4.471738E+00 | loss scale: 8192.0 | grad norm: 4.833 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.426 | tokens per gpu per second (tgs): 2011.283 | TFLOPs: 16.19 |
g0069: [2024-08-02 19:58:21,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=0, lr=[2.47988224e-05, 2.47988224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1420 loss: 4.3502 iter time (s): 4.095 samples/sec: 31.255
g0093:  iteration     1420/10000000 | consumed samples:       181760 | consumed tokens:    372244480 | elapsed time per iteration (ms): 4127.9 | learning rate: 2.480E-05 | global batch size:   128 | lm loss: 4.439635E+00 | loss scale: 8192.0 | grad norm: 3.988 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.009 | tokens per gpu per second (tgs): 1984.549 | TFLOPs: 15.97 |
g0069: [2024-08-02 19:59:03,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=0, lr=[2.4973585066666666e-05, 2.4973585066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1430 loss: 4.4083 iter time (s): 4.126 samples/sec: 31.025
g0093:  iteration     1430/10000000 | consumed samples:       183040 | consumed tokens:    374865920 | elapsed time per iteration (ms): 4158.4 | learning rate: 2.497E-05 | global batch size:   128 | lm loss: 4.393384E+00 | loss scale: 8192.0 | grad norm: 4.866 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.781 | tokens per gpu per second (tgs): 1970.007 | TFLOPs: 15.85 |
g0069: [2024-08-02 19:59:46,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=0, lr=[2.5148347733333333e-05, 2.5148347733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1440 loss: 4.3165 iter time (s): 4.333 samples/sec: 29.538
g0093:  iteration     1440/10000000 | consumed samples:       184320 | consumed tokens:    377487360 | elapsed time per iteration (ms): 4365.9 | learning rate: 2.515E-05 | global batch size:   128 | lm loss: 4.391653E+00 | loss scale: 8192.0 | grad norm: 3.449 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.318 | tokens per gpu per second (tgs): 1876.351 | TFLOPs: 15.10 |
g0069: [2024-08-02 20:00:27,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=0, lr=[2.53231104e-05, 2.53231104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1450 loss: 4.3390 iter time (s): 4.016 samples/sec: 31.872
g0093:  iteration     1450/10000000 | consumed samples:       185600 | consumed tokens:    380108800 | elapsed time per iteration (ms): 4048.6 | learning rate: 2.532E-05 | global batch size:   128 | lm loss: 4.356445E+00 | loss scale: 8192.0 | grad norm: 3.752 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.616 | tokens per gpu per second (tgs): 2023.423 | TFLOPs: 16.28 |
g0069: [2024-08-02 20:01:08,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=0, lr=[2.5497873066666668e-05, 2.5497873066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1460 loss: 4.3050 iter time (s): 4.095 samples/sec: 31.257
g0093:  iteration     1460/10000000 | consumed samples:       186880 | consumed tokens:    382730240 | elapsed time per iteration (ms): 4127.9 | learning rate: 2.550E-05 | global batch size:   128 | lm loss: 4.297253E+00 | loss scale: 8192.0 | grad norm: 3.946 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.009 | tokens per gpu per second (tgs): 1984.557 | TFLOPs: 15.97 |
g0069: [2024-08-02 20:01:49,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=0, lr=[2.5672635733333334e-05, 2.5672635733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1470 loss: 4.2699 iter time (s): 4.086 samples/sec: 31.328
g0093:  iteration     1470/10000000 | consumed samples:       188160 | consumed tokens:    385351680 | elapsed time per iteration (ms): 4118.6 | learning rate: 2.567E-05 | global batch size:   128 | lm loss: 4.290228E+00 | loss scale: 8192.0 | grad norm: 4.504 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.079 | tokens per gpu per second (tgs): 1989.042 | TFLOPs: 16.01 |
g0069: [2024-08-02 20:02:30,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=0, lr=[2.58473984e-05, 2.58473984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1480 loss: 4.2129 iter time (s): 4.015 samples/sec: 31.880
g0093:  iteration     1480/10000000 | consumed samples:       189440 | consumed tokens:    387973120 | elapsed time per iteration (ms): 4048.9 | learning rate: 2.585E-05 | global batch size:   128 | lm loss: 4.290141E+00 | loss scale: 8192.0 | grad norm: 4.127 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.614 | tokens per gpu per second (tgs): 2023.283 | TFLOPs: 16.28 |
g0069: [2024-08-02 20:03:10,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=0, lr=[2.6022161066666666e-05, 2.6022161066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1490 loss: 4.2414 iter time (s): 3.957 samples/sec: 32.349
g0093:  iteration     1490/10000000 | consumed samples:       190720 | consumed tokens:    390594560 | elapsed time per iteration (ms): 3990.0 | learning rate: 2.602E-05 | global batch size:   128 | lm loss: 4.247565E+00 | loss scale: 8192.0 | grad norm: 4.542 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.080 | tokens per gpu per second (tgs): 2053.139 | TFLOPs: 16.52 |
g0069: [2024-08-02 20:03:52,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=0, lr=[2.6196923733333336e-05, 2.6196923733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1500 loss: 4.2130 iter time (s): 4.182 samples/sec: 30.607
g0093:  iteration     1500/10000000 | consumed samples:       192000 | consumed tokens:    393216000 | elapsed time per iteration (ms): 4214.8 | learning rate: 2.620E-05 | global batch size:   128 | lm loss: 4.235884E+00 | loss scale: 8192.0 | grad norm: 3.651 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.369 | tokens per gpu per second (tgs): 1943.642 | TFLOPs: 15.64 |
g0092: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0090: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0092: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0090: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0091: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0069: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0069: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0091: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0091: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0091: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0091: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0093: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0086: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0087: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0093: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0087: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0093: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0086: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0088: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0093: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0088: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0086: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0088: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0093: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0087: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0087: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0069: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0069: [2024-08-02 20:03:56,456] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0090: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0087: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0093: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0069: [2024-08-02 20:03:56,457] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0069: [2024-08-02 20:04:31,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=0, lr=[2.6371686400000002e-05, 2.6371686400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1510 loss: 4.2220 iter time (s): 3.893 samples/sec: 32.876
g0093:  iteration     1510/10000000 | consumed samples:       193280 | consumed tokens:    395837440 | elapsed time per iteration (ms): 3926.9 | learning rate: 2.637E-05 | global batch size:   128 | lm loss: 4.214513E+00 | loss scale: 16384.0 | grad norm: 3.760 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.596 | tokens per gpu per second (tgs): 2086.116 | TFLOPs: 16.79 |
g0069: [2024-08-02 20:05:13,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=0, lr=[2.6546449066666668e-05, 2.6546449066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1520 loss: 4.1460 iter time (s): 4.162 samples/sec: 30.754
g0093:  iteration     1520/10000000 | consumed samples:       194560 | consumed tokens:    398458880 | elapsed time per iteration (ms): 4194.6 | learning rate: 2.655E-05 | global batch size:   128 | lm loss: 4.185472E+00 | loss scale: 16384.0 | grad norm: 4.476 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.515 | tokens per gpu per second (tgs): 1952.980 | TFLOPs: 15.72 |
g0069: [2024-08-02 20:05:54,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=0, lr=[2.6721211733333334e-05, 2.6721211733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1530 loss: 4.2010 iter time (s): 4.091 samples/sec: 31.289
g0093:  iteration     1530/10000000 | consumed samples:       195840 | consumed tokens:    401080320 | elapsed time per iteration (ms): 4124.9 | learning rate: 2.672E-05 | global batch size:   128 | lm loss: 4.190030E+00 | loss scale: 16384.0 | grad norm: 3.342 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.031 | tokens per gpu per second (tgs): 1986.010 | TFLOPs: 15.98 |
g0069: [2024-08-02 20:06:37,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=0, lr=[2.68959744e-05, 2.68959744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1540 loss: 4.1089 iter time (s): 4.191 samples/sec: 30.540
g0093:  iteration     1540/10000000 | consumed samples:       197120 | consumed tokens:    403701760 | elapsed time per iteration (ms): 4224.4 | learning rate: 2.690E-05 | global batch size:   128 | lm loss: 4.141425E+00 | loss scale: 16384.0 | grad norm: 4.477 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.300 | tokens per gpu per second (tgs): 1939.224 | TFLOPs: 15.61 |
g0069: [2024-08-02 20:07:19,325] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=0, lr=[2.707073706666667e-05, 2.707073706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1550 loss: 4.1683 iter time (s): 4.188 samples/sec: 30.564
g0093:  iteration     1550/10000000 | consumed samples:       198400 | consumed tokens:    406323200 | elapsed time per iteration (ms): 4221.6 | learning rate: 2.707E-05 | global batch size:   128 | lm loss: 4.105273E+00 | loss scale: 16384.0 | grad norm: 3.593 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.320 | tokens per gpu per second (tgs): 1940.511 | TFLOPs: 15.62 |
g0069: [2024-08-02 20:08:01,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=0, lr=[2.7245499733333335e-05, 2.7245499733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1560 loss: 4.1227 iter time (s): 4.169 samples/sec: 30.706
g0093:  iteration     1560/10000000 | consumed samples:       199680 | consumed tokens:    408944640 | elapsed time per iteration (ms): 4201.4 | learning rate: 2.725E-05 | global batch size:   128 | lm loss: 4.116554E+00 | loss scale: 16384.0 | grad norm: 4.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.466 | tokens per gpu per second (tgs): 1949.830 | TFLOPs: 15.69 |
g0069: [2024-08-02 20:08:42,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=0, lr=[2.74202624e-05, 2.74202624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1570 loss: 4.0714 iter time (s): 4.103 samples/sec: 31.197
g0093:  iteration     1570/10000000 | consumed samples:       200960 | consumed tokens:    411566080 | elapsed time per iteration (ms): 4136.8 | learning rate: 2.742E-05 | global batch size:   128 | lm loss: 4.073934E+00 | loss scale: 16384.0 | grad norm: 3.821 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.942 | tokens per gpu per second (tgs): 1980.256 | TFLOPs: 15.94 |
g0069: [2024-08-02 20:09:25,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=0, lr=[2.7595025066666668e-05, 2.7595025066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1580 loss: 4.0943 iter time (s): 4.242 samples/sec: 30.172
g0093:  iteration     1580/10000000 | consumed samples:       202240 | consumed tokens:    414187520 | elapsed time per iteration (ms): 4275.9 | learning rate: 2.760E-05 | global batch size:   128 | lm loss: 4.071507E+00 | loss scale: 16384.0 | grad norm: 4.112 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.935 | tokens per gpu per second (tgs): 1915.869 | TFLOPs: 15.42 |
g0069: [2024-08-02 20:10:04,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=0, lr=[2.7769787733333337e-05, 2.7769787733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1590 loss: 4.0118 iter time (s): 3.918 samples/sec: 32.669
g0093:  iteration     1590/10000000 | consumed samples:       203520 | consumed tokens:    416808960 | elapsed time per iteration (ms): 3950.5 | learning rate: 2.777E-05 | global batch size:   128 | lm loss: 4.046760E+00 | loss scale: 16384.0 | grad norm: 4.312 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.401 | tokens per gpu per second (tgs): 2073.648 | TFLOPs: 16.69 |
g0069: [2024-08-02 20:10:48,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=0, lr=[2.7944550400000003e-05, 2.7944550400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1600 loss: 3.9664 iter time (s): 4.282 samples/sec: 29.890
g0093:  iteration     1600/10000000 | consumed samples:       204800 | consumed tokens:    419430400 | elapsed time per iteration (ms): 4315.5 | learning rate: 2.794E-05 | global batch size:   128 | lm loss: 4.017205E+00 | loss scale: 16384.0 | grad norm: 3.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.660 | tokens per gpu per second (tgs): 1898.272 | TFLOPs: 15.28 |
g0069: [2024-08-02 20:11:30,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=0, lr=[2.811931306666667e-05, 2.811931306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1610 loss: 4.0203 iter time (s): 4.185 samples/sec: 30.585
g0093:  iteration     1610/10000000 | consumed samples:       206080 | consumed tokens:    422051840 | elapsed time per iteration (ms): 4218.1 | learning rate: 2.812E-05 | global batch size:   128 | lm loss: 4.015064E+00 | loss scale: 16384.0 | grad norm: 4.470 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.345 | tokens per gpu per second (tgs): 1942.098 | TFLOPs: 15.63 |
g0069: [2024-08-02 20:12:11,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=0, lr=[2.8294075733333335e-05, 2.8294075733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1620 loss: 4.0135 iter time (s): 4.084 samples/sec: 31.339
g0093:  iteration     1620/10000000 | consumed samples:       207360 | consumed tokens:    424673280 | elapsed time per iteration (ms): 4117.9 | learning rate: 2.829E-05 | global batch size:   128 | lm loss: 4.010163E+00 | loss scale: 16384.0 | grad norm: 3.414 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.083 | tokens per gpu per second (tgs): 1989.340 | TFLOPs: 16.01 |
g0069: [2024-08-02 20:12:52,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=0, lr=[2.84688384e-05, 2.84688384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1630 loss: 3.9601 iter time (s): 4.091 samples/sec: 31.288
g0093:  iteration     1630/10000000 | consumed samples:       208640 | consumed tokens:    427294720 | elapsed time per iteration (ms): 4124.9 | learning rate: 2.847E-05 | global batch size:   128 | lm loss: 3.983406E+00 | loss scale: 16384.0 | grad norm: 3.921 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.031 | tokens per gpu per second (tgs): 1985.989 | TFLOPs: 15.98 |
g0069: [2024-08-02 20:13:34,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=0, lr=[2.864360106666667e-05, 2.864360106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1640 loss: 3.9353 iter time (s): 4.171 samples/sec: 30.686
g0093:  iteration     1640/10000000 | consumed samples:       209920 | consumed tokens:    429916160 | elapsed time per iteration (ms): 4204.4 | learning rate: 2.864E-05 | global batch size:   128 | lm loss: 3.963605E+00 | loss scale: 16384.0 | grad norm: 5.464 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.445 | tokens per gpu per second (tgs): 1948.452 | TFLOPs: 15.68 |
g0069: [2024-08-02 20:14:15,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=0, lr=[2.8818363733333337e-05, 2.8818363733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1650 loss: 3.9639 iter time (s): 4.024 samples/sec: 31.809
g0093:  iteration     1650/10000000 | consumed samples:       211200 | consumed tokens:    432537600 | elapsed time per iteration (ms): 4057.3 | learning rate: 2.882E-05 | global batch size:   128 | lm loss: 3.944675E+00 | loss scale: 16384.0 | grad norm: 3.282 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.548 | tokens per gpu per second (tgs): 2019.062 | TFLOPs: 16.25 |
g0069: [2024-08-02 20:14:57,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=0, lr=[2.8993126400000003e-05, 2.8993126400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1660 loss: 3.9206 iter time (s): 4.147 samples/sec: 30.866
g0093:  iteration     1660/10000000 | consumed samples:       212480 | consumed tokens:    435159040 | elapsed time per iteration (ms): 4181.0 | learning rate: 2.899E-05 | global batch size:   128 | lm loss: 3.903601E+00 | loss scale: 16384.0 | grad norm: 4.324 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.615 | tokens per gpu per second (tgs): 1959.354 | TFLOPs: 15.77 |
g0069: [2024-08-02 20:15:39,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=0, lr=[2.916788906666667e-05, 2.916788906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1670 loss: 3.9190 iter time (s): 4.201 samples/sec: 30.472
g0093:  iteration     1670/10000000 | consumed samples:       213760 | consumed tokens:    437780480 | elapsed time per iteration (ms): 4234.6 | learning rate: 2.917E-05 | global batch size:   128 | lm loss: 3.908672E+00 | loss scale: 16384.0 | grad norm: 2.970 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.227 | tokens per gpu per second (tgs): 1934.519 | TFLOPs: 15.57 |
g0069: [2024-08-02 20:16:22,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=0, lr=[2.934265173333334e-05, 2.934265173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1680 loss: 3.8739 iter time (s): 4.235 samples/sec: 30.226
g0093:  iteration     1680/10000000 | consumed samples:       215040 | consumed tokens:    440401920 | elapsed time per iteration (ms): 4267.9 | learning rate: 2.934E-05 | global batch size:   128 | lm loss: 3.892837E+00 | loss scale: 16384.0 | grad norm: 3.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.991 | tokens per gpu per second (tgs): 1919.440 | TFLOPs: 15.45 |
g0069: [2024-08-02 20:17:02,027] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=0, lr=[2.9517414399999998e-05, 2.9517414399999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1690 loss: 3.8843 iter time (s): 3.951 samples/sec: 32.395
g0093:  iteration     1690/10000000 | consumed samples:       216320 | consumed tokens:    443023360 | elapsed time per iteration (ms): 3983.7 | learning rate: 2.952E-05 | global batch size:   128 | lm loss: 3.879862E+00 | loss scale: 16384.0 | grad norm: 3.102 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.131 | tokens per gpu per second (tgs): 2056.360 | TFLOPs: 16.55 |
g0069: [2024-08-02 20:17:43,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=0, lr=[2.9692177066666667e-05, 2.9692177066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1700 loss: 3.9109 iter time (s): 4.131 samples/sec: 30.987
g0093:  iteration     1700/10000000 | consumed samples:       217600 | consumed tokens:    445644800 | elapsed time per iteration (ms): 4163.6 | learning rate: 2.969E-05 | global batch size:   128 | lm loss: 3.888257E+00 | loss scale: 16384.0 | grad norm: 4.134 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.743 | tokens per gpu per second (tgs): 1967.525 | TFLOPs: 15.83 |
g0069: [2024-08-02 20:18:27,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=0, lr=[2.9866939733333333e-05, 2.9866939733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1710 loss: 3.8135 iter time (s): 4.350 samples/sec: 29.426
g0093:  iteration     1710/10000000 | consumed samples:       218880 | consumed tokens:    448266240 | elapsed time per iteration (ms): 4382.7 | learning rate: 2.987E-05 | global batch size:   128 | lm loss: 3.848153E+00 | loss scale: 16384.0 | grad norm: 3.292 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.206 | tokens per gpu per second (tgs): 1869.176 | TFLOPs: 15.04 |
g0069: [2024-08-02 20:19:09,202] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=0, lr=[3.00417024e-05, 3.00417024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1720 loss: 3.8411 iter time (s): 4.139 samples/sec: 30.928
g0093:  iteration     1720/10000000 | consumed samples:       220160 | consumed tokens:    450887680 | elapsed time per iteration (ms): 4171.2 | learning rate: 3.004E-05 | global batch size:   128 | lm loss: 3.841419E+00 | loss scale: 16384.0 | grad norm: 2.749 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.687 | tokens per gpu per second (tgs): 1963.953 | TFLOPs: 15.80 |
g0069: [2024-08-02 20:19:52,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=0, lr=[3.0216465066666665e-05, 3.0216465066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1730 loss: 3.7832 iter time (s): 4.325 samples/sec: 29.594
g0093:  iteration     1730/10000000 | consumed samples:       221440 | consumed tokens:    453509120 | elapsed time per iteration (ms): 4358.0 | learning rate: 3.022E-05 | global batch size:   128 | lm loss: 3.799123E+00 | loss scale: 16384.0 | grad norm: 2.866 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.371 | tokens per gpu per second (tgs): 1879.775 | TFLOPs: 15.13 |
g0069: [2024-08-02 20:20:34,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=0, lr=[3.0391227733333335e-05, 3.0391227733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1740 loss: 3.8539 iter time (s): 4.179 samples/sec: 30.627
g0093:  iteration     1740/10000000 | consumed samples:       222720 | consumed tokens:    456130560 | elapsed time per iteration (ms): 4212.2 | learning rate: 3.039E-05 | global batch size:   128 | lm loss: 3.822825E+00 | loss scale: 16384.0 | grad norm: 3.649 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.388 | tokens per gpu per second (tgs): 1944.843 | TFLOPs: 15.65 |
g0069: [2024-08-02 20:21:17,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=0, lr=[3.05659904e-05, 3.05659904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1750 loss: 3.7650 iter time (s): 4.245 samples/sec: 30.155
g0093:  iteration     1750/10000000 | consumed samples:       224000 | consumed tokens:    458752000 | elapsed time per iteration (ms): 4277.6 | learning rate: 3.057E-05 | global batch size:   128 | lm loss: 3.795212E+00 | loss scale: 16384.0 | grad norm: 3.368 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.923 | tokens per gpu per second (tgs): 1915.097 | TFLOPs: 15.41 |
g0069: [2024-08-02 20:22:02,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=0, lr=[3.0740753066666664e-05, 3.0740753066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1760 loss: 3.7843 iter time (s): 4.408 samples/sec: 29.040
g0093:  iteration     1760/10000000 | consumed samples:       225280 | consumed tokens:    461373440 | elapsed time per iteration (ms): 4459.5 | learning rate: 3.074E-05 | global batch size:   128 | lm loss: 3.787253E+00 | loss scale: 16384.0 | grad norm: 2.682 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.703 | tokens per gpu per second (tgs): 1836.986 | TFLOPs: 14.78 |
g0069: [2024-08-02 20:22:44,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=0, lr=[3.0915515733333336e-05, 3.0915515733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1770 loss: 3.7845 iter time (s): 4.142 samples/sec: 30.904
g0093:  iteration     1770/10000000 | consumed samples:       226560 | consumed tokens:    463994880 | elapsed time per iteration (ms): 4175.9 | learning rate: 3.092E-05 | global batch size:   128 | lm loss: 3.772577E+00 | loss scale: 16384.0 | grad norm: 3.070 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.652 | tokens per gpu per second (tgs): 1961.715 | TFLOPs: 15.79 |
g0069: [2024-08-02 20:23:27,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=0, lr=[3.10902784e-05, 3.10902784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1780 loss: 3.7318 iter time (s): 4.304 samples/sec: 29.743
g0093:  iteration     1780/10000000 | consumed samples:       227840 | consumed tokens:    466616320 | elapsed time per iteration (ms): 4336.2 | learning rate: 3.109E-05 | global batch size:   128 | lm loss: 3.753156E+00 | loss scale: 16384.0 | grad norm: 3.613 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.519 | tokens per gpu per second (tgs): 1889.203 | TFLOPs: 15.20 |
g0069: [2024-08-02 20:24:10,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=0, lr=[3.126504106666667e-05, 3.126504106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1790 loss: 3.7349 iter time (s): 4.280 samples/sec: 29.905
g0093:  iteration     1790/10000000 | consumed samples:       229120 | consumed tokens:    469237760 | elapsed time per iteration (ms): 4313.6 | learning rate: 3.127E-05 | global batch size:   128 | lm loss: 3.755521E+00 | loss scale: 16384.0 | grad norm: 3.159 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.674 | tokens per gpu per second (tgs): 1899.131 | TFLOPs: 15.28 |
g0069: [2024-08-02 20:24:57,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=0, lr=[3.1439803733333335e-05, 3.1439803733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1800 loss: 3.7703 iter time (s): 4.620 samples/sec: 27.703
g0093:  iteration     1800/10000000 | consumed samples:       230400 | consumed tokens:    471859200 | elapsed time per iteration (ms): 4653.3 | learning rate: 3.144E-05 | global batch size:   128 | lm loss: 3.732710E+00 | loss scale: 16384.0 | grad norm: 2.652 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.507 | tokens per gpu per second (tgs): 1760.461 | TFLOPs: 14.17 |
g0069: [2024-08-02 20:25:38,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=0, lr=[3.16145664e-05, 3.16145664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1810 loss: 3.7143 iter time (s): 4.154 samples/sec: 30.811
g0093:  iteration     1810/10000000 | consumed samples:       231680 | consumed tokens:    474480640 | elapsed time per iteration (ms): 4186.9 | learning rate: 3.161E-05 | global batch size:   128 | lm loss: 3.731757E+00 | loss scale: 16384.0 | grad norm: 3.116 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.571 | tokens per gpu per second (tgs): 1956.560 | TFLOPs: 15.74 |
g0069: [2024-08-02 20:26:21,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=0, lr=[3.178932906666667e-05, 3.178932906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1820 loss: 3.6685 iter time (s): 4.229 samples/sec: 30.266
g0093:  iteration     1820/10000000 | consumed samples:       232960 | consumed tokens:    477102080 | elapsed time per iteration (ms): 4263.2 | learning rate: 3.179E-05 | global batch size:   128 | lm loss: 3.698479E+00 | loss scale: 16384.0 | grad norm: 2.453 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.025 | tokens per gpu per second (tgs): 1921.570 | TFLOPs: 15.46 |
g0069: [2024-08-02 20:27:04,999] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=0, lr=[3.196409173333333e-05, 3.196409173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1830 loss: 3.6958 iter time (s): 4.309 samples/sec: 29.705
g0093:  iteration     1830/10000000 | consumed samples:       234240 | consumed tokens:    479723520 | elapsed time per iteration (ms): 4346.2 | learning rate: 3.196E-05 | global batch size:   128 | lm loss: 3.700312E+00 | loss scale: 16384.0 | grad norm: 3.399 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.451 | tokens per gpu per second (tgs): 1884.869 | TFLOPs: 15.17 |
g0069: [2024-08-02 20:27:47,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=0, lr=[3.21388544e-05, 3.21388544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1840 loss: 3.7161 iter time (s): 4.174 samples/sec: 30.669
g0093:  iteration     1840/10000000 | consumed samples:       235520 | consumed tokens:    482344960 | elapsed time per iteration (ms): 4206.2 | learning rate: 3.214E-05 | global batch size:   128 | lm loss: 3.699156E+00 | loss scale: 16384.0 | grad norm: 2.989 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.431 | tokens per gpu per second (tgs): 1947.585 | TFLOPs: 15.67 |
g0069: [2024-08-02 20:28:31,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=0, lr=[3.2313617066666665e-05, 3.2313617066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1850 loss: 3.6877 iter time (s): 4.412 samples/sec: 29.014
g0093:  iteration     1850/10000000 | consumed samples:       236800 | consumed tokens:    484966400 | elapsed time per iteration (ms): 4444.3 | learning rate: 3.231E-05 | global batch size:   128 | lm loss: 3.681678E+00 | loss scale: 16384.0 | grad norm: 2.684 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.801 | tokens per gpu per second (tgs): 1843.262 | TFLOPs: 14.83 |
g0069: [2024-08-02 20:29:14,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=0, lr=[3.248837973333334e-05, 3.248837973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1860 loss: 3.7141 iter time (s): 4.258 samples/sec: 30.064
g0093:  iteration     1860/10000000 | consumed samples:       238080 | consumed tokens:    487587840 | elapsed time per iteration (ms): 4290.1 | learning rate: 3.249E-05 | global batch size:   128 | lm loss: 3.661608E+00 | loss scale: 16384.0 | grad norm: 3.558 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.836 | tokens per gpu per second (tgs): 1909.527 | TFLOPs: 15.37 |
g0069: [2024-08-02 20:29:57,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=0, lr=[3.2663142400000004e-05, 3.2663142400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1870 loss: 3.6530 iter time (s): 4.322 samples/sec: 29.619
g0093:  iteration     1870/10000000 | consumed samples:       239360 | consumed tokens:    490209280 | elapsed time per iteration (ms): 4355.1 | learning rate: 3.266E-05 | global batch size:   128 | lm loss: 3.657730E+00 | loss scale: 16384.0 | grad norm: 2.853 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.391 | tokens per gpu per second (tgs): 1881.020 | TFLOPs: 15.14 |
g0069: [2024-08-02 20:30:41,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=0, lr=[3.283790506666667e-05, 3.283790506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1880 loss: 3.6541 iter time (s): 4.276 samples/sec: 29.935
g0093:  iteration     1880/10000000 | consumed samples:       240640 | consumed tokens:    492830720 | elapsed time per iteration (ms): 4308.9 | learning rate: 3.284E-05 | global batch size:   128 | lm loss: 3.639259E+00 | loss scale: 16384.0 | grad norm: 2.553 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.706 | tokens per gpu per second (tgs): 1901.184 | TFLOPs: 15.30 |
g0069: [2024-08-02 20:31:22,858] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=0, lr=[3.3012667733333336e-05, 3.3012667733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1890 loss: 3.5779 iter time (s): 4.145 samples/sec: 30.880
g0093:  iteration     1890/10000000 | consumed samples:       241920 | consumed tokens:    495452160 | elapsed time per iteration (ms): 4178.0 | learning rate: 3.301E-05 | global batch size:   128 | lm loss: 3.646185E+00 | loss scale: 16384.0 | grad norm: 2.817 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.636 | tokens per gpu per second (tgs): 1960.725 | TFLOPs: 15.78 |
g0069: [2024-08-02 20:32:04,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=0, lr=[3.31874304e-05, 3.31874304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1900 loss: 3.6121 iter time (s): 4.176 samples/sec: 30.648
g0093:  iteration     1900/10000000 | consumed samples:       243200 | consumed tokens:    498073600 | elapsed time per iteration (ms): 4209.0 | learning rate: 3.319E-05 | global batch size:   128 | lm loss: 3.632784E+00 | loss scale: 16384.0 | grad norm: 3.280 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.411 | tokens per gpu per second (tgs): 1946.298 | TFLOPs: 15.66 |
g0069: [2024-08-02 20:32:45,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=0, lr=[3.336219306666667e-05, 3.336219306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1910 loss: 3.6416 iter time (s): 4.062 samples/sec: 31.512
g0093:  iteration     1910/10000000 | consumed samples:       244480 | consumed tokens:    500695040 | elapsed time per iteration (ms): 4095.1 | learning rate: 3.336E-05 | global batch size:   128 | lm loss: 3.610167E+00 | loss scale: 16384.0 | grad norm: 3.112 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.257 | tokens per gpu per second (tgs): 2000.423 | TFLOPs: 16.10 |
g0069: [2024-08-02 20:33:26,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=0, lr=[3.3536955733333334e-05, 3.3536955733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1920 loss: 3.5346 iter time (s): 3.986 samples/sec: 32.112
g0093:  iteration     1920/10000000 | consumed samples:       245760 | consumed tokens:    503316480 | elapsed time per iteration (ms): 4020.3 | learning rate: 3.354E-05 | global batch size:   128 | lm loss: 3.594915E+00 | loss scale: 16384.0 | grad norm: 2.346 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.839 | tokens per gpu per second (tgs): 2037.684 | TFLOPs: 16.40 |
g0069: [2024-08-02 20:34:06,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=0, lr=[3.37117184e-05, 3.37117184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1930 loss: 3.6035 iter time (s): 4.040 samples/sec: 31.684
g0093:  iteration     1930/10000000 | consumed samples:       247040 | consumed tokens:    505937920 | elapsed time per iteration (ms): 4072.3 | learning rate: 3.371E-05 | global batch size:   128 | lm loss: 3.618340E+00 | loss scale: 16384.0 | grad norm: 3.272 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.432 | tokens per gpu per second (tgs): 2011.631 | TFLOPs: 16.19 |
g0069: [2024-08-02 20:34:49,514] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=0, lr=[3.388648106666667e-05, 3.388648106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1940 loss: 3.5839 iter time (s): 4.237 samples/sec: 30.213
g0093:  iteration     1940/10000000 | consumed samples:       248320 | consumed tokens:    508559360 | elapsed time per iteration (ms): 4269.0 | learning rate: 3.389E-05 | global batch size:   128 | lm loss: 3.593487E+00 | loss scale: 16384.0 | grad norm: 2.278 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.984 | tokens per gpu per second (tgs): 1918.945 | TFLOPs: 15.44 |
g0069: [2024-08-02 20:35:32,032] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=0, lr=[3.406124373333334e-05, 3.406124373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1950 loss: 3.5995 iter time (s): 4.219 samples/sec: 30.341
g0093:  iteration     1950/10000000 | consumed samples:       249600 | consumed tokens:    511180800 | elapsed time per iteration (ms): 4251.9 | learning rate: 3.406E-05 | global batch size:   128 | lm loss: 3.581808E+00 | loss scale: 16384.0 | grad norm: 2.744 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.104 | tokens per gpu per second (tgs): 1926.663 | TFLOPs: 15.50 |
g0069: [2024-08-02 20:36:10,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=0, lr=[3.4236006400000005e-05, 3.4236006400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1960 loss: 3.5879 iter time (s): 3.804 samples/sec: 33.646
g0093:  iteration     1960/10000000 | consumed samples:       250880 | consumed tokens:    513802240 | elapsed time per iteration (ms): 3837.8 | learning rate: 3.424E-05 | global batch size:   128 | lm loss: 3.593955E+00 | loss scale: 16384.0 | grad norm: 2.476 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.353 | tokens per gpu per second (tgs): 2134.584 | TFLOPs: 17.18 |
g0069: [2024-08-02 20:36:50,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=0, lr=[3.441076906666667e-05, 3.441076906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1970 loss: 3.5635 iter time (s): 3.996 samples/sec: 32.032
g0093:  iteration     1970/10000000 | consumed samples:       252160 | consumed tokens:    516423680 | elapsed time per iteration (ms): 4029.2 | learning rate: 3.441E-05 | global batch size:   128 | lm loss: 3.553267E+00 | loss scale: 16384.0 | grad norm: 2.286 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.768 | tokens per gpu per second (tgs): 2033.139 | TFLOPs: 16.36 |
g0069: [2024-08-02 20:37:32,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=0, lr=[3.458553173333334e-05, 3.458553173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1980 loss: 3.5797 iter time (s): 4.190 samples/sec: 30.547
g0093:  iteration     1980/10000000 | consumed samples:       253440 | consumed tokens:    519045120 | elapsed time per iteration (ms): 4223.6 | learning rate: 3.459E-05 | global batch size:   128 | lm loss: 3.567392E+00 | loss scale: 16384.0 | grad norm: 2.626 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.306 | tokens per gpu per second (tgs): 1939.556 | TFLOPs: 15.61 |
g0069: [2024-08-02 20:38:15,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=0, lr=[3.47602944e-05, 3.47602944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 1990 loss: 3.4979 iter time (s): 4.260 samples/sec: 30.048
g0093:  iteration     1990/10000000 | consumed samples:       254720 | consumed tokens:    521666560 | elapsed time per iteration (ms): 4292.3 | learning rate: 3.476E-05 | global batch size:   128 | lm loss: 3.557921E+00 | loss scale: 16384.0 | grad norm: 2.396 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.821 | tokens per gpu per second (tgs): 1908.518 | TFLOPs: 15.36 |
g0069: [2024-08-02 20:38:56,162] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=0, lr=[3.493505706666667e-05, 3.493505706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2000 loss: 3.5582 iter time (s): 3.997 samples/sec: 32.022
g0093:  iteration     2000/10000000 | consumed samples:       256000 | consumed tokens:    524288000 | elapsed time per iteration (ms): 4029.7 | learning rate: 3.494E-05 | global batch size:   128 | lm loss: 3.551015E+00 | loss scale: 16384.0 | grad norm: 2.714 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.764 | tokens per gpu per second (tgs): 2032.908 | TFLOPs: 16.36 |
g0093: ------------------------------------------------------------------------------------------------
g0093:  validation loss at iteration 2000 | lm loss value: 3.549829E+00 | lm loss PPL: 3.480736E+01 | 
g0093: ------------------------------------------------------------------------------------------------
g0069: saving checkpoint at iteration    2000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: [2024-08-02 20:45:23,855] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
g0093: [2024-08-02 20:45:23,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0093: [2024-08-02 20:45:23,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0093: [2024-08-02 20:45:23,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0069: [2024-08-02 20:45:23,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0069: [2024-08-02 20:45:23,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0069: [2024-08-02 20:45:23,862] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0087: [2024-08-02 20:45:23,863] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0087: [2024-08-02 20:45:23,863] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0087: [2024-08-02 20:45:23,863] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0092: [2024-08-02 20:45:23,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0092: [2024-08-02 20:45:23,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0091: [2024-08-02 20:45:23,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0091: [2024-08-02 20:45:23,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0092: [2024-08-02 20:45:23,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0091: [2024-08-02 20:45:23,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0090: [2024-08-02 20:45:23,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0088: [2024-08-02 20:45:23,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0088: [2024-08-02 20:45:23,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0088: [2024-08-02 20:45:23,864] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0090: [2024-08-02 20:45:23,865] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0090: [2024-08-02 20:45:23,865] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0086: [2024-08-02 20:45:23,865] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0086: [2024-08-02 20:45:23,865] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0086: [2024-08-02 20:45:23,865] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0093: [2024-08-02 20:45:23,890] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_23-model_00-model_states.pt...
g0087: [2024-08-02 20:45:23,898] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_08-model_00-model_states.pt...
g0090: [2024-08-02 20:45:23,898] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_14-model_00-model_states.pt...
g0088: [2024-08-02 20:45:23,898] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_11-model_00-model_states.pt...
g0092: [2024-08-02 20:45:23,904] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_20-model_00-model_states.pt...
g0086: [2024-08-02 20:45:23,904] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_05-model_00-model_states.pt...
g0091: [2024-08-02 20:45:23,904] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_17-model_00-model_states.pt...
g0069: [2024-08-02 20:45:23,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_01-model_00-model_states.pt...
g0086: [2024-08-02 20:45:24,010] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_05-model_00-model_states.pt.
g0088: [2024-08-02 20:45:24,044] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_11-model_00-model_states.pt.
g0086: [2024-08-02 20:45:24,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_06-model_00-model_states.pt...
g0087: [2024-08-02 20:45:24,060] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_08-model_00-model_states.pt.
g0091: [2024-08-02 20:45:24,075] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_17-model_00-model_states.pt.
g0090: [2024-08-02 20:45:24,077] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_14-model_00-model_states.pt.
g0088: [2024-08-02 20:45:24,078] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_12-model_00-model_states.pt...
g0087: [2024-08-02 20:45:24,095] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_09-model_00-model_states.pt...
g0090: [2024-08-02 20:45:24,112] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_15-model_00-model_states.pt...
g0091: [2024-08-02 20:45:24,115] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_18-model_00-model_states.pt...
g0086: [2024-08-02 20:45:24,149] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_06-model_00-model_states.pt.
g0086: [2024-08-02 20:45:24,184] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_07-model_00-model_states.pt...
g0088: [2024-08-02 20:45:24,194] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_12-model_00-model_states.pt.
g0092: [2024-08-02 20:45:24,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_20-model_00-model_states.pt.
g0093: [2024-08-02 20:45:24,216] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_23-model_00-model_states.pt.
g0093: [2024-08-02 20:45:24,217] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_24-model_00-model_states.pt...
g0093: [2024-08-02 20:45:24,218] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_24-model_00-model_states.pt.
g0087: [2024-08-02 20:45:24,222] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_09-model_00-model_states.pt.
g0088: [2024-08-02 20:45:24,224] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_13-model_00-model_states.pt...
g0092: [2024-08-02 20:45:24,237] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_21-model_00-model_states.pt...
g0087: [2024-08-02 20:45:24,248] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_10-model_00-model_states.pt...
g0069: [2024-08-02 20:45:24,258] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_01-model_00-model_states.pt.
g0093: [2024-08-02 20:45:24,271] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_25-model_00-model_states.pt...
g0090: [2024-08-02 20:45:24,285] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_15-model_00-model_states.pt.
g0069: [2024-08-02 20:45:24,286] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_02-model_00-model_states.pt...
g0091: [2024-08-02 20:45:24,291] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_18-model_00-model_states.pt.
g0086: [2024-08-02 20:45:24,305] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_07-model_00-model_states.pt.
g0086: [2024-08-02 20:45:24,308] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_01_model_states.pt...
g0090: [2024-08-02 20:45:24,311] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_16-model_00-model_states.pt...
g0091: [2024-08-02 20:45:24,327] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_19-model_00-model_states.pt...
g0092: [2024-08-02 20:45:24,345] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_21-model_00-model_states.pt.
g0087: [2024-08-02 20:45:24,350] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_10-model_00-model_states.pt.
g0087: [2024-08-02 20:45:24,352] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_02_model_states.pt...
g0088: [2024-08-02 20:45:24,359] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_13-model_00-model_states.pt.
g0088: [2024-08-02 20:45:24,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_03_model_states.pt...
g0092: [2024-08-02 20:45:24,380] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_22-model_00-model_states.pt...
g0090: [2024-08-02 20:45:24,445] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_16-model_00-model_states.pt.
g0090: [2024-08-02 20:45:24,447] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_04_model_states.pt...
g0069: [2024-08-02 20:45:24,461] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_02-model_00-model_states.pt.
g0093: [2024-08-02 20:45:24,464] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_25-model_00-model_states.pt.
g0093: [2024-08-02 20:45:24,465] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_07_model_states.pt...
g0069: [2024-08-02 20:45:24,494] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_03-model_00-model_states.pt...
g0091: [2024-08-02 20:45:24,547] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_19-model_00-model_states.pt.
g0091: [2024-08-02 20:45:24,549] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_05_model_states.pt...
g0092: [2024-08-02 20:45:24,606] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_22-model_00-model_states.pt.
g0092: [2024-08-02 20:45:24,608] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_06_model_states.pt...
g0069: [2024-08-02 20:45:24,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_03-model_00-model_states.pt.
g0069: [2024-08-02 20:45:24,653] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_04-model_00-model_states.pt...
g0069: [2024-08-02 20:45:24,816] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_04-model_00-model_states.pt.
g0069: [2024-08-02 20:45:24,818] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt
g0069: [2024-08-02 20:45:24,819] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt...
g0093: [2024-08-02 20:45:26,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_07_model_states.pt.
g0093: [2024-08-02 20:45:26,384] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0087: [2024-08-02 20:45:26,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_02_model_states.pt.
g0087: [2024-08-02 20:45:26,655] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0086: [2024-08-02 20:45:26,683] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_01_model_states.pt.
g0086: [2024-08-02 20:45:26,684] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0088: [2024-08-02 20:45:26,705] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_03_model_states.pt.
g0088: [2024-08-02 20:45:26,705] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0090: [2024-08-02 20:45:26,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_04_model_states.pt.
g0090: [2024-08-02 20:45:26,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0091: [2024-08-02 20:45:26,914] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_05_model_states.pt.
g0091: [2024-08-02 20:45:26,914] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0092: [2024-08-02 20:45:27,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_06_model_states.pt.
g0092: [2024-08-02 20:45:27,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0069: [2024-08-02 20:45:28,125] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt.
g0069: [2024-08-02 20:45:28,126] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0069:   successfully saved checkpoint at iteration    2000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: Checkpoint Save GB: 22.521, GB/Sec: 5.26, Latency(second): 4.284
g0093: (min, max) time across ranks (ms):
g0093:     save-checkpoint ................................: (4283.78, 4283.92)
g0090: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0090: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0090: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0092: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0086: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0092: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0092: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0092: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0086: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0086: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0069: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0091: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0088: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0091: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0091: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0090: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0087: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0092: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0087: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0091: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 20:45:32,762] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0069: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0087: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0087: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0087: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0093: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0093: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0093: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0093: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0091: [2024-08-02 20:45:32,761] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0092: [2024-08-02 20:45:32,762] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0088: [2024-08-02 20:45:32,762] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0093: [2024-08-02 20:45:32,762] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0069: [2024-08-02 20:46:11,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=0, lr=[3.5109819733333335e-05, 3.5109819733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2010 loss: 3.5487 iter time (s): 4.339 samples/sec: 29.502
g0093:  iteration     2010/10000000 | consumed samples:       257280 | consumed tokens:    526909440 | elapsed time per iteration (ms): 43563.9 | learning rate: 3.511E-05 | global batch size:   128 | lm loss: 3.528973E+00 | loss scale: 32768.0 | grad norm: 2.297 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.938 | tokens per gpu per second (tgs): 188.045 | TFLOPs: 1.51 |
g0069: [2024-08-02 20:46:53,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=0, lr=[3.52845824e-05, 3.52845824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2020 loss: 3.4726 iter time (s): 4.170 samples/sec: 30.694
g0093:  iteration     2020/10000000 | consumed samples:       258560 | consumed tokens:    529530880 | elapsed time per iteration (ms): 4202.9 | learning rate: 3.528E-05 | global batch size:   128 | lm loss: 3.514824E+00 | loss scale: 32768.0 | grad norm: 2.056 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.455 | tokens per gpu per second (tgs): 1949.109 | TFLOPs: 15.68 |
g0069: [2024-08-02 20:47:36,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=0, lr=[3.545934506666667e-05, 3.545934506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2030 loss: 3.4848 iter time (s): 4.184 samples/sec: 30.590
g0093:  iteration     2030/10000000 | consumed samples:       259840 | consumed tokens:    532152320 | elapsed time per iteration (ms): 4217.3 | learning rate: 3.546E-05 | global batch size:   128 | lm loss: 3.527138E+00 | loss scale: 32768.0 | grad norm: 2.411 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.351 | tokens per gpu per second (tgs): 1942.464 | TFLOPs: 15.63 |
g0069: [2024-08-02 20:48:17,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=0, lr=[3.5634107733333334e-05, 3.5634107733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2040 loss: 3.5143 iter time (s): 4.106 samples/sec: 31.176
g0093:  iteration     2040/10000000 | consumed samples:       261120 | consumed tokens:    534773760 | elapsed time per iteration (ms): 4139.0 | learning rate: 3.563E-05 | global batch size:   128 | lm loss: 3.509494E+00 | loss scale: 32768.0 | grad norm: 2.571 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.925 | tokens per gpu per second (tgs): 1979.221 | TFLOPs: 15.93 |
g0069: [2024-08-02 20:48:58,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=0, lr=[3.58088704e-05, 3.58088704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2050 loss: 3.4805 iter time (s): 4.097 samples/sec: 31.243
g0093:  iteration     2050/10000000 | consumed samples:       262400 | consumed tokens:    537395200 | elapsed time per iteration (ms): 4129.6 | learning rate: 3.581E-05 | global batch size:   128 | lm loss: 3.509192E+00 | loss scale: 32768.0 | grad norm: 2.510 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.996 | tokens per gpu per second (tgs): 1983.737 | TFLOPs: 15.96 |
g0069: [2024-08-02 20:49:39,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=0, lr=[3.5983633066666666e-05, 3.5983633066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2060 loss: 3.5269 iter time (s): 4.023 samples/sec: 31.814
g0093:  iteration     2060/10000000 | consumed samples:       263680 | consumed tokens:    540016640 | elapsed time per iteration (ms): 4057.2 | learning rate: 3.598E-05 | global batch size:   128 | lm loss: 3.503255E+00 | loss scale: 32768.0 | grad norm: 2.401 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.549 | tokens per gpu per second (tgs): 2019.146 | TFLOPs: 16.25 |
g0069: [2024-08-02 20:50:20,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=0, lr=[3.615839573333333e-05, 3.615839573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2070 loss: 3.4653 iter time (s): 4.075 samples/sec: 31.408
g0093:  iteration     2070/10000000 | consumed samples:       264960 | consumed tokens:    542638080 | elapsed time per iteration (ms): 4108.7 | learning rate: 3.616E-05 | global batch size:   128 | lm loss: 3.503210E+00 | loss scale: 32768.0 | grad norm: 2.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.154 | tokens per gpu per second (tgs): 1993.836 | TFLOPs: 16.04 |
g0069: [2024-08-02 20:51:01,120] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=0, lr=[3.63331584e-05, 3.63331584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2080 loss: 3.4184 iter time (s): 4.044 samples/sec: 31.649
g0093:  iteration     2080/10000000 | consumed samples:       266240 | consumed tokens:    545259520 | elapsed time per iteration (ms): 4077.1 | learning rate: 3.633E-05 | global batch size:   128 | lm loss: 3.472017E+00 | loss scale: 32768.0 | grad norm: 2.288 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.395 | tokens per gpu per second (tgs): 2009.273 | TFLOPs: 16.17 |
g0069: [2024-08-02 20:51:43,120] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=0, lr=[3.6507921066666664e-05, 3.6507921066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2090 loss: 3.4696 iter time (s): 4.167 samples/sec: 30.717
g0093:  iteration     2090/10000000 | consumed samples:       267520 | consumed tokens:    547880960 | elapsed time per iteration (ms): 4200.0 | learning rate: 3.651E-05 | global batch size:   128 | lm loss: 3.468369E+00 | loss scale: 32768.0 | grad norm: 2.490 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.477 | tokens per gpu per second (tgs): 1950.499 | TFLOPs: 15.70 |
g0069: [2024-08-02 20:52:25,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=0, lr=[3.668268373333334e-05, 3.668268373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2100 loss: 3.4724 iter time (s): 4.193 samples/sec: 30.525
g0093:  iteration     2100/10000000 | consumed samples:       268800 | consumed tokens:    550502400 | elapsed time per iteration (ms): 4225.8 | learning rate: 3.668E-05 | global batch size:   128 | lm loss: 3.481200E+00 | loss scale: 32768.0 | grad norm: 2.357 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.290 | tokens per gpu per second (tgs): 1938.587 | TFLOPs: 15.60 |
g0069: [2024-08-02 20:53:06,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=2110, skipped=0, lr=[3.68574464e-05, 3.68574464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2110 loss: 3.4518 iter time (s): 4.053 samples/sec: 31.584
g0093:  iteration     2110/10000000 | consumed samples:       270080 | consumed tokens:    553123840 | elapsed time per iteration (ms): 4085.6 | learning rate: 3.686E-05 | global batch size:   128 | lm loss: 3.468137E+00 | loss scale: 32768.0 | grad norm: 2.681 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.330 | tokens per gpu per second (tgs): 2005.104 | TFLOPs: 16.14 |
g0069: [2024-08-02 20:53:48,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=2120, skipped=0, lr=[3.703220906666667e-05, 3.703220906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2120 loss: 3.4578 iter time (s): 4.183 samples/sec: 30.597
g0093:  iteration     2120/10000000 | consumed samples:       271360 | consumed tokens:    555745280 | elapsed time per iteration (ms): 4215.8 | learning rate: 3.703E-05 | global batch size:   128 | lm loss: 3.467915E+00 | loss scale: 32768.0 | grad norm: 2.764 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.362 | tokens per gpu per second (tgs): 1943.150 | TFLOPs: 15.64 |
g0069: [2024-08-02 20:54:30,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=2130, skipped=0, lr=[3.7206971733333335e-05, 3.7206971733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2130 loss: 3.4183 iter time (s): 4.186 samples/sec: 30.580
g0093:  iteration     2130/10000000 | consumed samples:       272640 | consumed tokens:    558366720 | elapsed time per iteration (ms): 4218.4 | learning rate: 3.721E-05 | global batch size:   128 | lm loss: 3.454657E+00 | loss scale: 32768.0 | grad norm: 2.289 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.343 | tokens per gpu per second (tgs): 1941.962 | TFLOPs: 15.63 |
g0069: [2024-08-02 20:55:11,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=2140, skipped=0, lr=[3.73817344e-05, 3.73817344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2140 loss: 3.4138 iter time (s): 4.109 samples/sec: 31.154
g0093:  iteration     2140/10000000 | consumed samples:       273920 | consumed tokens:    560988160 | elapsed time per iteration (ms): 4141.0 | learning rate: 3.738E-05 | global batch size:   128 | lm loss: 3.445576E+00 | loss scale: 32768.0 | grad norm: 2.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.911 | tokens per gpu per second (tgs): 1978.275 | TFLOPs: 15.92 |
g0069: [2024-08-02 20:55:54,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=2150, skipped=0, lr=[3.755649706666667e-05, 3.755649706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2150 loss: 3.4816 iter time (s): 4.259 samples/sec: 30.055
g0093:  iteration     2150/10000000 | consumed samples:       275200 | consumed tokens:    563609600 | elapsed time per iteration (ms): 4292.0 | learning rate: 3.756E-05 | global batch size:   128 | lm loss: 3.450083E+00 | loss scale: 32768.0 | grad norm: 2.561 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.823 | tokens per gpu per second (tgs): 1908.651 | TFLOPs: 15.36 |
g0069: [2024-08-02 20:56:37,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=2160, skipped=0, lr=[3.773125973333333e-05, 3.773125973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2160 loss: 3.4430 iter time (s): 4.191 samples/sec: 30.538
g0093:  iteration     2160/10000000 | consumed samples:       276480 | consumed tokens:    566231040 | elapsed time per iteration (ms): 4223.7 | learning rate: 3.773E-05 | global batch size:   128 | lm loss: 3.419917E+00 | loss scale: 32768.0 | grad norm: 2.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.305 | tokens per gpu per second (tgs): 1939.515 | TFLOPs: 15.61 |
g0069: [2024-08-02 20:57:18,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=2170, skipped=0, lr=[3.79060224e-05, 3.79060224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2170 loss: 3.3809 iter time (s): 4.131 samples/sec: 30.986
g0093:  iteration     2170/10000000 | consumed samples:       277760 | consumed tokens:    568852480 | elapsed time per iteration (ms): 4163.4 | learning rate: 3.791E-05 | global batch size:   128 | lm loss: 3.428447E+00 | loss scale: 32768.0 | grad norm: 2.252 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.744 | tokens per gpu per second (tgs): 1967.640 | TFLOPs: 15.83 |
g0069: [2024-08-02 20:58:01,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=2180, skipped=0, lr=[3.8080785066666665e-05, 3.8080785066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2180 loss: 3.4158 iter time (s): 4.213 samples/sec: 30.380
g0093:  iteration     2180/10000000 | consumed samples:       279040 | consumed tokens:    571473920 | elapsed time per iteration (ms): 4245.8 | learning rate: 3.808E-05 | global batch size:   128 | lm loss: 3.413927E+00 | loss scale: 32768.0 | grad norm: 2.358 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.147 | tokens per gpu per second (tgs): 1929.431 | TFLOPs: 15.53 |
g0069: [2024-08-02 20:58:41,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=2190, skipped=0, lr=[3.825554773333334e-05, 3.825554773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2190 loss: 3.4245 iter time (s): 3.986 samples/sec: 32.114
g0093:  iteration     2190/10000000 | consumed samples:       280320 | consumed tokens:    574095360 | elapsed time per iteration (ms): 4018.3 | learning rate: 3.826E-05 | global batch size:   128 | lm loss: 3.391615E+00 | loss scale: 32768.0 | grad norm: 1.981 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.854 | tokens per gpu per second (tgs): 2038.657 | TFLOPs: 16.41 |
g0069: [2024-08-02 20:59:23,474] [INFO] [logging.py:96:log_dist] [Rank 0] step=2200, skipped=0, lr=[3.8430310400000004e-05, 3.8430310400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2200 loss: 3.4242 iter time (s): 4.173 samples/sec: 30.675
g0093:  iteration     2200/10000000 | consumed samples:       281600 | consumed tokens:    576716800 | elapsed time per iteration (ms): 4205.5 | learning rate: 3.843E-05 | global batch size:   128 | lm loss: 3.396916E+00 | loss scale: 32768.0 | grad norm: 2.293 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.437 | tokens per gpu per second (tgs): 1947.947 | TFLOPs: 15.68 |
g0069: [2024-08-02 21:00:04,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=2210, skipped=0, lr=[3.860507306666667e-05, 3.860507306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2210 loss: 3.3883 iter time (s): 4.075 samples/sec: 31.410
g0093:  iteration     2210/10000000 | consumed samples:       282880 | consumed tokens:    579338240 | elapsed time per iteration (ms): 4110.3 | learning rate: 3.861E-05 | global batch size:   128 | lm loss: 3.400927E+00 | loss scale: 32768.0 | grad norm: 2.313 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.141 | tokens per gpu per second (tgs): 1993.035 | TFLOPs: 16.04 |
g0069: [2024-08-02 21:00:46,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=2220, skipped=0, lr=[3.8779835733333336e-05, 3.8779835733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2220 loss: 3.3832 iter time (s): 4.142 samples/sec: 30.906
g0093:  iteration     2220/10000000 | consumed samples:       284160 | consumed tokens:    581959680 | elapsed time per iteration (ms): 4177.4 | learning rate: 3.878E-05 | global batch size:   128 | lm loss: 3.377108E+00 | loss scale: 32768.0 | grad norm: 2.169 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.641 | tokens per gpu per second (tgs): 1961.039 | TFLOPs: 15.78 |
g0069: [2024-08-02 21:01:27,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=2230, skipped=0, lr=[3.89545984e-05, 3.89545984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2230 loss: 3.3665 iter time (s): 4.122 samples/sec: 31.056
g0093:  iteration     2230/10000000 | consumed samples:       285440 | consumed tokens:    584581120 | elapsed time per iteration (ms): 4154.5 | learning rate: 3.895E-05 | global batch size:   128 | lm loss: 3.382669E+00 | loss scale: 32768.0 | grad norm: 2.145 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.810 | tokens per gpu per second (tgs): 1971.834 | TFLOPs: 15.87 |
g0069: [2024-08-02 21:02:09,761] [INFO] [logging.py:96:log_dist] [Rank 0] step=2240, skipped=0, lr=[3.912936106666667e-05, 3.912936106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2240 loss: 3.3427 iter time (s): 4.153 samples/sec: 30.819
g0093:  iteration     2240/10000000 | consumed samples:       286720 | consumed tokens:    587202560 | elapsed time per iteration (ms): 4186.5 | learning rate: 3.913E-05 | global batch size:   128 | lm loss: 3.369416E+00 | loss scale: 32768.0 | grad norm: 2.032 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.575 | tokens per gpu per second (tgs): 1956.781 | TFLOPs: 15.75 |
g0069: [2024-08-02 21:02:52,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=2250, skipped=0, lr=[3.9304123733333334e-05, 3.9304123733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2250 loss: 3.3628 iter time (s): 4.234 samples/sec: 30.230
g0093:  iteration     2250/10000000 | consumed samples:       288000 | consumed tokens:    589824000 | elapsed time per iteration (ms): 4266.7 | learning rate: 3.930E-05 | global batch size:   128 | lm loss: 3.367329E+00 | loss scale: 32768.0 | grad norm: 2.298 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.999 | tokens per gpu per second (tgs): 1919.963 | TFLOPs: 15.45 |
g0069: [2024-08-02 21:03:31,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=2260, skipped=0, lr=[3.94788864e-05, 3.94788864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2260 loss: 3.3299 iter time (s): 3.888 samples/sec: 32.922
g0093:  iteration     2260/10000000 | consumed samples:       289280 | consumed tokens:    592445440 | elapsed time per iteration (ms): 3920.7 | learning rate: 3.948E-05 | global batch size:   128 | lm loss: 3.362141E+00 | loss scale: 32768.0 | grad norm: 2.102 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.647 | tokens per gpu per second (tgs): 2089.412 | TFLOPs: 16.81 |
g0069: [2024-08-02 21:04:13,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=2270, skipped=0, lr=[3.965364906666667e-05, 3.965364906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2270 loss: 3.3565 iter time (s): 4.181 samples/sec: 30.618
g0093:  iteration     2270/10000000 | consumed samples:       290560 | consumed tokens:    595066880 | elapsed time per iteration (ms): 4213.0 | learning rate: 3.965E-05 | global batch size:   128 | lm loss: 3.342873E+00 | loss scale: 32768.0 | grad norm: 2.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.382 | tokens per gpu per second (tgs): 1944.439 | TFLOPs: 15.65 |
g0069: [2024-08-02 21:04:55,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=2280, skipped=0, lr=[3.982841173333334e-05, 3.982841173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2280 loss: 3.3181 iter time (s): 4.172 samples/sec: 30.681
g0093:  iteration     2280/10000000 | consumed samples:       291840 | consumed tokens:    597688320 | elapsed time per iteration (ms): 4208.5 | learning rate: 3.983E-05 | global batch size:   128 | lm loss: 3.342106E+00 | loss scale: 32768.0 | grad norm: 1.842 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.415 | tokens per gpu per second (tgs): 1946.533 | TFLOPs: 15.66 |
g0069: [2024-08-02 21:05:38,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=2290, skipped=0, lr=[4.0003174400000006e-05, 4.0003174400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2290 loss: 3.3333 iter time (s): 4.198 samples/sec: 30.493
g0093:  iteration     2290/10000000 | consumed samples:       293120 | consumed tokens:    600309760 | elapsed time per iteration (ms): 4230.8 | learning rate: 4.000E-05 | global batch size:   128 | lm loss: 3.352061E+00 | loss scale: 32768.0 | grad norm: 2.364 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.255 | tokens per gpu per second (tgs): 1936.297 | TFLOPs: 15.58 |
g0069: [2024-08-02 21:06:20,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=2300, skipped=0, lr=[4.017793706666667e-05, 4.017793706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2300 loss: 3.3212 iter time (s): 4.197 samples/sec: 30.501
g0093:  iteration     2300/10000000 | consumed samples:       294400 | consumed tokens:    602931200 | elapsed time per iteration (ms): 4229.6 | learning rate: 4.018E-05 | global batch size:   128 | lm loss: 3.335011E+00 | loss scale: 32768.0 | grad norm: 2.105 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.263 | tokens per gpu per second (tgs): 1936.842 | TFLOPs: 15.59 |
g0069: [2024-08-02 21:07:03,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=2310, skipped=0, lr=[4.035269973333334e-05, 4.035269973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2310 loss: 3.3642 iter time (s): 4.312 samples/sec: 29.683
g0093:  iteration     2310/10000000 | consumed samples:       295680 | consumed tokens:    605552640 | elapsed time per iteration (ms): 4345.1 | learning rate: 4.035E-05 | global batch size:   128 | lm loss: 3.327475E+00 | loss scale: 32768.0 | grad norm: 2.324 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.459 | tokens per gpu per second (tgs): 1885.358 | TFLOPs: 15.17 |
g0069: [2024-08-02 21:07:43,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=2320, skipped=0, lr=[4.0527462400000004e-05, 4.0527462400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2320 loss: 3.3382 iter time (s): 3.942 samples/sec: 32.469
g0093:  iteration     2320/10000000 | consumed samples:       296960 | consumed tokens:    608174080 | elapsed time per iteration (ms): 3974.7 | learning rate: 4.053E-05 | global batch size:   128 | lm loss: 3.324544E+00 | loss scale: 32768.0 | grad norm: 2.583 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.203 | tokens per gpu per second (tgs): 2061.011 | TFLOPs: 16.59 |
g0069: [2024-08-02 21:08:23,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=2330, skipped=0, lr=[4.070222506666667e-05, 4.070222506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2330 loss: 3.3131 iter time (s): 3.993 samples/sec: 32.057
g0093:  iteration     2330/10000000 | consumed samples:       298240 | consumed tokens:    610795520 | elapsed time per iteration (ms): 4025.5 | learning rate: 4.070E-05 | global batch size:   128 | lm loss: 3.327957E+00 | loss scale: 32768.0 | grad norm: 2.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.797 | tokens per gpu per second (tgs): 2035.038 | TFLOPs: 16.38 |
g0069: [2024-08-02 21:09:05,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=2340, skipped=0, lr=[4.0876987733333336e-05, 4.0876987733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2340 loss: 3.3068 iter time (s): 4.124 samples/sec: 31.036
g0093:  iteration     2340/10000000 | consumed samples:       299520 | consumed tokens:    613416960 | elapsed time per iteration (ms): 4156.6 | learning rate: 4.088E-05 | global batch size:   128 | lm loss: 3.294126E+00 | loss scale: 32768.0 | grad norm: 1.896 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.795 | tokens per gpu per second (tgs): 1970.865 | TFLOPs: 15.86 |
g0069: [2024-08-02 21:09:46,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=2350, skipped=0, lr=[4.10517504e-05, 4.10517504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2350 loss: 3.2904 iter time (s): 4.067 samples/sec: 31.473
g0093:  iteration     2350/10000000 | consumed samples:       300800 | consumed tokens:    616038400 | elapsed time per iteration (ms): 4099.3 | learning rate: 4.105E-05 | global batch size:   128 | lm loss: 3.314613E+00 | loss scale: 32768.0 | grad norm: 2.400 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.225 | tokens per gpu per second (tgs): 1998.379 | TFLOPs: 16.08 |
g0069: [2024-08-02 21:10:28,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=2360, skipped=0, lr=[4.122651306666667e-05, 4.122651306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2360 loss: 3.3160 iter time (s): 4.137 samples/sec: 30.943
g0093:  iteration     2360/10000000 | consumed samples:       302080 | consumed tokens:    618659840 | elapsed time per iteration (ms): 4169.6 | learning rate: 4.123E-05 | global batch size:   128 | lm loss: 3.298438E+00 | loss scale: 32768.0 | grad norm: 2.388 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.698 | tokens per gpu per second (tgs): 1964.679 | TFLOPs: 15.81 |
g0069: [2024-08-02 21:11:11,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=2370, skipped=0, lr=[4.1401275733333334e-05, 4.1401275733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2370 loss: 3.2620 iter time (s): 4.259 samples/sec: 30.056
g0093:  iteration     2370/10000000 | consumed samples:       303360 | consumed tokens:    621281280 | elapsed time per iteration (ms): 4291.5 | learning rate: 4.140E-05 | global batch size:   128 | lm loss: 3.297242E+00 | loss scale: 32768.0 | grad norm: 1.906 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.827 | tokens per gpu per second (tgs): 1908.911 | TFLOPs: 15.36 |
g0069: [2024-08-02 21:11:54,851] [INFO] [logging.py:96:log_dist] [Rank 0] step=2380, skipped=0, lr=[4.15760384e-05, 4.15760384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2380 loss: 3.3140 iter time (s): 4.345 samples/sec: 29.463
g0093:  iteration     2380/10000000 | consumed samples:       304640 | consumed tokens:    623902720 | elapsed time per iteration (ms): 4377.3 | learning rate: 4.158E-05 | global batch size:   128 | lm loss: 3.294066E+00 | loss scale: 32768.0 | grad norm: 2.119 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.242 | tokens per gpu per second (tgs): 1871.464 | TFLOPs: 15.06 |
g0069: [2024-08-02 21:12:36,688] [INFO] [logging.py:96:log_dist] [Rank 0] step=2390, skipped=0, lr=[4.1750801066666666e-05, 4.1750801066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2390 loss: 3.2920 iter time (s): 4.150 samples/sec: 30.841
g0093:  iteration     2390/10000000 | consumed samples:       305920 | consumed tokens:    626524160 | elapsed time per iteration (ms): 4183.5 | learning rate: 4.175E-05 | global batch size:   128 | lm loss: 3.286843E+00 | loss scale: 32768.0 | grad norm: 2.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.596 | tokens per gpu per second (tgs): 1958.161 | TFLOPs: 15.76 |
g0069: [2024-08-02 21:13:18,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=2400, skipped=0, lr=[4.192556373333333e-05, 4.192556373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2400 loss: 3.2561 iter time (s): 4.149 samples/sec: 30.853
g0093:  iteration     2400/10000000 | consumed samples:       307200 | consumed tokens:    629145600 | elapsed time per iteration (ms): 4181.6 | learning rate: 4.193E-05 | global batch size:   128 | lm loss: 3.288410E+00 | loss scale: 32768.0 | grad norm: 2.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.610 | tokens per gpu per second (tgs): 1959.063 | TFLOPs: 15.76 |
g0069: [2024-08-02 21:13:59,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=2410, skipped=0, lr=[4.21003264e-05, 4.21003264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2410 loss: 3.2132 iter time (s): 4.062 samples/sec: 31.509
g0093:  iteration     2410/10000000 | consumed samples:       308480 | consumed tokens:    631767040 | elapsed time per iteration (ms): 4094.8 | learning rate: 4.210E-05 | global batch size:   128 | lm loss: 3.263939E+00 | loss scale: 32768.0 | grad norm: 1.993 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.259 | tokens per gpu per second (tgs): 2000.567 | TFLOPs: 16.10 |
g0069: [2024-08-02 21:14:42,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=2420, skipped=0, lr=[4.2275089066666664e-05, 4.2275089066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2420 loss: 3.2813 iter time (s): 4.264 samples/sec: 30.021
g0093:  iteration     2420/10000000 | consumed samples:       309760 | consumed tokens:    634388480 | elapsed time per iteration (ms): 4296.6 | learning rate: 4.228E-05 | global batch size:   128 | lm loss: 3.264072E+00 | loss scale: 32768.0 | grad norm: 1.876 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.791 | tokens per gpu per second (tgs): 1906.633 | TFLOPs: 15.34 |
g0069: [2024-08-02 21:15:23,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=2430, skipped=0, lr=[4.244985173333334e-05, 4.244985173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2430 loss: 3.2617 iter time (s): 4.041 samples/sec: 31.675
g0093:  iteration     2430/10000000 | consumed samples:       311040 | consumed tokens:    637009920 | elapsed time per iteration (ms): 4074.1 | learning rate: 4.245E-05 | global batch size:   128 | lm loss: 3.255303E+00 | loss scale: 32768.0 | grad norm: 1.885 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.418 | tokens per gpu per second (tgs): 2010.752 | TFLOPs: 16.18 |
g0069: [2024-08-02 21:16:05,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=2440, skipped=0, lr=[4.26246144e-05, 4.26246144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2440 loss: 3.2773 iter time (s): 4.238 samples/sec: 30.202
g0093:  iteration     2440/10000000 | consumed samples:       312320 | consumed tokens:    639631360 | elapsed time per iteration (ms): 4270.9 | learning rate: 4.262E-05 | global batch size:   128 | lm loss: 3.255423E+00 | loss scale: 32768.0 | grad norm: 2.718 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.970 | tokens per gpu per second (tgs): 1918.097 | TFLOPs: 15.44 |
g0069: [2024-08-02 21:16:48,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=2450, skipped=0, lr=[4.279937706666667e-05, 4.279937706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2450 loss: 3.2337 iter time (s): 4.216 samples/sec: 30.362
g0093:  iteration     2450/10000000 | consumed samples:       313600 | consumed tokens:    642252800 | elapsed time per iteration (ms): 4249.4 | learning rate: 4.280E-05 | global batch size:   128 | lm loss: 3.255125E+00 | loss scale: 32768.0 | grad norm: 1.813 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.122 | tokens per gpu per second (tgs): 1927.811 | TFLOPs: 15.51 |
g0069: [2024-08-02 21:17:30,716] [INFO] [logging.py:96:log_dist] [Rank 0] step=2460, skipped=0, lr=[4.2974139733333335e-05, 4.2974139733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2460 loss: 3.3099 iter time (s): 4.203 samples/sec: 30.456
g0093:  iteration     2460/10000000 | consumed samples:       314880 | consumed tokens:    644874240 | elapsed time per iteration (ms): 4235.5 | learning rate: 4.297E-05 | global batch size:   128 | lm loss: 3.252255E+00 | loss scale: 32768.0 | grad norm: 2.496 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.220 | tokens per gpu per second (tgs): 1934.108 | TFLOPs: 15.56 |
g0069: [2024-08-02 21:18:10,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=2470, skipped=0, lr=[4.31489024e-05, 4.31489024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2470 loss: 3.2473 iter time (s): 3.974 samples/sec: 32.209
g0093:  iteration     2470/10000000 | consumed samples:       316160 | consumed tokens:    647495680 | elapsed time per iteration (ms): 4007.7 | learning rate: 4.315E-05 | global batch size:   128 | lm loss: 3.240836E+00 | loss scale: 32768.0 | grad norm: 2.047 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.939 | tokens per gpu per second (tgs): 2044.083 | TFLOPs: 16.45 |
g0069: [2024-08-02 21:18:52,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=2480, skipped=0, lr=[4.332366506666667e-05, 4.332366506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2480 loss: 3.2268 iter time (s): 4.099 samples/sec: 31.230
g0093:  iteration     2480/10000000 | consumed samples:       317440 | consumed tokens:    650117120 | elapsed time per iteration (ms): 4131.9 | learning rate: 4.332E-05 | global batch size:   128 | lm loss: 3.243962E+00 | loss scale: 32768.0 | grad norm: 1.887 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.979 | tokens per gpu per second (tgs): 1982.645 | TFLOPs: 15.95 |
g0069: [2024-08-02 21:19:33,081] [INFO] [logging.py:96:log_dist] [Rank 0] step=2490, skipped=0, lr=[4.3498427733333334e-05, 4.3498427733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2490 loss: 3.2220 iter time (s): 4.064 samples/sec: 31.494
g0093:  iteration     2490/10000000 | consumed samples:       318720 | consumed tokens:    652738560 | elapsed time per iteration (ms): 4096.8 | learning rate: 4.350E-05 | global batch size:   128 | lm loss: 3.225941E+00 | loss scale: 32768.0 | grad norm: 2.281 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.244 | tokens per gpu per second (tgs): 1999.587 | TFLOPs: 16.09 |
g0069: [2024-08-02 21:20:15,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=2500, skipped=0, lr=[4.36731904e-05, 4.36731904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2500 loss: 3.2482 iter time (s): 4.181 samples/sec: 30.616
g0093:  iteration     2500/10000000 | consumed samples:       320000 | consumed tokens:    655360000 | elapsed time per iteration (ms): 4213.5 | learning rate: 4.367E-05 | global batch size:   128 | lm loss: 3.228877E+00 | loss scale: 32768.0 | grad norm: 1.959 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.379 | tokens per gpu per second (tgs): 1944.248 | TFLOPs: 15.65 |
g0069: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0069: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0069: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0092: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0088: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0088: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0088: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0090: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0093: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0088: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0090: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0087: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0093: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0091: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0088: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0091: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0086: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0086: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0091: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0087: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0091: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0090: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0086: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0087: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0086: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0086: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0091: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0087: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0088: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0069: [2024-08-02 21:20:19,767] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0069: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0093: [2024-08-02 21:20:19,768] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 21:20:19,769] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0069: [2024-08-02 21:20:57,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=2510, skipped=0, lr=[4.3847953066666666e-05, 4.3847953066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2510 loss: 3.1771 iter time (s): 4.192 samples/sec: 30.531
g0093:  iteration     2510/10000000 | consumed samples:       321280 | consumed tokens:    657981440 | elapsed time per iteration (ms): 4225.1 | learning rate: 4.385E-05 | global batch size:   128 | lm loss: 3.206357E+00 | loss scale: 65536.0 | grad norm: 1.966 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.295 | tokens per gpu per second (tgs): 1938.909 | TFLOPs: 15.60 |
g0069: [2024-08-02 21:21:42,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=2520, skipped=0, lr=[4.402271573333334e-05, 4.402271573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2520 loss: 3.2355 iter time (s): 4.520 samples/sec: 28.316
g0093:  iteration     2520/10000000 | consumed samples:       322560 | consumed tokens:    660602880 | elapsed time per iteration (ms): 4552.9 | learning rate: 4.402E-05 | global batch size:   128 | lm loss: 3.214661E+00 | loss scale: 65536.0 | grad norm: 2.307 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.114 | tokens per gpu per second (tgs): 1799.280 | TFLOPs: 14.48 |
g0069: [2024-08-02 21:22:27,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=2530, skipped=0, lr=[4.4197478400000005e-05, 4.4197478400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2530 loss: 3.2159 iter time (s): 4.415 samples/sec: 28.992
g0093:  iteration     2530/10000000 | consumed samples:       323840 | consumed tokens:    663224320 | elapsed time per iteration (ms): 4448.2 | learning rate: 4.420E-05 | global batch size:   128 | lm loss: 3.201489E+00 | loss scale: 65536.0 | grad norm: 2.093 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.776 | tokens per gpu per second (tgs): 1841.633 | TFLOPs: 14.82 |
g0069: [2024-08-02 21:23:12,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=2540, skipped=0, lr=[4.437224106666667e-05, 4.437224106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2540 loss: 3.2218 iter time (s): 4.423 samples/sec: 28.942
g0093:  iteration     2540/10000000 | consumed samples:       325120 | consumed tokens:    665845760 | elapsed time per iteration (ms): 4454.8 | learning rate: 4.437E-05 | global batch size:   128 | lm loss: 3.204215E+00 | loss scale: 65536.0 | grad norm: 2.109 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.733 | tokens per gpu per second (tgs): 1838.916 | TFLOPs: 14.80 |
g0069: [2024-08-02 21:23:54,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=2550, skipped=0, lr=[4.454700373333334e-05, 4.454700373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2550 loss: 3.1677 iter time (s): 4.168 samples/sec: 30.710
g0093:  iteration     2550/10000000 | consumed samples:       326400 | consumed tokens:    668467200 | elapsed time per iteration (ms): 4200.8 | learning rate: 4.455E-05 | global batch size:   128 | lm loss: 3.200272E+00 | loss scale: 65536.0 | grad norm: 1.805 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.470 | tokens per gpu per second (tgs): 1950.099 | TFLOPs: 15.69 |
g0069: [2024-08-02 21:24:38,392] [INFO] [logging.py:96:log_dist] [Rank 0] step=2560, skipped=0, lr=[4.47217664e-05, 4.47217664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2560 loss: 3.2234 iter time (s): 4.403 samples/sec: 29.070
g0093:  iteration     2560/10000000 | consumed samples:       327680 | consumed tokens:    671088640 | elapsed time per iteration (ms): 4435.6 | learning rate: 4.472E-05 | global batch size:   128 | lm loss: 3.212914E+00 | loss scale: 65536.0 | grad norm: 1.726 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.857 | tokens per gpu per second (tgs): 1846.856 | TFLOPs: 14.86 |
g0069: [2024-08-02 21:25:20,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=2570, skipped=0, lr=[4.489652906666667e-05, 4.489652906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2570 loss: 3.1895 iter time (s): 4.167 samples/sec: 30.714
g0093:  iteration     2570/10000000 | consumed samples:       328960 | consumed tokens:    673710080 | elapsed time per iteration (ms): 4199.8 | learning rate: 4.490E-05 | global batch size:   128 | lm loss: 3.187852E+00 | loss scale: 65536.0 | grad norm: 1.916 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.478 | tokens per gpu per second (tgs): 1950.586 | TFLOPs: 15.70 |
g0069: [2024-08-02 21:26:02,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=2580, skipped=0, lr=[4.5071291733333335e-05, 4.5071291733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2580 loss: 3.1741 iter time (s): 4.224 samples/sec: 30.305
g0093:  iteration     2580/10000000 | consumed samples:       330240 | consumed tokens:    676331520 | elapsed time per iteration (ms): 4256.5 | learning rate: 4.507E-05 | global batch size:   128 | lm loss: 3.181140E+00 | loss scale: 65536.0 | grad norm: 1.747 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.072 | tokens per gpu per second (tgs): 1924.585 | TFLOPs: 15.49 |
g0069: [2024-08-02 21:26:47,033] [INFO] [logging.py:96:log_dist] [Rank 0] step=2590, skipped=0, lr=[4.52460544e-05, 4.52460544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2590 loss: 3.1306 iter time (s): 4.360 samples/sec: 29.361
g0093:  iteration     2590/10000000 | consumed samples:       331520 | consumed tokens:    678952960 | elapsed time per iteration (ms): 4409.8 | learning rate: 4.525E-05 | global batch size:   128 | lm loss: 3.169211E+00 | loss scale: 65536.0 | grad norm: 1.798 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.026 | tokens per gpu per second (tgs): 1857.665 | TFLOPs: 14.95 |
g0069: [2024-08-02 21:27:31,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=2600, skipped=0, lr=[4.542081706666667e-05, 4.542081706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2600 loss: 3.1669 iter time (s): 4.450 samples/sec: 28.762
g0093:  iteration     2600/10000000 | consumed samples:       332800 | consumed tokens:    681574400 | elapsed time per iteration (ms): 4483.1 | learning rate: 4.542E-05 | global batch size:   128 | lm loss: 3.164240E+00 | loss scale: 65536.0 | grad norm: 2.060 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.552 | tokens per gpu per second (tgs): 1827.310 | TFLOPs: 14.70 |
g0069: [2024-08-02 21:28:14,137] [INFO] [logging.py:96:log_dist] [Rank 0] step=2610, skipped=0, lr=[4.559557973333334e-05, 4.559557973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2610 loss: 3.1727 iter time (s): 4.189 samples/sec: 30.553
g0093:  iteration     2610/10000000 | consumed samples:       334080 | consumed tokens:    684195840 | elapsed time per iteration (ms): 4225.1 | learning rate: 4.560E-05 | global batch size:   128 | lm loss: 3.162723E+00 | loss scale: 65536.0 | grad norm: 1.885 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.295 | tokens per gpu per second (tgs): 1938.911 | TFLOPs: 15.60 |
g0069: [2024-08-02 21:28:56,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=2620, skipped=0, lr=[4.5770342400000006e-05, 4.5770342400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2620 loss: 3.1528 iter time (s): 4.221 samples/sec: 30.323
g0093:  iteration     2620/10000000 | consumed samples:       335360 | consumed tokens:    686817280 | elapsed time per iteration (ms): 4253.9 | learning rate: 4.577E-05 | global batch size:   128 | lm loss: 3.161300E+00 | loss scale: 65536.0 | grad norm: 1.744 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.090 | tokens per gpu per second (tgs): 1925.783 | TFLOPs: 15.50 |
g0069: [2024-08-02 21:29:40,472] [INFO] [logging.py:96:log_dist] [Rank 0] step=2630, skipped=0, lr=[4.594510506666667e-05, 4.594510506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2630 loss: 3.1681 iter time (s): 4.347 samples/sec: 29.446
g0093:  iteration     2630/10000000 | consumed samples:       336640 | consumed tokens:    689438720 | elapsed time per iteration (ms): 4379.7 | learning rate: 4.595E-05 | global batch size:   128 | lm loss: 3.154267E+00 | loss scale: 65536.0 | grad norm: 2.089 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.226 | tokens per gpu per second (tgs): 1870.469 | TFLOPs: 15.05 |
g0069: [2024-08-02 21:30:23,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=2640, skipped=0, lr=[4.611986773333334e-05, 4.611986773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2640 loss: 3.1500 iter time (s): 4.306 samples/sec: 29.728
g0093:  iteration     2640/10000000 | consumed samples:       337920 | consumed tokens:    692060160 | elapsed time per iteration (ms): 4340.1 | learning rate: 4.612E-05 | global batch size:   128 | lm loss: 3.143727E+00 | loss scale: 65536.0 | grad norm: 1.982 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.493 | tokens per gpu per second (tgs): 1887.525 | TFLOPs: 15.19 |
g0069: [2024-08-02 21:31:07,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=2650, skipped=0, lr=[4.6294630400000004e-05, 4.6294630400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2650 loss: 3.1627 iter time (s): 4.303 samples/sec: 29.748
g0093:  iteration     2650/10000000 | consumed samples:       339200 | consumed tokens:    694681600 | elapsed time per iteration (ms): 4335.4 | learning rate: 4.629E-05 | global batch size:   128 | lm loss: 3.145449E+00 | loss scale: 65536.0 | grad norm: 1.927 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.524 | tokens per gpu per second (tgs): 1889.561 | TFLOPs: 15.21 |
g0069: [2024-08-02 21:31:50,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=2660, skipped=0, lr=[4.646939306666667e-05, 4.646939306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2660 loss: 3.1423 iter time (s): 4.303 samples/sec: 29.747
g0093:  iteration     2660/10000000 | consumed samples:       340480 | consumed tokens:    697303040 | elapsed time per iteration (ms): 4335.8 | learning rate: 4.647E-05 | global batch size:   128 | lm loss: 3.132052E+00 | loss scale: 65536.0 | grad norm: 1.949 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.522 | tokens per gpu per second (tgs): 1889.396 | TFLOPs: 15.20 |
g0069: [2024-08-02 21:32:32,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=2670, skipped=0, lr=[4.6644155733333336e-05, 4.6644155733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2670 loss: 3.1193 iter time (s): 4.154 samples/sec: 30.817
g0093:  iteration     2670/10000000 | consumed samples:       341760 | consumed tokens:    699924480 | elapsed time per iteration (ms): 4191.6 | learning rate: 4.664E-05 | global batch size:   128 | lm loss: 3.130514E+00 | loss scale: 65536.0 | grad norm: 1.902 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.538 | tokens per gpu per second (tgs): 1954.407 | TFLOPs: 15.73 |
g0069: [2024-08-02 21:33:17,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=2680, skipped=0, lr=[4.68189184e-05, 4.68189184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2680 loss: 3.1140 iter time (s): 4.445 samples/sec: 28.795
g0093:  iteration     2680/10000000 | consumed samples:       343040 | consumed tokens:    702545920 | elapsed time per iteration (ms): 4478.2 | learning rate: 4.682E-05 | global batch size:   128 | lm loss: 3.134696E+00 | loss scale: 65536.0 | grad norm: 1.870 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.583 | tokens per gpu per second (tgs): 1829.306 | TFLOPs: 14.72 |
g0069: [2024-08-02 21:33:59,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=2690, skipped=0, lr=[4.699368106666667e-05, 4.699368106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2690 loss: 3.1224 iter time (s): 4.209 samples/sec: 30.412
g0093:  iteration     2690/10000000 | consumed samples:       344320 | consumed tokens:    705167360 | elapsed time per iteration (ms): 4241.5 | learning rate: 4.699E-05 | global batch size:   128 | lm loss: 3.119703E+00 | loss scale: 65536.0 | grad norm: 1.864 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.178 | tokens per gpu per second (tgs): 1931.376 | TFLOPs: 15.54 |
g0069: [2024-08-02 21:34:44,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=2700, skipped=0, lr=[4.716844373333334e-05, 4.716844373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2700 loss: 3.1241 iter time (s): 4.428 samples/sec: 28.910
g0093:  iteration     2700/10000000 | consumed samples:       345600 | consumed tokens:    707788800 | elapsed time per iteration (ms): 4460.2 | learning rate: 4.717E-05 | global batch size:   128 | lm loss: 3.131292E+00 | loss scale: 65536.0 | grad norm: 2.027 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.699 | tokens per gpu per second (tgs): 1836.707 | TFLOPs: 14.78 |
g0069: [2024-08-02 21:35:25,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=2710, skipped=0, lr=[4.734320640000001e-05, 4.734320640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2710 loss: 3.1422 iter time (s): 4.121 samples/sec: 31.057
g0093:  iteration     2710/10000000 | consumed samples:       346880 | consumed tokens:    710410240 | elapsed time per iteration (ms): 4153.7 | learning rate: 4.734E-05 | global batch size:   128 | lm loss: 3.118738E+00 | loss scale: 65536.0 | grad norm: 2.121 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.816 | tokens per gpu per second (tgs): 1972.194 | TFLOPs: 15.87 |
g0069: [2024-08-02 21:36:09,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=2720, skipped=0, lr=[4.751796906666667e-05, 4.751796906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2720 loss: 3.1175 iter time (s): 4.383 samples/sec: 29.204
g0093:  iteration     2720/10000000 | consumed samples:       348160 | consumed tokens:    713031680 | elapsed time per iteration (ms): 4415.4 | learning rate: 4.752E-05 | global batch size:   128 | lm loss: 3.118651E+00 | loss scale: 65536.0 | grad norm: 1.946 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.990 | tokens per gpu per second (tgs): 1855.329 | TFLOPs: 14.93 |
g0069: [2024-08-02 21:36:57,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=2730, skipped=0, lr=[4.769273173333334e-05, 4.769273173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2730 loss: 3.1099 iter time (s): 4.767 samples/sec: 26.853
g0093:  iteration     2730/10000000 | consumed samples:       349440 | consumed tokens:    715653120 | elapsed time per iteration (ms): 4799.2 | learning rate: 4.769E-05 | global batch size:   128 | lm loss: 3.096778E+00 | loss scale: 65536.0 | grad norm: 1.983 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.671 | tokens per gpu per second (tgs): 1706.941 | TFLOPs: 13.74 |
g0069: [2024-08-02 21:37:43,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=2740, skipped=0, lr=[4.7867494400000005e-05, 4.7867494400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2740 loss: 3.0958 iter time (s): 4.471 samples/sec: 28.630
g0093:  iteration     2740/10000000 | consumed samples:       350720 | consumed tokens:    718274560 | elapsed time per iteration (ms): 4503.1 | learning rate: 4.787E-05 | global batch size:   128 | lm loss: 3.105428E+00 | loss scale: 65536.0 | grad norm: 2.023 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.425 | tokens per gpu per second (tgs): 1819.199 | TFLOPs: 14.64 |
g0069: [2024-08-02 21:38:24,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=2750, skipped=0, lr=[4.804225706666667e-05, 4.804225706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2750 loss: 3.1062 iter time (s): 4.139 samples/sec: 30.923
g0093:  iteration     2750/10000000 | consumed samples:       352000 | consumed tokens:    720896000 | elapsed time per iteration (ms): 4172.4 | learning rate: 4.804E-05 | global batch size:   128 | lm loss: 3.090975E+00 | loss scale: 65536.0 | grad norm: 1.905 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.678 | tokens per gpu per second (tgs): 1963.363 | TFLOPs: 15.80 |
g0069: [2024-08-02 21:39:07,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=2760, skipped=0, lr=[4.821701973333334e-05, 4.821701973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2760 loss: 3.0971 iter time (s): 4.224 samples/sec: 30.306
g0093:  iteration     2760/10000000 | consumed samples:       353280 | consumed tokens:    723517440 | elapsed time per iteration (ms): 4256.1 | learning rate: 4.822E-05 | global batch size:   128 | lm loss: 3.081113E+00 | loss scale: 65536.0 | grad norm: 1.853 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.075 | tokens per gpu per second (tgs): 1924.785 | TFLOPs: 15.49 |
g0069: [2024-08-02 21:39:48,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=2770, skipped=0, lr=[4.8391782400000004e-05, 4.8391782400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2770 loss: 3.1202 iter time (s): 4.107 samples/sec: 31.165
g0093:  iteration     2770/10000000 | consumed samples:       354560 | consumed tokens:    726138880 | elapsed time per iteration (ms): 4139.7 | learning rate: 4.839E-05 | global batch size:   128 | lm loss: 3.103116E+00 | loss scale: 65536.0 | grad norm: 1.763 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.920 | tokens per gpu per second (tgs): 1978.866 | TFLOPs: 15.92 |
g0069: [2024-08-02 21:40:31,946] [INFO] [logging.py:96:log_dist] [Rank 0] step=2780, skipped=0, lr=[4.856654506666667e-05, 4.856654506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2780 loss: 3.1018 iter time (s): 4.292 samples/sec: 29.823
g0093:  iteration     2780/10000000 | consumed samples:       355840 | consumed tokens:    728760320 | elapsed time per iteration (ms): 4324.7 | learning rate: 4.857E-05 | global batch size:   128 | lm loss: 3.085719E+00 | loss scale: 65536.0 | grad norm: 1.699 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.597 | tokens per gpu per second (tgs): 1894.233 | TFLOPs: 15.24 |
g0069: [2024-08-02 21:41:13,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=2790, skipped=0, lr=[4.874130773333334e-05, 4.874130773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2790 loss: 3.0788 iter time (s): 4.155 samples/sec: 30.809
g0093:  iteration     2790/10000000 | consumed samples:       357120 | consumed tokens:    731381760 | elapsed time per iteration (ms): 4187.0 | learning rate: 4.874E-05 | global batch size:   128 | lm loss: 3.069925E+00 | loss scale: 65536.0 | grad norm: 1.705 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.571 | tokens per gpu per second (tgs): 1956.515 | TFLOPs: 15.74 |
g0069: [2024-08-02 21:41:58,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=2800, skipped=0, lr=[4.891607040000001e-05, 4.891607040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2800 loss: 3.0828 iter time (s): 4.428 samples/sec: 28.910
g0093:  iteration     2800/10000000 | consumed samples:       358400 | consumed tokens:    734003200 | elapsed time per iteration (ms): 4460.3 | learning rate: 4.892E-05 | global batch size:   128 | lm loss: 3.088176E+00 | loss scale: 65536.0 | grad norm: 1.849 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.698 | tokens per gpu per second (tgs): 1836.647 | TFLOPs: 14.78 |
g0069: [2024-08-02 21:42:42,680] [INFO] [logging.py:96:log_dist] [Rank 0] step=2810, skipped=0, lr=[4.9090833066666675e-05, 4.9090833066666675e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2810 loss: 3.0558 iter time (s): 4.393 samples/sec: 29.135
g0093:  iteration     2810/10000000 | consumed samples:       359680 | consumed tokens:    736624640 | elapsed time per iteration (ms): 4426.1 | learning rate: 4.909E-05 | global batch size:   128 | lm loss: 3.083835E+00 | loss scale: 65536.0 | grad norm: 1.721 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.919 | tokens per gpu per second (tgs): 1850.824 | TFLOPs: 14.89 |
g0069: [2024-08-02 21:43:24,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=2820, skipped=0, lr=[4.926559573333334e-05, 4.926559573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2820 loss: 3.0210 iter time (s): 4.100 samples/sec: 31.220
g0093:  iteration     2820/10000000 | consumed samples:       360960 | consumed tokens:    739246080 | elapsed time per iteration (ms): 4132.7 | learning rate: 4.927E-05 | global batch size:   128 | lm loss: 3.062489E+00 | loss scale: 65536.0 | grad norm: 1.578 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.972 | tokens per gpu per second (tgs): 1982.232 | TFLOPs: 15.95 |
g0069: [2024-08-02 21:44:04,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=2830, skipped=0, lr=[4.944035840000001e-05, 4.944035840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2830 loss: 3.0262 iter time (s): 4.031 samples/sec: 31.751
g0093:  iteration     2830/10000000 | consumed samples:       362240 | consumed tokens:    741867520 | elapsed time per iteration (ms): 4065.6 | learning rate: 4.944E-05 | global batch size:   128 | lm loss: 3.068420E+00 | loss scale: 65536.0 | grad norm: 1.562 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.483 | tokens per gpu per second (tgs): 2014.942 | TFLOPs: 16.21 |
g0069: [2024-08-02 21:44:49,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=2840, skipped=0, lr=[4.961512106666667e-05, 4.961512106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2840 loss: 3.0592 iter time (s): 4.461 samples/sec: 28.692
g0093:  iteration     2840/10000000 | consumed samples:       363520 | consumed tokens:    744488960 | elapsed time per iteration (ms): 4493.9 | learning rate: 4.962E-05 | global batch size:   128 | lm loss: 3.065842E+00 | loss scale: 65536.0 | grad norm: 2.063 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.483 | tokens per gpu per second (tgs): 1822.918 | TFLOPs: 14.67 |
g0069: [2024-08-02 21:45:32,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=2850, skipped=0, lr=[4.978988373333333e-05, 4.978988373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2850 loss: 3.0264 iter time (s): 4.251 samples/sec: 30.109
g0093:  iteration     2850/10000000 | consumed samples:       364800 | consumed tokens:    747110400 | elapsed time per iteration (ms): 4284.5 | learning rate: 4.979E-05 | global batch size:   128 | lm loss: 3.056460E+00 | loss scale: 65536.0 | grad norm: 1.615 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.875 | tokens per gpu per second (tgs): 1912.001 | TFLOPs: 15.39 |
g0069: [2024-08-02 21:46:13,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=2860, skipped=0, lr=[4.99646464e-05, 4.99646464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2860 loss: 3.0468 iter time (s): 4.116 samples/sec: 31.102
g0093:  iteration     2860/10000000 | consumed samples:       366080 | consumed tokens:    749731840 | elapsed time per iteration (ms): 4149.4 | learning rate: 4.996E-05 | global batch size:   128 | lm loss: 3.048343E+00 | loss scale: 65536.0 | grad norm: 1.584 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.848 | tokens per gpu per second (tgs): 1974.261 | TFLOPs: 15.89 |
g0069: [2024-08-02 21:46:54,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=2870, skipped=0, lr=[5.0139409066666664e-05, 5.0139409066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2870 loss: 3.0446 iter time (s): 4.023 samples/sec: 31.815
g0093:  iteration     2870/10000000 | consumed samples:       367360 | consumed tokens:    752353280 | elapsed time per iteration (ms): 4055.6 | learning rate: 5.014E-05 | global batch size:   128 | lm loss: 3.040741E+00 | loss scale: 65536.0 | grad norm: 2.079 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.561 | tokens per gpu per second (tgs): 2019.900 | TFLOPs: 16.25 |
g0069: [2024-08-02 21:47:34,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=2880, skipped=0, lr=[5.031417173333333e-05, 5.031417173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2880 loss: 3.0701 iter time (s): 3.930 samples/sec: 32.570
g0093:  iteration     2880/10000000 | consumed samples:       368640 | consumed tokens:    754974720 | elapsed time per iteration (ms): 3963.2 | learning rate: 5.031E-05 | global batch size:   128 | lm loss: 3.058057E+00 | loss scale: 65536.0 | grad norm: 1.977 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.297 | tokens per gpu per second (tgs): 2067.035 | TFLOPs: 16.63 |
g0069: [2024-08-02 21:48:16,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=2890, skipped=0, lr=[5.0488934399999996e-05, 5.0488934399999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2890 loss: 3.0404 iter time (s): 4.217 samples/sec: 30.356
g0093:  iteration     2890/10000000 | consumed samples:       369920 | consumed tokens:    757596160 | elapsed time per iteration (ms): 4249.8 | learning rate: 5.049E-05 | global batch size:   128 | lm loss: 3.020030E+00 | loss scale: 65536.0 | grad norm: 1.704 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.119 | tokens per gpu per second (tgs): 1927.621 | TFLOPs: 15.51 |
g0069: [2024-08-02 21:49:00,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=2900, skipped=0, lr=[5.066369706666666e-05, 5.066369706666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2900 loss: 3.0139 iter time (s): 4.353 samples/sec: 29.403
g0093:  iteration     2900/10000000 | consumed samples:       371200 | consumed tokens:    760217600 | elapsed time per iteration (ms): 4385.8 | learning rate: 5.066E-05 | global batch size:   128 | lm loss: 3.028935E+00 | loss scale: 65536.0 | grad norm: 1.803 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.185 | tokens per gpu per second (tgs): 1867.852 | TFLOPs: 15.03 |
g0069: [2024-08-02 21:49:41,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=2910, skipped=0, lr=[5.083845973333333e-05, 5.083845973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2910 loss: 3.0258 iter time (s): 4.040 samples/sec: 31.680
g0093:  iteration     2910/10000000 | consumed samples:       372480 | consumed tokens:    762839040 | elapsed time per iteration (ms): 4073.8 | learning rate: 5.084E-05 | global batch size:   128 | lm loss: 3.026214E+00 | loss scale: 65536.0 | grad norm: 1.648 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.420 | tokens per gpu per second (tgs): 2010.905 | TFLOPs: 16.18 |
g0069: [2024-08-02 21:50:22,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=2920, skipped=0, lr=[5.10132224e-05, 5.10132224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2920 loss: 3.0758 iter time (s): 4.110 samples/sec: 31.144
g0093:  iteration     2920/10000000 | consumed samples:       373760 | consumed tokens:    765460480 | elapsed time per iteration (ms): 4142.7 | learning rate: 5.101E-05 | global batch size:   128 | lm loss: 3.027518E+00 | loss scale: 65536.0 | grad norm: 1.678 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.898 | tokens per gpu per second (tgs): 1977.468 | TFLOPs: 15.91 |
g0069: [2024-08-02 21:51:03,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=2930, skipped=0, lr=[5.118798506666667e-05, 5.118798506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2930 loss: 2.9814 iter time (s): 4.091 samples/sec: 31.289
g0093:  iteration     2930/10000000 | consumed samples:       375040 | consumed tokens:    768081920 | elapsed time per iteration (ms): 4123.7 | learning rate: 5.119E-05 | global batch size:   128 | lm loss: 3.031525E+00 | loss scale: 65536.0 | grad norm: 1.545 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.040 | tokens per gpu per second (tgs): 1986.566 | TFLOPs: 15.99 |
g0069: [2024-08-02 21:51:45,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=2940, skipped=0, lr=[5.1362747733333334e-05, 5.1362747733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2940 loss: 3.0468 iter time (s): 4.115 samples/sec: 31.104
g0093:  iteration     2940/10000000 | consumed samples:       376320 | consumed tokens:    770703360 | elapsed time per iteration (ms): 4147.5 | learning rate: 5.136E-05 | global batch size:   128 | lm loss: 3.013631E+00 | loss scale: 65536.0 | grad norm: 1.760 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.862 | tokens per gpu per second (tgs): 1975.168 | TFLOPs: 15.89 |
g0069: [2024-08-02 21:52:27,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=2950, skipped=0, lr=[5.15375104e-05, 5.15375104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2950 loss: 2.9954 iter time (s): 4.174 samples/sec: 30.662
g0093:  iteration     2950/10000000 | consumed samples:       377600 | consumed tokens:    773324800 | elapsed time per iteration (ms): 4206.8 | learning rate: 5.154E-05 | global batch size:   128 | lm loss: 3.005100E+00 | loss scale: 65536.0 | grad norm: 1.759 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.427 | tokens per gpu per second (tgs): 1947.305 | TFLOPs: 15.67 |
g0069: [2024-08-02 21:53:09,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=2960, skipped=0, lr=[5.1712273066666666e-05, 5.1712273066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2960 loss: 3.0110 iter time (s): 4.173 samples/sec: 30.670
g0093:  iteration     2960/10000000 | consumed samples:       378880 | consumed tokens:    775946240 | elapsed time per iteration (ms): 4205.8 | learning rate: 5.171E-05 | global batch size:   128 | lm loss: 2.994553E+00 | loss scale: 65536.0 | grad norm: 1.653 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.434 | tokens per gpu per second (tgs): 1947.766 | TFLOPs: 15.67 |
g0069: [2024-08-02 21:53:50,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=2970, skipped=0, lr=[5.188703573333333e-05, 5.188703573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2970 loss: 3.0134 iter time (s): 4.106 samples/sec: 31.175
g0093:  iteration     2970/10000000 | consumed samples:       380160 | consumed tokens:    778567680 | elapsed time per iteration (ms): 4138.4 | learning rate: 5.189E-05 | global batch size:   128 | lm loss: 3.014186E+00 | loss scale: 65536.0 | grad norm: 1.590 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.930 | tokens per gpu per second (tgs): 1979.530 | TFLOPs: 15.93 |
g0069: [2024-08-02 21:54:32,721] [INFO] [logging.py:96:log_dist] [Rank 0] step=2980, skipped=0, lr=[5.20617984e-05, 5.20617984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2980 loss: 3.0471 iter time (s): 4.152 samples/sec: 30.832
g0093:  iteration     2980/10000000 | consumed samples:       381440 | consumed tokens:    781189120 | elapsed time per iteration (ms): 4184.4 | learning rate: 5.206E-05 | global batch size:   128 | lm loss: 2.997883E+00 | loss scale: 65536.0 | grad norm: 1.964 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.590 | tokens per gpu per second (tgs): 1957.734 | TFLOPs: 15.75 |
g0069: [2024-08-02 21:55:13,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=2990, skipped=0, lr=[5.2236561066666664e-05, 5.2236561066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 2990 loss: 3.0513 iter time (s): 4.007 samples/sec: 31.946
g0093:  iteration     2990/10000000 | consumed samples:       382720 | consumed tokens:    783810560 | elapsed time per iteration (ms): 4039.3 | learning rate: 5.224E-05 | global batch size:   128 | lm loss: 3.001040E+00 | loss scale: 65536.0 | grad norm: 1.732 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.688 | tokens per gpu per second (tgs): 2028.058 | TFLOPs: 16.32 |
g0069: [2024-08-02 21:55:53,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=0, lr=[5.241132373333334e-05, 5.241132373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3000 loss: 3.0068 iter time (s): 4.039 samples/sec: 31.694
g0093:  iteration     3000/10000000 | consumed samples:       384000 | consumed tokens:    786432000 | elapsed time per iteration (ms): 4071.4 | learning rate: 5.241E-05 | global batch size:   128 | lm loss: 2.990743E+00 | loss scale: 65536.0 | grad norm: 1.526 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.439 | tokens per gpu per second (tgs): 2012.091 | TFLOPs: 16.19 |
g0093: ------------------------------------------------------------------------------------------------
g0093:  validation loss at iteration 3000 | lm loss value: 2.989098E+00 | lm loss PPL: 1.986775E+01 | 
g0093: ------------------------------------------------------------------------------------------------
g0069: saving checkpoint at iteration    3000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: [2024-08-02 22:02:20,911] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3000 is about to be saved!
g0069: [2024-08-02 22:02:20,915] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0088: [2024-08-02 22:02:20,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0088: [2024-08-02 22:02:20,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0088: [2024-08-02 22:02:20,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0069: [2024-08-02 22:02:20,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0069: [2024-08-02 22:02:20,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0093: [2024-08-02 22:02:20,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0093: [2024-08-02 22:02:20,918] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0093: [2024-08-02 22:02:20,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0086: [2024-08-02 22:02:20,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0086: [2024-08-02 22:02:20,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0087: [2024-08-02 22:02:20,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0086: [2024-08-02 22:02:20,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0087: [2024-08-02 22:02:20,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0087: [2024-08-02 22:02:20,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0090: [2024-08-02 22:02:20,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0090: [2024-08-02 22:02:20,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0090: [2024-08-02 22:02:20,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0092: [2024-08-02 22:02:20,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0092: [2024-08-02 22:02:20,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0091: [2024-08-02 22:02:20,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0091: [2024-08-02 22:02:20,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0091: [2024-08-02 22:02:20,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0092: [2024-08-02 22:02:20,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0093: [2024-08-02 22:02:20,942] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_23-model_00-model_states.pt...
g0088: [2024-08-02 22:02:20,949] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_11-model_00-model_states.pt...
g0090: [2024-08-02 22:02:20,952] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_14-model_00-model_states.pt...
g0087: [2024-08-02 22:02:20,952] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_08-model_00-model_states.pt...
g0086: [2024-08-02 22:02:20,955] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_05-model_00-model_states.pt...
g0091: [2024-08-02 22:02:20,956] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_17-model_00-model_states.pt...
g0092: [2024-08-02 22:02:20,958] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_20-model_00-model_states.pt...
g0069: [2024-08-02 22:02:20,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_01-model_00-model_states.pt...
g0088: [2024-08-02 22:02:21,050] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_11-model_00-model_states.pt.
g0093: [2024-08-02 22:02:21,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_23-model_00-model_states.pt.
g0093: [2024-08-02 22:02:21,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_24-model_00-model_states.pt...
g0093: [2024-08-02 22:02:21,069] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_24-model_00-model_states.pt.
g0091: [2024-08-02 22:02:21,074] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_17-model_00-model_states.pt.
g0088: [2024-08-02 22:02:21,086] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_12-model_00-model_states.pt...
g0087: [2024-08-02 22:02:21,104] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_08-model_00-model_states.pt.
g0090: [2024-08-02 22:02:21,108] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_14-model_00-model_states.pt.
g0091: [2024-08-02 22:02:21,114] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_18-model_00-model_states.pt...
g0093: [2024-08-02 22:02:21,121] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_25-model_00-model_states.pt...
g0092: [2024-08-02 22:02:21,138] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_20-model_00-model_states.pt.
g0087: [2024-08-02 22:02:21,139] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_09-model_00-model_states.pt...
g0090: [2024-08-02 22:02:21,143] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_15-model_00-model_states.pt...
g0086: [2024-08-02 22:02:21,155] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_05-model_00-model_states.pt.
g0092: [2024-08-02 22:02:21,177] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_21-model_00-model_states.pt...
g0088: [2024-08-02 22:02:21,186] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_12-model_00-model_states.pt.
g0086: [2024-08-02 22:02:21,194] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_06-model_00-model_states.pt...
g0088: [2024-08-02 22:02:21,217] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_13-model_00-model_states.pt...
g0091: [2024-08-02 22:02:21,225] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_18-model_00-model_states.pt.
g0069: [2024-08-02 22:02:21,225] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_01-model_00-model_states.pt.
g0069: [2024-08-02 22:02:21,253] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_02-model_00-model_states.pt...
g0091: [2024-08-02 22:02:21,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_19-model_00-model_states.pt...
g0087: [2024-08-02 22:02:21,261] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_09-model_00-model_states.pt.
g0092: [2024-08-02 22:02:21,281] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_21-model_00-model_states.pt.
g0087: [2024-08-02 22:02:21,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_10-model_00-model_states.pt...
g0093: [2024-08-02 22:02:21,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_25-model_00-model_states.pt.
g0093: [2024-08-02 22:02:21,300] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_07_model_states.pt...
g0090: [2024-08-02 22:02:21,307] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_15-model_00-model_states.pt.
g0092: [2024-08-02 22:02:21,316] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_22-model_00-model_states.pt...
g0090: [2024-08-02 22:02:21,333] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_16-model_00-model_states.pt...
g0086: [2024-08-02 22:02:21,351] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_06-model_00-model_states.pt.
g0069: [2024-08-02 22:02:21,357] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_02-model_00-model_states.pt.
g0091: [2024-08-02 22:02:21,376] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_19-model_00-model_states.pt.
g0091: [2024-08-02 22:02:21,378] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_05_model_states.pt...
g0086: [2024-08-02 22:02:21,385] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_07-model_00-model_states.pt...
g0069: [2024-08-02 22:02:21,390] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_03-model_00-model_states.pt...
g0088: [2024-08-02 22:02:21,390] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_13-model_00-model_states.pt.
g0088: [2024-08-02 22:02:21,392] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_03_model_states.pt...
g0087: [2024-08-02 22:02:21,418] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_10-model_00-model_states.pt.
g0087: [2024-08-02 22:02:21,420] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_02_model_states.pt...
g0092: [2024-08-02 22:02:21,450] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_22-model_00-model_states.pt.
g0092: [2024-08-02 22:02:21,452] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_06_model_states.pt...
g0086: [2024-08-02 22:02:21,492] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_07-model_00-model_states.pt.
g0069: [2024-08-02 22:02:21,494] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_03-model_00-model_states.pt.
g0086: [2024-08-02 22:02:21,494] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_01_model_states.pt...
g0069: [2024-08-02 22:02:21,518] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_04-model_00-model_states.pt...
g0090: [2024-08-02 22:02:21,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_16-model_00-model_states.pt.
g0090: [2024-08-02 22:02:21,538] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_04_model_states.pt...
g0069: [2024-08-02 22:02:21,644] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_04-model_00-model_states.pt.
g0069: [2024-08-02 22:02:21,646] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt
g0069: [2024-08-02 22:02:21,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt...
g0093: [2024-08-02 22:02:23,262] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_07_model_states.pt.
g0093: [2024-08-02 22:02:23,263] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0087: [2024-08-02 22:02:23,703] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_02_model_states.pt.
g0087: [2024-08-02 22:02:23,704] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0091: [2024-08-02 22:02:23,758] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_05_model_states.pt.
g0091: [2024-08-02 22:02:23,759] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0086: [2024-08-02 22:02:23,901] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_01_model_states.pt.
g0086: [2024-08-02 22:02:23,902] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0092: [2024-08-02 22:02:23,931] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_06_model_states.pt.
g0092: [2024-08-02 22:02:23,932] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0088: [2024-08-02 22:02:24,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_03_model_states.pt.
g0088: [2024-08-02 22:02:24,016] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0090: [2024-08-02 22:02:25,225] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_04_model_states.pt.
g0090: [2024-08-02 22:02:25,226] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0069: [2024-08-02 22:02:25,266] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt.
g0069: [2024-08-02 22:02:25,267] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0069:   successfully saved checkpoint at iteration    3000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: Checkpoint Save GB: 22.521, GB/Sec: 5.15, Latency(second): 4.374
g0093: (min, max) time across ranks (ms):
g0093:     save-checkpoint ................................: (4373.97, 4374.10)
g0069: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0091: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0087: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0069: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0093: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0069: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0093: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0086: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 22:02:29,161] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0086: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0086: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0087: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0091: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0090: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0091: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0091: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0093: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0093: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 22:02:29,161] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0090: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0086: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0093: [2024-08-02 22:02:29,161] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0091: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0090: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0086: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0092: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0086: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0092: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0092: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0087: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0087: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0090: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0092: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0092: [2024-08-02 22:02:29,160] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0069: [2024-08-02 22:03:04,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=3010, skipped=0, lr=[5.25860864e-05, 5.25860864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3010 loss: 2.9714 iter time (s): 3.909 samples/sec: 32.741
g0093:  iteration     3010/10000000 | consumed samples:       385280 | consumed tokens:    789053440 | elapsed time per iteration (ms): 43082.4 | learning rate: 5.259E-05 | global batch size:   128 | lm loss: 2.985154E+00 | loss scale: 131072.0 | grad norm: 1.581 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.971 | tokens per gpu per second (tgs): 190.147 | TFLOPs: 1.53 |
g0069: [2024-08-02 22:03:46,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=3020, skipped=0, lr=[5.276084906666667e-05, 5.276084906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3020 loss: 2.9831 iter time (s): 4.135 samples/sec: 30.952
g0093:  iteration     3020/10000000 | consumed samples:       386560 | consumed tokens:    791674880 | elapsed time per iteration (ms): 4167.7 | learning rate: 5.276E-05 | global batch size:   128 | lm loss: 2.988528E+00 | loss scale: 131072.0 | grad norm: 1.549 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.712 | tokens per gpu per second (tgs): 1965.576 | TFLOPs: 15.82 |
g0069: [2024-08-02 22:04:27,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=3030, skipped=0, lr=[5.2935611733333335e-05, 5.2935611733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3030 loss: 2.9507 iter time (s): 4.087 samples/sec: 31.315
g0093:  iteration     3030/10000000 | consumed samples:       387840 | consumed tokens:    794296320 | elapsed time per iteration (ms): 4120.3 | learning rate: 5.294E-05 | global batch size:   128 | lm loss: 2.976974E+00 | loss scale: 131072.0 | grad norm: 1.687 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.065 | tokens per gpu per second (tgs): 1988.185 | TFLOPs: 16.00 |
g0069: [2024-08-02 22:05:08,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=3040, skipped=0, lr=[5.31103744e-05, 5.31103744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3040 loss: 2.9657 iter time (s): 4.099 samples/sec: 31.230
g0093:  iteration     3040/10000000 | consumed samples:       389120 | consumed tokens:    796917760 | elapsed time per iteration (ms): 4131.0 | learning rate: 5.311E-05 | global batch size:   128 | lm loss: 2.979826E+00 | loss scale: 131072.0 | grad norm: 1.536 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.985 | tokens per gpu per second (tgs): 1983.071 | TFLOPs: 15.96 |
g0069: [2024-08-02 22:05:49,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=3050, skipped=0, lr=[5.328513706666667e-05, 5.328513706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3050 loss: 2.9830 iter time (s): 4.036 samples/sec: 31.716
g0093:  iteration     3050/10000000 | consumed samples:       390400 | consumed tokens:    799539200 | elapsed time per iteration (ms): 4068.4 | learning rate: 5.329E-05 | global batch size:   128 | lm loss: 2.970001E+00 | loss scale: 131072.0 | grad norm: 1.710 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.462 | tokens per gpu per second (tgs): 2013.561 | TFLOPs: 16.20 |
g0069: [2024-08-02 22:06:30,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=3060, skipped=0, lr=[5.345989973333333e-05, 5.345989973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3060 loss: 2.9620 iter time (s): 4.089 samples/sec: 31.304
g0093:  iteration     3060/10000000 | consumed samples:       391680 | consumed tokens:    802160640 | elapsed time per iteration (ms): 4121.4 | learning rate: 5.346E-05 | global batch size:   128 | lm loss: 2.971644E+00 | loss scale: 131072.0 | grad norm: 1.511 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.057 | tokens per gpu per second (tgs): 1987.667 | TFLOPs: 16.00 |
g0069: [2024-08-02 22:07:11,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=3070, skipped=0, lr=[5.36346624e-05, 5.36346624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3070 loss: 2.9864 iter time (s): 4.008 samples/sec: 31.937
g0093:  iteration     3070/10000000 | consumed samples:       392960 | consumed tokens:    804782080 | elapsed time per iteration (ms): 4040.1 | learning rate: 5.363E-05 | global batch size:   128 | lm loss: 2.974123E+00 | loss scale: 131072.0 | grad norm: 1.688 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.682 | tokens per gpu per second (tgs): 2027.673 | TFLOPs: 16.32 |
g0069: [2024-08-02 22:07:52,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=3080, skipped=0, lr=[5.3809425066666665e-05, 5.3809425066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3080 loss: 2.9399 iter time (s): 4.151 samples/sec: 30.836
g0093:  iteration     3080/10000000 | consumed samples:       394240 | consumed tokens:    807403520 | elapsed time per iteration (ms): 4183.3 | learning rate: 5.381E-05 | global batch size:   128 | lm loss: 2.960365E+00 | loss scale: 131072.0 | grad norm: 2.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.598 | tokens per gpu per second (tgs): 1958.252 | TFLOPs: 15.76 |
g0069: [2024-08-02 22:08:31,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=3090, skipped=0, lr=[5.398418773333334e-05, 5.398418773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3090 loss: 2.9825 iter time (s): 3.830 samples/sec: 33.420
g0093:  iteration     3090/10000000 | consumed samples:       395520 | consumed tokens:    810024960 | elapsed time per iteration (ms): 3862.2 | learning rate: 5.398E-05 | global batch size:   128 | lm loss: 2.955874E+00 | loss scale: 131072.0 | grad norm: 1.497 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.141 | tokens per gpu per second (tgs): 2121.053 | TFLOPs: 17.07 |
g0069: [2024-08-02 22:09:14,596] [INFO] [logging.py:96:log_dist] [Rank 0] step=3100, skipped=0, lr=[5.4158950400000004e-05, 5.4158950400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3100 loss: 2.9372 iter time (s): 4.267 samples/sec: 29.998
g0093:  iteration     3100/10000000 | consumed samples:       396800 | consumed tokens:    812646400 | elapsed time per iteration (ms): 4299.7 | learning rate: 5.416E-05 | global batch size:   128 | lm loss: 2.939686E+00 | loss scale: 131072.0 | grad norm: 1.560 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.769 | tokens per gpu per second (tgs): 1905.242 | TFLOPs: 15.33 |
g0069: [2024-08-02 22:09:55,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=3110, skipped=0, lr=[5.433371306666667e-05, 5.433371306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3110 loss: 2.8980 iter time (s): 4.042 samples/sec: 31.669
g0093:  iteration     3110/10000000 | consumed samples:       398080 | consumed tokens:    815267840 | elapsed time per iteration (ms): 4074.1 | learning rate: 5.433E-05 | global batch size:   128 | lm loss: 2.952392E+00 | loss scale: 131072.0 | grad norm: 1.571 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.418 | tokens per gpu per second (tgs): 2010.732 | TFLOPs: 16.18 |
g0069: [2024-08-02 22:10:36,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=3120, skipped=0, lr=[5.4508475733333336e-05, 5.4508475733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3120 loss: 2.9320 iter time (s): 4.116 samples/sec: 31.100
g0093:  iteration     3120/10000000 | consumed samples:       399360 | consumed tokens:    817889280 | elapsed time per iteration (ms): 4148.2 | learning rate: 5.451E-05 | global batch size:   128 | lm loss: 2.946954E+00 | loss scale: 131072.0 | grad norm: 1.587 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.856 | tokens per gpu per second (tgs): 1974.815 | TFLOPs: 15.89 |
g0069: [2024-08-02 22:11:18,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=3130, skipped=0, lr=[5.46832384e-05, 5.46832384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3130 loss: 2.9282 iter time (s): 4.107 samples/sec: 31.165
g0093:  iteration     3130/10000000 | consumed samples:       400640 | consumed tokens:    820510720 | elapsed time per iteration (ms): 4139.7 | learning rate: 5.468E-05 | global batch size:   128 | lm loss: 2.929462E+00 | loss scale: 131072.0 | grad norm: 1.680 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.920 | tokens per gpu per second (tgs): 1978.881 | TFLOPs: 15.92 |
g0069: [2024-08-02 22:11:59,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=3140, skipped=0, lr=[5.485800106666667e-05, 5.485800106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3140 loss: 2.9715 iter time (s): 4.141 samples/sec: 30.909
g0093:  iteration     3140/10000000 | consumed samples:       401920 | consumed tokens:    823132160 | elapsed time per iteration (ms): 4174.2 | learning rate: 5.486E-05 | global batch size:   128 | lm loss: 2.934221E+00 | loss scale: 131072.0 | grad norm: 1.659 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.665 | tokens per gpu per second (tgs): 1962.533 | TFLOPs: 15.79 |
g0069: [2024-08-02 22:12:41,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=3150, skipped=0, lr=[5.5032763733333334e-05, 5.5032763733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3150 loss: 2.8975 iter time (s): 4.107 samples/sec: 31.163
g0093:  iteration     3150/10000000 | consumed samples:       403200 | consumed tokens:    825753600 | elapsed time per iteration (ms): 4146.2 | learning rate: 5.503E-05 | global batch size:   128 | lm loss: 2.935610E+00 | loss scale: 131072.0 | grad norm: 1.438 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.872 | tokens per gpu per second (tgs): 1975.808 | TFLOPs: 15.90 |
g0069: [2024-08-02 22:13:20,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=3160, skipped=0, lr=[5.52075264e-05, 5.52075264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3160 loss: 2.8979 iter time (s): 3.908 samples/sec: 32.755
g0093:  iteration     3160/10000000 | consumed samples:       404480 | consumed tokens:    828375040 | elapsed time per iteration (ms): 3940.9 | learning rate: 5.521E-05 | global batch size:   128 | lm loss: 2.930153E+00 | loss scale: 131072.0 | grad norm: 1.619 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.479 | tokens per gpu per second (tgs): 2078.687 | TFLOPs: 16.73 |
g0069: [2024-08-02 22:14:01,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=3170, skipped=0, lr=[5.5382289066666667e-05, 5.5382289066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3170 loss: 2.9173 iter time (s): 4.026 samples/sec: 31.795
g0093:  iteration     3170/10000000 | consumed samples:       405760 | consumed tokens:    830996480 | elapsed time per iteration (ms): 4058.3 | learning rate: 5.538E-05 | global batch size:   128 | lm loss: 2.923620E+00 | loss scale: 131072.0 | grad norm: 1.520 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.540 | tokens per gpu per second (tgs): 2018.563 | TFLOPs: 16.24 |
g0069: [2024-08-02 22:14:43,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=3180, skipped=0, lr=[5.555705173333334e-05, 5.555705173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3180 loss: 2.9119 iter time (s): 4.142 samples/sec: 30.907
g0093:  iteration     3180/10000000 | consumed samples:       407040 | consumed tokens:    833617920 | elapsed time per iteration (ms): 4173.7 | learning rate: 5.556E-05 | global batch size:   128 | lm loss: 2.914162E+00 | loss scale: 131072.0 | grad norm: 1.455 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.668 | tokens per gpu per second (tgs): 1962.756 | TFLOPs: 15.79 |
g0069: [2024-08-02 22:15:24,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=3190, skipped=0, lr=[5.5731814400000005e-05, 5.5731814400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3190 loss: 2.8778 iter time (s): 4.102 samples/sec: 31.202
g0093:  iteration     3190/10000000 | consumed samples:       408320 | consumed tokens:    836239360 | elapsed time per iteration (ms): 4135.6 | learning rate: 5.573E-05 | global batch size:   128 | lm loss: 2.922780E+00 | loss scale: 131072.0 | grad norm: 1.441 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.951 | tokens per gpu per second (tgs): 1980.839 | TFLOPs: 15.94 |
g0069: [2024-08-02 22:16:05,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=3200, skipped=0, lr=[5.590657706666667e-05, 5.590657706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3200 loss: 2.9169 iter time (s): 4.080 samples/sec: 31.376
g0093:  iteration     3200/10000000 | consumed samples:       409600 | consumed tokens:    838860800 | elapsed time per iteration (ms): 4113.6 | learning rate: 5.591E-05 | global batch size:   128 | lm loss: 2.903348E+00 | loss scale: 131072.0 | grad norm: 1.665 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.116 | tokens per gpu per second (tgs): 1991.430 | TFLOPs: 16.03 |
g0069: [2024-08-02 22:16:47,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=3210, skipped=0, lr=[5.608133973333334e-05, 5.608133973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3210 loss: 2.8847 iter time (s): 4.134 samples/sec: 30.963
g0093:  iteration     3210/10000000 | consumed samples:       410880 | consumed tokens:    841482240 | elapsed time per iteration (ms): 4167.7 | learning rate: 5.608E-05 | global batch size:   128 | lm loss: 2.909607E+00 | loss scale: 131072.0 | grad norm: 1.391 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.712 | tokens per gpu per second (tgs): 1965.598 | TFLOPs: 15.82 |
g0069: [2024-08-02 22:17:28,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=3220, skipped=0, lr=[5.6256102400000004e-05, 5.6256102400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3220 loss: 2.9596 iter time (s): 4.059 samples/sec: 31.533
g0093:  iteration     3220/10000000 | consumed samples:       412160 | consumed tokens:    844103680 | elapsed time per iteration (ms): 4091.7 | learning rate: 5.626E-05 | global batch size:   128 | lm loss: 2.897548E+00 | loss scale: 131072.0 | grad norm: 1.728 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.283 | tokens per gpu per second (tgs): 2002.110 | TFLOPs: 16.11 |
g0069: [2024-08-02 22:18:10,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=3230, skipped=0, lr=[5.643086506666667e-05, 5.643086506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3230 loss: 2.9444 iter time (s): 4.159 samples/sec: 30.779
g0093:  iteration     3230/10000000 | consumed samples:       413440 | consumed tokens:    846725120 | elapsed time per iteration (ms): 4191.2 | learning rate: 5.643E-05 | global batch size:   128 | lm loss: 2.906956E+00 | loss scale: 131072.0 | grad norm: 1.651 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.540 | tokens per gpu per second (tgs): 1954.589 | TFLOPs: 15.73 |
g0069: [2024-08-02 22:18:53,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=3240, skipped=0, lr=[5.6605627733333336e-05, 5.6605627733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3240 loss: 2.8768 iter time (s): 4.335 samples/sec: 29.528
g0093:  iteration     3240/10000000 | consumed samples:       414720 | consumed tokens:    849346560 | elapsed time per iteration (ms): 4367.2 | learning rate: 5.661E-05 | global batch size:   128 | lm loss: 2.893843E+00 | loss scale: 131072.0 | grad norm: 1.603 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.309 | tokens per gpu per second (tgs): 1875.807 | TFLOPs: 15.09 |
g0069: [2024-08-02 22:19:36,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=3250, skipped=0, lr=[5.67803904e-05, 5.67803904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3250 loss: 2.8973 iter time (s): 4.234 samples/sec: 30.228
g0093:  iteration     3250/10000000 | consumed samples:       416000 | consumed tokens:    851968000 | elapsed time per iteration (ms): 4267.3 | learning rate: 5.678E-05 | global batch size:   128 | lm loss: 2.890942E+00 | loss scale: 131072.0 | grad norm: 1.689 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.995 | tokens per gpu per second (tgs): 1919.695 | TFLOPs: 15.45 |
g0069: [2024-08-02 22:20:18,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=3260, skipped=0, lr=[5.695515306666667e-05, 5.695515306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3260 loss: 2.8754 iter time (s): 4.157 samples/sec: 30.788
g0093:  iteration     3260/10000000 | consumed samples:       417280 | consumed tokens:    854589440 | elapsed time per iteration (ms): 4190.1 | learning rate: 5.696E-05 | global batch size:   128 | lm loss: 2.877772E+00 | loss scale: 131072.0 | grad norm: 1.469 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.548 | tokens per gpu per second (tgs): 1955.084 | TFLOPs: 15.73 |
g0069: [2024-08-02 22:20:58,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=3270, skipped=0, lr=[5.712991573333334e-05, 5.712991573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3270 loss: 2.8885 iter time (s): 4.005 samples/sec: 31.958
g0093:  iteration     3270/10000000 | consumed samples:       418560 | consumed tokens:    857210880 | elapsed time per iteration (ms): 4038.0 | learning rate: 5.713E-05 | global batch size:   128 | lm loss: 2.885948E+00 | loss scale: 131072.0 | grad norm: 1.415 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.699 | tokens per gpu per second (tgs): 2028.746 | TFLOPs: 16.33 |
g0069: [2024-08-02 22:21:41,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=3280, skipped=0, lr=[5.730467840000001e-05, 5.730467840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3280 loss: 2.8595 iter time (s): 4.276 samples/sec: 29.936
g0093:  iteration     3280/10000000 | consumed samples:       419840 | consumed tokens:    859832320 | elapsed time per iteration (ms): 4308.6 | learning rate: 5.730E-05 | global batch size:   128 | lm loss: 2.864431E+00 | loss scale: 131072.0 | grad norm: 1.430 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.708 | tokens per gpu per second (tgs): 1901.313 | TFLOPs: 15.30 |
g0069: [2024-08-02 22:22:25,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=3290, skipped=0, lr=[5.747944106666667e-05, 5.747944106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3290 loss: 2.9062 iter time (s): 4.283 samples/sec: 29.885
g0093:  iteration     3290/10000000 | consumed samples:       421120 | consumed tokens:    862453760 | elapsed time per iteration (ms): 4315.9 | learning rate: 5.748E-05 | global batch size:   128 | lm loss: 2.868096E+00 | loss scale: 131072.0 | grad norm: 1.608 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.658 | tokens per gpu per second (tgs): 1898.102 | TFLOPs: 15.27 |
g0069: [2024-08-02 22:23:07,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=3300, skipped=0, lr=[5.765420373333334e-05, 5.765420373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3300 loss: 2.8757 iter time (s): 4.192 samples/sec: 30.533
g0093:  iteration     3300/10000000 | consumed samples:       422400 | consumed tokens:    865075200 | elapsed time per iteration (ms): 4224.7 | learning rate: 5.765E-05 | global batch size:   128 | lm loss: 2.862326E+00 | loss scale: 131072.0 | grad norm: 1.544 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.298 | tokens per gpu per second (tgs): 1939.082 | TFLOPs: 15.60 |
g0069: [2024-08-02 22:23:49,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=3310, skipped=0, lr=[5.7828966400000005e-05, 5.7828966400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3310 loss: 2.8165 iter time (s): 4.232 samples/sec: 30.246
g0093:  iteration     3310/10000000 | consumed samples:       423680 | consumed tokens:    867696640 | elapsed time per iteration (ms): 4264.7 | learning rate: 5.783E-05 | global batch size:   128 | lm loss: 2.849648E+00 | loss scale: 131072.0 | grad norm: 1.518 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.014 | tokens per gpu per second (tgs): 1920.905 | TFLOPs: 15.46 |
g0069: [2024-08-02 22:24:32,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=3320, skipped=0, lr=[5.800372906666667e-05, 5.800372906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3320 loss: 2.8538 iter time (s): 4.272 samples/sec: 29.960
g0093:  iteration     3320/10000000 | consumed samples:       424960 | consumed tokens:    870318080 | elapsed time per iteration (ms): 4305.3 | learning rate: 5.800E-05 | global batch size:   128 | lm loss: 2.857161E+00 | loss scale: 131072.0 | grad norm: 1.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.731 | tokens per gpu per second (tgs): 1902.792 | TFLOPs: 15.31 |
g0069: [2024-08-02 22:25:15,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=3330, skipped=0, lr=[5.817849173333334e-05, 5.817849173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3330 loss: 2.8565 iter time (s): 4.207 samples/sec: 30.425
g0093:  iteration     3330/10000000 | consumed samples:       426240 | consumed tokens:    872939520 | elapsed time per iteration (ms): 4240.9 | learning rate: 5.818E-05 | global batch size:   128 | lm loss: 2.835384E+00 | loss scale: 131072.0 | grad norm: 1.489 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.182 | tokens per gpu per second (tgs): 1931.657 | TFLOPs: 15.54 |
g0069: [2024-08-02 22:25:58,597] [INFO] [logging.py:96:log_dist] [Rank 0] step=3340, skipped=0, lr=[5.83532544e-05, 5.83532544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3340 loss: 2.8415 iter time (s): 4.290 samples/sec: 29.840
g0093:  iteration     3340/10000000 | consumed samples:       427520 | consumed tokens:    875560960 | elapsed time per iteration (ms): 4321.9 | learning rate: 5.835E-05 | global batch size:   128 | lm loss: 2.849956E+00 | loss scale: 131072.0 | grad norm: 1.399 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.616 | tokens per gpu per second (tgs): 1895.453 | TFLOPs: 15.25 |
g0069: [2024-08-02 22:26:40,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=3350, skipped=0, lr=[5.852801706666667e-05, 5.852801706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3350 loss: 2.8405 iter time (s): 4.168 samples/sec: 30.707
g0093:  iteration     3350/10000000 | consumed samples:       428800 | consumed tokens:    878182400 | elapsed time per iteration (ms): 4200.6 | learning rate: 5.853E-05 | global batch size:   128 | lm loss: 2.837426E+00 | loss scale: 131072.0 | grad norm: 1.515 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.472 | tokens per gpu per second (tgs): 1950.185 | TFLOPs: 15.69 |
g0069: [2024-08-02 22:27:22,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=3360, skipped=0, lr=[5.870277973333334e-05, 5.870277973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3360 loss: 2.8402 iter time (s): 4.124 samples/sec: 31.040
g0093:  iteration     3360/10000000 | consumed samples:       430080 | consumed tokens:    880803840 | elapsed time per iteration (ms): 4156.2 | learning rate: 5.870E-05 | global batch size:   128 | lm loss: 2.852903E+00 | loss scale: 131072.0 | grad norm: 1.349 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.797 | tokens per gpu per second (tgs): 1971.016 | TFLOPs: 15.86 |
g0069: [2024-08-02 22:28:04,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=3370, skipped=0, lr=[5.887754240000001e-05, 5.887754240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3370 loss: 2.8494 iter time (s): 4.183 samples/sec: 30.597
g0093:  iteration     3370/10000000 | consumed samples:       431360 | consumed tokens:    883425280 | elapsed time per iteration (ms): 4215.7 | learning rate: 5.888E-05 | global batch size:   128 | lm loss: 2.844041E+00 | loss scale: 131072.0 | grad norm: 1.416 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.362 | tokens per gpu per second (tgs): 1943.190 | TFLOPs: 15.64 |
g0069: [2024-08-02 22:28:46,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=3380, skipped=0, lr=[5.9052305066666674e-05, 5.9052305066666674e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3380 loss: 2.8408 iter time (s): 4.167 samples/sec: 30.721
g0093:  iteration     3380/10000000 | consumed samples:       432640 | consumed tokens:    886046720 | elapsed time per iteration (ms): 4199.0 | learning rate: 5.905E-05 | global batch size:   128 | lm loss: 2.839651E+00 | loss scale: 131072.0 | grad norm: 1.454 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.484 | tokens per gpu per second (tgs): 1950.959 | TFLOPs: 15.70 |
g0069: [2024-08-02 22:29:27,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=3390, skipped=0, lr=[5.922706773333334e-05, 5.922706773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3390 loss: 2.8145 iter time (s): 4.062 samples/sec: 31.511
g0093:  iteration     3390/10000000 | consumed samples:       433920 | consumed tokens:    888668160 | elapsed time per iteration (ms): 4094.4 | learning rate: 5.923E-05 | global batch size:   128 | lm loss: 2.833265E+00 | loss scale: 131072.0 | grad norm: 1.423 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.262 | tokens per gpu per second (tgs): 2000.785 | TFLOPs: 16.10 |
g0069: [2024-08-02 22:30:09,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=3400, skipped=0, lr=[5.9401830400000006e-05, 5.9401830400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3400 loss: 2.8222 iter time (s): 4.216 samples/sec: 30.363
g0093:  iteration     3400/10000000 | consumed samples:       435200 | consumed tokens:    891289600 | elapsed time per iteration (ms): 4248.0 | learning rate: 5.940E-05 | global batch size:   128 | lm loss: 2.821394E+00 | loss scale: 131072.0 | grad norm: 1.333 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.132 | tokens per gpu per second (tgs): 1928.421 | TFLOPs: 15.52 |
g0069: [2024-08-02 22:30:50,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=3410, skipped=0, lr=[5.957659306666667e-05, 5.957659306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3410 loss: 2.8186 iter time (s): 4.069 samples/sec: 31.457
g0093:  iteration     3410/10000000 | consumed samples:       436480 | consumed tokens:    893911040 | elapsed time per iteration (ms): 4101.6 | learning rate: 5.958E-05 | global batch size:   128 | lm loss: 2.821302E+00 | loss scale: 131072.0 | grad norm: 1.408 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.208 | tokens per gpu per second (tgs): 1997.283 | TFLOPs: 16.07 |
g0069: [2024-08-02 22:31:32,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=3420, skipped=0, lr=[5.975135573333334e-05, 5.975135573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3420 loss: 2.8021 iter time (s): 4.148 samples/sec: 30.855
g0093:  iteration     3420/10000000 | consumed samples:       437760 | consumed tokens:    896532480 | elapsed time per iteration (ms): 4190.3 | learning rate: 5.975E-05 | global batch size:   128 | lm loss: 2.810761E+00 | loss scale: 131072.0 | grad norm: 1.457 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.546 | tokens per gpu per second (tgs): 1954.975 | TFLOPs: 15.73 |
g0069: [2024-08-02 22:32:14,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=3430, skipped=0, lr=[5.9926118400000004e-05, 5.9926118400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3430 loss: 2.8365 iter time (s): 4.097 samples/sec: 31.240
g0093:  iteration     3430/10000000 | consumed samples:       439040 | consumed tokens:    899153920 | elapsed time per iteration (ms): 4144.2 | learning rate: 5.993E-05 | global batch size:   128 | lm loss: 2.822132E+00 | loss scale: 131072.0 | grad norm: 1.721 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.887 | tokens per gpu per second (tgs): 1976.758 | TFLOPs: 15.91 |
g0069: [2024-08-02 22:32:55,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=3440, skipped=0, lr=[6.010088106666667e-05, 6.010088106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3440 loss: 2.7845 iter time (s): 4.070 samples/sec: 31.446
g0093:  iteration     3440/10000000 | consumed samples:       440320 | consumed tokens:    901775360 | elapsed time per iteration (ms): 4104.7 | learning rate: 6.010E-05 | global batch size:   128 | lm loss: 2.813292E+00 | loss scale: 131072.0 | grad norm: 1.439 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.184 | tokens per gpu per second (tgs): 1995.773 | TFLOPs: 16.06 |
g0069: [2024-08-02 22:33:37,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=3450, skipped=0, lr=[6.027564373333334e-05, 6.027564373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3450 loss: 2.7836 iter time (s): 4.156 samples/sec: 30.796
g0093:  iteration     3450/10000000 | consumed samples:       441600 | consumed tokens:    904396800 | elapsed time per iteration (ms): 4189.2 | learning rate: 6.028E-05 | global batch size:   128 | lm loss: 2.813873E+00 | loss scale: 131072.0 | grad norm: 1.360 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.555 | tokens per gpu per second (tgs): 1955.524 | TFLOPs: 15.74 |
g0069: [2024-08-02 22:34:18,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=3460, skipped=0, lr=[6.045040640000001e-05, 6.045040640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3460 loss: 2.8182 iter time (s): 4.127 samples/sec: 31.019
g0093:  iteration     3460/10000000 | consumed samples:       442880 | consumed tokens:    907018240 | elapsed time per iteration (ms): 4159.1 | learning rate: 6.045E-05 | global batch size:   128 | lm loss: 2.805948E+00 | loss scale: 131072.0 | grad norm: 1.598 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.776 | tokens per gpu per second (tgs): 1969.638 | TFLOPs: 15.85 |
g0069: [2024-08-02 22:35:00,622] [INFO] [logging.py:96:log_dist] [Rank 0] step=3470, skipped=0, lr=[6.0625169066666676e-05, 6.0625169066666676e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3470 loss: 2.8084 iter time (s): 4.166 samples/sec: 30.726
g0093:  iteration     3470/10000000 | consumed samples:       444160 | consumed tokens:    909639680 | elapsed time per iteration (ms): 4199.2 | learning rate: 6.063E-05 | global batch size:   128 | lm loss: 2.799790E+00 | loss scale: 131072.0 | grad norm: 1.474 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.482 | tokens per gpu per second (tgs): 1950.831 | TFLOPs: 15.70 |
g0069: [2024-08-02 22:35:41,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=3480, skipped=0, lr=[6.079993173333334e-05, 6.079993173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3480 loss: 2.7802 iter time (s): 4.080 samples/sec: 31.369
g0093:  iteration     3480/10000000 | consumed samples:       445440 | consumed tokens:    912261120 | elapsed time per iteration (ms): 4113.6 | learning rate: 6.080E-05 | global batch size:   128 | lm loss: 2.794353E+00 | loss scale: 131072.0 | grad norm: 1.367 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.116 | tokens per gpu per second (tgs): 1991.439 | TFLOPs: 16.03 |
g0069: [2024-08-02 22:36:22,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=3490, skipped=0, lr=[6.097469440000001e-05, 6.097469440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3490 loss: 2.8169 iter time (s): 4.031 samples/sec: 31.750
g0093:  iteration     3490/10000000 | consumed samples:       446720 | consumed tokens:    914882560 | elapsed time per iteration (ms): 4063.8 | learning rate: 6.097E-05 | global batch size:   128 | lm loss: 2.790664E+00 | loss scale: 131072.0 | grad norm: 1.471 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.497 | tokens per gpu per second (tgs): 2015.823 | TFLOPs: 16.22 |
g0069: [2024-08-02 22:37:02,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=3500, skipped=0, lr=[6.114945706666668e-05, 6.114945706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3500 loss: 2.8051 iter time (s): 4.025 samples/sec: 31.801
g0093:  iteration     3500/10000000 | consumed samples:       448000 | consumed tokens:    917504000 | elapsed time per iteration (ms): 4058.1 | learning rate: 6.115E-05 | global batch size:   128 | lm loss: 2.787151E+00 | loss scale: 131072.0 | grad norm: 1.585 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.542 | tokens per gpu per second (tgs): 2018.684 | TFLOPs: 16.24 |
g0069: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0086: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0088: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0086: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0087: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0088: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0087: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0087: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0087: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0086: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0069: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0092: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0090: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0088: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0069: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0092: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0093: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0069: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0093: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0093: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0093: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0091: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0092: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0069: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0093: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0090: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0092: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0069: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0090: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0090: [2024-08-02 22:37:07,007] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0093: [2024-08-02 22:37:07,008] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0091: [2024-08-02 22:37:07,009] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0069: [2024-08-02 22:37:43,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=3510, skipped=0, lr=[6.132421973333333e-05, 6.132421973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3510 loss: 2.8196 iter time (s): 4.006 samples/sec: 31.950
g0093:  iteration     3510/10000000 | consumed samples:       449280 | consumed tokens:    920125440 | elapsed time per iteration (ms): 4052.4 | learning rate: 6.132E-05 | global batch size:   128 | lm loss: 2.776180E+00 | loss scale: 262144.0 | grad norm: 1.471 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.586 | tokens per gpu per second (tgs): 2021.498 | TFLOPs: 16.27 |
g0069: [2024-08-02 22:38:23,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=3520, skipped=0, lr=[6.14989824e-05, 6.14989824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3520 loss: 2.7569 iter time (s): 3.971 samples/sec: 32.230
g0093:  iteration     3520/10000000 | consumed samples:       450560 | consumed tokens:    922746880 | elapsed time per iteration (ms): 4005.6 | learning rate: 6.150E-05 | global batch size:   128 | lm loss: 2.783964E+00 | loss scale: 262144.0 | grad norm: 1.301 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.955 | tokens per gpu per second (tgs): 2045.139 | TFLOPs: 16.46 |
g0069: [2024-08-02 22:39:04,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=3530, skipped=0, lr=[6.167374506666667e-05, 6.167374506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3530 loss: 2.7421 iter time (s): 4.073 samples/sec: 31.425
g0093:  iteration     3530/10000000 | consumed samples:       451840 | consumed tokens:    925368320 | elapsed time per iteration (ms): 4105.6 | learning rate: 6.167E-05 | global batch size:   128 | lm loss: 2.777080E+00 | loss scale: 262144.0 | grad norm: 1.349 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.177 | tokens per gpu per second (tgs): 1995.310 | TFLOPs: 16.06 |
g0069: [2024-08-02 22:39:46,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=3540, skipped=0, lr=[6.184850773333333e-05, 6.184850773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3540 loss: 2.7887 iter time (s): 4.123 samples/sec: 31.048
g0093:  iteration     3540/10000000 | consumed samples:       453120 | consumed tokens:    927989760 | elapsed time per iteration (ms): 4155.9 | learning rate: 6.185E-05 | global batch size:   128 | lm loss: 2.779205E+00 | loss scale: 262144.0 | grad norm: 1.454 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.799 | tokens per gpu per second (tgs): 1971.167 | TFLOPs: 15.86 |
g0069: [2024-08-02 22:40:27,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=3550, skipped=0, lr=[6.20232704e-05, 6.20232704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3550 loss: 2.7732 iter time (s): 4.148 samples/sec: 30.859
g0093:  iteration     3550/10000000 | consumed samples:       454400 | consumed tokens:    930611200 | elapsed time per iteration (ms): 4180.0 | learning rate: 6.202E-05 | global batch size:   128 | lm loss: 2.784211E+00 | loss scale: 262144.0 | grad norm: 1.405 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.622 | tokens per gpu per second (tgs): 1959.794 | TFLOPs: 15.77 |
g0069: [2024-08-02 22:41:09,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=3560, skipped=0, lr=[6.219803306666666e-05, 6.219803306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3560 loss: 2.7283 iter time (s): 4.088 samples/sec: 31.308
g0093:  iteration     3560/10000000 | consumed samples:       455680 | consumed tokens:    933232640 | elapsed time per iteration (ms): 4120.9 | learning rate: 6.220E-05 | global batch size:   128 | lm loss: 2.756951E+00 | loss scale: 262144.0 | grad norm: 1.334 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.061 | tokens per gpu per second (tgs): 1987.927 | TFLOPs: 16.00 |
g0069: [2024-08-02 22:41:50,266] [INFO] [logging.py:96:log_dist] [Rank 0] step=3570, skipped=0, lr=[6.237279573333333e-05, 6.237279573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3570 loss: 2.7453 iter time (s): 4.075 samples/sec: 31.409
g0093:  iteration     3570/10000000 | consumed samples:       456960 | consumed tokens:    935854080 | elapsed time per iteration (ms): 4108.1 | learning rate: 6.237E-05 | global batch size:   128 | lm loss: 2.757050E+00 | loss scale: 262144.0 | grad norm: 1.323 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.158 | tokens per gpu per second (tgs): 1994.121 | TFLOPs: 16.05 |
g0069: [2024-08-02 22:42:31,990] [INFO] [logging.py:96:log_dist] [Rank 0] step=3580, skipped=0, lr=[6.25475584e-05, 6.25475584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3580 loss: 2.7456 iter time (s): 4.140 samples/sec: 30.921
g0093:  iteration     3580/10000000 | consumed samples:       458240 | consumed tokens:    938475520 | elapsed time per iteration (ms): 4172.4 | learning rate: 6.255E-05 | global batch size:   128 | lm loss: 2.752401E+00 | loss scale: 262144.0 | grad norm: 1.306 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.677 | tokens per gpu per second (tgs): 1963.358 | TFLOPs: 15.80 |
g0069: [2024-08-02 22:43:12,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=3590, skipped=0, lr=[6.272232106666666e-05, 6.272232106666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3590 loss: 2.7737 iter time (s): 3.994 samples/sec: 32.050
g0093:  iteration     3590/10000000 | consumed samples:       459520 | consumed tokens:    941096960 | elapsed time per iteration (ms): 4027.1 | learning rate: 6.272E-05 | global batch size:   128 | lm loss: 2.751514E+00 | loss scale: 262144.0 | grad norm: 1.504 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.785 | tokens per gpu per second (tgs): 2034.242 | TFLOPs: 16.37 |
g0069: [2024-08-02 22:43:55,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=3600, skipped=0, lr=[6.289708373333333e-05, 6.289708373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3600 loss: 2.7646 iter time (s): 4.306 samples/sec: 29.725
g0093:  iteration     3600/10000000 | consumed samples:       460800 | consumed tokens:    943718400 | elapsed time per iteration (ms): 4338.9 | learning rate: 6.290E-05 | global batch size:   128 | lm loss: 2.746614E+00 | loss scale: 262144.0 | grad norm: 1.302 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.500 | tokens per gpu per second (tgs): 1888.015 | TFLOPs: 15.19 |
g0069: [2024-08-02 22:44:36,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=3610, skipped=0, lr=[6.30718464e-05, 6.30718464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3610 loss: 2.6988 iter time (s): 4.017 samples/sec: 31.864
g0093:  iteration     3610/10000000 | consumed samples:       462080 | consumed tokens:    946339840 | elapsed time per iteration (ms): 4049.7 | learning rate: 6.307E-05 | global batch size:   128 | lm loss: 2.749516E+00 | loss scale: 262144.0 | grad norm: 1.303 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.607 | tokens per gpu per second (tgs): 2022.880 | TFLOPs: 16.28 |
g0069: [2024-08-02 22:45:18,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=3620, skipped=0, lr=[6.324660906666667e-05, 6.324660906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3620 loss: 2.7406 iter time (s): 4.207 samples/sec: 30.425
g0093:  iteration     3620/10000000 | consumed samples:       463360 | consumed tokens:    948961280 | elapsed time per iteration (ms): 4240.0 | learning rate: 6.325E-05 | global batch size:   128 | lm loss: 2.736737E+00 | loss scale: 262144.0 | grad norm: 1.452 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.189 | tokens per gpu per second (tgs): 1932.077 | TFLOPs: 15.55 |
g0069: [2024-08-02 22:46:00,486] [INFO] [logging.py:96:log_dist] [Rank 0] step=3630, skipped=0, lr=[6.342137173333334e-05, 6.342137173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3630 loss: 2.7194 iter time (s): 4.161 samples/sec: 30.760
g0093:  iteration     3630/10000000 | consumed samples:       464640 | consumed tokens:    951582720 | elapsed time per iteration (ms): 4193.9 | learning rate: 6.342E-05 | global batch size:   128 | lm loss: 2.738238E+00 | loss scale: 262144.0 | grad norm: 1.326 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.520 | tokens per gpu per second (tgs): 1953.310 | TFLOPs: 15.72 |
g0069: [2024-08-02 22:46:39,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=3640, skipped=0, lr=[6.35961344e-05, 6.35961344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3640 loss: 2.7287 iter time (s): 3.906 samples/sec: 32.773
g0093:  iteration     3640/10000000 | consumed samples:       465920 | consumed tokens:    954204160 | elapsed time per iteration (ms): 3938.6 | learning rate: 6.360E-05 | global batch size:   128 | lm loss: 2.731554E+00 | loss scale: 262144.0 | grad norm: 1.303 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.499 | tokens per gpu per second (tgs): 2079.907 | TFLOPs: 16.74 |
g0069: [2024-08-02 22:47:20,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=3650, skipped=0, lr=[6.377089706666667e-05, 6.377089706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3650 loss: 2.7172 iter time (s): 3.997 samples/sec: 32.023
g0093:  iteration     3650/10000000 | consumed samples:       467200 | consumed tokens:    956825600 | elapsed time per iteration (ms): 4029.9 | learning rate: 6.377E-05 | global batch size:   128 | lm loss: 2.728197E+00 | loss scale: 262144.0 | grad norm: 1.293 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.763 | tokens per gpu per second (tgs): 2032.820 | TFLOPs: 16.36 |
g0069: [2024-08-02 22:48:01,465] [INFO] [logging.py:96:log_dist] [Rank 0] step=3660, skipped=0, lr=[6.394565973333334e-05, 6.394565973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3660 loss: 2.7192 iter time (s): 4.097 samples/sec: 31.244
g0093:  iteration     3660/10000000 | consumed samples:       468480 | consumed tokens:    959447040 | elapsed time per iteration (ms): 4129.4 | learning rate: 6.395E-05 | global batch size:   128 | lm loss: 2.736908E+00 | loss scale: 262144.0 | grad norm: 1.471 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.997 | tokens per gpu per second (tgs): 1983.833 | TFLOPs: 15.96 |
g0069: [2024-08-02 22:48:43,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=3670, skipped=0, lr=[6.41204224e-05, 6.41204224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3670 loss: 2.7128 iter time (s): 4.124 samples/sec: 31.041
g0093:  iteration     3670/10000000 | consumed samples:       469760 | consumed tokens:    962068480 | elapsed time per iteration (ms): 4156.4 | learning rate: 6.412E-05 | global batch size:   128 | lm loss: 2.728085E+00 | loss scale: 262144.0 | grad norm: 1.254 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.796 | tokens per gpu per second (tgs): 1970.937 | TFLOPs: 15.86 |
g0069: [2024-08-02 22:49:23,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=3680, skipped=0, lr=[6.429518506666667e-05, 6.429518506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3680 loss: 2.6917 iter time (s): 4.021 samples/sec: 31.833
g0093:  iteration     3680/10000000 | consumed samples:       471040 | consumed tokens:    964689920 | elapsed time per iteration (ms): 4053.9 | learning rate: 6.430E-05 | global batch size:   128 | lm loss: 2.712767E+00 | loss scale: 262144.0 | grad norm: 1.276 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.575 | tokens per gpu per second (tgs): 2020.785 | TFLOPs: 16.26 |
g0069: [2024-08-02 22:50:05,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=3690, skipped=0, lr=[6.446994773333334e-05, 6.446994773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3690 loss: 2.7192 iter time (s): 4.174 samples/sec: 30.667
g0093:  iteration     3690/10000000 | consumed samples:       472320 | consumed tokens:    967311360 | elapsed time per iteration (ms): 4207.6 | learning rate: 6.447E-05 | global batch size:   128 | lm loss: 2.704918E+00 | loss scale: 262144.0 | grad norm: 1.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.421 | tokens per gpu per second (tgs): 1946.956 | TFLOPs: 15.67 |
g0069: [2024-08-02 22:50:46,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=3700, skipped=0, lr=[6.46447104e-05, 6.46447104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3700 loss: 2.7335 iter time (s): 4.046 samples/sec: 31.638
g0093:  iteration     3700/10000000 | consumed samples:       473600 | consumed tokens:    969932800 | elapsed time per iteration (ms): 4080.0 | learning rate: 6.464E-05 | global batch size:   128 | lm loss: 2.721853E+00 | loss scale: 262144.0 | grad norm: 1.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.373 | tokens per gpu per second (tgs): 2007.846 | TFLOPs: 16.16 |
g0069: [2024-08-02 22:51:26,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=3710, skipped=0, lr=[6.481947306666667e-05, 6.481947306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3710 loss: 2.7111 iter time (s): 4.006 samples/sec: 31.953
g0093:  iteration     3710/10000000 | consumed samples:       474880 | consumed tokens:    972554240 | elapsed time per iteration (ms): 4038.5 | learning rate: 6.482E-05 | global batch size:   128 | lm loss: 2.711069E+00 | loss scale: 262144.0 | grad norm: 1.318 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.695 | tokens per gpu per second (tgs): 2028.463 | TFLOPs: 16.32 |
g0069: [2024-08-02 22:52:07,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=3720, skipped=0, lr=[6.499423573333333e-05, 6.499423573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3720 loss: 2.6801 iter time (s): 4.025 samples/sec: 31.799
g0093:  iteration     3720/10000000 | consumed samples:       476160 | consumed tokens:    975175680 | elapsed time per iteration (ms): 4059.2 | learning rate: 6.499E-05 | global batch size:   128 | lm loss: 2.698986E+00 | loss scale: 262144.0 | grad norm: 1.516 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.533 | tokens per gpu per second (tgs): 2018.122 | TFLOPs: 16.24 |
g0069: [2024-08-02 22:52:49,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=3730, skipped=0, lr=[6.51689984e-05, 6.51689984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3730 loss: 2.6872 iter time (s): 4.129 samples/sec: 31.003
g0093:  iteration     3730/10000000 | consumed samples:       477440 | consumed tokens:    977797120 | elapsed time per iteration (ms): 4162.1 | learning rate: 6.517E-05 | global batch size:   128 | lm loss: 2.697143E+00 | loss scale: 262144.0 | grad norm: 1.392 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.754 | tokens per gpu per second (tgs): 1968.254 | TFLOPs: 15.84 |
g0069: [2024-08-02 22:53:31,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=3740, skipped=0, lr=[6.534376106666667e-05, 6.534376106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3740 loss: 2.7121 iter time (s): 4.180 samples/sec: 30.620
g0093:  iteration     3740/10000000 | consumed samples:       478720 | consumed tokens:    980418560 | elapsed time per iteration (ms): 4213.0 | learning rate: 6.534E-05 | global batch size:   128 | lm loss: 2.687903E+00 | loss scale: 262144.0 | grad norm: 1.390 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.382 | tokens per gpu per second (tgs): 1944.460 | TFLOPs: 15.65 |
g0069: [2024-08-02 22:54:14,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=3750, skipped=0, lr=[6.551852373333333e-05, 6.551852373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3750 loss: 2.6579 iter time (s): 4.286 samples/sec: 29.862
g0093:  iteration     3750/10000000 | consumed samples:       480000 | consumed tokens:    983040000 | elapsed time per iteration (ms): 4318.7 | learning rate: 6.552E-05 | global batch size:   128 | lm loss: 2.685172E+00 | loss scale: 262144.0 | grad norm: 1.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.638 | tokens per gpu per second (tgs): 1896.851 | TFLOPs: 15.26 |
g0069: [2024-08-02 22:54:56,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=3760, skipped=0, lr=[6.56932864e-05, 6.56932864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3760 loss: 2.7032 iter time (s): 4.170 samples/sec: 30.696
g0093:  iteration     3760/10000000 | consumed samples:       481280 | consumed tokens:    985661440 | elapsed time per iteration (ms): 4202.2 | learning rate: 6.569E-05 | global batch size:   128 | lm loss: 2.686724E+00 | loss scale: 262144.0 | grad norm: 1.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.461 | tokens per gpu per second (tgs): 1949.476 | TFLOPs: 15.69 |
g0069: [2024-08-02 22:55:38,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=3770, skipped=0, lr=[6.586804906666666e-05, 6.586804906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3770 loss: 2.6810 iter time (s): 4.134 samples/sec: 30.966
g0093:  iteration     3770/10000000 | consumed samples:       482560 | consumed tokens:    988282880 | elapsed time per iteration (ms): 4165.8 | learning rate: 6.587E-05 | global batch size:   128 | lm loss: 2.689938E+00 | loss scale: 262144.0 | grad norm: 1.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.726 | tokens per gpu per second (tgs): 1966.476 | TFLOPs: 15.82 |
g0069: [2024-08-02 22:56:19,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=3780, skipped=0, lr=[6.604281173333333e-05, 6.604281173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3780 loss: 2.6897 iter time (s): 4.095 samples/sec: 31.259
g0093:  iteration     3780/10000000 | consumed samples:       483840 | consumed tokens:    990904320 | elapsed time per iteration (ms): 4128.0 | learning rate: 6.604E-05 | global batch size:   128 | lm loss: 2.681046E+00 | loss scale: 262144.0 | grad norm: 1.277 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.008 | tokens per gpu per second (tgs): 1984.510 | TFLOPs: 15.97 |
g0069: [2024-08-02 22:56:59,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=3790, skipped=0, lr=[6.62175744e-05, 6.62175744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3790 loss: 2.6646 iter time (s): 3.969 samples/sec: 32.253
g0093:  iteration     3790/10000000 | consumed samples:       485120 | consumed tokens:    993525760 | elapsed time per iteration (ms): 4001.0 | learning rate: 6.622E-05 | global batch size:   128 | lm loss: 2.671097E+00 | loss scale: 262144.0 | grad norm: 1.289 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.992 | tokens per gpu per second (tgs): 2047.482 | TFLOPs: 16.48 |
g0069: [2024-08-02 22:57:40,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=3800, skipped=0, lr=[6.639233706666668e-05, 6.639233706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3800 loss: 2.6562 iter time (s): 4.095 samples/sec: 31.260
g0093:  iteration     3800/10000000 | consumed samples:       486400 | consumed tokens:    996147200 | elapsed time per iteration (ms): 4127.3 | learning rate: 6.639E-05 | global batch size:   128 | lm loss: 2.670012E+00 | loss scale: 262144.0 | grad norm: 1.269 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.013 | tokens per gpu per second (tgs): 1984.826 | TFLOPs: 15.97 |
g0069: [2024-08-02 22:58:22,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=3810, skipped=0, lr=[6.656709973333334e-05, 6.656709973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3810 loss: 2.6946 iter time (s): 4.121 samples/sec: 31.060
g0093:  iteration     3810/10000000 | consumed samples:       487680 | consumed tokens:    998768640 | elapsed time per iteration (ms): 4153.6 | learning rate: 6.657E-05 | global batch size:   128 | lm loss: 2.677788E+00 | loss scale: 262144.0 | grad norm: 1.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.817 | tokens per gpu per second (tgs): 1972.283 | TFLOPs: 15.87 |
g0069: [2024-08-02 22:59:02,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=3820, skipped=0, lr=[6.674186240000001e-05, 6.674186240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3820 loss: 2.6683 iter time (s): 4.007 samples/sec: 31.944
g0093:  iteration     3820/10000000 | consumed samples:       488960 | consumed tokens:   1001390080 | elapsed time per iteration (ms): 4039.6 | learning rate: 6.674E-05 | global batch size:   128 | lm loss: 2.665004E+00 | loss scale: 262144.0 | grad norm: 1.369 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.687 | tokens per gpu per second (tgs): 2027.940 | TFLOPs: 16.32 |
g0069: [2024-08-02 22:59:44,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=3830, skipped=0, lr=[6.691662506666667e-05, 6.691662506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3830 loss: 2.6539 iter time (s): 4.144 samples/sec: 30.886
g0093:  iteration     3830/10000000 | consumed samples:       490240 | consumed tokens:   1004011520 | elapsed time per iteration (ms): 4176.5 | learning rate: 6.692E-05 | global batch size:   128 | lm loss: 2.653525E+00 | loss scale: 262144.0 | grad norm: 1.243 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.647 | tokens per gpu per second (tgs): 1961.435 | TFLOPs: 15.78 |
g0069: [2024-08-02 23:00:25,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=3840, skipped=0, lr=[6.709138773333334e-05, 6.709138773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3840 loss: 2.6449 iter time (s): 4.097 samples/sec: 31.245
g0093:  iteration     3840/10000000 | consumed samples:       491520 | consumed tokens:   1006632960 | elapsed time per iteration (ms): 4130.2 | learning rate: 6.709E-05 | global batch size:   128 | lm loss: 2.661284E+00 | loss scale: 262144.0 | grad norm: 1.291 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.991 | tokens per gpu per second (tgs): 1983.432 | TFLOPs: 15.96 |
g0069: [2024-08-02 23:01:06,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=3850, skipped=0, lr=[6.72661504e-05, 6.72661504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3850 loss: 2.6592 iter time (s): 4.041 samples/sec: 31.678
g0093:  iteration     3850/10000000 | consumed samples:       492800 | consumed tokens:   1009254400 | elapsed time per iteration (ms): 4073.6 | learning rate: 6.727E-05 | global batch size:   128 | lm loss: 2.653499E+00 | loss scale: 262144.0 | grad norm: 1.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.422 | tokens per gpu per second (tgs): 2011.009 | TFLOPs: 16.18 |
g0069: [2024-08-02 23:01:47,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=3860, skipped=0, lr=[6.744091306666667e-05, 6.744091306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3860 loss: 2.6353 iter time (s): 4.086 samples/sec: 31.330
g0093:  iteration     3860/10000000 | consumed samples:       494080 | consumed tokens:   1011875840 | elapsed time per iteration (ms): 4118.0 | learning rate: 6.744E-05 | global batch size:   128 | lm loss: 2.641749E+00 | loss scale: 262144.0 | grad norm: 1.267 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.083 | tokens per gpu per second (tgs): 1989.330 | TFLOPs: 16.01 |
g0069: [2024-08-02 23:02:29,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=3870, skipped=0, lr=[6.761567573333334e-05, 6.761567573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3870 loss: 2.6145 iter time (s): 4.123 samples/sec: 31.048
g0093:  iteration     3870/10000000 | consumed samples:       495360 | consumed tokens:   1014497280 | elapsed time per iteration (ms): 4155.4 | learning rate: 6.762E-05 | global batch size:   128 | lm loss: 2.625038E+00 | loss scale: 262144.0 | grad norm: 1.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.803 | tokens per gpu per second (tgs): 1971.392 | TFLOPs: 15.86 |
g0069: [2024-08-02 23:03:10,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=3880, skipped=0, lr=[6.77904384e-05, 6.77904384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3880 loss: 2.6328 iter time (s): 4.094 samples/sec: 31.262
g0093:  iteration     3880/10000000 | consumed samples:       496640 | consumed tokens:   1017118720 | elapsed time per iteration (ms): 4127.3 | learning rate: 6.779E-05 | global batch size:   128 | lm loss: 2.649426E+00 | loss scale: 262144.0 | grad norm: 1.238 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.013 | tokens per gpu per second (tgs): 1984.809 | TFLOPs: 15.97 |
g0069: [2024-08-02 23:03:51,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=3890, skipped=0, lr=[6.796520106666667e-05, 6.796520106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3890 loss: 2.6202 iter time (s): 4.034 samples/sec: 31.732
g0093:  iteration     3890/10000000 | consumed samples:       497920 | consumed tokens:   1019740160 | elapsed time per iteration (ms): 4066.8 | learning rate: 6.797E-05 | global batch size:   128 | lm loss: 2.632319E+00 | loss scale: 262144.0 | grad norm: 1.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.474 | tokens per gpu per second (tgs): 2014.356 | TFLOPs: 16.21 |
g0069: [2024-08-02 23:04:34,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=3900, skipped=0, lr=[6.813996373333334e-05, 6.813996373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3900 loss: 2.6748 iter time (s): 4.311 samples/sec: 29.693
g0093:  iteration     3900/10000000 | consumed samples:       499200 | consumed tokens:   1022361600 | elapsed time per iteration (ms): 4343.6 | learning rate: 6.814E-05 | global batch size:   128 | lm loss: 2.637542E+00 | loss scale: 262144.0 | grad norm: 2.045 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.469 | tokens per gpu per second (tgs): 1885.996 | TFLOPs: 15.18 |
g0069: [2024-08-02 23:05:16,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=3910, skipped=0, lr=[6.83147264e-05, 6.83147264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3910 loss: 2.6137 iter time (s): 4.202 samples/sec: 30.462
g0093:  iteration     3910/10000000 | consumed samples:       500480 | consumed tokens:   1024983040 | elapsed time per iteration (ms): 4235.2 | learning rate: 6.831E-05 | global batch size:   128 | lm loss: 2.620239E+00 | loss scale: 262144.0 | grad norm: 1.268 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.223 | tokens per gpu per second (tgs): 1934.273 | TFLOPs: 15.57 |
g0069: [2024-08-02 23:05:55,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=3920, skipped=0, lr=[6.848948906666667e-05, 6.848948906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3920 loss: 2.6518 iter time (s): 3.858 samples/sec: 33.175
g0093:  iteration     3920/10000000 | consumed samples:       501760 | consumed tokens:   1027604480 | elapsed time per iteration (ms): 3890.5 | learning rate: 6.849E-05 | global batch size:   128 | lm loss: 2.620183E+00 | loss scale: 262144.0 | grad norm: 1.683 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.900 | tokens per gpu per second (tgs): 2105.629 | TFLOPs: 16.94 |
g0069: [2024-08-02 23:06:36,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=3930, skipped=0, lr=[6.866425173333333e-05, 6.866425173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3930 loss: 2.6014 iter time (s): 4.065 samples/sec: 31.490
g0093:  iteration     3930/10000000 | consumed samples:       503040 | consumed tokens:   1030225920 | elapsed time per iteration (ms): 4097.5 | learning rate: 6.866E-05 | global batch size:   128 | lm loss: 2.620758E+00 | loss scale: 262144.0 | grad norm: 1.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.239 | tokens per gpu per second (tgs): 1999.279 | TFLOPs: 16.09 |
g0069: [2024-08-02 23:07:18,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=3940, skipped=0, lr=[6.88390144e-05, 6.88390144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3940 loss: 2.5960 iter time (s): 4.134 samples/sec: 30.965
g0093:  iteration     3940/10000000 | consumed samples:       504320 | consumed tokens:   1032847360 | elapsed time per iteration (ms): 4166.5 | learning rate: 6.884E-05 | global batch size:   128 | lm loss: 2.602226E+00 | loss scale: 262144.0 | grad norm: 1.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.721 | tokens per gpu per second (tgs): 1966.159 | TFLOPs: 15.82 |
g0069: [2024-08-02 23:07:59,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=3950, skipped=0, lr=[6.901377706666667e-05, 6.901377706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3950 loss: 2.5692 iter time (s): 4.037 samples/sec: 31.708
g0093:  iteration     3950/10000000 | consumed samples:       505600 | consumed tokens:   1035468800 | elapsed time per iteration (ms): 4069.2 | learning rate: 6.901E-05 | global batch size:   128 | lm loss: 2.604046E+00 | loss scale: 262144.0 | grad norm: 1.388 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.456 | tokens per gpu per second (tgs): 2013.192 | TFLOPs: 16.20 |
g0069: [2024-08-02 23:08:39,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=3960, skipped=0, lr=[6.918853973333333e-05, 6.918853973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3960 loss: 2.5939 iter time (s): 3.976 samples/sec: 32.189
g0093:  iteration     3960/10000000 | consumed samples:       506880 | consumed tokens:   1038090240 | elapsed time per iteration (ms): 4009.0 | learning rate: 6.919E-05 | global batch size:   128 | lm loss: 2.595697E+00 | loss scale: 262144.0 | grad norm: 1.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.928 | tokens per gpu per second (tgs): 2043.396 | TFLOPs: 16.44 |
g0069: [2024-08-02 23:09:19,464] [INFO] [logging.py:96:log_dist] [Rank 0] step=3970, skipped=0, lr=[6.93633024e-05, 6.93633024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3970 loss: 2.5581 iter time (s): 4.000 samples/sec: 31.999
g0093:  iteration     3970/10000000 | consumed samples:       508160 | consumed tokens:   1040711680 | elapsed time per iteration (ms): 4033.3 | learning rate: 6.936E-05 | global batch size:   128 | lm loss: 2.590650E+00 | loss scale: 262144.0 | grad norm: 1.121 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.736 | tokens per gpu per second (tgs): 2031.086 | TFLOPs: 16.34 |
g0069: [2024-08-02 23:10:00,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=3980, skipped=0, lr=[6.953806506666668e-05, 6.953806506666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3980 loss: 2.6005 iter time (s): 4.042 samples/sec: 31.670
g0093:  iteration     3980/10000000 | consumed samples:       509440 | consumed tokens:   1043333120 | elapsed time per iteration (ms): 4074.4 | learning rate: 6.954E-05 | global batch size:   128 | lm loss: 2.589454E+00 | loss scale: 262144.0 | grad norm: 1.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.416 | tokens per gpu per second (tgs): 2010.608 | TFLOPs: 16.18 |
g0069: [2024-08-02 23:10:42,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=3990, skipped=0, lr=[6.971282773333334e-05, 6.971282773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 3990 loss: 2.6043 iter time (s): 4.210 samples/sec: 30.404
g0093:  iteration     3990/10000000 | consumed samples:       510720 | consumed tokens:   1045954560 | elapsed time per iteration (ms): 4243.0 | learning rate: 6.971E-05 | global batch size:   128 | lm loss: 2.592370E+00 | loss scale: 262144.0 | grad norm: 1.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.167 | tokens per gpu per second (tgs): 1930.689 | TFLOPs: 15.54 |
g0069: [2024-08-02 23:11:24,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=0, lr=[6.988759040000001e-05, 6.988759040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4000 loss: 2.5749 iter time (s): 4.170 samples/sec: 30.698
g0093:  iteration     4000/10000000 | consumed samples:       512000 | consumed tokens:   1048576000 | elapsed time per iteration (ms): 4202.6 | learning rate: 6.989E-05 | global batch size:   128 | lm loss: 2.598429E+00 | loss scale: 262144.0 | grad norm: 1.102 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.457 | tokens per gpu per second (tgs): 1949.275 | TFLOPs: 15.69 |
g0093: ------------------------------------------------------------------------------------------------
g0093:  validation loss at iteration 4000 | lm loss value: 2.584207E+00 | lm loss PPL: 1.325278E+01 | 
g0093: ------------------------------------------------------------------------------------------------
g0069: saving checkpoint at iteration    4000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: [2024-08-02 23:17:53,012] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4000 is about to be saved!
g0090: [2024-08-02 23:17:53,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0093: [2024-08-02 23:17:53,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0069: [2024-08-02 23:17:53,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0069: [2024-08-02 23:17:53,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0069: [2024-08-02 23:17:53,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0090: [2024-08-02 23:17:53,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0090: [2024-08-02 23:17:53,019] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0093: [2024-08-02 23:17:53,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0093: [2024-08-02 23:17:53,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0092: [2024-08-02 23:17:53,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0092: [2024-08-02 23:17:53,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0092: [2024-08-02 23:17:53,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0086: [2024-08-02 23:17:53,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0086: [2024-08-02 23:17:53,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0086: [2024-08-02 23:17:53,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0091: [2024-08-02 23:17:53,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0091: [2024-08-02 23:17:53,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0091: [2024-08-02 23:17:53,020] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0088: [2024-08-02 23:17:53,023] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0088: [2024-08-02 23:17:53,023] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0088: [2024-08-02 23:17:53,023] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0087: [2024-08-02 23:17:53,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0087: [2024-08-02 23:17:53,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0087: [2024-08-02 23:17:53,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0093: [2024-08-02 23:17:53,042] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_23-model_00-model_states.pt...
g0090: [2024-08-02 23:17:53,052] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_14-model_00-model_states.pt...
g0088: [2024-08-02 23:17:53,056] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_11-model_00-model_states.pt...
g0086: [2024-08-02 23:17:53,056] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_05-model_00-model_states.pt...
g0092: [2024-08-02 23:17:53,057] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_20-model_00-model_states.pt...
g0087: [2024-08-02 23:17:53,058] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_08-model_00-model_states.pt...
g0091: [2024-08-02 23:17:53,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_17-model_00-model_states.pt...
g0069: [2024-08-02 23:17:53,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_01-model_00-model_states.pt...
g0090: [2024-08-02 23:17:53,169] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_14-model_00-model_states.pt.
g0087: [2024-08-02 23:17:53,201] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_08-model_00-model_states.pt.
g0090: [2024-08-02 23:17:53,203] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_15-model_00-model_states.pt...
g0086: [2024-08-02 23:17:53,209] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_05-model_00-model_states.pt.
g0088: [2024-08-02 23:17:53,212] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_11-model_00-model_states.pt.
g0092: [2024-08-02 23:17:53,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_20-model_00-model_states.pt.
g0087: [2024-08-02 23:17:53,236] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_09-model_00-model_states.pt...
g0088: [2024-08-02 23:17:53,246] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_12-model_00-model_states.pt...
g0091: [2024-08-02 23:17:53,248] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_17-model_00-model_states.pt.
g0086: [2024-08-02 23:17:53,249] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_06-model_00-model_states.pt...
g0092: [2024-08-02 23:17:53,252] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_21-model_00-model_states.pt...
g0069: [2024-08-02 23:17:53,262] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_01-model_00-model_states.pt.
g0093: [2024-08-02 23:17:53,279] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_23-model_00-model_states.pt.
g0093: [2024-08-02 23:17:53,280] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_24-model_00-model_states.pt...
g0093: [2024-08-02 23:17:53,282] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_24-model_00-model_states.pt.
g0091: [2024-08-02 23:17:53,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_18-model_00-model_states.pt...
g0069: [2024-08-02 23:17:53,291] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_02-model_00-model_states.pt...
g0093: [2024-08-02 23:17:53,328] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_25-model_00-model_states.pt...
g0086: [2024-08-02 23:17:53,355] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_06-model_00-model_states.pt.
g0086: [2024-08-02 23:17:53,386] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_07-model_00-model_states.pt...
g0087: [2024-08-02 23:17:53,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_09-model_00-model_states.pt.
g0090: [2024-08-02 23:17:53,416] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_15-model_00-model_states.pt.
g0087: [2024-08-02 23:17:53,419] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_10-model_00-model_states.pt...
g0091: [2024-08-02 23:17:53,428] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_18-model_00-model_states.pt.
g0090: [2024-08-02 23:17:53,442] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_16-model_00-model_states.pt...
g0091: [2024-08-02 23:17:53,463] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_19-model_00-model_states.pt...
g0088: [2024-08-02 23:17:53,470] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_12-model_00-model_states.pt.
g0093: [2024-08-02 23:17:53,487] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_25-model_00-model_states.pt.
g0093: [2024-08-02 23:17:53,488] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_07_model_states.pt...
g0086: [2024-08-02 23:17:53,489] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_07-model_00-model_states.pt.
g0086: [2024-08-02 23:17:53,491] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_01_model_states.pt...
g0088: [2024-08-02 23:17:53,500] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_13-model_00-model_states.pt...
g0092: [2024-08-02 23:17:53,511] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_21-model_00-model_states.pt.
g0087: [2024-08-02 23:17:53,522] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_10-model_00-model_states.pt.
g0087: [2024-08-02 23:17:53,524] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_02_model_states.pt...
g0092: [2024-08-02 23:17:53,546] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_22-model_00-model_states.pt...
g0091: [2024-08-02 23:17:53,553] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_19-model_00-model_states.pt.
g0090: [2024-08-02 23:17:53,554] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_16-model_00-model_states.pt.
g0091: [2024-08-02 23:17:53,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_05_model_states.pt...
g0090: [2024-08-02 23:17:53,556] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_04_model_states.pt...
g0088: [2024-08-02 23:17:54,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_13-model_00-model_states.pt.
g0088: [2024-08-02 23:17:54,055] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_03_model_states.pt...
g0069: [2024-08-02 23:17:54,178] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_02-model_00-model_states.pt.
g0069: [2024-08-02 23:17:54,213] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_03-model_00-model_states.pt...
g0069: [2024-08-02 23:17:54,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_03-model_00-model_states.pt.
g0069: [2024-08-02 23:17:54,326] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_04-model_00-model_states.pt...
g0069: [2024-08-02 23:17:54,412] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_04-model_00-model_states.pt.
g0069: [2024-08-02 23:17:54,414] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt
g0069: [2024-08-02 23:17:54,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt...
g0092: [2024-08-02 23:17:54,598] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_22-model_00-model_states.pt.
g0092: [2024-08-02 23:17:54,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_06_model_states.pt...
g0093: [2024-08-02 23:17:55,460] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_07_model_states.pt.
g0093: [2024-08-02 23:17:55,461] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0090: [2024-08-02 23:17:55,832] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_04_model_states.pt.
g0090: [2024-08-02 23:17:55,833] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0087: [2024-08-02 23:17:55,904] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_02_model_states.pt.
g0087: [2024-08-02 23:17:55,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0091: [2024-08-02 23:17:55,924] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_05_model_states.pt.
g0091: [2024-08-02 23:17:55,924] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0086: [2024-08-02 23:17:55,984] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_01_model_states.pt.
g0086: [2024-08-02 23:17:55,985] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0088: [2024-08-02 23:17:56,324] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_03_model_states.pt.
g0088: [2024-08-02 23:17:56,325] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0092: [2024-08-02 23:17:56,968] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_06_model_states.pt.
g0092: [2024-08-02 23:17:56,968] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0069: [2024-08-02 23:17:57,769] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt.
g0069: [2024-08-02 23:17:57,770] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0069:   successfully saved checkpoint at iteration    4000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: Checkpoint Save GB: 22.521, GB/Sec: 4.72, Latency(second): 4.772
g0093: (min, max) time across ranks (ms):
g0093:     save-checkpoint ................................: (4772.14, 4772.35)
g0088: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0088: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0093: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0093: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0087: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0091: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0090: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0091: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0091: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0087: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0090: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0092: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0092: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0069: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0090: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0069: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0090: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0093: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0093: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0069: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0069: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0086: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0086: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0086: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0090: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-02 23:18:01,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0091: [2024-08-02 23:18:01,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0086: [2024-08-02 23:18:01,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0069: [2024-08-02 23:18:01,655] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-02 23:18:01,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0092: [2024-08-02 23:18:01,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0088: [2024-08-02 23:18:01,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0093: [2024-08-02 23:18:01,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0069: [2024-08-02 23:18:01,656] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0069: [2024-08-02 23:18:39,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=4010, skipped=0, lr=[7.006235306666668e-05, 7.006235306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4010 loss: 2.6331 iter time (s): 4.152 samples/sec: 30.829
g0093:  iteration     4010/10000000 | consumed samples:       513280 | consumed tokens:   1051197440 | elapsed time per iteration (ms): 43492.4 | learning rate: 7.006E-05 | global batch size:   128 | lm loss: 2.592822E+00 | loss scale: 524288.0 | grad norm: 1.360 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.943 | tokens per gpu per second (tgs): 188.355 | TFLOPs: 1.52 |
g0069: [2024-08-02 23:19:20,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=4020, skipped=0, lr=[7.023711573333334e-05, 7.023711573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4020 loss: 2.5887 iter time (s): 4.034 samples/sec: 31.731
g0093:  iteration     4020/10000000 | consumed samples:       514560 | consumed tokens:   1053818880 | elapsed time per iteration (ms): 4066.4 | learning rate: 7.024E-05 | global batch size:   128 | lm loss: 2.589321E+00 | loss scale: 524288.0 | grad norm: 1.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.477 | tokens per gpu per second (tgs): 2014.549 | TFLOPs: 16.21 |
g0069: [2024-08-02 23:20:01,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=4030, skipped=0, lr=[7.041187840000001e-05, 7.041187840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4030 loss: 2.5916 iter time (s): 4.077 samples/sec: 31.393
g0093:  iteration     4030/10000000 | consumed samples:       515840 | consumed tokens:   1056440320 | elapsed time per iteration (ms): 4110.0 | learning rate: 7.041E-05 | global batch size:   128 | lm loss: 2.575245E+00 | loss scale: 524288.0 | grad norm: 1.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.143 | tokens per gpu per second (tgs): 1993.182 | TFLOPs: 16.04 |
g0069: [2024-08-02 23:20:43,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=4040, skipped=0, lr=[7.058664106666667e-05, 7.058664106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4040 loss: 2.5265 iter time (s): 4.135 samples/sec: 30.953
g0093:  iteration     4040/10000000 | consumed samples:       517120 | consumed tokens:   1059061760 | elapsed time per iteration (ms): 4168.6 | learning rate: 7.059E-05 | global batch size:   128 | lm loss: 2.568810E+00 | loss scale: 524288.0 | grad norm: 1.142 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.706 | tokens per gpu per second (tgs): 1965.152 | TFLOPs: 15.81 |
g0069: [2024-08-02 23:21:24,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=4050, skipped=0, lr=[7.076140373333334e-05, 7.076140373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4050 loss: 2.5329 iter time (s): 4.093 samples/sec: 31.270
g0093:  iteration     4050/10000000 | consumed samples:       518400 | consumed tokens:   1061683200 | elapsed time per iteration (ms): 4126.3 | learning rate: 7.076E-05 | global batch size:   128 | lm loss: 2.559204E+00 | loss scale: 524288.0 | grad norm: 1.124 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.020 | tokens per gpu per second (tgs): 1985.303 | TFLOPs: 15.98 |
g0069: [2024-08-02 23:22:05,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=4060, skipped=0, lr=[7.093616640000001e-05, 7.093616640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4060 loss: 2.5551 iter time (s): 4.135 samples/sec: 30.956
g0093:  iteration     4060/10000000 | consumed samples:       519680 | consumed tokens:   1064304640 | elapsed time per iteration (ms): 4167.5 | learning rate: 7.094E-05 | global batch size:   128 | lm loss: 2.572590E+00 | loss scale: 524288.0 | grad norm: 1.132 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.714 | tokens per gpu per second (tgs): 1965.669 | TFLOPs: 15.82 |
g0069: [2024-08-02 23:22:46,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=4070, skipped=0, lr=[7.111092906666667e-05, 7.111092906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4070 loss: 2.5572 iter time (s): 3.979 samples/sec: 32.170
g0093:  iteration     4070/10000000 | consumed samples:       520960 | consumed tokens:   1066926080 | elapsed time per iteration (ms): 4011.4 | learning rate: 7.111E-05 | global batch size:   128 | lm loss: 2.570160E+00 | loss scale: 524288.0 | grad norm: 1.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.909 | tokens per gpu per second (tgs): 2042.163 | TFLOPs: 16.43 |
g0069: [2024-08-02 23:23:26,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=4080, skipped=0, lr=[7.128569173333334e-05, 7.128569173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4080 loss: 2.5407 iter time (s): 4.010 samples/sec: 31.918
g0093:  iteration     4080/10000000 | consumed samples:       522240 | consumed tokens:   1069547520 | elapsed time per iteration (ms): 4042.8 | learning rate: 7.129E-05 | global batch size:   128 | lm loss: 2.539542E+00 | loss scale: 524288.0 | grad norm: 1.158 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.662 | tokens per gpu per second (tgs): 2026.338 | TFLOPs: 16.31 |
g0069: [2024-08-02 23:24:09,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=4090, skipped=0, lr=[7.14604544e-05, 7.14604544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4090 loss: 2.5390 iter time (s): 4.275 samples/sec: 29.939
g0093:  iteration     4090/10000000 | consumed samples:       523520 | consumed tokens:   1072168960 | elapsed time per iteration (ms): 4307.8 | learning rate: 7.146E-05 | global batch size:   128 | lm loss: 2.548783E+00 | loss scale: 524288.0 | grad norm: 1.166 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.713 | tokens per gpu per second (tgs): 1901.650 | TFLOPs: 15.30 |
g0069: [2024-08-02 23:24:51,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=4100, skipped=0, lr=[7.163521706666667e-05, 7.163521706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4100 loss: 2.5296 iter time (s): 4.114 samples/sec: 31.116
g0093:  iteration     4100/10000000 | consumed samples:       524800 | consumed tokens:   1074790400 | elapsed time per iteration (ms): 4146.0 | learning rate: 7.164E-05 | global batch size:   128 | lm loss: 2.545129E+00 | loss scale: 524288.0 | grad norm: 1.133 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.873 | tokens per gpu per second (tgs): 1975.859 | TFLOPs: 15.90 |
g0069: [2024-08-02 23:25:32,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=4110, skipped=0, lr=[7.180997973333334e-05, 7.180997973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4110 loss: 2.5117 iter time (s): 4.093 samples/sec: 31.277
g0093:  iteration     4110/10000000 | consumed samples:       526080 | consumed tokens:   1077411840 | elapsed time per iteration (ms): 4125.1 | learning rate: 7.181E-05 | global batch size:   128 | lm loss: 2.520926E+00 | loss scale: 524288.0 | grad norm: 1.100 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.029 | tokens per gpu per second (tgs): 1985.874 | TFLOPs: 15.98 |
g0069: [2024-08-02 23:26:14,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=4120, skipped=0, lr=[7.19847424e-05, 7.19847424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4120 loss: 2.5059 iter time (s): 4.145 samples/sec: 30.879
g0093:  iteration     4120/10000000 | consumed samples:       527360 | consumed tokens:   1080033280 | elapsed time per iteration (ms): 4177.8 | learning rate: 7.198E-05 | global batch size:   128 | lm loss: 2.531127E+00 | loss scale: 524288.0 | grad norm: 1.084 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.638 | tokens per gpu per second (tgs): 1960.860 | TFLOPs: 15.78 |
g0069: [2024-08-02 23:26:54,812] [INFO] [logging.py:96:log_dist] [Rank 0] step=4130, skipped=0, lr=[7.215950506666667e-05, 7.215950506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4130 loss: 2.5424 iter time (s): 4.039 samples/sec: 31.691
g0093:  iteration     4130/10000000 | consumed samples:       528640 | consumed tokens:   1082654720 | elapsed time per iteration (ms): 4072.2 | learning rate: 7.216E-05 | global batch size:   128 | lm loss: 2.513024E+00 | loss scale: 524288.0 | grad norm: 1.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.433 | tokens per gpu per second (tgs): 2011.692 | TFLOPs: 16.19 |
g0069: [2024-08-02 23:27:35,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=4140, skipped=0, lr=[7.233426773333334e-05, 7.233426773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4140 loss: 2.5035 iter time (s): 4.069 samples/sec: 31.461
g0093:  iteration     4140/10000000 | consumed samples:       529920 | consumed tokens:   1085276160 | elapsed time per iteration (ms): 4101.6 | learning rate: 7.233E-05 | global batch size:   128 | lm loss: 2.529577E+00 | loss scale: 524288.0 | grad norm: 1.123 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.208 | tokens per gpu per second (tgs): 1997.292 | TFLOPs: 16.07 |
g0069: [2024-08-02 23:28:15,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=4150, skipped=0, lr=[7.25090304e-05, 7.25090304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4150 loss: 2.5317 iter time (s): 3.982 samples/sec: 32.146
g0093:  iteration     4150/10000000 | consumed samples:       531200 | consumed tokens:   1087897600 | elapsed time per iteration (ms): 4014.8 | learning rate: 7.251E-05 | global batch size:   128 | lm loss: 2.510095E+00 | loss scale: 524288.0 | grad norm: 1.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.882 | tokens per gpu per second (tgs): 2040.467 | TFLOPs: 16.42 |
g0069: [2024-08-02 23:28:57,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=4160, skipped=0, lr=[7.268379306666668e-05, 7.268379306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4160 loss: 2.4902 iter time (s): 4.141 samples/sec: 30.912
g0093:  iteration     4160/10000000 | consumed samples:       532480 | consumed tokens:   1090519040 | elapsed time per iteration (ms): 4173.1 | learning rate: 7.268E-05 | global batch size:   128 | lm loss: 2.510355E+00 | loss scale: 524288.0 | grad norm: 1.149 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.673 | tokens per gpu per second (tgs): 1963.055 | TFLOPs: 15.80 |
g0069: [2024-08-02 23:29:40,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=4170, skipped=0, lr=[7.285855573333333e-05, 7.285855573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4170 loss: 2.5175 iter time (s): 4.222 samples/sec: 30.314
g0093:  iteration     4170/10000000 | consumed samples:       533760 | consumed tokens:   1093140480 | elapsed time per iteration (ms): 4256.1 | learning rate: 7.286E-05 | global batch size:   128 | lm loss: 2.505835E+00 | loss scale: 524288.0 | grad norm: 1.145 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.074 | tokens per gpu per second (tgs): 1924.762 | TFLOPs: 15.49 |
g0069: [2024-08-02 23:30:20,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=4180, skipped=0, lr=[7.30333184e-05, 7.30333184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4180 loss: 2.4886 iter time (s): 4.023 samples/sec: 31.820
g0093:  iteration     4180/10000000 | consumed samples:       535040 | consumed tokens:   1095761920 | elapsed time per iteration (ms): 4056.0 | learning rate: 7.303E-05 | global batch size:   128 | lm loss: 2.494441E+00 | loss scale: 524288.0 | grad norm: 1.140 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.558 | tokens per gpu per second (tgs): 2019.735 | TFLOPs: 16.25 |
g0069: [2024-08-02 23:31:02,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=4190, skipped=0, lr=[7.320808106666667e-05, 7.320808106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4190 loss: 2.5568 iter time (s): 4.143 samples/sec: 30.897
g0093:  iteration     4190/10000000 | consumed samples:       536320 | consumed tokens:   1098383360 | elapsed time per iteration (ms): 4175.8 | learning rate: 7.321E-05 | global batch size:   128 | lm loss: 2.497435E+00 | loss scale: 524288.0 | grad norm: 1.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.653 | tokens per gpu per second (tgs): 1961.792 | TFLOPs: 15.79 |
g0069: [2024-08-02 23:31:43,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=4200, skipped=0, lr=[7.338284373333333e-05, 7.338284373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4200 loss: 2.4614 iter time (s): 4.052 samples/sec: 31.588
g0093:  iteration     4200/10000000 | consumed samples:       537600 | consumed tokens:   1101004800 | elapsed time per iteration (ms): 4086.4 | learning rate: 7.338E-05 | global batch size:   128 | lm loss: 2.501338E+00 | loss scale: 524288.0 | grad norm: 1.148 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.323 | tokens per gpu per second (tgs): 2004.697 | TFLOPs: 16.13 |
g0069: [2024-08-02 23:32:26,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=4210, skipped=0, lr=[7.35576064e-05, 7.35576064e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4210 loss: 2.4834 iter time (s): 4.280 samples/sec: 29.908
g0093:  iteration     4210/10000000 | consumed samples:       538880 | consumed tokens:   1103626240 | elapsed time per iteration (ms): 4313.2 | learning rate: 7.356E-05 | global batch size:   128 | lm loss: 2.484714E+00 | loss scale: 524288.0 | grad norm: 1.100 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.676 | tokens per gpu per second (tgs): 1899.287 | TFLOPs: 15.28 |
g0069: [2024-08-02 23:33:08,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=4220, skipped=0, lr=[7.373236906666666e-05, 7.373236906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4220 loss: 2.4721 iter time (s): 4.199 samples/sec: 30.483
g0093:  iteration     4220/10000000 | consumed samples:       540160 | consumed tokens:   1106247680 | elapsed time per iteration (ms): 4232.9 | learning rate: 7.373E-05 | global batch size:   128 | lm loss: 2.487387E+00 | loss scale: 524288.0 | grad norm: 1.077 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.239 | tokens per gpu per second (tgs): 1935.323 | TFLOPs: 15.57 |
g0069: [2024-08-02 23:33:49,603] [INFO] [logging.py:96:log_dist] [Rank 0] step=4230, skipped=0, lr=[7.390713173333333e-05, 7.390713173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4230 loss: 2.5024 iter time (s): 4.037 samples/sec: 31.709
g0093:  iteration     4230/10000000 | consumed samples:       541440 | consumed tokens:   1108869120 | elapsed time per iteration (ms): 4069.1 | learning rate: 7.391E-05 | global batch size:   128 | lm loss: 2.464934E+00 | loss scale: 524288.0 | grad norm: 1.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.456 | tokens per gpu per second (tgs): 2013.198 | TFLOPs: 16.20 |
g0069: [2024-08-02 23:34:30,117] [INFO] [logging.py:96:log_dist] [Rank 0] step=4240, skipped=0, lr=[7.40818944e-05, 7.40818944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4240 loss: 2.4620 iter time (s): 4.018 samples/sec: 31.856
g0093:  iteration     4240/10000000 | consumed samples:       542720 | consumed tokens:   1111490560 | elapsed time per iteration (ms): 4051.5 | learning rate: 7.408E-05 | global batch size:   128 | lm loss: 2.453922E+00 | loss scale: 524288.0 | grad norm: 1.066 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.593 | tokens per gpu per second (tgs): 2021.962 | TFLOPs: 16.27 |
g0069: [2024-08-02 23:35:10,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=4250, skipped=0, lr=[7.425665706666666e-05, 7.425665706666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4250 loss: 2.4766 iter time (s): 3.995 samples/sec: 32.040
g0093:  iteration     4250/10000000 | consumed samples:       544000 | consumed tokens:   1114112000 | elapsed time per iteration (ms): 4027.8 | learning rate: 7.426E-05 | global batch size:   128 | lm loss: 2.463159E+00 | loss scale: 524288.0 | grad norm: 1.370 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.779 | tokens per gpu per second (tgs): 2033.881 | TFLOPs: 16.37 |
g0069: [2024-08-02 23:35:51,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=4260, skipped=0, lr=[7.443141973333333e-05, 7.443141973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4260 loss: 2.4307 iter time (s): 4.116 samples/sec: 31.101
g0093:  iteration     4260/10000000 | consumed samples:       545280 | consumed tokens:   1116733440 | elapsed time per iteration (ms): 4148.8 | learning rate: 7.443E-05 | global batch size:   128 | lm loss: 2.460577E+00 | loss scale: 524288.0 | grad norm: 1.027 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.852 | tokens per gpu per second (tgs): 1974.553 | TFLOPs: 15.89 |
g0069: [2024-08-02 23:36:34,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=4270, skipped=0, lr=[7.46061824e-05, 7.46061824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4270 loss: 2.4085 iter time (s): 4.187 samples/sec: 30.574
g0093:  iteration     4270/10000000 | consumed samples:       546560 | consumed tokens:   1119354880 | elapsed time per iteration (ms): 4224.6 | learning rate: 7.461E-05 | global batch size:   128 | lm loss: 2.436960E+00 | loss scale: 524288.0 | grad norm: 1.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.299 | tokens per gpu per second (tgs): 1939.115 | TFLOPs: 15.60 |
g0069: [2024-08-02 23:37:16,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=4280, skipped=0, lr=[7.478094506666666e-05, 7.478094506666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4280 loss: 2.4370 iter time (s): 4.178 samples/sec: 30.635
g0093:  iteration     4280/10000000 | consumed samples:       547840 | consumed tokens:   1121976320 | elapsed time per iteration (ms): 4226.5 | learning rate: 7.478E-05 | global batch size:   128 | lm loss: 2.443814E+00 | loss scale: 524288.0 | grad norm: 1.156 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.285 | tokens per gpu per second (tgs): 1938.243 | TFLOPs: 15.60 |
g0069: [2024-08-02 23:37:57,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=4290, skipped=0, lr=[7.495570773333334e-05, 7.495570773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4290 loss: 2.4420 iter time (s): 4.123 samples/sec: 31.046
g0093:  iteration     4290/10000000 | consumed samples:       549120 | consumed tokens:   1124597760 | elapsed time per iteration (ms): 4157.6 | learning rate: 7.496E-05 | global batch size:   128 | lm loss: 2.433035E+00 | loss scale: 524288.0 | grad norm: 1.069 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.787 | tokens per gpu per second (tgs): 1970.390 | TFLOPs: 15.86 |
g0069: [2024-08-02 23:38:37,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=4300, skipped=0, lr=[7.51304704e-05, 7.51304704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4300 loss: 2.4323 iter time (s): 3.902 samples/sec: 32.802
g0093:  iteration     4300/10000000 | consumed samples:       550400 | consumed tokens:   1127219200 | elapsed time per iteration (ms): 3934.6 | learning rate: 7.513E-05 | global batch size:   128 | lm loss: 2.440274E+00 | loss scale: 524288.0 | grad norm: 1.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.532 | tokens per gpu per second (tgs): 2082.049 | TFLOPs: 16.75 |
g0069: [2024-08-02 23:39:16,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=4310, skipped=0, lr=[7.530523306666667e-05, 7.530523306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4310 loss: 2.4335 iter time (s): 3.916 samples/sec: 32.690
g0093:  iteration     4310/10000000 | consumed samples:       551680 | consumed tokens:   1129840640 | elapsed time per iteration (ms): 3948.5 | learning rate: 7.531E-05 | global batch size:   128 | lm loss: 2.412150E+00 | loss scale: 524288.0 | grad norm: 1.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.418 | tokens per gpu per second (tgs): 2074.733 | TFLOPs: 16.70 |
g0069: [2024-08-02 23:39:57,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=4320, skipped=0, lr=[7.547999573333334e-05, 7.547999573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4320 loss: 2.3770 iter time (s): 4.046 samples/sec: 31.635
g0093:  iteration     4320/10000000 | consumed samples:       552960 | consumed tokens:   1132462080 | elapsed time per iteration (ms): 4079.1 | learning rate: 7.548E-05 | global batch size:   128 | lm loss: 2.407305E+00 | loss scale: 524288.0 | grad norm: 1.119 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.380 | tokens per gpu per second (tgs): 2008.305 | TFLOPs: 16.16 |
g0069: [2024-08-02 23:40:38,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=4330, skipped=0, lr=[7.56547584e-05, 7.56547584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4330 loss: 2.4111 iter time (s): 4.014 samples/sec: 31.892
g0093:  iteration     4330/10000000 | consumed samples:       554240 | consumed tokens:   1135083520 | elapsed time per iteration (ms): 4046.8 | learning rate: 7.565E-05 | global batch size:   128 | lm loss: 2.401809E+00 | loss scale: 524288.0 | grad norm: 1.142 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.630 | tokens per gpu per second (tgs): 2024.318 | TFLOPs: 16.29 |
g0069: [2024-08-02 23:41:19,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=4340, skipped=0, lr=[7.582952106666667e-05, 7.582952106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4340 loss: 2.4165 iter time (s): 4.085 samples/sec: 31.335
g0093:  iteration     4340/10000000 | consumed samples:       555520 | consumed tokens:   1137704960 | elapsed time per iteration (ms): 4117.6 | learning rate: 7.583E-05 | global batch size:   128 | lm loss: 2.404162E+00 | loss scale: 524288.0 | grad norm: 1.101 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.086 | tokens per gpu per second (tgs): 1989.509 | TFLOPs: 16.01 |
g0069: [2024-08-02 23:41:59,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=4350, skipped=0, lr=[7.600428373333334e-05, 7.600428373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4350 loss: 2.4276 iter time (s): 4.013 samples/sec: 31.893
g0093:  iteration     4350/10000000 | consumed samples:       556800 | consumed tokens:   1140326400 | elapsed time per iteration (ms): 4045.7 | learning rate: 7.600E-05 | global batch size:   128 | lm loss: 2.398753E+00 | loss scale: 524288.0 | grad norm: 1.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.639 | tokens per gpu per second (tgs): 2024.873 | TFLOPs: 16.29 |
g0069: [2024-08-02 23:42:40,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=4360, skipped=0, lr=[7.61790464e-05, 7.61790464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4360 loss: 2.3613 iter time (s): 4.055 samples/sec: 31.568
g0093:  iteration     4360/10000000 | consumed samples:       558080 | consumed tokens:   1142947840 | elapsed time per iteration (ms): 4091.3 | learning rate: 7.618E-05 | global batch size:   128 | lm loss: 2.396966E+00 | loss scale: 524288.0 | grad norm: 1.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.286 | tokens per gpu per second (tgs): 2002.293 | TFLOPs: 16.11 |
g0069: [2024-08-02 23:43:22,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=4370, skipped=0, lr=[7.635380906666667e-05, 7.635380906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4370 loss: 2.3602 iter time (s): 4.167 samples/sec: 30.716
g0093:  iteration     4370/10000000 | consumed samples:       559360 | consumed tokens:   1145569280 | elapsed time per iteration (ms): 4199.9 | learning rate: 7.635E-05 | global batch size:   128 | lm loss: 2.372850E+00 | loss scale: 524288.0 | grad norm: 1.141 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.477 | tokens per gpu per second (tgs): 1950.508 | TFLOPs: 15.70 |
g0069: [2024-08-02 23:44:03,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=4380, skipped=0, lr=[7.652857173333333e-05, 7.652857173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4380 loss: 2.3313 iter time (s): 4.070 samples/sec: 31.453
g0093:  iteration     4380/10000000 | consumed samples:       560640 | consumed tokens:   1148190720 | elapsed time per iteration (ms): 4102.4 | learning rate: 7.653E-05 | global batch size:   128 | lm loss: 2.350413E+00 | loss scale: 524288.0 | grad norm: 1.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.202 | tokens per gpu per second (tgs): 1996.901 | TFLOPs: 16.07 |
g0069: [2024-08-02 23:44:44,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=4390, skipped=0, lr=[7.67033344e-05, 7.67033344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4390 loss: 2.3585 iter time (s): 4.035 samples/sec: 31.721
g0093:  iteration     4390/10000000 | consumed samples:       561920 | consumed tokens:   1150812160 | elapsed time per iteration (ms): 4068.5 | learning rate: 7.670E-05 | global batch size:   128 | lm loss: 2.365075E+00 | loss scale: 524288.0 | grad norm: 1.265 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.461 | tokens per gpu per second (tgs): 2013.501 | TFLOPs: 16.20 |
g0069: [2024-08-02 23:45:24,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=4400, skipped=0, lr=[7.687809706666667e-05, 7.687809706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4400 loss: 2.3437 iter time (s): 3.974 samples/sec: 32.210
g0093:  iteration     4400/10000000 | consumed samples:       563200 | consumed tokens:   1153433600 | elapsed time per iteration (ms): 4006.7 | learning rate: 7.688E-05 | global batch size:   128 | lm loss: 2.343714E+00 | loss scale: 524288.0 | grad norm: 1.147 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.946 | tokens per gpu per second (tgs): 2044.557 | TFLOPs: 16.45 |
g0069: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: Grad overflow on iteration 4405
g0069: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: Grad overflow on iteration 4405
g0069: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: Grad overflow on iteration 4405
g0069: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: Grad overflow on iteration 4405
g0091: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 4405
g0091: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 4405
g0091: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 4405
g0091: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0069: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0091: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0091: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0091: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 4405
g0069: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0069: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0088: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 4405
g0088: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 4405
g0088: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 4405
g0087: Grad overflow on iteration 4405
g0090: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: Grad overflow on iteration 4405
g0087: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 4405
g0087: Grad overflow on iteration 4405
g0088: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0087: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0087: Grad overflow on iteration 4405
g0088: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0087: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0069: [2024-08-02 23:45:49,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0088: [2024-08-02 23:45:49,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0087: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0092: Grad overflow on iteration 4405
g0090: Grad overflow on iteration 4405
g0087: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0090: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 4405
g0087: [2024-08-02 23:45:49,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0092: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: Grad overflow on iteration 4405
g0090: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 4405
g0092: Grad overflow on iteration 4405
g0086: Grad overflow on iteration 4405
g0086: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: Grad overflow on iteration 4405
g0090: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0093: Grad overflow on iteration 4405
g0086: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: Grad overflow on iteration 4405
g0086: Grad overflow on iteration 4405
g0092: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0086: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: Grad overflow on iteration 4405
g0093: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: Grad overflow on iteration 4405
g0086: Grad overflow on iteration 4405
g0093: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0086: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0090: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: Grad overflow on iteration 4405
g0092: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0086: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0093: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0092: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0090: Grad overflow on iteration 4405
g0093: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0086: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0086: [2024-08-02 23:45:49,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0090: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0093: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0090: [2024-08-02 23:45:49,039] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0093: [2024-08-02 23:45:49,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0090: [2024-08-02 23:45:49,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0069: [2024-08-02 23:45:49,040] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288, reducing to 262144.0
g0091: [2024-08-02 23:45:49,040] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288 to 262144.0
g0069: [2024-08-02 23:46:05,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=4410, skipped=1, lr=[7.705285973333333e-05, 7.705285973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4410 loss: 2.2955 iter time (s): 4.031 samples/sec: 31.756
g0093:  iteration     4410/10000000 | consumed samples:       564480 | consumed tokens:   1156055040 | elapsed time per iteration (ms): 4064.5 | learning rate: 7.705E-05 | global batch size:   128 | lm loss: 2.331944E+00 | loss scale: 262144.0 | grad norm: 1.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.492 | tokens per gpu per second (tgs): 2015.509 | TFLOPs: 16.22 |
g0069: [2024-08-02 23:46:46,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=4420, skipped=1, lr=[7.72276224e-05, 7.72276224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4420 loss: 2.2964 iter time (s): 4.114 samples/sec: 31.114
g0093:  iteration     4420/10000000 | consumed samples:       565760 | consumed tokens:   1158676480 | elapsed time per iteration (ms): 4146.9 | learning rate: 7.723E-05 | global batch size:   128 | lm loss: 2.324099E+00 | loss scale: 262144.0 | grad norm: 1.085 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.866 | tokens per gpu per second (tgs): 1975.450 | TFLOPs: 15.90 |
g0069: [2024-08-02 23:47:27,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=4430, skipped=1, lr=[7.740238506666667e-05, 7.740238506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4430 loss: 2.3151 iter time (s): 4.057 samples/sec: 31.550
g0093:  iteration     4430/10000000 | consumed samples:       567040 | consumed tokens:   1161297920 | elapsed time per iteration (ms): 4090.0 | learning rate: 7.740E-05 | global batch size:   128 | lm loss: 2.314706E+00 | loss scale: 262144.0 | grad norm: 1.110 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.296 | tokens per gpu per second (tgs): 2002.923 | TFLOPs: 16.12 |
g0069: [2024-08-02 23:48:10,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=4440, skipped=1, lr=[7.757714773333333e-05, 7.757714773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4440 loss: 2.2619 iter time (s): 4.229 samples/sec: 30.265
g0093:  iteration     4440/10000000 | consumed samples:       568320 | consumed tokens:   1163919360 | elapsed time per iteration (ms): 4262.4 | learning rate: 7.758E-05 | global batch size:   128 | lm loss: 2.289848E+00 | loss scale: 262144.0 | grad norm: 1.125 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.030 | tokens per gpu per second (tgs): 1921.943 | TFLOPs: 15.47 |
g0069: [2024-08-02 23:48:49,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=4450, skipped=1, lr=[7.77519104e-05, 7.77519104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4450 loss: 2.2719 iter time (s): 3.946 samples/sec: 32.442
g0093:  iteration     4450/10000000 | consumed samples:       569600 | consumed tokens:   1166540800 | elapsed time per iteration (ms): 3978.5 | learning rate: 7.775E-05 | global batch size:   128 | lm loss: 2.267012E+00 | loss scale: 262144.0 | grad norm: 1.150 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.173 | tokens per gpu per second (tgs): 2059.085 | TFLOPs: 16.57 |
g0069: [2024-08-02 23:49:30,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=4460, skipped=1, lr=[7.792667306666666e-05, 7.792667306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4460 loss: 2.2749 iter time (s): 4.040 samples/sec: 31.685
g0093:  iteration     4460/10000000 | consumed samples:       570880 | consumed tokens:   1169162240 | elapsed time per iteration (ms): 4072.8 | learning rate: 7.793E-05 | global batch size:   128 | lm loss: 2.253293E+00 | loss scale: 262144.0 | grad norm: 1.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.428 | tokens per gpu per second (tgs): 2011.412 | TFLOPs: 16.19 |
g0069: [2024-08-02 23:50:13,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=4470, skipped=1, lr=[7.810143573333334e-05, 7.810143573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4470 loss: 2.2086 iter time (s): 4.216 samples/sec: 30.363
g0093:  iteration     4470/10000000 | consumed samples:       572160 | consumed tokens:   1171783680 | elapsed time per iteration (ms): 4249.3 | learning rate: 7.810E-05 | global batch size:   128 | lm loss: 2.234465E+00 | loss scale: 262144.0 | grad norm: 1.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.123 | tokens per gpu per second (tgs): 1927.865 | TFLOPs: 15.51 |
g0069: [2024-08-02 23:50:55,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=4480, skipped=1, lr=[7.827619840000001e-05, 7.827619840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4480 loss: 2.1769 iter time (s): 4.168 samples/sec: 30.709
g0093:  iteration     4480/10000000 | consumed samples:       573440 | consumed tokens:   1174405120 | elapsed time per iteration (ms): 4201.1 | learning rate: 7.828E-05 | global batch size:   128 | lm loss: 2.207689E+00 | loss scale: 262144.0 | grad norm: 1.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.468 | tokens per gpu per second (tgs): 1949.951 | TFLOPs: 15.69 |
g0069: [2024-08-02 23:51:36,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=4490, skipped=1, lr=[7.845096106666667e-05, 7.845096106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4490 loss: 2.1370 iter time (s): 4.158 samples/sec: 30.786
g0093:  iteration     4490/10000000 | consumed samples:       574720 | consumed tokens:   1177026560 | elapsed time per iteration (ms): 4190.5 | learning rate: 7.845E-05 | global batch size:   128 | lm loss: 2.173988E+00 | loss scale: 262144.0 | grad norm: 1.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.545 | tokens per gpu per second (tgs): 1954.898 | TFLOPs: 15.73 |
g0069: [2024-08-02 23:52:16,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=4500, skipped=1, lr=[7.862572373333334e-05, 7.862572373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4500 loss: 2.1405 iter time (s): 3.914 samples/sec: 32.703
g0093:  iteration     4500/10000000 | consumed samples:       576000 | consumed tokens:   1179648000 | elapsed time per iteration (ms): 3946.5 | learning rate: 7.863E-05 | global batch size:   128 | lm loss: 2.153909E+00 | loss scale: 262144.0 | grad norm: 1.275 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.434 | tokens per gpu per second (tgs): 2075.754 | TFLOPs: 16.70 |
g0069: [2024-08-02 23:52:56,717] [INFO] [logging.py:96:log_dist] [Rank 0] step=4510, skipped=1, lr=[7.880048640000001e-05, 7.880048640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4510 loss: 2.1010 iter time (s): 3.998 samples/sec: 32.019
g0093:  iteration     4510/10000000 | consumed samples:       577280 | consumed tokens:   1182269440 | elapsed time per iteration (ms): 4030.6 | learning rate: 7.880E-05 | global batch size:   128 | lm loss: 2.135359E+00 | loss scale: 262144.0 | grad norm: 1.251 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.757 | tokens per gpu per second (tgs): 2032.436 | TFLOPs: 16.36 |
g0069: [2024-08-02 23:53:37,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=4520, skipped=1, lr=[7.897524906666667e-05, 7.897524906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4520 loss: 2.0634 iter time (s): 4.090 samples/sec: 31.296
g0093:  iteration     4520/10000000 | consumed samples:       578560 | consumed tokens:   1184890880 | elapsed time per iteration (ms): 4122.3 | learning rate: 7.898E-05 | global batch size:   128 | lm loss: 2.111191E+00 | loss scale: 262144.0 | grad norm: 1.380 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.050 | tokens per gpu per second (tgs): 1987.230 | TFLOPs: 15.99 |
g0069: [2024-08-02 23:54:19,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=4530, skipped=1, lr=[7.915001173333334e-05, 7.915001173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4530 loss: 2.0705 iter time (s): 4.111 samples/sec: 31.137
g0093:  iteration     4530/10000000 | consumed samples:       579840 | consumed tokens:   1187512320 | elapsed time per iteration (ms): 4144.0 | learning rate: 7.915E-05 | global batch size:   128 | lm loss: 2.065000E+00 | loss scale: 262144.0 | grad norm: 1.668 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.888 | tokens per gpu per second (tgs): 1976.845 | TFLOPs: 15.91 |
g0069: [2024-08-02 23:55:00,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=4540, skipped=1, lr=[7.93247744e-05, 7.93247744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4540 loss: 2.0968 iter time (s): 4.039 samples/sec: 31.691
g0093:  iteration     4540/10000000 | consumed samples:       581120 | consumed tokens:   1190133760 | elapsed time per iteration (ms): 4072.2 | learning rate: 7.932E-05 | global batch size:   128 | lm loss: 2.070571E+00 | loss scale: 262144.0 | grad norm: 1.575 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.433 | tokens per gpu per second (tgs): 2011.704 | TFLOPs: 16.19 |
g0069: [2024-08-02 23:55:42,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=4550, skipped=1, lr=[7.949953706666667e-05, 7.949953706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4550 loss: 2.0132 iter time (s): 4.161 samples/sec: 30.765
g0093:  iteration     4550/10000000 | consumed samples:       582400 | consumed tokens:   1192755200 | elapsed time per iteration (ms): 4193.3 | learning rate: 7.950E-05 | global batch size:   128 | lm loss: 2.045288E+00 | loss scale: 262144.0 | grad norm: 1.282 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.525 | tokens per gpu per second (tgs): 1953.609 | TFLOPs: 15.72 |
g0069: [2024-08-02 23:56:23,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=4560, skipped=1, lr=[7.967429973333334e-05, 7.967429973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4560 loss: 2.0246 iter time (s): 4.158 samples/sec: 30.786
g0093:  iteration     4560/10000000 | consumed samples:       583680 | consumed tokens:   1195376640 | elapsed time per iteration (ms): 4190.3 | learning rate: 7.967E-05 | global batch size:   128 | lm loss: 2.020498E+00 | loss scale: 262144.0 | grad norm: 1.136 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.547 | tokens per gpu per second (tgs): 1954.991 | TFLOPs: 15.73 |
g0069: [2024-08-02 23:57:06,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=4570, skipped=1, lr=[7.98490624e-05, 7.98490624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4570 loss: 1.9860 iter time (s): 4.202 samples/sec: 30.464
g0093:  iteration     4570/10000000 | consumed samples:       584960 | consumed tokens:   1197998080 | elapsed time per iteration (ms): 4235.2 | learning rate: 7.985E-05 | global batch size:   128 | lm loss: 1.999472E+00 | loss scale: 262144.0 | grad norm: 1.306 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.223 | tokens per gpu per second (tgs): 1934.251 | TFLOPs: 15.57 |
g0069: [2024-08-02 23:57:47,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=4580, skipped=1, lr=[8.002382506666667e-05, 8.002382506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4580 loss: 1.9907 iter time (s): 4.102 samples/sec: 31.201
g0093:  iteration     4580/10000000 | consumed samples:       586240 | consumed tokens:   1200619520 | elapsed time per iteration (ms): 4134.6 | learning rate: 8.002E-05 | global batch size:   128 | lm loss: 1.999313E+00 | loss scale: 262144.0 | grad norm: 1.277 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.958 | tokens per gpu per second (tgs): 1981.344 | TFLOPs: 15.94 |
g0069: [2024-08-02 23:58:29,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=4590, skipped=1, lr=[8.019858773333334e-05, 8.019858773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4590 loss: 1.9403 iter time (s): 4.134 samples/sec: 30.960
g0093:  iteration     4590/10000000 | consumed samples:       587520 | consumed tokens:   1203240960 | elapsed time per iteration (ms): 4167.9 | learning rate: 8.020E-05 | global batch size:   128 | lm loss: 1.964873E+00 | loss scale: 262144.0 | grad norm: 1.531 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.711 | tokens per gpu per second (tgs): 1965.511 | TFLOPs: 15.82 |
g0069: [2024-08-02 23:59:08,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=4600, skipped=1, lr=[8.03733504e-05, 8.03733504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4600 loss: 1.9387 iter time (s): 3.859 samples/sec: 33.168
g0093:  iteration     4600/10000000 | consumed samples:       588800 | consumed tokens:   1205862400 | elapsed time per iteration (ms): 3892.1 | learning rate: 8.037E-05 | global batch size:   128 | lm loss: 1.959384E+00 | loss scale: 262144.0 | grad norm: 1.236 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.887 | tokens per gpu per second (tgs): 2104.761 | TFLOPs: 16.94 |
g0069: [2024-08-02 23:59:47,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=4610, skipped=1, lr=[8.054811306666667e-05, 8.054811306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4610 loss: 1.9238 iter time (s): 3.929 samples/sec: 32.580
g0093:  iteration     4610/10000000 | consumed samples:       590080 | consumed tokens:   1208483840 | elapsed time per iteration (ms): 3960.9 | learning rate: 8.055E-05 | global batch size:   128 | lm loss: 1.932059E+00 | loss scale: 262144.0 | grad norm: 1.526 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.316 | tokens per gpu per second (tgs): 2068.210 | TFLOPs: 16.64 |
g0069: [2024-08-03 00:00:28,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=4620, skipped=1, lr=[8.072287573333333e-05, 8.072287573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4620 loss: 1.8548 iter time (s): 4.048 samples/sec: 31.618
g0093:  iteration     4620/10000000 | consumed samples:       591360 | consumed tokens:   1211105280 | elapsed time per iteration (ms): 4081.6 | learning rate: 8.072E-05 | global batch size:   128 | lm loss: 1.933112E+00 | loss scale: 262144.0 | grad norm: 1.254 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.360 | tokens per gpu per second (tgs): 2007.063 | TFLOPs: 16.15 |
g0069: [2024-08-03 00:01:10,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=4630, skipped=1, lr=[8.08976384e-05, 8.08976384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4630 loss: 1.9443 iter time (s): 4.160 samples/sec: 30.768
g0093:  iteration     4630/10000000 | consumed samples:       592640 | consumed tokens:   1213726720 | elapsed time per iteration (ms): 4193.9 | learning rate: 8.090E-05 | global batch size:   128 | lm loss: 1.902239E+00 | loss scale: 262144.0 | grad norm: 1.459 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.520 | tokens per gpu per second (tgs): 1953.311 | TFLOPs: 15.72 |
g0069: [2024-08-03 00:01:52,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=4640, skipped=1, lr=[8.107240106666668e-05, 8.107240106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4640 loss: 1.8719 iter time (s): 4.163 samples/sec: 30.748
g0093:  iteration     4640/10000000 | consumed samples:       593920 | consumed tokens:   1216348160 | elapsed time per iteration (ms): 4196.0 | learning rate: 8.107E-05 | global batch size:   128 | lm loss: 1.899550E+00 | loss scale: 262144.0 | grad norm: 1.428 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.505 | tokens per gpu per second (tgs): 1952.332 | TFLOPs: 15.71 |
g0069: [2024-08-03 00:02:34,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=4650, skipped=1, lr=[8.124716373333335e-05, 8.124716373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4650 loss: 1.8913 iter time (s): 4.115 samples/sec: 31.106
g0093:  iteration     4650/10000000 | consumed samples:       595200 | consumed tokens:   1218969600 | elapsed time per iteration (ms): 4148.0 | learning rate: 8.125E-05 | global batch size:   128 | lm loss: 1.881720E+00 | loss scale: 262144.0 | grad norm: 1.655 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.858 | tokens per gpu per second (tgs): 1974.925 | TFLOPs: 15.89 |
g0069: [2024-08-03 00:03:16,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=4660, skipped=1, lr=[8.142192640000001e-05, 8.142192640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4660 loss: 1.8452 iter time (s): 4.208 samples/sec: 30.421
g0093:  iteration     4660/10000000 | consumed samples:       596480 | consumed tokens:   1221591040 | elapsed time per iteration (ms): 4240.4 | learning rate: 8.142E-05 | global batch size:   128 | lm loss: 1.882517E+00 | loss scale: 262144.0 | grad norm: 1.386 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.186 | tokens per gpu per second (tgs): 1931.916 | TFLOPs: 15.55 |
g0069: [2024-08-03 00:03:57,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=4670, skipped=1, lr=[8.159668906666668e-05, 8.159668906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4670 loss: 1.8732 iter time (s): 4.120 samples/sec: 31.068
g0093:  iteration     4670/10000000 | consumed samples:       597760 | consumed tokens:   1224212480 | elapsed time per iteration (ms): 4152.8 | learning rate: 8.160E-05 | global batch size:   128 | lm loss: 1.858920E+00 | loss scale: 262144.0 | grad norm: 1.712 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.823 | tokens per gpu per second (tgs): 1972.645 | TFLOPs: 15.87 |
g0069: [2024-08-03 00:04:40,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=4680, skipped=1, lr=[8.177145173333334e-05, 8.177145173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4680 loss: 1.8540 iter time (s): 4.184 samples/sec: 30.592
g0093:  iteration     4680/10000000 | consumed samples:       599040 | consumed tokens:   1226833920 | elapsed time per iteration (ms): 4216.6 | learning rate: 8.177E-05 | global batch size:   128 | lm loss: 1.857383E+00 | loss scale: 262144.0 | grad norm: 1.536 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.356 | tokens per gpu per second (tgs): 1942.784 | TFLOPs: 15.63 |
g0069: [2024-08-03 00:05:21,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=4690, skipped=1, lr=[8.194621440000001e-05, 8.194621440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4690 loss: 1.8072 iter time (s): 4.089 samples/sec: 31.300
g0093:  iteration     4690/10000000 | consumed samples:       600320 | consumed tokens:   1229455360 | elapsed time per iteration (ms): 4122.1 | learning rate: 8.195E-05 | global batch size:   128 | lm loss: 1.845532E+00 | loss scale: 262144.0 | grad norm: 1.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.052 | tokens per gpu per second (tgs): 1987.353 | TFLOPs: 15.99 |
g0069: [2024-08-03 00:06:03,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=4700, skipped=1, lr=[8.212097706666668e-05, 8.212097706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4700 loss: 1.8038 iter time (s): 4.141 samples/sec: 30.909
g0093:  iteration     4700/10000000 | consumed samples:       601600 | consumed tokens:   1232076800 | elapsed time per iteration (ms): 4173.5 | learning rate: 8.212E-05 | global batch size:   128 | lm loss: 1.833427E+00 | loss scale: 262144.0 | grad norm: 1.351 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.670 | tokens per gpu per second (tgs): 1962.879 | TFLOPs: 15.80 |
g0069: [2024-08-03 00:06:45,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=4710, skipped=1, lr=[8.229573973333334e-05, 8.229573973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4710 loss: 1.8181 iter time (s): 4.161 samples/sec: 30.761
g0093:  iteration     4710/10000000 | consumed samples:       602880 | consumed tokens:   1234698240 | elapsed time per iteration (ms): 4193.5 | learning rate: 8.230E-05 | global batch size:   128 | lm loss: 1.813419E+00 | loss scale: 262144.0 | grad norm: 1.603 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.523 | tokens per gpu per second (tgs): 1953.477 | TFLOPs: 15.72 |
g0069: [2024-08-03 00:07:25,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=4720, skipped=1, lr=[8.247050240000001e-05, 8.247050240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4720 loss: 1.8131 iter time (s): 4.060 samples/sec: 31.531
g0093:  iteration     4720/10000000 | consumed samples:       604160 | consumed tokens:   1237319680 | elapsed time per iteration (ms): 4094.0 | learning rate: 8.247E-05 | global batch size:   128 | lm loss: 1.825212E+00 | loss scale: 262144.0 | grad norm: 1.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.265 | tokens per gpu per second (tgs): 2000.979 | TFLOPs: 16.10 |
g0069: [2024-08-03 00:08:06,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=4730, skipped=1, lr=[8.264526506666667e-05, 8.264526506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4730 loss: 1.8239 iter time (s): 4.043 samples/sec: 31.661
g0093:  iteration     4730/10000000 | consumed samples:       605440 | consumed tokens:   1239941120 | elapsed time per iteration (ms): 4076.2 | learning rate: 8.265E-05 | global batch size:   128 | lm loss: 1.811790E+00 | loss scale: 262144.0 | grad norm: 1.362 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.402 | tokens per gpu per second (tgs): 2009.730 | TFLOPs: 16.17 |
g0069: [2024-08-03 00:08:48,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=4740, skipped=1, lr=[8.282002773333334e-05, 8.282002773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4740 loss: 1.7855 iter time (s): 4.171 samples/sec: 30.691
g0093:  iteration     4740/10000000 | consumed samples:       606720 | consumed tokens:   1242562560 | elapsed time per iteration (ms): 4202.8 | learning rate: 8.282E-05 | global batch size:   128 | lm loss: 1.791214E+00 | loss scale: 262144.0 | grad norm: 1.634 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.456 | tokens per gpu per second (tgs): 1949.154 | TFLOPs: 15.69 |
g0069: [2024-08-03 00:09:32,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=4750, skipped=1, lr=[8.29947904e-05, 8.29947904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4750 loss: 1.7343 iter time (s): 4.383 samples/sec: 29.201
g0093:  iteration     4750/10000000 | consumed samples:       608000 | consumed tokens:   1245184000 | elapsed time per iteration (ms): 4417.2 | learning rate: 8.299E-05 | global batch size:   128 | lm loss: 1.779732E+00 | loss scale: 262144.0 | grad norm: 1.279 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.978 | tokens per gpu per second (tgs): 1854.570 | TFLOPs: 14.92 |
g0069: [2024-08-03 00:10:13,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=4760, skipped=1, lr=[8.316955306666667e-05, 8.316955306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4760 loss: 1.7516 iter time (s): 4.047 samples/sec: 31.631
g0093:  iteration     4760/10000000 | consumed samples:       609280 | consumed tokens:   1247805440 | elapsed time per iteration (ms): 4079.8 | learning rate: 8.317E-05 | global batch size:   128 | lm loss: 1.758332E+00 | loss scale: 262144.0 | grad norm: 1.240 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.374 | tokens per gpu per second (tgs): 2007.947 | TFLOPs: 16.16 |
g0069: [2024-08-03 00:10:55,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=4770, skipped=1, lr=[8.334431573333334e-05, 8.334431573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4770 loss: 1.7529 iter time (s): 4.165 samples/sec: 30.732
g0093:  iteration     4770/10000000 | consumed samples:       610560 | consumed tokens:   1250426880 | elapsed time per iteration (ms): 4198.6 | learning rate: 8.334E-05 | global batch size:   128 | lm loss: 1.768201E+00 | loss scale: 262144.0 | grad norm: 1.097 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.486 | tokens per gpu per second (tgs): 1951.127 | TFLOPs: 15.70 |
g0069: [2024-08-03 00:11:38,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=4780, skipped=1, lr=[8.35190784e-05, 8.35190784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4780 loss: 1.7486 iter time (s): 4.272 samples/sec: 29.965
g0093:  iteration     4780/10000000 | consumed samples:       611840 | consumed tokens:   1253048320 | elapsed time per iteration (ms): 4303.9 | learning rate: 8.352E-05 | global batch size:   128 | lm loss: 1.746224E+00 | loss scale: 262144.0 | grad norm: 1.350 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.741 | tokens per gpu per second (tgs): 1903.393 | TFLOPs: 15.32 |
g0069: [2024-08-03 00:12:21,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=4790, skipped=1, lr=[8.369384106666667e-05, 8.369384106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4790 loss: 1.7209 iter time (s): 4.224 samples/sec: 30.301
g0093:  iteration     4790/10000000 | consumed samples:       613120 | consumed tokens:   1255669760 | elapsed time per iteration (ms): 4256.6 | learning rate: 8.369E-05 | global batch size:   128 | lm loss: 1.738421E+00 | loss scale: 262144.0 | grad norm: 1.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.071 | tokens per gpu per second (tgs): 1924.537 | TFLOPs: 15.49 |
g0069: [2024-08-03 00:13:04,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=4800, skipped=1, lr=[8.386860373333334e-05, 8.386860373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4800 loss: 1.7312 iter time (s): 4.250 samples/sec: 30.119
g0093:  iteration     4800/10000000 | consumed samples:       614400 | consumed tokens:   1258291200 | elapsed time per iteration (ms): 4282.6 | learning rate: 8.387E-05 | global batch size:   128 | lm loss: 1.738489E+00 | loss scale: 262144.0 | grad norm: 1.609 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.888 | tokens per gpu per second (tgs): 1912.843 | TFLOPs: 15.39 |
g0069: [2024-08-03 00:13:46,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=4810, skipped=1, lr=[8.40433664e-05, 8.40433664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4810 loss: 1.6985 iter time (s): 4.205 samples/sec: 30.438
g0093:  iteration     4810/10000000 | consumed samples:       615680 | consumed tokens:   1260912640 | elapsed time per iteration (ms): 4238.2 | learning rate: 8.404E-05 | global batch size:   128 | lm loss: 1.735946E+00 | loss scale: 262144.0 | grad norm: 1.065 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.201 | tokens per gpu per second (tgs): 1932.883 | TFLOPs: 15.55 |
g0069: [2024-08-03 00:14:27,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=4820, skipped=1, lr=[8.421812906666668e-05, 8.421812906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4820 loss: 1.6879 iter time (s): 4.110 samples/sec: 31.143
g0093:  iteration     4820/10000000 | consumed samples:       616960 | consumed tokens:   1263534080 | elapsed time per iteration (ms): 4142.9 | learning rate: 8.422E-05 | global batch size:   128 | lm loss: 1.711455E+00 | loss scale: 262144.0 | grad norm: 1.276 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.896 | tokens per gpu per second (tgs): 1977.371 | TFLOPs: 15.91 |
g0069: [2024-08-03 00:15:12,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=4830, skipped=1, lr=[8.439289173333333e-05, 8.439289173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4830 loss: 1.7017 iter time (s): 4.397 samples/sec: 29.112
g0093:  iteration     4830/10000000 | consumed samples:       618240 | consumed tokens:   1266155520 | elapsed time per iteration (ms): 4429.2 | learning rate: 8.439E-05 | global batch size:   128 | lm loss: 1.694886E+00 | loss scale: 262144.0 | grad norm: 1.330 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.899 | tokens per gpu per second (tgs): 1849.562 | TFLOPs: 14.88 |
g0069: [2024-08-03 00:15:54,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=4840, skipped=1, lr=[8.45676544e-05, 8.45676544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4840 loss: 1.6934 iter time (s): 4.203 samples/sec: 30.458
g0093:  iteration     4840/10000000 | consumed samples:       619520 | consumed tokens:   1268776960 | elapsed time per iteration (ms): 4237.0 | learning rate: 8.457E-05 | global batch size:   128 | lm loss: 1.707845E+00 | loss scale: 262144.0 | grad norm: 1.079 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.210 | tokens per gpu per second (tgs): 1933.439 | TFLOPs: 15.56 |
g0069: [2024-08-03 00:16:36,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=4850, skipped=1, lr=[8.474241706666667e-05, 8.474241706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4850 loss: 1.6790 iter time (s): 4.169 samples/sec: 30.705
g0093:  iteration     4850/10000000 | consumed samples:       620800 | consumed tokens:   1271398400 | elapsed time per iteration (ms): 4203.0 | learning rate: 8.474E-05 | global batch size:   128 | lm loss: 1.688181E+00 | loss scale: 262144.0 | grad norm: 1.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.455 | tokens per gpu per second (tgs): 1949.090 | TFLOPs: 15.68 |
g0069: [2024-08-03 00:17:19,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=4860, skipped=1, lr=[8.491717973333333e-05, 8.491717973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4860 loss: 1.6629 iter time (s): 4.262 samples/sec: 30.031
g0093:  iteration     4860/10000000 | consumed samples:       622080 | consumed tokens:   1274019840 | elapsed time per iteration (ms): 4295.5 | learning rate: 8.492E-05 | global batch size:   128 | lm loss: 1.666948E+00 | loss scale: 262144.0 | grad norm: 1.066 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.799 | tokens per gpu per second (tgs): 1907.112 | TFLOPs: 15.35 |
g0069: [2024-08-03 00:18:01,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=4870, skipped=1, lr=[8.50919424e-05, 8.50919424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4870 loss: 1.6676 iter time (s): 4.149 samples/sec: 30.851
g0093:  iteration     4870/10000000 | consumed samples:       623360 | consumed tokens:   1276641280 | elapsed time per iteration (ms): 4181.4 | learning rate: 8.509E-05 | global batch size:   128 | lm loss: 1.673363E+00 | loss scale: 262144.0 | grad norm: 1.070 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.612 | tokens per gpu per second (tgs): 1959.139 | TFLOPs: 15.77 |
g0069: [2024-08-03 00:18:43,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=4880, skipped=1, lr=[8.526670506666666e-05, 8.526670506666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4880 loss: 1.6808 iter time (s): 4.142 samples/sec: 30.905
g0093:  iteration     4880/10000000 | consumed samples:       624640 | consumed tokens:   1279262720 | elapsed time per iteration (ms): 4174.5 | learning rate: 8.527E-05 | global batch size:   128 | lm loss: 1.670209E+00 | loss scale: 262144.0 | grad norm: 1.134 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.662 | tokens per gpu per second (tgs): 1962.380 | TFLOPs: 15.79 |
g0069: [2024-08-03 00:19:24,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=4890, skipped=1, lr=[8.544146773333333e-05, 8.544146773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4890 loss: 1.6528 iter time (s): 4.094 samples/sec: 31.264
g0093:  iteration     4890/10000000 | consumed samples:       625920 | consumed tokens:   1281884160 | elapsed time per iteration (ms): 4127.3 | learning rate: 8.544E-05 | global batch size:   128 | lm loss: 1.656911E+00 | loss scale: 262144.0 | grad norm: 1.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.013 | tokens per gpu per second (tgs): 1984.813 | TFLOPs: 15.97 |
g0069: [2024-08-03 00:20:04,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=4900, skipped=1, lr=[8.56162304e-05, 8.56162304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4900 loss: 1.5988 iter time (s): 3.996 samples/sec: 32.035
g0093:  iteration     4900/10000000 | consumed samples:       627200 | consumed tokens:   1284505600 | elapsed time per iteration (ms): 4028.1 | learning rate: 8.562E-05 | global batch size:   128 | lm loss: 1.641221E+00 | loss scale: 262144.0 | grad norm: 1.122 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.777 | tokens per gpu per second (tgs): 2033.700 | TFLOPs: 16.37 |
g0086: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0086: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0086: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0092: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0088: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0091: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0088: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0069: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0088: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0092: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0087: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0091: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0093: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0069: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0090: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0093: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0069: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0092: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0092: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0090: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0087: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0087: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0091: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0090: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0090: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0090: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0091: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0090: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0093: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0088: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0086: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0069: [2024-08-03 00:20:31,597] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0069: [2024-08-03 00:20:43,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=4910, skipped=1, lr=[8.579099306666666e-05, 8.579099306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4910 loss: 1.6779 iter time (s): 3.875 samples/sec: 33.034
g0093:  iteration     4910/10000000 | consumed samples:       628480 | consumed tokens:   1287127040 | elapsed time per iteration (ms): 3906.9 | learning rate: 8.579E-05 | global batch size:   128 | lm loss: 1.641832E+00 | loss scale: 524288.0 | grad norm: 1.938 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.762 | tokens per gpu per second (tgs): 2096.783 | TFLOPs: 16.87 |
g0069: [2024-08-03 00:21:24,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=4920, skipped=1, lr=[8.596575573333333e-05, 8.596575573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4920 loss: 1.6128 iter time (s): 4.033 samples/sec: 31.740
g0093:  iteration     4920/10000000 | consumed samples:       629760 | consumed tokens:   1289748480 | elapsed time per iteration (ms): 4065.1 | learning rate: 8.597E-05 | global batch size:   128 | lm loss: 1.649959E+00 | loss scale: 524288.0 | grad norm: 1.024 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.488 | tokens per gpu per second (tgs): 2015.214 | TFLOPs: 16.22 |
g0069: [2024-08-03 00:22:07,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=4930, skipped=1, lr=[8.61405184e-05, 8.61405184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4930 loss: 1.6346 iter time (s): 4.257 samples/sec: 30.065
g0093:  iteration     4930/10000000 | consumed samples:       631040 | consumed tokens:   1292369920 | elapsed time per iteration (ms): 4290.5 | learning rate: 8.614E-05 | global batch size:   128 | lm loss: 1.622827E+00 | loss scale: 524288.0 | grad norm: 1.146 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.833 | tokens per gpu per second (tgs): 1909.341 | TFLOPs: 15.36 |
g0069: [2024-08-03 00:22:48,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=4940, skipped=1, lr=[8.631528106666666e-05, 8.631528106666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4940 loss: 1.6254 iter time (s): 4.097 samples/sec: 31.239
g0093:  iteration     4940/10000000 | consumed samples:       632320 | consumed tokens:   1294991360 | elapsed time per iteration (ms): 4132.1 | learning rate: 8.632E-05 | global batch size:   128 | lm loss: 1.631315E+00 | loss scale: 524288.0 | grad norm: 0.992 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.977 | tokens per gpu per second (tgs): 1982.520 | TFLOPs: 15.95 |
g0093: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: Grad overflow on iteration 4945
g0093: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: Grad overflow on iteration 4945
g0093: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0090: Grad overflow on iteration 4945
g0093: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0091: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 4945
g0086: Grad overflow on iteration 4945
g0092: Grad overflow on iteration 4945
g0069: Grad overflow on iteration 4945
g0086: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: Grad overflow on iteration 4945
g0087: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: Grad overflow on iteration 4945
g0087: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: Grad overflow on iteration 4945
g0088: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 4945
g0086: Grad overflow on iteration 4945
g0088: Grad overflow on iteration 4945
g0092: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: Grad overflow on iteration 4945
g0086: Grad overflow on iteration 4945
g0087: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0086: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0088: Grad overflow on iteration 4945
g0088: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 4945
g0069: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 4945
g0088: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: Grad overflow on iteration 4945
g0086: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0091: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0088: Grad overflow on iteration 4945
g0088: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0069: Grad overflow on iteration 4945
g0069: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: Grad overflow on iteration 4945
g0069: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0069: Grad overflow on iteration 4945
g0090: Grad overflow on iteration 4945
g0086: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0069: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0069: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0069: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0090: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0090: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0069: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0093: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0093: Grad overflow on iteration 4945
g0093: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0091: Grad overflow on iteration 4945
g0091: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0090: Grad overflow on iteration 4945
g0088: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0087: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0091: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0088: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0093: Grad overflow on iteration 4945
g0090: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0093: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0091: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0090: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0091: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 4945
g0069: [2024-08-03 00:23:12,633] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0092: Grad overflow on iteration 4945
g0092: [2024-08-03 00:23:12,632] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: Grad overflow on iteration 4945
g0092: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0092: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0092: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0092: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: Grad overflow on iteration 4945
g0092: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0091: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0093: [2024-08-03 00:23:12,633] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0069: [2024-08-03 00:23:29,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=4950, skipped=2, lr=[8.649004373333334e-05, 8.649004373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4950 loss: 1.6075 iter time (s): 4.063 samples/sec: 31.501
g0093:  iteration     4950/10000000 | consumed samples:       633600 | consumed tokens:   1297612800 | elapsed time per iteration (ms): 4095.9 | learning rate: 8.649E-05 | global batch size:   128 | lm loss: 1.618957E+00 | loss scale: 262144.0 | grad norm: 1.255 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.250 | tokens per gpu per second (tgs): 2000.030 | TFLOPs: 16.09 |
g0069: [2024-08-03 00:24:11,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=4960, skipped=2, lr=[8.666480640000001e-05, 8.666480640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4960 loss: 1.6140 iter time (s): 4.188 samples/sec: 30.564
g0093:  iteration     4960/10000000 | consumed samples:       634880 | consumed tokens:   1300234240 | elapsed time per iteration (ms): 4220.2 | learning rate: 8.666E-05 | global batch size:   128 | lm loss: 1.609296E+00 | loss scale: 262144.0 | grad norm: 1.144 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.330 | tokens per gpu per second (tgs): 1941.148 | TFLOPs: 15.62 |
g0069: [2024-08-03 00:24:53,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=4970, skipped=2, lr=[8.683956906666667e-05, 8.683956906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4970 loss: 1.6065 iter time (s): 4.092 samples/sec: 31.284
g0093:  iteration     4970/10000000 | consumed samples:       636160 | consumed tokens:   1302855680 | elapsed time per iteration (ms): 4124.4 | learning rate: 8.684E-05 | global batch size:   128 | lm loss: 1.596041E+00 | loss scale: 262144.0 | grad norm: 1.078 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.035 | tokens per gpu per second (tgs): 1986.230 | TFLOPs: 15.98 |
g0069: [2024-08-03 00:25:36,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=4980, skipped=2, lr=[8.701433173333334e-05, 8.701433173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4980 loss: 1.5970 iter time (s): 4.341 samples/sec: 29.487
g0093:  iteration     4980/10000000 | consumed samples:       637440 | consumed tokens:   1305477120 | elapsed time per iteration (ms): 4373.4 | learning rate: 8.701E-05 | global batch size:   128 | lm loss: 1.603332E+00 | loss scale: 262144.0 | grad norm: 1.133 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.268 | tokens per gpu per second (tgs): 1873.123 | TFLOPs: 15.07 |
g0069: [2024-08-03 00:26:19,669] [INFO] [logging.py:96:log_dist] [Rank 0] step=4990, skipped=2, lr=[8.71890944e-05, 8.71890944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 4990 loss: 1.5985 iter time (s): 4.252 samples/sec: 30.106
g0093:  iteration     4990/10000000 | consumed samples:       638720 | consumed tokens:   1308098560 | elapsed time per iteration (ms): 4284.5 | learning rate: 8.719E-05 | global batch size:   128 | lm loss: 1.602458E+00 | loss scale: 262144.0 | grad norm: 1.326 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.875 | tokens per gpu per second (tgs): 1912.016 | TFLOPs: 15.39 |
g0069: [2024-08-03 00:27:00,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=2, lr=[8.736385706666667e-05, 8.736385706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5000 loss: 1.6175 iter time (s): 4.090 samples/sec: 31.295
g0093:  iteration     5000/10000000 | consumed samples:       640000 | consumed tokens:   1310720000 | elapsed time per iteration (ms): 4124.1 | learning rate: 8.736E-05 | global batch size:   128 | lm loss: 1.581276E+00 | loss scale: 262144.0 | grad norm: 1.368 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.037 | tokens per gpu per second (tgs): 1986.382 | TFLOPs: 15.98 |
g0093: ------------------------------------------------------------------------------------------------
g0093:  validation loss at iteration 5000 | lm loss value: 1.583299E+00 | lm loss PPL: 4.870999E+00 | 
g0093: ------------------------------------------------------------------------------------------------
g0069: saving checkpoint at iteration    5000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: [2024-08-03 00:33:20,936] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5000 is about to be saved!
g0093: [2024-08-03 00:33:20,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0093: [2024-08-03 00:33:20,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0069: [2024-08-03 00:33:20,942] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0069: [2024-08-03 00:33:20,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0093: [2024-08-03 00:33:20,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0087: [2024-08-03 00:33:20,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0087: [2024-08-03 00:33:20,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0087: [2024-08-03 00:33:20,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0090: [2024-08-03 00:33:20,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0091: [2024-08-03 00:33:20,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0091: [2024-08-03 00:33:20,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0086: [2024-08-03 00:33:20,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0086: [2024-08-03 00:33:20,944] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0086: [2024-08-03 00:33:20,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0069: [2024-08-03 00:33:20,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0092: [2024-08-03 00:33:20,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0090: [2024-08-03 00:33:20,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0091: [2024-08-03 00:33:20,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0092: [2024-08-03 00:33:20,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0090: [2024-08-03 00:33:20,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0092: [2024-08-03 00:33:20,945] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0088: [2024-08-03 00:33:20,946] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0088: [2024-08-03 00:33:20,946] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0088: [2024-08-03 00:33:20,946] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0093: [2024-08-03 00:33:20,968] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_23-model_00-model_states.pt...
g0090: [2024-08-03 00:33:20,977] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_14-model_00-model_states.pt...
g0087: [2024-08-03 00:33:20,978] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_08-model_00-model_states.pt...
g0088: [2024-08-03 00:33:20,979] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_11-model_00-model_states.pt...
g0086: [2024-08-03 00:33:20,982] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_05-model_00-model_states.pt...
g0092: [2024-08-03 00:33:20,983] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_20-model_00-model_states.pt...
g0091: [2024-08-03 00:33:20,983] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_17-model_00-model_states.pt...
g0069: [2024-08-03 00:33:20,989] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_01-model_00-model_states.pt...
g0090: [2024-08-03 00:33:21,139] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_14-model_00-model_states.pt.
g0088: [2024-08-03 00:33:21,139] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_11-model_00-model_states.pt.
g0093: [2024-08-03 00:33:21,140] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_23-model_00-model_states.pt.
g0093: [2024-08-03 00:33:21,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_24-model_00-model_states.pt...
g0093: [2024-08-03 00:33:21,143] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_24-model_00-model_states.pt.
g0090: [2024-08-03 00:33:21,174] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_15-model_00-model_states.pt...
g0088: [2024-08-03 00:33:21,174] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_12-model_00-model_states.pt...
g0093: [2024-08-03 00:33:21,187] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_25-model_00-model_states.pt...
g0087: [2024-08-03 00:33:21,209] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_08-model_00-model_states.pt.
g0091: [2024-08-03 00:33:21,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_17-model_00-model_states.pt.
g0087: [2024-08-03 00:33:21,245] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_09-model_00-model_states.pt...
g0069: [2024-08-03 00:33:21,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_01-model_00-model_states.pt.
g0092: [2024-08-03 00:33:21,264] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_20-model_00-model_states.pt.
g0091: [2024-08-03 00:33:21,272] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_18-model_00-model_states.pt...
g0069: [2024-08-03 00:33:21,280] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_02-model_00-model_states.pt...
g0092: [2024-08-03 00:33:21,304] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_21-model_00-model_states.pt...
g0090: [2024-08-03 00:33:21,328] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_15-model_00-model_states.pt.
g0086: [2024-08-03 00:33:21,343] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_05-model_00-model_states.pt.
g0088: [2024-08-03 00:33:21,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_12-model_00-model_states.pt.
g0090: [2024-08-03 00:33:21,355] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_16-model_00-model_states.pt...
g0086: [2024-08-03 00:33:21,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_06-model_00-model_states.pt...
g0088: [2024-08-03 00:33:21,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_13-model_00-model_states.pt...
g0091: [2024-08-03 00:33:21,388] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_18-model_00-model_states.pt.
g0069: [2024-08-03 00:33:21,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_02-model_00-model_states.pt.
g0091: [2024-08-03 00:33:21,423] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_19-model_00-model_states.pt...
g0092: [2024-08-03 00:33:21,424] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_21-model_00-model_states.pt.
g0069: [2024-08-03 00:33:21,442] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_03-model_00-model_states.pt...
g0090: [2024-08-03 00:33:21,448] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_16-model_00-model_states.pt.
g0090: [2024-08-03 00:33:21,450] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_04_model_states.pt...
g0092: [2024-08-03 00:33:21,459] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_22-model_00-model_states.pt...
g0086: [2024-08-03 00:33:21,487] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_06-model_00-model_states.pt.
g0086: [2024-08-03 00:33:21,522] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_07-model_00-model_states.pt...
g0088: [2024-08-03 00:33:21,561] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_13-model_00-model_states.pt.
g0088: [2024-08-03 00:33:21,563] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_03_model_states.pt...
g0091: [2024-08-03 00:33:21,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_19-model_00-model_states.pt.
g0091: [2024-08-03 00:33:21,566] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_05_model_states.pt...
g0087: [2024-08-03 00:33:21,566] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_09-model_00-model_states.pt.
g0092: [2024-08-03 00:33:21,579] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_22-model_00-model_states.pt.
g0092: [2024-08-03 00:33:21,582] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_06_model_states.pt...
g0087: [2024-08-03 00:33:21,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_10-model_00-model_states.pt...
g0086: [2024-08-03 00:33:21,627] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_07-model_00-model_states.pt.
g0086: [2024-08-03 00:33:21,628] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_01_model_states.pt...
g0087: [2024-08-03 00:33:21,698] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_10-model_00-model_states.pt.
g0087: [2024-08-03 00:33:21,700] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_02_model_states.pt...
g0093: [2024-08-03 00:33:22,349] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_25-model_00-model_states.pt.
g0093: [2024-08-03 00:33:22,351] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_07_model_states.pt...
g0090: [2024-08-03 00:33:23,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_04_model_states.pt.
g0090: [2024-08-03 00:33:23,798] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0088: [2024-08-03 00:33:23,877] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_03_model_states.pt.
g0088: [2024-08-03 00:33:23,878] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0086: [2024-08-03 00:33:23,923] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_01_model_states.pt.
g0086: [2024-08-03 00:33:23,924] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0092: [2024-08-03 00:33:23,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_06_model_states.pt.
g0092: [2024-08-03 00:33:23,943] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0087: [2024-08-03 00:33:24,033] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_02_model_states.pt.
g0087: [2024-08-03 00:33:24,034] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0069: [2024-08-03 00:33:24,060] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_03-model_00-model_states.pt.
g0069: [2024-08-03 00:33:24,085] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_04-model_00-model_states.pt...
g0093: [2024-08-03 00:33:24,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_07_model_states.pt.
g0093: [2024-08-03 00:33:24,134] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0091: [2024-08-03 00:33:24,142] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_05_model_states.pt.
g0091: [2024-08-03 00:33:24,143] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0069: [2024-08-03 00:33:24,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_04-model_00-model_states.pt.
g0069: [2024-08-03 00:33:24,190] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt
g0069: [2024-08-03 00:33:24,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt...
g0069: [2024-08-03 00:33:27,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt.
g0069: [2024-08-03 00:33:27,563] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0069:   successfully saved checkpoint at iteration    5000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: Checkpoint Save GB: 22.521, GB/Sec: 3.39, Latency(second): 6.643
g0093: (min, max) time across ranks (ms):
g0093:     save-checkpoint ................................: (6642.99, 6643.15)
g0069: [2024-08-03 00:34:08,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=5010, skipped=2, lr=[8.753861973333334e-05, 8.753861973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5010 loss: 1.5653 iter time (s): 4.093 samples/sec: 31.276
g0093:  iteration     5010/10000000 | consumed samples:       641280 | consumed tokens:   1313341440 | elapsed time per iteration (ms): 42787.7 | learning rate: 8.754E-05 | global batch size:   128 | lm loss: 1.573999E+00 | loss scale: 262144.0 | grad norm: 1.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.992 | tokens per gpu per second (tgs): 191.457 | TFLOPs: 1.54 |
g0069: [2024-08-03 00:34:52,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=5020, skipped=2, lr=[8.77133824e-05, 8.77133824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5020 loss: 1.5826 iter time (s): 4.308 samples/sec: 29.710
g0093:  iteration     5020/10000000 | consumed samples:       642560 | consumed tokens:   1315962880 | elapsed time per iteration (ms): 4341.2 | learning rate: 8.771E-05 | global batch size:   128 | lm loss: 1.579554E+00 | loss scale: 262144.0 | grad norm: 1.165 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.485 | tokens per gpu per second (tgs): 1887.038 | TFLOPs: 15.19 |
g0069: [2024-08-03 00:35:33,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=5030, skipped=2, lr=[8.788814506666667e-05, 8.788814506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5030 loss: 1.5978 iter time (s): 4.118 samples/sec: 31.082
g0093:  iteration     5030/10000000 | consumed samples:       643840 | consumed tokens:   1318584320 | elapsed time per iteration (ms): 4151.4 | learning rate: 8.789E-05 | global batch size:   128 | lm loss: 1.574604E+00 | loss scale: 262144.0 | grad norm: 1.338 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.833 | tokens per gpu per second (tgs): 1973.304 | TFLOPs: 15.88 |
g0088: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 5031
g0088: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 5031
g0088: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0088: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0088: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 5031
g0088: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0091: Grad overflow on iteration 5031
g0090: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: Grad overflow on iteration 5031
g0086: Grad overflow on iteration 5031
g0091: Grad overflow on iteration 5031
g0087: Grad overflow on iteration 5031
g0069: Grad overflow on iteration 5031
g0086: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: Grad overflow on iteration 5031
g0086: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: Grad overflow on iteration 5031
g0087: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0092: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: Grad overflow on iteration 5031
g0069: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0091: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 5031
g0069: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: Grad overflow on iteration 5031
g0093: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0093: Grad overflow on iteration 5031
g0088: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 5031
g0069: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 5031
g0086: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0087: Grad overflow on iteration 5031
g0087: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0087: Grad overflow on iteration 5031
g0069: Grad overflow on iteration 5031
g0069: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0069: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0086: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: Grad overflow on iteration 5031
g0086: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0090: Grad overflow on iteration 5031
g0086: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0090: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 5031
g0090: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0090: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0091: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0090: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0092: Grad overflow on iteration 5031
g0087: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0091: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0092: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: Grad overflow on iteration 5031
g0093: Grad overflow on iteration 5031
g0092: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0092: Grad overflow on iteration 5031
g0092: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0090: Grad overflow on iteration 5031
g0090: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0091: Grad overflow on iteration 5031
g0090: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0087: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0091: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0092: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: Grad overflow on iteration 5031
g0092: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0087: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0091: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0087: Grad overflow on iteration 5031
g0087: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0092: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0087: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0069: [2024-08-03 00:35:41,935] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0093: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: Grad overflow on iteration 5031
g0088: [2024-08-03 00:35:41,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0093: [2024-08-03 00:35:41,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0069: [2024-08-03 00:36:15,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=5040, skipped=3, lr=[8.806290773333334e-05, 8.806290773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5040 loss: 1.5512 iter time (s): 4.126 samples/sec: 31.021
g0093:  iteration     5040/10000000 | consumed samples:       645120 | consumed tokens:   1321205760 | elapsed time per iteration (ms): 4158.9 | learning rate: 8.806E-05 | global batch size:   128 | lm loss: 1.565263E+00 | loss scale: 131072.0 | grad norm: 1.097 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.778 | tokens per gpu per second (tgs): 1969.762 | TFLOPs: 15.85 |
g0069: [2024-08-03 00:36:55,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=5050, skipped=3, lr=[8.82376704e-05, 8.82376704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5050 loss: 1.5928 iter time (s): 4.036 samples/sec: 31.715
g0093:  iteration     5050/10000000 | consumed samples:       646400 | consumed tokens:   1323827200 | elapsed time per iteration (ms): 4068.6 | learning rate: 8.824E-05 | global batch size:   128 | lm loss: 1.562869E+00 | loss scale: 131072.0 | grad norm: 1.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.460 | tokens per gpu per second (tgs): 2013.469 | TFLOPs: 16.20 |
g0069: [2024-08-03 00:37:37,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=5060, skipped=3, lr=[8.841243306666667e-05, 8.841243306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5060 loss: 1.5612 iter time (s): 4.073 samples/sec: 31.423
g0093:  iteration     5060/10000000 | consumed samples:       647680 | consumed tokens:   1326448640 | elapsed time per iteration (ms): 4106.2 | learning rate: 8.841E-05 | global batch size:   128 | lm loss: 1.548335E+00 | loss scale: 131072.0 | grad norm: 1.406 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.172 | tokens per gpu per second (tgs): 1995.015 | TFLOPs: 16.05 |
g0069: [2024-08-03 00:38:17,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=5070, skipped=3, lr=[8.858719573333333e-05, 8.858719573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5070 loss: 1.5563 iter time (s): 4.052 samples/sec: 31.591
g0093:  iteration     5070/10000000 | consumed samples:       648960 | consumed tokens:   1329070080 | elapsed time per iteration (ms): 4085.2 | learning rate: 8.859E-05 | global batch size:   128 | lm loss: 1.547994E+00 | loss scale: 131072.0 | grad norm: 1.007 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.333 | tokens per gpu per second (tgs): 2005.287 | TFLOPs: 16.14 |
g0069: [2024-08-03 00:38:59,598] [INFO] [logging.py:96:log_dist] [Rank 0] step=5080, skipped=3, lr=[8.87619584e-05, 8.87619584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5080 loss: 1.5407 iter time (s): 4.136 samples/sec: 30.947
g0093:  iteration     5080/10000000 | consumed samples:       650240 | consumed tokens:   1331691520 | elapsed time per iteration (ms): 4169.5 | learning rate: 8.876E-05 | global batch size:   128 | lm loss: 1.546393E+00 | loss scale: 131072.0 | grad norm: 1.009 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.699 | tokens per gpu per second (tgs): 1964.722 | TFLOPs: 15.81 |
g0069: [2024-08-03 00:39:41,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=5090, skipped=3, lr=[8.893672106666667e-05, 8.893672106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5090 loss: 1.5434 iter time (s): 4.164 samples/sec: 30.742
g0093:  iteration     5090/10000000 | consumed samples:       651520 | consumed tokens:   1334312960 | elapsed time per iteration (ms): 4197.2 | learning rate: 8.894E-05 | global batch size:   128 | lm loss: 1.534652E+00 | loss scale: 131072.0 | grad norm: 1.098 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.497 | tokens per gpu per second (tgs): 1951.785 | TFLOPs: 15.71 |
g0069: [2024-08-03 00:40:23,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=5100, skipped=3, lr=[8.911148373333333e-05, 8.911148373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5100 loss: 1.5251 iter time (s): 4.132 samples/sec: 30.981
g0093:  iteration     5100/10000000 | consumed samples:       652800 | consumed tokens:   1336934400 | elapsed time per iteration (ms): 4164.5 | learning rate: 8.911E-05 | global batch size:   128 | lm loss: 1.533160E+00 | loss scale: 131072.0 | grad norm: 1.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.736 | tokens per gpu per second (tgs): 1967.102 | TFLOPs: 15.83 |
g0069: [2024-08-03 00:41:02,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=5110, skipped=3, lr=[8.92862464e-05, 8.92862464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5110 loss: 1.5575 iter time (s): 3.892 samples/sec: 32.888
g0093:  iteration     5110/10000000 | consumed samples:       654080 | consumed tokens:   1339555840 | elapsed time per iteration (ms): 3924.6 | learning rate: 8.929E-05 | global batch size:   128 | lm loss: 1.534175E+00 | loss scale: 131072.0 | grad norm: 1.018 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.615 | tokens per gpu per second (tgs): 2087.358 | TFLOPs: 16.80 |
g0069: [2024-08-03 00:41:45,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=5120, skipped=3, lr=[8.946100906666666e-05, 8.946100906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5120 loss: 1.5445 iter time (s): 4.257 samples/sec: 30.067
g0093:  iteration     5120/10000000 | consumed samples:       655360 | consumed tokens:   1342177280 | elapsed time per iteration (ms): 4310.8 | learning rate: 8.946E-05 | global batch size:   128 | lm loss: 1.534108E+00 | loss scale: 131072.0 | grad norm: 0.996 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.693 | tokens per gpu per second (tgs): 1900.323 | TFLOPs: 15.29 |
g0069: [2024-08-03 00:42:26,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=5130, skipped=3, lr=[8.963577173333334e-05, 8.963577173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5130 loss: 1.5400 iter time (s): 4.013 samples/sec: 31.895
g0093:  iteration     5130/10000000 | consumed samples:       656640 | consumed tokens:   1344798720 | elapsed time per iteration (ms): 4047.1 | learning rate: 8.964E-05 | global batch size:   128 | lm loss: 1.508982E+00 | loss scale: 131072.0 | grad norm: 1.115 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.628 | tokens per gpu per second (tgs): 2024.163 | TFLOPs: 16.29 |
g0069: [2024-08-03 00:43:05,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=5140, skipped=3, lr=[8.981053440000001e-05, 8.981053440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5140 loss: 1.5143 iter time (s): 3.938 samples/sec: 32.502
g0093:  iteration     5140/10000000 | consumed samples:       657920 | consumed tokens:   1347420160 | elapsed time per iteration (ms): 3972.1 | learning rate: 8.981E-05 | global batch size:   128 | lm loss: 1.515688E+00 | loss scale: 131072.0 | grad norm: 1.012 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.225 | tokens per gpu per second (tgs): 2062.377 | TFLOPs: 16.60 |
g0069: [2024-08-03 00:43:45,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=5150, skipped=3, lr=[8.998529706666668e-05, 8.998529706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5150 loss: 1.5845 iter time (s): 3.967 samples/sec: 32.266
g0093:  iteration     5150/10000000 | consumed samples:       659200 | consumed tokens:   1350041600 | elapsed time per iteration (ms): 3999.6 | learning rate: 8.999E-05 | global batch size:   128 | lm loss: 1.523615E+00 | loss scale: 131072.0 | grad norm: 1.003 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.003 | tokens per gpu per second (tgs): 2048.203 | TFLOPs: 16.48 |
g0069: [2024-08-03 00:44:24,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=5160, skipped=3, lr=[9.016005973333334e-05, 9.016005973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5160 loss: 1.5205 iter time (s): 3.824 samples/sec: 33.470
g0093:  iteration     5160/10000000 | consumed samples:       660480 | consumed tokens:   1352663040 | elapsed time per iteration (ms): 3857.4 | learning rate: 9.016E-05 | global batch size:   128 | lm loss: 1.518677E+00 | loss scale: 131072.0 | grad norm: 1.071 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.183 | tokens per gpu per second (tgs): 2123.685 | TFLOPs: 17.09 |
g0069: [2024-08-03 00:45:04,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=5170, skipped=3, lr=[9.033482240000001e-05, 9.033482240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5170 loss: 1.4917 iter time (s): 4.001 samples/sec: 31.989
g0093:  iteration     5170/10000000 | consumed samples:       661760 | consumed tokens:   1355284480 | elapsed time per iteration (ms): 4034.0 | learning rate: 9.033E-05 | global batch size:   128 | lm loss: 1.487015E+00 | loss scale: 131072.0 | grad norm: 0.913 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.731 | tokens per gpu per second (tgs): 2030.761 | TFLOPs: 16.34 |
g0069: [2024-08-03 00:45:44,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=5180, skipped=3, lr=[9.050958506666667e-05, 9.050958506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5180 loss: 1.5172 iter time (s): 3.997 samples/sec: 32.021
g0093:  iteration     5180/10000000 | consumed samples:       663040 | consumed tokens:   1357905920 | elapsed time per iteration (ms): 4029.9 | learning rate: 9.051E-05 | global batch size:   128 | lm loss: 1.512677E+00 | loss scale: 131072.0 | grad norm: 0.954 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.762 | tokens per gpu per second (tgs): 2032.792 | TFLOPs: 16.36 |
g0069: [2024-08-03 00:46:26,642] [INFO] [logging.py:96:log_dist] [Rank 0] step=5190, skipped=3, lr=[9.068434773333334e-05, 9.068434773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5190 loss: 1.4727 iter time (s): 4.134 samples/sec: 30.962
g0093:  iteration     5190/10000000 | consumed samples:       664320 | consumed tokens:   1360527360 | elapsed time per iteration (ms): 4166.8 | learning rate: 9.068E-05 | global batch size:   128 | lm loss: 1.484555E+00 | loss scale: 131072.0 | grad norm: 0.962 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.719 | tokens per gpu per second (tgs): 1966.034 | TFLOPs: 15.82 |
g0069: [2024-08-03 00:47:07,076] [INFO] [logging.py:96:log_dist] [Rank 0] step=5200, skipped=3, lr=[9.08591104e-05, 9.08591104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5200 loss: 1.4756 iter time (s): 4.010 samples/sec: 31.920
g0093:  iteration     5200/10000000 | consumed samples:       665600 | consumed tokens:   1363148800 | elapsed time per iteration (ms): 4043.4 | learning rate: 9.086E-05 | global batch size:   128 | lm loss: 1.485279E+00 | loss scale: 131072.0 | grad norm: 1.293 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.656 | tokens per gpu per second (tgs): 2025.997 | TFLOPs: 16.30 |
g0069: [2024-08-03 00:47:50,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=5210, skipped=3, lr=[9.103387306666667e-05, 9.103387306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5210 loss: 1.4971 iter time (s): 4.290 samples/sec: 29.839
g0093:  iteration     5210/10000000 | consumed samples:       666880 | consumed tokens:   1365770240 | elapsed time per iteration (ms): 4328.4 | learning rate: 9.103E-05 | global batch size:   128 | lm loss: 1.495109E+00 | loss scale: 131072.0 | grad norm: 0.878 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.572 | tokens per gpu per second (tgs): 1892.621 | TFLOPs: 15.23 |
g0069: [2024-08-03 00:48:31,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=5220, skipped=3, lr=[9.120863573333334e-05, 9.120863573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5220 loss: 1.4890 iter time (s): 4.051 samples/sec: 31.595
g0093:  iteration     5220/10000000 | consumed samples:       668160 | consumed tokens:   1368391680 | elapsed time per iteration (ms): 4083.6 | learning rate: 9.121E-05 | global batch size:   128 | lm loss: 1.504204E+00 | loss scale: 131072.0 | grad norm: 0.997 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.345 | tokens per gpu per second (tgs): 2006.079 | TFLOPs: 16.14 |
g0069: [2024-08-03 00:49:12,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=5230, skipped=3, lr=[9.13833984e-05, 9.13833984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5230 loss: 1.4957 iter time (s): 4.110 samples/sec: 31.142
g0093:  iteration     5230/10000000 | consumed samples:       669440 | consumed tokens:   1371013120 | elapsed time per iteration (ms): 4142.7 | learning rate: 9.138E-05 | global batch size:   128 | lm loss: 1.496384E+00 | loss scale: 131072.0 | grad norm: 1.034 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.898 | tokens per gpu per second (tgs): 1977.457 | TFLOPs: 15.91 |
g0069: [2024-08-03 00:49:55,506] [INFO] [logging.py:96:log_dist] [Rank 0] step=5240, skipped=3, lr=[9.155816106666667e-05, 9.155816106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5240 loss: 1.4663 iter time (s): 4.256 samples/sec: 30.078
g0093:  iteration     5240/10000000 | consumed samples:       670720 | consumed tokens:   1373634560 | elapsed time per iteration (ms): 4288.2 | learning rate: 9.156E-05 | global batch size:   128 | lm loss: 1.481712E+00 | loss scale: 131072.0 | grad norm: 0.814 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.850 | tokens per gpu per second (tgs): 1910.373 | TFLOPs: 15.37 |
g0069: [2024-08-03 00:50:38,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=5250, skipped=3, lr=[9.173292373333334e-05, 9.173292373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5250 loss: 1.5292 iter time (s): 4.259 samples/sec: 30.051
g0093:  iteration     5250/10000000 | consumed samples:       672000 | consumed tokens:   1376256000 | elapsed time per iteration (ms): 4292.1 | learning rate: 9.173E-05 | global batch size:   128 | lm loss: 1.486892E+00 | loss scale: 131072.0 | grad norm: 1.295 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.822 | tokens per gpu per second (tgs): 1908.629 | TFLOPs: 15.36 |
g0069: [2024-08-03 00:51:20,208] [INFO] [logging.py:96:log_dist] [Rank 0] step=5260, skipped=3, lr=[9.19076864e-05, 9.19076864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5260 loss: 1.4908 iter time (s): 4.145 samples/sec: 30.878
g0093:  iteration     5260/10000000 | consumed samples:       673280 | consumed tokens:   1378877440 | elapsed time per iteration (ms): 4178.1 | learning rate: 9.191E-05 | global batch size:   128 | lm loss: 1.456339E+00 | loss scale: 131072.0 | grad norm: 0.991 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.636 | tokens per gpu per second (tgs): 1960.680 | TFLOPs: 15.78 |
g0069: [2024-08-03 00:52:01,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=5270, skipped=3, lr=[9.208244906666667e-05, 9.208244906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5270 loss: 1.4614 iter time (s): 4.106 samples/sec: 31.177
g0093:  iteration     5270/10000000 | consumed samples:       674560 | consumed tokens:   1381498880 | elapsed time per iteration (ms): 4138.6 | learning rate: 9.208E-05 | global batch size:   128 | lm loss: 1.471602E+00 | loss scale: 131072.0 | grad norm: 0.950 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.928 | tokens per gpu per second (tgs): 1979.405 | TFLOPs: 15.93 |
g0069: [2024-08-03 00:52:44,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=5280, skipped=3, lr=[9.225721173333333e-05, 9.225721173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5280 loss: 1.4824 iter time (s): 4.259 samples/sec: 30.054
g0093:  iteration     5280/10000000 | consumed samples:       675840 | consumed tokens:   1384120320 | elapsed time per iteration (ms): 4291.6 | learning rate: 9.226E-05 | global batch size:   128 | lm loss: 1.470551E+00 | loss scale: 131072.0 | grad norm: 1.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.826 | tokens per gpu per second (tgs): 1908.847 | TFLOPs: 15.36 |
g0069: [2024-08-03 00:53:29,095] [INFO] [logging.py:96:log_dist] [Rank 0] step=5290, skipped=3, lr=[9.24319744e-05, 9.24319744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5290 loss: 1.4442 iter time (s): 4.426 samples/sec: 28.920
g0093:  iteration     5290/10000000 | consumed samples:       677120 | consumed tokens:   1386741760 | elapsed time per iteration (ms): 4458.4 | learning rate: 9.243E-05 | global batch size:   128 | lm loss: 1.449334E+00 | loss scale: 131072.0 | grad norm: 1.140 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.710 | tokens per gpu per second (tgs): 1837.417 | TFLOPs: 14.79 |
g0069: [2024-08-03 00:54:07,429] [INFO] [logging.py:96:log_dist] [Rank 0] step=5300, skipped=3, lr=[9.260673706666667e-05, 9.260673706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5300 loss: 1.4436 iter time (s): 3.800 samples/sec: 33.685
g0093:  iteration     5300/10000000 | consumed samples:       678400 | consumed tokens:   1389363200 | elapsed time per iteration (ms): 3833.6 | learning rate: 9.261E-05 | global batch size:   128 | lm loss: 1.462958E+00 | loss scale: 131072.0 | grad norm: 1.038 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.389 | tokens per gpu per second (tgs): 2136.870 | TFLOPs: 17.20 |
g0069: [2024-08-03 00:54:48,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=5310, skipped=3, lr=[9.278149973333335e-05, 9.278149973333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5310 loss: 1.4210 iter time (s): 4.094 samples/sec: 31.263
g0093:  iteration     5310/10000000 | consumed samples:       679680 | consumed tokens:   1391984640 | elapsed time per iteration (ms): 4127.8 | learning rate: 9.278E-05 | global batch size:   128 | lm loss: 1.450970E+00 | loss scale: 131072.0 | grad norm: 1.113 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.009 | tokens per gpu per second (tgs): 1984.574 | TFLOPs: 15.97 |
g0069: [2024-08-03 00:55:28,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=5320, skipped=3, lr=[9.295626240000001e-05, 9.295626240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5320 loss: 1.4431 iter time (s): 3.988 samples/sec: 32.097
g0093:  iteration     5320/10000000 | consumed samples:       680960 | consumed tokens:   1394606080 | elapsed time per iteration (ms): 4020.5 | learning rate: 9.296E-05 | global batch size:   128 | lm loss: 1.444962E+00 | loss scale: 131072.0 | grad norm: 0.999 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.837 | tokens per gpu per second (tgs): 2037.558 | TFLOPs: 16.40 |
g0069: [2024-08-03 00:56:09,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=5330, skipped=3, lr=[9.313102506666668e-05, 9.313102506666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5330 loss: 1.4260 iter time (s): 4.040 samples/sec: 31.682
g0093:  iteration     5330/10000000 | consumed samples:       682240 | consumed tokens:   1397227520 | elapsed time per iteration (ms): 4073.3 | learning rate: 9.313E-05 | global batch size:   128 | lm loss: 1.456945E+00 | loss scale: 131072.0 | grad norm: 0.977 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.424 | tokens per gpu per second (tgs): 2011.149 | TFLOPs: 16.18 |
g0069: [2024-08-03 00:56:50,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=5340, skipped=3, lr=[9.330578773333334e-05, 9.330578773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5340 loss: 1.4045 iter time (s): 4.022 samples/sec: 31.827
g0093:  iteration     5340/10000000 | consumed samples:       683520 | consumed tokens:   1399848960 | elapsed time per iteration (ms): 4054.4 | learning rate: 9.331E-05 | global batch size:   128 | lm loss: 1.443786E+00 | loss scale: 131072.0 | grad norm: 0.948 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.571 | tokens per gpu per second (tgs): 2020.515 | TFLOPs: 16.26 |
g0069: [2024-08-03 00:57:31,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=5350, skipped=3, lr=[9.348055040000001e-05, 9.348055040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5350 loss: 1.4594 iter time (s): 4.088 samples/sec: 31.312
g0093:  iteration     5350/10000000 | consumed samples:       684800 | consumed tokens:   1402470400 | elapsed time per iteration (ms): 4120.5 | learning rate: 9.348E-05 | global batch size:   128 | lm loss: 1.450554E+00 | loss scale: 131072.0 | grad norm: 1.084 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.065 | tokens per gpu per second (tgs): 1988.131 | TFLOPs: 16.00 |
g0069: [2024-08-03 00:58:11,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=5360, skipped=3, lr=[9.365531306666668e-05, 9.365531306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5360 loss: 1.4664 iter time (s): 3.998 samples/sec: 32.019
g0093:  iteration     5360/10000000 | consumed samples:       686080 | consumed tokens:   1405091840 | elapsed time per iteration (ms): 4030.1 | learning rate: 9.366E-05 | global batch size:   128 | lm loss: 1.470417E+00 | loss scale: 131072.0 | grad norm: 0.991 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.761 | tokens per gpu per second (tgs): 2032.705 | TFLOPs: 16.36 |
g0069: [2024-08-03 00:58:53,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=5370, skipped=3, lr=[9.383007573333334e-05, 9.383007573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5370 loss: 1.4043 iter time (s): 4.179 samples/sec: 30.633
g0093:  iteration     5370/10000000 | consumed samples:       687360 | consumed tokens:   1407713280 | elapsed time per iteration (ms): 4211.5 | learning rate: 9.383E-05 | global batch size:   128 | lm loss: 1.433023E+00 | loss scale: 131072.0 | grad norm: 0.974 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.393 | tokens per gpu per second (tgs): 1945.141 | TFLOPs: 15.65 |
g0069: [2024-08-03 00:59:36,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=5380, skipped=3, lr=[9.400483840000001e-05, 9.400483840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5380 loss: 1.4806 iter time (s): 4.196 samples/sec: 30.506
g0093:  iteration     5380/10000000 | consumed samples:       688640 | consumed tokens:   1410334720 | elapsed time per iteration (ms): 4228.3 | learning rate: 9.400E-05 | global batch size:   128 | lm loss: 1.426227E+00 | loss scale: 131072.0 | grad norm: 1.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.272 | tokens per gpu per second (tgs): 1937.401 | TFLOPs: 15.59 |
g0069: [2024-08-03 01:00:16,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=5390, skipped=3, lr=[9.417960106666667e-05, 9.417960106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5390 loss: 1.4207 iter time (s): 4.025 samples/sec: 31.800
g0093:  iteration     5390/10000000 | consumed samples:       689920 | consumed tokens:   1412956160 | elapsed time per iteration (ms): 4058.1 | learning rate: 9.418E-05 | global batch size:   128 | lm loss: 1.416804E+00 | loss scale: 131072.0 | grad norm: 0.927 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.542 | tokens per gpu per second (tgs): 2018.668 | TFLOPs: 16.24 |
g0069: [2024-08-03 01:00:58,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=5400, skipped=3, lr=[9.435436373333334e-05, 9.435436373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5400 loss: 1.3951 iter time (s): 4.153 samples/sec: 30.818
g0093:  iteration     5400/10000000 | consumed samples:       691200 | consumed tokens:   1415577600 | elapsed time per iteration (ms): 4186.0 | learning rate: 9.435E-05 | global batch size:   128 | lm loss: 1.413001E+00 | loss scale: 131072.0 | grad norm: 0.943 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.578 | tokens per gpu per second (tgs): 1957.004 | TFLOPs: 15.75 |
g0069: [2024-08-03 01:01:38,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=5410, skipped=3, lr=[9.452912640000001e-05, 9.452912640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5410 loss: 1.4513 iter time (s): 4.010 samples/sec: 31.923
g0093:  iteration     5410/10000000 | consumed samples:       692480 | consumed tokens:   1418199040 | elapsed time per iteration (ms): 4043.5 | learning rate: 9.453E-05 | global batch size:   128 | lm loss: 1.424172E+00 | loss scale: 131072.0 | grad norm: 1.047 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.655 | tokens per gpu per second (tgs): 2025.946 | TFLOPs: 16.30 |
g0069: [2024-08-03 01:02:18,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=5420, skipped=3, lr=[9.470388906666667e-05, 9.470388906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5420 loss: 1.4299 iter time (s): 3.923 samples/sec: 32.628
g0093:  iteration     5420/10000000 | consumed samples:       693760 | consumed tokens:   1420820480 | elapsed time per iteration (ms): 3956.0 | learning rate: 9.470E-05 | global batch size:   128 | lm loss: 1.411415E+00 | loss scale: 131072.0 | grad norm: 0.852 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.356 | tokens per gpu per second (tgs): 2070.784 | TFLOPs: 16.66 |
g0069: [2024-08-03 01:02:58,399] [INFO] [logging.py:96:log_dist] [Rank 0] step=5430, skipped=3, lr=[9.487865173333334e-05, 9.487865173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5430 loss: 1.3959 iter time (s): 3.953 samples/sec: 32.381
g0093:  iteration     5430/10000000 | consumed samples:       695040 | consumed tokens:   1423441920 | elapsed time per iteration (ms): 3986.5 | learning rate: 9.488E-05 | global batch size:   128 | lm loss: 1.419355E+00 | loss scale: 131072.0 | grad norm: 0.835 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.108 | tokens per gpu per second (tgs): 2054.943 | TFLOPs: 16.54 |
g0069: [2024-08-03 01:03:38,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=5440, skipped=3, lr=[9.50534144e-05, 9.50534144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5440 loss: 1.4328 iter time (s): 4.008 samples/sec: 31.937
g0093:  iteration     5440/10000000 | consumed samples:       696320 | consumed tokens:   1426063360 | elapsed time per iteration (ms): 4040.5 | learning rate: 9.505E-05 | global batch size:   128 | lm loss: 1.402717E+00 | loss scale: 131072.0 | grad norm: 0.826 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.679 | tokens per gpu per second (tgs): 2027.458 | TFLOPs: 16.32 |
g0069: [2024-08-03 01:04:18,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=5450, skipped=3, lr=[9.522817706666667e-05, 9.522817706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5450 loss: 1.4422 iter time (s): 3.956 samples/sec: 32.352
g0093:  iteration     5450/10000000 | consumed samples:       697600 | consumed tokens:   1428684800 | elapsed time per iteration (ms): 3989.0 | learning rate: 9.523E-05 | global batch size:   128 | lm loss: 1.406034E+00 | loss scale: 131072.0 | grad norm: 0.821 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.088 | tokens per gpu per second (tgs): 2053.647 | TFLOPs: 16.53 |
g0069: [2024-08-03 01:05:00,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=5460, skipped=3, lr=[9.540293973333334e-05, 9.540293973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5460 loss: 1.4266 iter time (s): 4.159 samples/sec: 30.778
g0093:  iteration     5460/10000000 | consumed samples:       698880 | consumed tokens:   1431306240 | elapsed time per iteration (ms): 4192.0 | learning rate: 9.540E-05 | global batch size:   128 | lm loss: 1.414811E+00 | loss scale: 131072.0 | grad norm: 0.836 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.535 | tokens per gpu per second (tgs): 1954.218 | TFLOPs: 15.73 |
g0069: [2024-08-03 01:05:40,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=5470, skipped=3, lr=[9.55777024e-05, 9.55777024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5470 loss: 1.4107 iter time (s): 3.927 samples/sec: 32.591
g0093:  iteration     5470/10000000 | consumed samples:       700160 | consumed tokens:   1433927680 | elapsed time per iteration (ms): 3960.2 | learning rate: 9.558E-05 | global batch size:   128 | lm loss: 1.399568E+00 | loss scale: 131072.0 | grad norm: 0.858 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.321 | tokens per gpu per second (tgs): 2068.561 | TFLOPs: 16.65 |
g0069: [2024-08-03 01:06:20,617] [INFO] [logging.py:96:log_dist] [Rank 0] step=5480, skipped=3, lr=[9.575246506666667e-05, 9.575246506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5480 loss: 1.3815 iter time (s): 4.006 samples/sec: 31.949
g0093:  iteration     5480/10000000 | consumed samples:       701440 | consumed tokens:   1436549120 | elapsed time per iteration (ms): 4039.9 | learning rate: 9.575E-05 | global batch size:   128 | lm loss: 1.407277E+00 | loss scale: 131072.0 | grad norm: 0.918 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.684 | tokens per gpu per second (tgs): 2027.757 | TFLOPs: 16.32 |
g0069: [2024-08-03 01:07:03,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=5490, skipped=3, lr=[9.592722773333335e-05, 9.592722773333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5490 loss: 1.3953 iter time (s): 4.240 samples/sec: 30.186
g0093:  iteration     5490/10000000 | consumed samples:       702720 | consumed tokens:   1439170560 | elapsed time per iteration (ms): 4272.7 | learning rate: 9.593E-05 | global batch size:   128 | lm loss: 1.396560E+00 | loss scale: 131072.0 | grad norm: 0.913 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.957 | tokens per gpu per second (tgs): 1917.276 | TFLOPs: 15.43 |
g0069: [2024-08-03 01:07:41,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=5500, skipped=3, lr=[9.610199040000002e-05, 9.610199040000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5500 loss: 1.3916 iter time (s): 3.828 samples/sec: 33.436
g0093:  iteration     5500/10000000 | consumed samples:       704000 | consumed tokens:   1441792000 | elapsed time per iteration (ms): 3861.1 | learning rate: 9.610E-05 | global batch size:   128 | lm loss: 1.390702E+00 | loss scale: 131072.0 | grad norm: 0.802 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.151 | tokens per gpu per second (tgs): 2121.672 | TFLOPs: 17.07 |
g0069: [2024-08-03 01:08:23,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=5510, skipped=3, lr=[9.627675306666668e-05, 9.627675306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5510 loss: 1.4141 iter time (s): 4.092 samples/sec: 31.277
g0093:  iteration     5510/10000000 | consumed samples:       705280 | consumed tokens:   1444413440 | elapsed time per iteration (ms): 4125.5 | learning rate: 9.628E-05 | global batch size:   128 | lm loss: 1.392930E+00 | loss scale: 131072.0 | grad norm: 0.927 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.026 | tokens per gpu per second (tgs): 1985.688 | TFLOPs: 15.98 |
g0069: [2024-08-03 01:09:04,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=5520, skipped=3, lr=[9.645151573333335e-05, 9.645151573333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5520 loss: 1.4193 iter time (s): 4.112 samples/sec: 31.125
g0093:  iteration     5520/10000000 | consumed samples:       706560 | consumed tokens:   1447034880 | elapsed time per iteration (ms): 4145.4 | learning rate: 9.645E-05 | global batch size:   128 | lm loss: 1.388862E+00 | loss scale: 131072.0 | grad norm: 0.933 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.877 | tokens per gpu per second (tgs): 1976.146 | TFLOPs: 15.90 |
g0069: [2024-08-03 01:09:44,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=5530, skipped=3, lr=[9.662627840000001e-05, 9.662627840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5530 loss: 1.3953 iter time (s): 3.969 samples/sec: 32.253
g0093:  iteration     5530/10000000 | consumed samples:       707840 | consumed tokens:   1449656320 | elapsed time per iteration (ms): 4001.1 | learning rate: 9.663E-05 | global batch size:   128 | lm loss: 1.403601E+00 | loss scale: 131072.0 | grad norm: 0.819 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.992 | tokens per gpu per second (tgs): 2047.461 | TFLOPs: 16.48 |
g0069: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0069: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0092: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0092: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0086: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0086: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0086: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0090: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0090: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0088: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0090: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0088: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0088: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0088: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0093: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 01:09:56,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0087: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 01:09:56,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0093: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 01:09:56,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0090: [2024-08-03 01:09:56,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0087: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0087: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0087: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0093: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0087: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0091: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0087: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0091: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0087: [2024-08-03 01:09:56,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0091: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0091: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0069: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0093: [2024-08-03 01:09:56,437] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 01:09:56,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0093: [2024-08-03 01:09:56,438] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0069: [2024-08-03 01:10:25,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=5540, skipped=3, lr=[9.680104106666668e-05, 9.680104106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5540 loss: 1.4265 iter time (s): 4.044 samples/sec: 31.650
g0093:  iteration     5540/10000000 | consumed samples:       709120 | consumed tokens:   1452277760 | elapsed time per iteration (ms): 4076.7 | learning rate: 9.680E-05 | global batch size:   128 | lm loss: 1.401974E+00 | loss scale: 262144.0 | grad norm: 0.899 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.398 | tokens per gpu per second (tgs): 2009.486 | TFLOPs: 16.17 |
g0069: [2024-08-03 01:11:06,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=5550, skipped=3, lr=[9.697580373333335e-05, 9.697580373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5550 loss: 1.3694 iter time (s): 4.066 samples/sec: 31.478
g0093:  iteration     5550/10000000 | consumed samples:       710400 | consumed tokens:   1454899200 | elapsed time per iteration (ms): 4099.0 | learning rate: 9.698E-05 | global batch size:   128 | lm loss: 1.376843E+00 | loss scale: 262144.0 | grad norm: 0.910 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.227 | tokens per gpu per second (tgs): 1998.526 | TFLOPs: 16.08 |
g0069: [2024-08-03 01:11:49,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=5560, skipped=3, lr=[9.715056640000001e-05, 9.715056640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5560 loss: 1.3633 iter time (s): 4.273 samples/sec: 29.953
g0093:  iteration     5560/10000000 | consumed samples:       711680 | consumed tokens:   1457520640 | elapsed time per iteration (ms): 4305.8 | learning rate: 9.715E-05 | global batch size:   128 | lm loss: 1.390151E+00 | loss scale: 262144.0 | grad norm: 0.851 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.727 | tokens per gpu per second (tgs): 1902.536 | TFLOPs: 15.31 |
g0069: [2024-08-03 01:12:29,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=5570, skipped=3, lr=[9.732532906666668e-05, 9.732532906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5570 loss: 1.3336 iter time (s): 3.971 samples/sec: 32.234
g0093:  iteration     5570/10000000 | consumed samples:       712960 | consumed tokens:   1460142080 | elapsed time per iteration (ms): 4003.9 | learning rate: 9.733E-05 | global batch size:   128 | lm loss: 1.386610E+00 | loss scale: 262144.0 | grad norm: 0.883 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.969 | tokens per gpu per second (tgs): 2046.023 | TFLOPs: 16.46 |
g0069: [2024-08-03 01:13:09,406] [INFO] [logging.py:96:log_dist] [Rank 0] step=5580, skipped=3, lr=[9.750009173333334e-05, 9.750009173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5580 loss: 1.3673 iter time (s): 3.955 samples/sec: 32.364
g0093:  iteration     5580/10000000 | consumed samples:       714240 | consumed tokens:   1462763520 | elapsed time per iteration (ms): 3987.6 | learning rate: 9.750E-05 | global batch size:   128 | lm loss: 1.373345E+00 | loss scale: 262144.0 | grad norm: 0.718 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.100 | tokens per gpu per second (tgs): 2054.375 | TFLOPs: 16.53 |
g0069: [2024-08-03 01:13:53,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=5590, skipped=3, lr=[9.767485440000001e-05, 9.767485440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5590 loss: 1.3747 iter time (s): 4.366 samples/sec: 29.315
g0093:  iteration     5590/10000000 | consumed samples:       715520 | consumed tokens:   1465384960 | elapsed time per iteration (ms): 4398.8 | learning rate: 9.767E-05 | global batch size:   128 | lm loss: 1.388843E+00 | loss scale: 262144.0 | grad norm: 0.960 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.099 | tokens per gpu per second (tgs): 1862.335 | TFLOPs: 14.99 |
g0069: [2024-08-03 01:14:33,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=5600, skipped=3, lr=[9.784961706666668e-05, 9.784961706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5600 loss: 1.3812 iter time (s): 4.019 samples/sec: 31.848
g0093:  iteration     5600/10000000 | consumed samples:       716800 | consumed tokens:   1468006400 | elapsed time per iteration (ms): 4051.5 | learning rate: 9.785E-05 | global batch size:   128 | lm loss: 1.371668E+00 | loss scale: 262144.0 | grad norm: 0.983 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.593 | tokens per gpu per second (tgs): 2021.948 | TFLOPs: 16.27 |
g0069: [2024-08-03 01:15:17,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=5610, skipped=3, lr=[9.802437973333334e-05, 9.802437973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5610 loss: 1.3362 iter time (s): 4.300 samples/sec: 29.768
g0093:  iteration     5610/10000000 | consumed samples:       718080 | consumed tokens:   1470627840 | elapsed time per iteration (ms): 4332.5 | learning rate: 9.802E-05 | global batch size:   128 | lm loss: 1.371379E+00 | loss scale: 262144.0 | grad norm: 0.715 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.544 | tokens per gpu per second (tgs): 1890.826 | TFLOPs: 15.22 |
g0069: [2024-08-03 01:15:58,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=5620, skipped=3, lr=[9.819914240000001e-05, 9.819914240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5620 loss: 1.3908 iter time (s): 4.075 samples/sec: 31.414
g0093:  iteration     5620/10000000 | consumed samples:       719360 | consumed tokens:   1473249280 | elapsed time per iteration (ms): 4107.5 | learning rate: 9.820E-05 | global batch size:   128 | lm loss: 1.375972E+00 | loss scale: 262144.0 | grad norm: 0.920 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.162 | tokens per gpu per second (tgs): 1994.380 | TFLOPs: 16.05 |
g0069: [2024-08-03 01:16:38,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=5630, skipped=3, lr=[9.837390506666667e-05, 9.837390506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5630 loss: 1.4001 iter time (s): 3.968 samples/sec: 32.260
g0093:  iteration     5630/10000000 | consumed samples:       720640 | consumed tokens:   1475870720 | elapsed time per iteration (ms): 4000.4 | learning rate: 9.837E-05 | global batch size:   128 | lm loss: 1.365283E+00 | loss scale: 262144.0 | grad norm: 0.891 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.997 | tokens per gpu per second (tgs): 2047.817 | TFLOPs: 16.48 |
g0069: [2024-08-03 01:17:20,894] [INFO] [logging.py:96:log_dist] [Rank 0] step=5640, skipped=3, lr=[9.854866773333334e-05, 9.854866773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5640 loss: 1.3645 iter time (s): 4.226 samples/sec: 30.292
g0093:  iteration     5640/10000000 | consumed samples:       721920 | consumed tokens:   1478492160 | elapsed time per iteration (ms): 4258.0 | learning rate: 9.855E-05 | global batch size:   128 | lm loss: 1.365953E+00 | loss scale: 262144.0 | grad norm: 0.671 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.061 | tokens per gpu per second (tgs): 1923.914 | TFLOPs: 15.48 |
g0069: [2024-08-03 01:18:02,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=5650, skipped=3, lr=[9.87234304e-05, 9.87234304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5650 loss: 1.3599 iter time (s): 4.100 samples/sec: 31.220
g0093:  iteration     5650/10000000 | consumed samples:       723200 | consumed tokens:   1481113600 | elapsed time per iteration (ms): 4132.3 | learning rate: 9.872E-05 | global batch size:   128 | lm loss: 1.364499E+00 | loss scale: 262144.0 | grad norm: 0.888 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.976 | tokens per gpu per second (tgs): 1982.454 | TFLOPs: 15.95 |
g0069: [2024-08-03 01:18:42,750] [INFO] [logging.py:96:log_dist] [Rank 0] step=5660, skipped=3, lr=[9.889819306666667e-05, 9.889819306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5660 loss: 1.3577 iter time (s): 4.021 samples/sec: 31.834
g0093:  iteration     5660/10000000 | consumed samples:       724480 | consumed tokens:   1483735040 | elapsed time per iteration (ms): 4053.3 | learning rate: 9.890E-05 | global batch size:   128 | lm loss: 1.362725E+00 | loss scale: 262144.0 | grad norm: 0.804 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.579 | tokens per gpu per second (tgs): 2021.049 | TFLOPs: 16.26 |
g0069: [2024-08-03 01:19:23,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=5670, skipped=3, lr=[9.907295573333335e-05, 9.907295573333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5670 loss: 1.3698 iter time (s): 4.003 samples/sec: 31.975
g0093:  iteration     5670/10000000 | consumed samples:       725760 | consumed tokens:   1486356480 | elapsed time per iteration (ms): 4037.2 | learning rate: 9.907E-05 | global batch size:   128 | lm loss: 1.367493E+00 | loss scale: 262144.0 | grad norm: 0.758 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.705 | tokens per gpu per second (tgs): 2029.112 | TFLOPs: 16.33 |
g0069: [2024-08-03 01:20:05,470] [INFO] [logging.py:96:log_dist] [Rank 0] step=5680, skipped=3, lr=[9.924771840000002e-05, 9.924771840000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5680 loss: 1.3559 iter time (s): 4.201 samples/sec: 30.466
g0093:  iteration     5680/10000000 | consumed samples:       727040 | consumed tokens:   1488977920 | elapsed time per iteration (ms): 4234.7 | learning rate: 9.925E-05 | global batch size:   128 | lm loss: 1.365357E+00 | loss scale: 262144.0 | grad norm: 0.807 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.226 | tokens per gpu per second (tgs): 1934.489 | TFLOPs: 15.57 |
g0069: [2024-08-03 01:20:46,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=5690, skipped=3, lr=[9.942248106666668e-05, 9.942248106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5690 loss: 1.3804 iter time (s): 4.065 samples/sec: 31.491
g0093:  iteration     5690/10000000 | consumed samples:       728320 | consumed tokens:   1491599360 | elapsed time per iteration (ms): 4097.2 | learning rate: 9.942E-05 | global batch size:   128 | lm loss: 1.363214E+00 | loss scale: 262144.0 | grad norm: 0.967 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.241 | tokens per gpu per second (tgs): 1999.431 | TFLOPs: 16.09 |
g0069: [2024-08-03 01:21:28,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=5700, skipped=3, lr=[9.959724373333335e-05, 9.959724373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5700 loss: 1.3821 iter time (s): 4.166 samples/sec: 30.727
g0093:  iteration     5700/10000000 | consumed samples:       729600 | consumed tokens:   1494220800 | elapsed time per iteration (ms): 4198.2 | learning rate: 9.960E-05 | global batch size:   128 | lm loss: 1.371316E+00 | loss scale: 262144.0 | grad norm: 0.802 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.490 | tokens per gpu per second (tgs): 1951.335 | TFLOPs: 15.70 |
g0069: [2024-08-03 01:22:10,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=5710, skipped=3, lr=[9.977200640000002e-05, 9.977200640000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5710 loss: 1.3100 iter time (s): 4.193 samples/sec: 30.529
g0093:  iteration     5710/10000000 | consumed samples:       730880 | consumed tokens:   1496842240 | elapsed time per iteration (ms): 4225.5 | learning rate: 9.977E-05 | global batch size:   128 | lm loss: 1.338726E+00 | loss scale: 262144.0 | grad norm: 0.889 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.293 | tokens per gpu per second (tgs): 1938.723 | TFLOPs: 15.60 |
g0069: [2024-08-03 01:22:49,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=5720, skipped=3, lr=[9.994676906666668e-05, 9.994676906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5720 loss: 1.3557 iter time (s): 3.824 samples/sec: 33.476
g0093:  iteration     5720/10000000 | consumed samples:       732160 | consumed tokens:   1499463680 | elapsed time per iteration (ms): 3856.5 | learning rate: 9.995E-05 | global batch size:   128 | lm loss: 1.341047E+00 | loss scale: 262144.0 | grad norm: 0.805 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.191 | tokens per gpu per second (tgs): 2124.206 | TFLOPs: 17.09 |
g0069: [2024-08-03 01:23:28,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=5730, skipped=3, lr=[0.00010012153173333335, 0.00010012153173333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5730 loss: 1.3748 iter time (s): 3.869 samples/sec: 33.086
g0093:  iteration     5730/10000000 | consumed samples:       733440 | consumed tokens:   1502085120 | elapsed time per iteration (ms): 3903.0 | learning rate: 1.001E-04 | global batch size:   128 | lm loss: 1.355290E+00 | loss scale: 262144.0 | grad norm: 0.891 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.795 | tokens per gpu per second (tgs): 2098.911 | TFLOPs: 16.89 |
g0069: [2024-08-03 01:24:09,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=5740, skipped=3, lr=[0.00010029629440000001, 0.00010029629440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5740 loss: 1.3548 iter time (s): 4.040 samples/sec: 31.685
g0093:  iteration     5740/10000000 | consumed samples:       734720 | consumed tokens:   1504706560 | elapsed time per iteration (ms): 4073.0 | learning rate: 1.003E-04 | global batch size:   128 | lm loss: 1.352816E+00 | loss scale: 262144.0 | grad norm: 0.760 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.426 | tokens per gpu per second (tgs): 2011.293 | TFLOPs: 16.19 |
g0069: [2024-08-03 01:24:50,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=5750, skipped=3, lr=[0.00010047105706666668, 0.00010047105706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5750 loss: 1.3164 iter time (s): 4.068 samples/sec: 31.467
g0093:  iteration     5750/10000000 | consumed samples:       736000 | consumed tokens:   1507328000 | elapsed time per iteration (ms): 4100.8 | learning rate: 1.005E-04 | global batch size:   128 | lm loss: 1.344978E+00 | loss scale: 262144.0 | grad norm: 0.757 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.214 | tokens per gpu per second (tgs): 1997.665 | TFLOPs: 16.08 |
g0069: [2024-08-03 01:25:31,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=5760, skipped=3, lr=[0.00010064581973333335, 0.00010064581973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5760 loss: 1.3388 iter time (s): 4.073 samples/sec: 31.423
g0093:  iteration     5760/10000000 | consumed samples:       737280 | consumed tokens:   1509949440 | elapsed time per iteration (ms): 4107.1 | learning rate: 1.006E-04 | global batch size:   128 | lm loss: 1.346778E+00 | loss scale: 262144.0 | grad norm: 0.728 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.166 | tokens per gpu per second (tgs): 1994.605 | TFLOPs: 16.05 |
g0069: [2024-08-03 01:26:13,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=5770, skipped=3, lr=[0.00010082058240000001, 0.00010082058240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5770 loss: 1.3356 iter time (s): 4.232 samples/sec: 30.248
g0093:  iteration     5770/10000000 | consumed samples:       738560 | consumed tokens:   1512570880 | elapsed time per iteration (ms): 4265.4 | learning rate: 1.008E-04 | global batch size:   128 | lm loss: 1.344298E+00 | loss scale: 262144.0 | grad norm: 0.754 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.009 | tokens per gpu per second (tgs): 1920.571 | TFLOPs: 15.46 |
g0069: [2024-08-03 01:26:55,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=5780, skipped=3, lr=[0.00010099534506666668, 0.00010099534506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5780 loss: 1.3357 iter time (s): 4.131 samples/sec: 30.987
g0093:  iteration     5780/10000000 | consumed samples:       739840 | consumed tokens:   1515192320 | elapsed time per iteration (ms): 4177.2 | learning rate: 1.010E-04 | global batch size:   128 | lm loss: 1.338117E+00 | loss scale: 262144.0 | grad norm: 0.832 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.643 | tokens per gpu per second (tgs): 1961.134 | TFLOPs: 15.78 |
g0069: [2024-08-03 01:27:36,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=5790, skipped=3, lr=[0.00010117010773333334, 0.00010117010773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5790 loss: 1.2790 iter time (s): 4.040 samples/sec: 31.680
g0093:  iteration     5790/10000000 | consumed samples:       741120 | consumed tokens:   1517813760 | elapsed time per iteration (ms): 4073.9 | learning rate: 1.012E-04 | global batch size:   128 | lm loss: 1.326935E+00 | loss scale: 262144.0 | grad norm: 0.761 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.419 | tokens per gpu per second (tgs): 2010.832 | TFLOPs: 16.18 |
g0069: [2024-08-03 01:28:17,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=5800, skipped=3, lr=[0.00010134487040000001, 0.00010134487040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5800 loss: 1.3392 iter time (s): 4.057 samples/sec: 31.552
g0093:  iteration     5800/10000000 | consumed samples:       742400 | consumed tokens:   1520435200 | elapsed time per iteration (ms): 4089.4 | learning rate: 1.013E-04 | global batch size:   128 | lm loss: 1.335510E+00 | loss scale: 262144.0 | grad norm: 0.957 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.300 | tokens per gpu per second (tgs): 2003.230 | TFLOPs: 16.12 |
g0069: [2024-08-03 01:28:58,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=5810, skipped=3, lr=[0.00010151963306666668, 0.00010151963306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5810 loss: 1.3366 iter time (s): 4.101 samples/sec: 31.211
g0093:  iteration     5810/10000000 | consumed samples:       743680 | consumed tokens:   1523056640 | elapsed time per iteration (ms): 4133.7 | learning rate: 1.015E-04 | global batch size:   128 | lm loss: 1.325512E+00 | loss scale: 262144.0 | grad norm: 0.709 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.965 | tokens per gpu per second (tgs): 1981.777 | TFLOPs: 15.95 |
g0069: [2024-08-03 01:29:38,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=5820, skipped=3, lr=[0.00010169439573333333, 0.00010169439573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5820 loss: 1.3287 iter time (s): 4.008 samples/sec: 31.933
g0093:  iteration     5820/10000000 | consumed samples:       744960 | consumed tokens:   1525678080 | elapsed time per iteration (ms): 4040.8 | learning rate: 1.017E-04 | global batch size:   128 | lm loss: 1.327955E+00 | loss scale: 262144.0 | grad norm: 0.882 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.677 | tokens per gpu per second (tgs): 2027.307 | TFLOPs: 16.31 |
g0069: [2024-08-03 01:30:19,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=5830, skipped=3, lr=[0.0001018691584, 0.0001018691584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5830 loss: 1.3531 iter time (s): 4.069 samples/sec: 31.454
g0093:  iteration     5830/10000000 | consumed samples:       746240 | consumed tokens:   1528299520 | elapsed time per iteration (ms): 4102.2 | learning rate: 1.019E-04 | global batch size:   128 | lm loss: 1.333683E+00 | loss scale: 262144.0 | grad norm: 0.849 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.202 | tokens per gpu per second (tgs): 1996.959 | TFLOPs: 16.07 |
g0069: [2024-08-03 01:31:02,069] [INFO] [logging.py:96:log_dist] [Rank 0] step=5840, skipped=3, lr=[0.00010204392106666666, 0.00010204392106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5840 loss: 1.3186 iter time (s): 4.183 samples/sec: 30.600
g0093:  iteration     5840/10000000 | consumed samples:       747520 | consumed tokens:   1530920960 | elapsed time per iteration (ms): 4215.9 | learning rate: 1.020E-04 | global batch size:   128 | lm loss: 1.327411E+00 | loss scale: 262144.0 | grad norm: 0.726 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.361 | tokens per gpu per second (tgs): 1943.106 | TFLOPs: 15.64 |
g0069: [2024-08-03 01:31:43,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=5850, skipped=3, lr=[0.00010221868373333333, 0.00010221868373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5850 loss: 1.3344 iter time (s): 4.066 samples/sec: 31.480
g0093:  iteration     5850/10000000 | consumed samples:       748800 | consumed tokens:   1533542400 | elapsed time per iteration (ms): 4098.9 | learning rate: 1.022E-04 | global batch size:   128 | lm loss: 1.320442E+00 | loss scale: 262144.0 | grad norm: 0.858 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.228 | tokens per gpu per second (tgs): 1998.597 | TFLOPs: 16.08 |
g0069: [2024-08-03 01:32:23,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=5860, skipped=3, lr=[0.0001023934464, 0.0001023934464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5860 loss: 1.3025 iter time (s): 3.968 samples/sec: 32.260
g0093:  iteration     5860/10000000 | consumed samples:       750080 | consumed tokens:   1536163840 | elapsed time per iteration (ms): 4000.7 | learning rate: 1.024E-04 | global batch size:   128 | lm loss: 1.311412E+00 | loss scale: 262144.0 | grad norm: 0.609 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.994 | tokens per gpu per second (tgs): 2047.645 | TFLOPs: 16.48 |
g0069: [2024-08-03 01:33:04,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=5870, skipped=3, lr=[0.00010256820906666666, 0.00010256820906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5870 loss: 1.3346 iter time (s): 4.109 samples/sec: 31.150
g0093:  iteration     5870/10000000 | consumed samples:       751360 | consumed tokens:   1538785280 | elapsed time per iteration (ms): 4142.3 | learning rate: 1.026E-04 | global batch size:   128 | lm loss: 1.326857E+00 | loss scale: 262144.0 | grad norm: 0.762 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.900 | tokens per gpu per second (tgs): 1977.624 | TFLOPs: 15.91 |
g0069: [2024-08-03 01:33:45,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=5880, skipped=3, lr=[0.00010274297173333333, 0.00010274297173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5880 loss: 1.3667 iter time (s): 4.071 samples/sec: 31.446
g0093:  iteration     5880/10000000 | consumed samples:       752640 | consumed tokens:   1541406720 | elapsed time per iteration (ms): 4103.4 | learning rate: 1.027E-04 | global batch size:   128 | lm loss: 1.335956E+00 | loss scale: 262144.0 | grad norm: 0.859 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.193 | tokens per gpu per second (tgs): 1996.382 | TFLOPs: 16.07 |
g0069: [2024-08-03 01:34:26,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=5890, skipped=3, lr=[0.00010291773439999999, 0.00010291773439999999], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5890 loss: 1.3096 iter time (s): 4.029 samples/sec: 31.774
g0093:  iteration     5890/10000000 | consumed samples:       753920 | consumed tokens:   1544028160 | elapsed time per iteration (ms): 4060.8 | learning rate: 1.029E-04 | global batch size:   128 | lm loss: 1.308015E+00 | loss scale: 262144.0 | grad norm: 0.656 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.521 | tokens per gpu per second (tgs): 2017.326 | TFLOPs: 16.23 |
g0069: [2024-08-03 01:35:06,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=5900, skipped=3, lr=[0.00010309249706666666, 0.00010309249706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5900 loss: 1.3487 iter time (s): 4.000 samples/sec: 31.996
g0093:  iteration     5900/10000000 | consumed samples:       755200 | consumed tokens:   1546649600 | elapsed time per iteration (ms): 4032.9 | learning rate: 1.031E-04 | global batch size:   128 | lm loss: 1.302177E+00 | loss scale: 262144.0 | grad norm: 0.748 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.739 | tokens per gpu per second (tgs): 2031.307 | TFLOPs: 16.35 |
g0069: [2024-08-03 01:35:48,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=5910, skipped=3, lr=[0.00010326725973333332, 0.00010326725973333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5910 loss: 1.2881 iter time (s): 4.140 samples/sec: 30.919
g0093:  iteration     5910/10000000 | consumed samples:       756480 | consumed tokens:   1549271040 | elapsed time per iteration (ms): 4172.3 | learning rate: 1.033E-04 | global batch size:   128 | lm loss: 1.308868E+00 | loss scale: 262144.0 | grad norm: 0.636 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.678 | tokens per gpu per second (tgs): 1963.410 | TFLOPs: 15.80 |
g0069: [2024-08-03 01:36:29,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=5920, skipped=3, lr=[0.0001034420224, 0.0001034420224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5920 loss: 1.3396 iter time (s): 4.135 samples/sec: 30.955
g0093:  iteration     5920/10000000 | consumed samples:       757760 | consumed tokens:   1551892480 | elapsed time per iteration (ms): 4167.2 | learning rate: 1.034E-04 | global batch size:   128 | lm loss: 1.319722E+00 | loss scale: 262144.0 | grad norm: 0.760 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.716 | tokens per gpu per second (tgs): 1965.833 | TFLOPs: 15.82 |
g0069: [2024-08-03 01:37:09,493] [INFO] [logging.py:96:log_dist] [Rank 0] step=5930, skipped=3, lr=[0.00010361678506666667, 0.00010361678506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5930 loss: 1.3310 iter time (s): 3.932 samples/sec: 32.556
g0093:  iteration     5930/10000000 | consumed samples:       759040 | consumed tokens:   1554513920 | elapsed time per iteration (ms): 3963.7 | learning rate: 1.036E-04 | global batch size:   128 | lm loss: 1.315546E+00 | loss scale: 262144.0 | grad norm: 0.822 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.293 | tokens per gpu per second (tgs): 2066.767 | TFLOPs: 16.63 |
g0069: [2024-08-03 01:37:51,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=5940, skipped=3, lr=[0.00010379154773333334, 0.00010379154773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5940 loss: 1.3337 iter time (s): 4.138 samples/sec: 30.933
g0093:  iteration     5940/10000000 | consumed samples:       760320 | consumed tokens:   1557135360 | elapsed time per iteration (ms): 4170.5 | learning rate: 1.038E-04 | global batch size:   128 | lm loss: 1.310384E+00 | loss scale: 262144.0 | grad norm: 0.787 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.692 | tokens per gpu per second (tgs): 1964.288 | TFLOPs: 15.81 |
g0069: [2024-08-03 01:38:33,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=5950, skipped=3, lr=[0.0001039663104, 0.0001039663104], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5950 loss: 1.2954 iter time (s): 4.176 samples/sec: 30.649
g0093:  iteration     5950/10000000 | consumed samples:       761600 | consumed tokens:   1559756800 | elapsed time per iteration (ms): 4208.8 | learning rate: 1.040E-04 | global batch size:   128 | lm loss: 1.308493E+00 | loss scale: 262144.0 | grad norm: 0.728 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.413 | tokens per gpu per second (tgs): 1946.415 | TFLOPs: 15.66 |
g0069: [2024-08-03 01:39:13,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=5960, skipped=3, lr=[0.00010414107306666667, 0.00010414107306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5960 loss: 1.3146 iter time (s): 4.036 samples/sec: 31.716
g0093:  iteration     5960/10000000 | consumed samples:       762880 | consumed tokens:   1562378240 | elapsed time per iteration (ms): 4068.0 | learning rate: 1.041E-04 | global batch size:   128 | lm loss: 1.307601E+00 | loss scale: 262144.0 | grad norm: 0.610 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.465 | tokens per gpu per second (tgs): 2013.768 | TFLOPs: 16.21 |
g0069: [2024-08-03 01:39:54,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=5970, skipped=3, lr=[0.00010431583573333333, 0.00010431583573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5970 loss: 1.3090 iter time (s): 4.051 samples/sec: 31.601
g0093:  iteration     5970/10000000 | consumed samples:       764160 | consumed tokens:   1564999680 | elapsed time per iteration (ms): 4082.7 | learning rate: 1.043E-04 | global batch size:   128 | lm loss: 1.311521E+00 | loss scale: 262144.0 | grad norm: 0.898 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.352 | tokens per gpu per second (tgs): 2006.497 | TFLOPs: 16.15 |
g0069: [2024-08-03 01:40:35,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=5980, skipped=3, lr=[0.0001044905984, 0.0001044905984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5980 loss: 1.2842 iter time (s): 3.997 samples/sec: 32.024
g0093:  iteration     5980/10000000 | consumed samples:       765440 | consumed tokens:   1567621120 | elapsed time per iteration (ms): 4029.1 | learning rate: 1.045E-04 | global batch size:   128 | lm loss: 1.306278E+00 | loss scale: 262144.0 | grad norm: 0.663 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.769 | tokens per gpu per second (tgs): 2033.213 | TFLOPs: 16.36 |
g0069: [2024-08-03 01:41:16,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=5990, skipped=3, lr=[0.00010466536106666667, 0.00010466536106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 5990 loss: 1.2869 iter time (s): 4.143 samples/sec: 30.896
g0093:  iteration     5990/10000000 | consumed samples:       766720 | consumed tokens:   1570242560 | elapsed time per iteration (ms): 4175.1 | learning rate: 1.047E-04 | global batch size:   128 | lm loss: 1.304871E+00 | loss scale: 262144.0 | grad norm: 0.640 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.658 | tokens per gpu per second (tgs): 1962.121 | TFLOPs: 15.79 |
g0069: [2024-08-03 01:41:56,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=3, lr=[0.00010484012373333333, 0.00010484012373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6000 loss: 1.2996 iter time (s): 3.892 samples/sec: 32.887
g0093:  iteration     6000/10000000 | consumed samples:       768000 | consumed tokens:   1572864000 | elapsed time per iteration (ms): 3924.3 | learning rate: 1.048E-04 | global batch size:   128 | lm loss: 1.281166E+00 | loss scale: 262144.0 | grad norm: 0.777 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.617 | tokens per gpu per second (tgs): 2087.518 | TFLOPs: 16.80 |
g0093: ------------------------------------------------------------------------------------------------
g0093:  validation loss at iteration 6000 | lm loss value: 1.301427E+00 | lm loss PPL: 3.674537E+00 | 
g0093: ------------------------------------------------------------------------------------------------
g0069: saving checkpoint at iteration    6000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: [2024-08-03 01:48:25,951] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6000 is about to be saved!
g0093: [2024-08-03 01:48:25,963] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0093: [2024-08-03 01:48:25,963] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0069: [2024-08-03 01:48:25,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0093: [2024-08-03 01:48:25,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0069: [2024-08-03 01:48:25,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0069: [2024-08-03 01:48:25,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0090: [2024-08-03 01:48:25,964] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0087: [2024-08-03 01:48:25,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0090: [2024-08-03 01:48:25,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0087: [2024-08-03 01:48:25,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0087: [2024-08-03 01:48:25,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0090: [2024-08-03 01:48:25,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0091: [2024-08-03 01:48:25,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0091: [2024-08-03 01:48:25,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0091: [2024-08-03 01:48:25,965] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0092: [2024-08-03 01:48:25,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0092: [2024-08-03 01:48:25,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0092: [2024-08-03 01:48:25,967] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0088: [2024-08-03 01:48:25,970] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0088: [2024-08-03 01:48:25,970] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0088: [2024-08-03 01:48:25,970] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0086: [2024-08-03 01:48:25,972] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0086: [2024-08-03 01:48:25,972] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0086: [2024-08-03 01:48:25,972] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0093: [2024-08-03 01:48:25,984] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_23-model_00-model_states.pt...
g0087: [2024-08-03 01:48:25,997] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_08-model_00-model_states.pt...
g0090: [2024-08-03 01:48:25,998] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_14-model_00-model_states.pt...
g0091: [2024-08-03 01:48:26,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_17-model_00-model_states.pt...
g0088: [2024-08-03 01:48:26,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_11-model_00-model_states.pt...
g0092: [2024-08-03 01:48:26,004] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_20-model_00-model_states.pt...
g0086: [2024-08-03 01:48:26,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_05-model_00-model_states.pt...
g0069: [2024-08-03 01:48:26,010] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_01-model_00-model_states.pt...
g0091: [2024-08-03 01:48:26,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_17-model_00-model_states.pt.
g0091: [2024-08-03 01:48:26,174] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_18-model_00-model_states.pt...
g0093: [2024-08-03 01:48:26,204] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_23-model_00-model_states.pt.
g0093: [2024-08-03 01:48:26,204] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_24-model_00-model_states.pt...
g0093: [2024-08-03 01:48:26,206] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_24-model_00-model_states.pt.
g0092: [2024-08-03 01:48:26,258] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_20-model_00-model_states.pt.
g0093: [2024-08-03 01:48:26,259] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_25-model_00-model_states.pt...
g0069: [2024-08-03 01:48:26,290] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_01-model_00-model_states.pt.
g0092: [2024-08-03 01:48:26,297] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_21-model_00-model_states.pt...
g0069: [2024-08-03 01:48:26,319] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_02-model_00-model_states.pt...
g0086: [2024-08-03 01:48:26,372] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_05-model_00-model_states.pt.
g0091: [2024-08-03 01:48:26,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_18-model_00-model_states.pt.
g0086: [2024-08-03 01:48:26,412] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_06-model_00-model_states.pt...
g0091: [2024-08-03 01:48:26,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_19-model_00-model_states.pt...
g0088: [2024-08-03 01:48:26,468] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_11-model_00-model_states.pt.
g0088: [2024-08-03 01:48:26,502] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_12-model_00-model_states.pt...
g0069: [2024-08-03 01:48:26,505] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_02-model_00-model_states.pt.
g0092: [2024-08-03 01:48:26,505] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_21-model_00-model_states.pt.
g0090: [2024-08-03 01:48:26,519] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_14-model_00-model_states.pt.
g0091: [2024-08-03 01:48:26,524] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_19-model_00-model_states.pt.
g0091: [2024-08-03 01:48:26,526] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_05_model_states.pt...
g0086: [2024-08-03 01:48:26,533] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_06-model_00-model_states.pt.
g0069: [2024-08-03 01:48:26,540] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_03-model_00-model_states.pt...
g0092: [2024-08-03 01:48:26,540] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_22-model_00-model_states.pt...
g0087: [2024-08-03 01:48:26,545] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_08-model_00-model_states.pt.
g0090: [2024-08-03 01:48:26,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_15-model_00-model_states.pt...
g0086: [2024-08-03 01:48:26,568] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_07-model_00-model_states.pt...
g0087: [2024-08-03 01:48:26,580] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_09-model_00-model_states.pt...
g0088: [2024-08-03 01:48:26,609] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_12-model_00-model_states.pt.
g0088: [2024-08-03 01:48:26,639] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_13-model_00-model_states.pt...
g0086: [2024-08-03 01:48:26,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_07-model_00-model_states.pt.
g0090: [2024-08-03 01:48:26,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_15-model_00-model_states.pt.
g0086: [2024-08-03 01:48:26,697] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_01_model_states.pt...
g0093: [2024-08-03 01:48:26,716] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_25-model_00-model_states.pt.
g0087: [2024-08-03 01:48:26,717] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_09-model_00-model_states.pt.
g0093: [2024-08-03 01:48:26,717] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_07_model_states.pt...
g0090: [2024-08-03 01:48:26,723] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_16-model_00-model_states.pt...
g0088: [2024-08-03 01:48:26,731] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_13-model_00-model_states.pt.
g0088: [2024-08-03 01:48:26,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_03_model_states.pt...
g0069: [2024-08-03 01:48:26,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_03-model_00-model_states.pt.
g0087: [2024-08-03 01:48:26,743] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_10-model_00-model_states.pt...
g0069: [2024-08-03 01:48:26,768] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_04-model_00-model_states.pt...
g0090: [2024-08-03 01:48:26,861] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_16-model_00-model_states.pt.
g0090: [2024-08-03 01:48:26,863] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_04_model_states.pt...
g0069: [2024-08-03 01:48:26,885] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_04-model_00-model_states.pt.
g0069: [2024-08-03 01:48:26,887] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt
g0069: [2024-08-03 01:48:26,887] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt...
g0087: [2024-08-03 01:48:26,960] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_10-model_00-model_states.pt.
g0087: [2024-08-03 01:48:26,961] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_02_model_states.pt...
g0092: [2024-08-03 01:48:27,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_22-model_00-model_states.pt.
g0092: [2024-08-03 01:48:27,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_06_model_states.pt...
g0091: [2024-08-03 01:48:28,931] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_05_model_states.pt.
g0091: [2024-08-03 01:48:28,932] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0088: [2024-08-03 01:48:29,141] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_03_model_states.pt.
g0088: [2024-08-03 01:48:29,141] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0093: [2024-08-03 01:48:29,154] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_07_model_states.pt.
g0093: [2024-08-03 01:48:29,155] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0090: [2024-08-03 01:48:29,189] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_04_model_states.pt.
g0090: [2024-08-03 01:48:29,189] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0087: [2024-08-03 01:48:29,231] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_02_model_states.pt.
g0087: [2024-08-03 01:48:29,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0086: [2024-08-03 01:48:29,245] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_01_model_states.pt.
g0086: [2024-08-03 01:48:29,246] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0092: [2024-08-03 01:48:29,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_06_model_states.pt.
g0092: [2024-08-03 01:48:29,696] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0069: [2024-08-03 01:48:30,272] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt.
g0069: [2024-08-03 01:48:30,272] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0069:   successfully saved checkpoint at iteration    6000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: Checkpoint Save GB: 22.521, GB/Sec: 5.19, Latency(second): 4.335
g0093: (min, max) time across ranks (ms):
g0093:     save-checkpoint ................................: (4334.35, 4335.21)
g0069: [2024-08-03 01:49:10,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=6010, skipped=3, lr=[0.0001050148864, 0.0001050148864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6010 loss: 1.3065 iter time (s): 4.012 samples/sec: 31.905
g0093:  iteration     6010/10000000 | consumed samples:       769280 | consumed tokens:   1575485440 | elapsed time per iteration (ms): 43464.0 | learning rate: 1.050E-04 | global batch size:   128 | lm loss: 1.305902E+00 | loss scale: 262144.0 | grad norm: 0.750 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.945 | tokens per gpu per second (tgs): 188.478 | TFLOPs: 1.52 |
g0069: [2024-08-03 01:49:51,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=6020, skipped=3, lr=[0.00010518964906666666, 0.00010518964906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6020 loss: 1.3119 iter time (s): 4.087 samples/sec: 31.317
g0093:  iteration     6020/10000000 | consumed samples:       770560 | consumed tokens:   1578106880 | elapsed time per iteration (ms): 4119.2 | learning rate: 1.052E-04 | global batch size:   128 | lm loss: 1.306474E+00 | loss scale: 262144.0 | grad norm: 1.134 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.074 | tokens per gpu per second (tgs): 1988.752 | TFLOPs: 16.00 |
g0069: [2024-08-03 01:50:34,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=6030, skipped=3, lr=[0.00010536441173333333, 0.00010536441173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6030 loss: 1.3037 iter time (s): 4.199 samples/sec: 30.484
g0093:  iteration     6030/10000000 | consumed samples:       771840 | consumed tokens:   1580728320 | elapsed time per iteration (ms): 4231.0 | learning rate: 1.054E-04 | global batch size:   128 | lm loss: 1.285135E+00 | loss scale: 262144.0 | grad norm: 0.720 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.253 | tokens per gpu per second (tgs): 1936.172 | TFLOPs: 15.58 |
g0087: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0090: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0090: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0092: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0087: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0090: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0091: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0086: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0092: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0088: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0087: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0088: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0069: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0092: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0092: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0088: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0088: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0069: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0091: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0092: [2024-08-03 01:50:46,185] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0091: [2024-08-03 01:50:46,185] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0093: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0086: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0086: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0093: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0088: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0069: [2024-08-03 01:50:46,185] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0069: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 01:50:46,185] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0069: [2024-08-03 01:50:46,185] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0090: [2024-08-03 01:50:46,185] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0093: [2024-08-03 01:50:46,185] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 01:50:46,184] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 01:50:46,185] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0093: [2024-08-03 01:50:46,186] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0069: [2024-08-03 01:51:15,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=6040, skipped=3, lr=[0.0001055391744, 0.0001055391744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6040 loss: 1.2873 iter time (s): 4.056 samples/sec: 31.555
g0093:  iteration     6040/10000000 | consumed samples:       773120 | consumed tokens:   1583349760 | elapsed time per iteration (ms): 4088.5 | learning rate: 1.055E-04 | global batch size:   128 | lm loss: 1.293404E+00 | loss scale: 524288.0 | grad norm: 0.746 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.308 | tokens per gpu per second (tgs): 2003.687 | TFLOPs: 16.12 |
g0069: [2024-08-03 01:51:56,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=6050, skipped=3, lr=[0.00010571393706666666, 0.00010571393706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6050 loss: 1.2862 iter time (s): 4.143 samples/sec: 30.895
g0093:  iteration     6050/10000000 | consumed samples:       774400 | consumed tokens:   1585971200 | elapsed time per iteration (ms): 4175.4 | learning rate: 1.057E-04 | global batch size:   128 | lm loss: 1.294873E+00 | loss scale: 524288.0 | grad norm: 0.792 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.655 | tokens per gpu per second (tgs): 1961.952 | TFLOPs: 15.79 |
g0069: [2024-08-03 01:52:36,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=6060, skipped=3, lr=[0.00010588869973333333, 0.00010588869973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6060 loss: 1.2982 iter time (s): 3.891 samples/sec: 32.897
g0093:  iteration     6060/10000000 | consumed samples:       775680 | consumed tokens:   1588592640 | elapsed time per iteration (ms): 3927.9 | learning rate: 1.059E-04 | global batch size:   128 | lm loss: 1.284243E+00 | loss scale: 524288.0 | grad norm: 0.702 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.587 | tokens per gpu per second (tgs): 2085.567 | TFLOPs: 16.78 |
g0069: [2024-08-03 01:53:16,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=6070, skipped=3, lr=[0.0001060634624, 0.0001060634624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6070 loss: 1.2801 iter time (s): 3.991 samples/sec: 32.073
g0093:  iteration     6070/10000000 | consumed samples:       776960 | consumed tokens:   1591214080 | elapsed time per iteration (ms): 4023.0 | learning rate: 1.061E-04 | global batch size:   128 | lm loss: 1.289938E+00 | loss scale: 524288.0 | grad norm: 0.740 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.817 | tokens per gpu per second (tgs): 2036.301 | TFLOPs: 16.39 |
g0069: [2024-08-03 01:53:58,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=6080, skipped=3, lr=[0.00010623822506666666, 0.00010623822506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6080 loss: 1.3194 iter time (s): 4.192 samples/sec: 30.532
g0093:  iteration     6080/10000000 | consumed samples:       778240 | consumed tokens:   1593835520 | elapsed time per iteration (ms): 4224.5 | learning rate: 1.062E-04 | global batch size:   128 | lm loss: 1.278925E+00 | loss scale: 524288.0 | grad norm: 0.612 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.299 | tokens per gpu per second (tgs): 1939.143 | TFLOPs: 15.60 |
g0069: [2024-08-03 01:54:42,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=6090, skipped=3, lr=[0.00010641298773333333, 0.00010641298773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6090 loss: 1.3167 iter time (s): 4.400 samples/sec: 29.088
g0093:  iteration     6090/10000000 | consumed samples:       779520 | consumed tokens:   1596456960 | elapsed time per iteration (ms): 4432.6 | learning rate: 1.064E-04 | global batch size:   128 | lm loss: 1.288662E+00 | loss scale: 524288.0 | grad norm: 0.673 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.877 | tokens per gpu per second (tgs): 1848.132 | TFLOPs: 14.87 |
g0069: [2024-08-03 01:55:26,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=6100, skipped=3, lr=[0.0001065877504, 0.0001065877504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6100 loss: 1.2948 iter time (s): 4.298 samples/sec: 29.784
g0093:  iteration     6100/10000000 | consumed samples:       780800 | consumed tokens:   1599078400 | elapsed time per iteration (ms): 4329.8 | learning rate: 1.066E-04 | global batch size:   128 | lm loss: 1.283161E+00 | loss scale: 524288.0 | grad norm: 0.682 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.563 | tokens per gpu per second (tgs): 1892.013 | TFLOPs: 15.23 |
g0069: [2024-08-03 01:56:08,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=6110, skipped=3, lr=[0.00010676251306666667, 0.00010676251306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6110 loss: 1.2923 iter time (s): 4.166 samples/sec: 30.721
g0093:  iteration     6110/10000000 | consumed samples:       782080 | consumed tokens:   1601699840 | elapsed time per iteration (ms): 4198.6 | learning rate: 1.068E-04 | global batch size:   128 | lm loss: 1.284429E+00 | loss scale: 524288.0 | grad norm: 0.691 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.486 | tokens per gpu per second (tgs): 1951.118 | TFLOPs: 15.70 |
g0069: [2024-08-03 01:56:48,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=6120, skipped=3, lr=[0.00010693727573333334, 0.00010693727573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6120 loss: 1.2762 iter time (s): 3.977 samples/sec: 32.189
g0093:  iteration     6120/10000000 | consumed samples:       783360 | consumed tokens:   1604321280 | elapsed time per iteration (ms): 4008.6 | learning rate: 1.069E-04 | global batch size:   128 | lm loss: 1.279438E+00 | loss scale: 524288.0 | grad norm: 0.669 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.931 | tokens per gpu per second (tgs): 2043.593 | TFLOPs: 16.45 |
g0069: [2024-08-03 01:57:29,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=6130, skipped=3, lr=[0.0001071120384, 0.0001071120384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6130 loss: 1.2505 iter time (s): 4.056 samples/sec: 31.560
g0093:  iteration     6130/10000000 | consumed samples:       784640 | consumed tokens:   1606942720 | elapsed time per iteration (ms): 4089.1 | learning rate: 1.071E-04 | global batch size:   128 | lm loss: 1.288675E+00 | loss scale: 524288.0 | grad norm: 0.607 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.303 | tokens per gpu per second (tgs): 2003.365 | TFLOPs: 16.12 |
g0069: [2024-08-03 01:58:13,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=6140, skipped=3, lr=[0.00010728680106666667, 0.00010728680106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6140 loss: 1.2887 iter time (s): 4.351 samples/sec: 29.416
g0093:  iteration     6140/10000000 | consumed samples:       785920 | consumed tokens:   1609564160 | elapsed time per iteration (ms): 4384.3 | learning rate: 1.073E-04 | global batch size:   128 | lm loss: 1.279398E+00 | loss scale: 524288.0 | grad norm: 0.783 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.195 | tokens per gpu per second (tgs): 1868.500 | TFLOPs: 15.04 |
g0069: [2024-08-03 01:58:54,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=6150, skipped=3, lr=[0.00010746156373333334, 0.00010746156373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6150 loss: 1.2783 iter time (s): 4.124 samples/sec: 31.040
g0093:  iteration     6150/10000000 | consumed samples:       787200 | consumed tokens:   1612185600 | elapsed time per iteration (ms): 4157.3 | learning rate: 1.075E-04 | global batch size:   128 | lm loss: 1.277070E+00 | loss scale: 524288.0 | grad norm: 0.668 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.789 | tokens per gpu per second (tgs): 1970.492 | TFLOPs: 15.86 |
g0069: [2024-08-03 01:59:34,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=6160, skipped=3, lr=[0.0001076363264, 0.0001076363264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6160 loss: 1.3133 iter time (s): 3.979 samples/sec: 32.170
g0093:  iteration     6160/10000000 | consumed samples:       788480 | consumed tokens:   1614807040 | elapsed time per iteration (ms): 4011.2 | learning rate: 1.076E-04 | global batch size:   128 | lm loss: 1.288885E+00 | loss scale: 524288.0 | grad norm: 0.711 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.910 | tokens per gpu per second (tgs): 2042.262 | TFLOPs: 16.43 |
g0069: [2024-08-03 02:00:14,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=6170, skipped=3, lr=[0.00010781108906666667, 0.00010781108906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6170 loss: 1.2926 iter time (s): 3.973 samples/sec: 32.214
g0093:  iteration     6170/10000000 | consumed samples:       789760 | consumed tokens:   1617428480 | elapsed time per iteration (ms): 4006.1 | learning rate: 1.078E-04 | global batch size:   128 | lm loss: 1.268735E+00 | loss scale: 524288.0 | grad norm: 0.613 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.951 | tokens per gpu per second (tgs): 2044.883 | TFLOPs: 16.46 |
g0069: [2024-08-03 02:00:56,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=6180, skipped=3, lr=[0.00010798585173333333, 0.00010798585173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6180 loss: 1.2946 iter time (s): 4.093 samples/sec: 31.275
g0093:  iteration     6180/10000000 | consumed samples:       791040 | consumed tokens:   1620049920 | elapsed time per iteration (ms): 4125.3 | learning rate: 1.080E-04 | global batch size:   128 | lm loss: 1.268333E+00 | loss scale: 524288.0 | grad norm: 0.683 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.028 | tokens per gpu per second (tgs): 1985.810 | TFLOPs: 15.98 |
g0069: [2024-08-03 02:01:37,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=6190, skipped=3, lr=[0.0001081606144, 0.0001081606144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6190 loss: 1.2729 iter time (s): 4.115 samples/sec: 31.108
g0093:  iteration     6190/10000000 | consumed samples:       792320 | consumed tokens:   1622671360 | elapsed time per iteration (ms): 4147.0 | learning rate: 1.082E-04 | global batch size:   128 | lm loss: 1.267900E+00 | loss scale: 524288.0 | grad norm: 0.666 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.866 | tokens per gpu per second (tgs): 1975.420 | TFLOPs: 15.90 |
g0069: [2024-08-03 02:02:20,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=6200, skipped=3, lr=[0.00010833537706666667, 0.00010833537706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6200 loss: 1.2514 iter time (s): 4.264 samples/sec: 30.015
g0093:  iteration     6200/10000000 | consumed samples:       793600 | consumed tokens:   1625292800 | elapsed time per iteration (ms): 4297.7 | learning rate: 1.083E-04 | global batch size:   128 | lm loss: 1.267737E+00 | loss scale: 524288.0 | grad norm: 0.619 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.783 | tokens per gpu per second (tgs): 1906.133 | TFLOPs: 15.34 |
g0069: [2024-08-03 02:03:02,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=6210, skipped=3, lr=[0.00010851013973333333, 0.00010851013973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6210 loss: 1.2937 iter time (s): 4.186 samples/sec: 30.580
g0093:  iteration     6210/10000000 | consumed samples:       794880 | consumed tokens:   1627914240 | elapsed time per iteration (ms): 4219.3 | learning rate: 1.085E-04 | global batch size:   128 | lm loss: 1.286857E+00 | loss scale: 524288.0 | grad norm: 0.795 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.337 | tokens per gpu per second (tgs): 1941.549 | TFLOPs: 15.62 |
g0069: [2024-08-03 02:03:44,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=6220, skipped=3, lr=[0.0001086849024, 0.0001086849024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6220 loss: 1.3048 iter time (s): 4.127 samples/sec: 31.019
g0093:  iteration     6220/10000000 | consumed samples:       796160 | consumed tokens:   1630535680 | elapsed time per iteration (ms): 4159.2 | learning rate: 1.087E-04 | global batch size:   128 | lm loss: 1.270346E+00 | loss scale: 524288.0 | grad norm: 0.621 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.775 | tokens per gpu per second (tgs): 1969.632 | TFLOPs: 15.85 |
g0069: [2024-08-03 02:04:24,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=6230, skipped=3, lr=[0.00010885966506666666, 0.00010885966506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6230 loss: 1.2691 iter time (s): 4.025 samples/sec: 31.801
g0093:  iteration     6230/10000000 | consumed samples:       797440 | consumed tokens:   1633157120 | elapsed time per iteration (ms): 4057.4 | learning rate: 1.089E-04 | global batch size:   128 | lm loss: 1.276256E+00 | loss scale: 524288.0 | grad norm: 0.694 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.548 | tokens per gpu per second (tgs): 2019.051 | TFLOPs: 16.25 |
g0069: [2024-08-03 02:05:08,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=6240, skipped=3, lr=[0.00010903442773333333, 0.00010903442773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6240 loss: 1.2436 iter time (s): 4.340 samples/sec: 29.494
g0093:  iteration     6240/10000000 | consumed samples:       798720 | consumed tokens:   1635778560 | elapsed time per iteration (ms): 4372.1 | learning rate: 1.090E-04 | global batch size:   128 | lm loss: 1.262252E+00 | loss scale: 524288.0 | grad norm: 0.716 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.277 | tokens per gpu per second (tgs): 1873.706 | TFLOPs: 15.08 |
g0069: [2024-08-03 02:05:50,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=6250, skipped=3, lr=[0.0001092091904, 0.0001092091904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6250 loss: 1.3073 iter time (s): 4.150 samples/sec: 30.841
g0093:  iteration     6250/10000000 | consumed samples:       800000 | consumed tokens:   1638400000 | elapsed time per iteration (ms): 4184.2 | learning rate: 1.092E-04 | global batch size:   128 | lm loss: 1.259578E+00 | loss scale: 524288.0 | grad norm: 0.965 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.591 | tokens per gpu per second (tgs): 1957.826 | TFLOPs: 15.75 |
g0069: [2024-08-03 02:06:33,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=6260, skipped=3, lr=[0.00010938395306666666, 0.00010938395306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6260 loss: 1.2441 iter time (s): 4.283 samples/sec: 29.885
g0093:  iteration     6260/10000000 | consumed samples:       801280 | consumed tokens:   1641021440 | elapsed time per iteration (ms): 4315.9 | learning rate: 1.094E-04 | global batch size:   128 | lm loss: 1.260668E+00 | loss scale: 524288.0 | grad norm: 0.665 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.658 | tokens per gpu per second (tgs): 1898.085 | TFLOPs: 15.27 |
g0069: [2024-08-03 02:07:13,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=6270, skipped=3, lr=[0.00010955871573333333, 0.00010955871573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6270 loss: 1.2399 iter time (s): 3.974 samples/sec: 32.208
g0093:  iteration     6270/10000000 | consumed samples:       802560 | consumed tokens:   1643642880 | elapsed time per iteration (ms): 4006.2 | learning rate: 1.096E-04 | global batch size:   128 | lm loss: 1.265784E+00 | loss scale: 524288.0 | grad norm: 0.593 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.950 | tokens per gpu per second (tgs): 2044.820 | TFLOPs: 16.46 |
g0069: [2024-08-03 02:07:53,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=6280, skipped=3, lr=[0.00010973347840000001, 0.00010973347840000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6280 loss: 1.2286 iter time (s): 3.954 samples/sec: 32.369
g0093:  iteration     6280/10000000 | consumed samples:       803840 | consumed tokens:   1646264320 | elapsed time per iteration (ms): 3986.5 | learning rate: 1.097E-04 | global batch size:   128 | lm loss: 1.255150E+00 | loss scale: 524288.0 | grad norm: 0.625 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.108 | tokens per gpu per second (tgs): 2054.911 | TFLOPs: 16.54 |
g0069: [2024-08-03 02:08:35,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=6290, skipped=3, lr=[0.00010990824106666667, 0.00010990824106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6290 loss: 1.2389 iter time (s): 4.126 samples/sec: 31.022
g0093:  iteration     6290/10000000 | consumed samples:       805120 | consumed tokens:   1648885760 | elapsed time per iteration (ms): 4160.0 | learning rate: 1.099E-04 | global batch size:   128 | lm loss: 1.267920E+00 | loss scale: 524288.0 | grad norm: 0.640 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.769 | tokens per gpu per second (tgs): 1969.212 | TFLOPs: 15.85 |
g0069: [2024-08-03 02:09:16,354] [INFO] [logging.py:96:log_dist] [Rank 0] step=6300, skipped=3, lr=[0.00011008300373333334, 0.00011008300373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6300 loss: 1.2440 iter time (s): 4.093 samples/sec: 31.274
g0093:  iteration     6300/10000000 | consumed samples:       806400 | consumed tokens:   1651507200 | elapsed time per iteration (ms): 4125.1 | learning rate: 1.101E-04 | global batch size:   128 | lm loss: 1.258337E+00 | loss scale: 524288.0 | grad norm: 0.709 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.029 | tokens per gpu per second (tgs): 1985.870 | TFLOPs: 15.98 |
g0069: [2024-08-03 02:09:57,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=6310, skipped=3, lr=[0.00011025776640000001, 0.00011025776640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6310 loss: 1.2528 iter time (s): 4.093 samples/sec: 31.274
g0093:  iteration     6310/10000000 | consumed samples:       807680 | consumed tokens:   1654128640 | elapsed time per iteration (ms): 4125.2 | learning rate: 1.103E-04 | global batch size:   128 | lm loss: 1.245297E+00 | loss scale: 524288.0 | grad norm: 0.703 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.029 | tokens per gpu per second (tgs): 1985.860 | TFLOPs: 15.98 |
g0069: [2024-08-03 02:10:39,180] [INFO] [logging.py:96:log_dist] [Rank 0] step=6320, skipped=3, lr=[0.00011043252906666667, 0.00011043252906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6320 loss: 1.2569 iter time (s): 4.125 samples/sec: 31.031
g0093:  iteration     6320/10000000 | consumed samples:       808960 | consumed tokens:   1656750080 | elapsed time per iteration (ms): 4157.3 | learning rate: 1.104E-04 | global batch size:   128 | lm loss: 1.256363E+00 | loss scale: 524288.0 | grad norm: 0.694 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.789 | tokens per gpu per second (tgs): 1970.514 | TFLOPs: 15.86 |
g0069: [2024-08-03 02:11:19,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=6330, skipped=3, lr=[0.00011060729173333334, 0.00011060729173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6330 loss: 1.2172 iter time (s): 3.952 samples/sec: 32.392
g0093:  iteration     6330/10000000 | consumed samples:       810240 | consumed tokens:   1659371520 | elapsed time per iteration (ms): 3984.3 | learning rate: 1.106E-04 | global batch size:   128 | lm loss: 1.249632E+00 | loss scale: 524288.0 | grad norm: 0.684 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.126 | tokens per gpu per second (tgs): 2056.070 | TFLOPs: 16.55 |
g0069: [2024-08-03 02:11:59,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=6340, skipped=3, lr=[0.0001107820544, 0.0001107820544], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6340 loss: 1.2890 iter time (s): 4.056 samples/sec: 31.561
g0093:  iteration     6340/10000000 | consumed samples:       811520 | consumed tokens:   1661992960 | elapsed time per iteration (ms): 4088.1 | learning rate: 1.108E-04 | global batch size:   128 | lm loss: 1.261337E+00 | loss scale: 524288.0 | grad norm: 0.617 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.310 | tokens per gpu per second (tgs): 2003.856 | TFLOPs: 16.13 |
g0069: [2024-08-03 02:12:41,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=6350, skipped=3, lr=[0.00011095681706666667, 0.00011095681706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6350 loss: 1.2462 iter time (s): 4.157 samples/sec: 30.793
g0093:  iteration     6350/10000000 | consumed samples:       812800 | consumed tokens:   1664614400 | elapsed time per iteration (ms): 4189.2 | learning rate: 1.110E-04 | global batch size:   128 | lm loss: 1.253281E+00 | loss scale: 524288.0 | grad norm: 0.626 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.555 | tokens per gpu per second (tgs): 1955.488 | TFLOPs: 15.74 |
g0069: [2024-08-03 02:13:22,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=6360, skipped=3, lr=[0.00011113157973333334, 0.00011113157973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6360 loss: 1.2464 iter time (s): 4.057 samples/sec: 31.549
g0093:  iteration     6360/10000000 | consumed samples:       814080 | consumed tokens:   1667235840 | elapsed time per iteration (ms): 4089.2 | learning rate: 1.111E-04 | global batch size:   128 | lm loss: 1.234102E+00 | loss scale: 524288.0 | grad norm: 0.764 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.302 | tokens per gpu per second (tgs): 2003.321 | TFLOPs: 16.12 |
g0069: [2024-08-03 02:14:05,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=6370, skipped=3, lr=[0.0001113063424, 0.0001113063424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6370 loss: 1.2199 iter time (s): 4.246 samples/sec: 30.143
g0093:  iteration     6370/10000000 | consumed samples:       815360 | consumed tokens:   1669857280 | elapsed time per iteration (ms): 4278.6 | learning rate: 1.113E-04 | global batch size:   128 | lm loss: 1.240877E+00 | loss scale: 524288.0 | grad norm: 0.652 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.916 | tokens per gpu per second (tgs): 1914.643 | TFLOPs: 15.41 |
g0069: [2024-08-03 02:14:47,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=6380, skipped=3, lr=[0.00011148110506666667, 0.00011148110506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6380 loss: 1.2165 iter time (s): 4.138 samples/sec: 30.933
g0093:  iteration     6380/10000000 | consumed samples:       816640 | consumed tokens:   1672478720 | elapsed time per iteration (ms): 4170.0 | learning rate: 1.115E-04 | global batch size:   128 | lm loss: 1.258648E+00 | loss scale: 524288.0 | grad norm: 0.576 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.696 | tokens per gpu per second (tgs): 1964.522 | TFLOPs: 15.81 |
g0069: [2024-08-03 02:15:25,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=6390, skipped=3, lr=[0.00011165586773333334, 0.00011165586773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6390 loss: 1.2618 iter time (s): 3.844 samples/sec: 33.299
g0093:  iteration     6390/10000000 | consumed samples:       817920 | consumed tokens:   1675100160 | elapsed time per iteration (ms): 3876.5 | learning rate: 1.117E-04 | global batch size:   128 | lm loss: 1.233342E+00 | loss scale: 524288.0 | grad norm: 0.658 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.019 | tokens per gpu per second (tgs): 2113.228 | TFLOPs: 17.01 |
g0069: [2024-08-03 02:16:05,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=6400, skipped=3, lr=[0.0001118306304, 0.0001118306304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6400 loss: 1.2315 iter time (s): 3.875 samples/sec: 33.031
g0093:  iteration     6400/10000000 | consumed samples:       819200 | consumed tokens:   1677721600 | elapsed time per iteration (ms): 3908.1 | learning rate: 1.118E-04 | global batch size:   128 | lm loss: 1.241712E+00 | loss scale: 524288.0 | grad norm: 0.656 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.752 | tokens per gpu per second (tgs): 2096.160 | TFLOPs: 16.87 |
g0069: [2024-08-03 02:16:47,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=6410, skipped=3, lr=[0.00011200539306666667, 0.00011200539306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6410 loss: 1.2468 iter time (s): 4.230 samples/sec: 30.258
g0093:  iteration     6410/10000000 | consumed samples:       820480 | consumed tokens:   1680343040 | elapsed time per iteration (ms): 4262.3 | learning rate: 1.120E-04 | global batch size:   128 | lm loss: 1.233126E+00 | loss scale: 524288.0 | grad norm: 0.785 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.031 | tokens per gpu per second (tgs): 1921.955 | TFLOPs: 15.47 |
g0069: [2024-08-03 02:17:30,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=6420, skipped=3, lr=[0.00011218015573333333, 0.00011218015573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6420 loss: 1.2718 iter time (s): 4.280 samples/sec: 29.906
g0093:  iteration     6420/10000000 | consumed samples:       821760 | consumed tokens:   1682964480 | elapsed time per iteration (ms): 4312.2 | learning rate: 1.122E-04 | global batch size:   128 | lm loss: 1.251549E+00 | loss scale: 524288.0 | grad norm: 0.648 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.683 | tokens per gpu per second (tgs): 1899.728 | TFLOPs: 15.29 |
g0069: [2024-08-03 02:18:12,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=6430, skipped=3, lr=[0.0001123549184, 0.0001123549184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6430 loss: 1.2284 iter time (s): 4.160 samples/sec: 30.771
g0093:  iteration     6430/10000000 | consumed samples:       823040 | consumed tokens:   1685585920 | elapsed time per iteration (ms): 4193.1 | learning rate: 1.124E-04 | global batch size:   128 | lm loss: 1.240701E+00 | loss scale: 524288.0 | grad norm: 0.663 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.526 | tokens per gpu per second (tgs): 1953.675 | TFLOPs: 15.72 |
g0069: [2024-08-03 02:18:56,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=6440, skipped=3, lr=[0.00011252968106666667, 0.00011252968106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6440 loss: 1.2129 iter time (s): 4.304 samples/sec: 29.743
g0093:  iteration     6440/10000000 | consumed samples:       824320 | consumed tokens:   1688207360 | elapsed time per iteration (ms): 4336.2 | learning rate: 1.125E-04 | global batch size:   128 | lm loss: 1.240768E+00 | loss scale: 524288.0 | grad norm: 0.664 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.519 | tokens per gpu per second (tgs): 1889.205 | TFLOPs: 15.20 |
g0069: [2024-08-03 02:19:38,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=6450, skipped=3, lr=[0.00011270444373333333, 0.00011270444373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6450 loss: 1.2484 iter time (s): 4.178 samples/sec: 30.636
g0093:  iteration     6450/10000000 | consumed samples:       825600 | consumed tokens:   1690828800 | elapsed time per iteration (ms): 4210.6 | learning rate: 1.127E-04 | global batch size:   128 | lm loss: 1.246712E+00 | loss scale: 524288.0 | grad norm: 0.759 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.399 | tokens per gpu per second (tgs): 1945.550 | TFLOPs: 15.66 |
g0069: [2024-08-03 02:20:19,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=6460, skipped=3, lr=[0.00011287920640000001, 0.00011287920640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6460 loss: 1.2381 iter time (s): 4.052 samples/sec: 31.593
g0093:  iteration     6460/10000000 | consumed samples:       826880 | consumed tokens:   1693450240 | elapsed time per iteration (ms): 4084.6 | learning rate: 1.129E-04 | global batch size:   128 | lm loss: 1.234261E+00 | loss scale: 524288.0 | grad norm: 0.553 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.338 | tokens per gpu per second (tgs): 2005.604 | TFLOPs: 16.14 |
g0069: [2024-08-03 02:20:59,581] [INFO] [logging.py:96:log_dist] [Rank 0] step=6470, skipped=3, lr=[0.00011305396906666668, 0.00011305396906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6470 loss: 1.2330 iter time (s): 4.024 samples/sec: 31.809
g0093:  iteration     6470/10000000 | consumed samples:       828160 | consumed tokens:   1696071680 | elapsed time per iteration (ms): 4056.8 | learning rate: 1.131E-04 | global batch size:   128 | lm loss: 1.240444E+00 | loss scale: 524288.0 | grad norm: 0.583 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.552 | tokens per gpu per second (tgs): 2019.308 | TFLOPs: 16.25 |
g0069: [2024-08-03 02:21:49,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=6480, skipped=3, lr=[0.00011322873173333334, 0.00011322873173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6480 loss: 1.2032 iter time (s): 4.926 samples/sec: 25.984
g0093:  iteration     6480/10000000 | consumed samples:       829440 | consumed tokens:   1698693120 | elapsed time per iteration (ms): 4958.3 | learning rate: 1.132E-04 | global batch size:   128 | lm loss: 1.223621E+00 | loss scale: 524288.0 | grad norm: 0.721 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.815 | tokens per gpu per second (tgs): 1652.169 | TFLOPs: 13.30 |
g0069: [2024-08-03 02:22:45,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=6490, skipped=3, lr=[0.00011340349440000001, 0.00011340349440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6490 loss: 1.2223 iter time (s): 5.640 samples/sec: 22.695
g0093:  iteration     6490/10000000 | consumed samples:       830720 | consumed tokens:   1701314560 | elapsed time per iteration (ms): 5673.1 | learning rate: 1.134E-04 | global batch size:   128 | lm loss: 1.236121E+00 | loss scale: 524288.0 | grad norm: 0.596 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.563 | tokens per gpu per second (tgs): 1444.015 | TFLOPs: 11.62 |
g0069: [2024-08-03 02:23:42,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=6500, skipped=3, lr=[0.00011357825706666668, 0.00011357825706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6500 loss: 1.2326 iter time (s): 5.653 samples/sec: 22.641
g0093:  iteration     6500/10000000 | consumed samples:       832000 | consumed tokens:   1703936000 | elapsed time per iteration (ms): 5685.4 | learning rate: 1.136E-04 | global batch size:   128 | lm loss: 1.219290E+00 | loss scale: 524288.0 | grad norm: 0.637 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.514 | tokens per gpu per second (tgs): 1440.896 | TFLOPs: 11.60 |
g0069: [2024-08-03 02:24:35,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=6510, skipped=3, lr=[0.00011375301973333334, 0.00011375301973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6510 loss: 1.1829 iter time (s): 5.259 samples/sec: 24.339
g0093:  iteration     6510/10000000 | consumed samples:       833280 | consumed tokens:   1706557440 | elapsed time per iteration (ms): 5291.9 | learning rate: 1.138E-04 | global batch size:   128 | lm loss: 1.226612E+00 | loss scale: 524288.0 | grad norm: 0.531 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.188 | tokens per gpu per second (tgs): 1548.034 | TFLOPs: 12.46 |
g0069: [2024-08-03 02:25:19,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=6520, skipped=3, lr=[0.00011392778240000001, 0.00011392778240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6520 loss: 1.2294 iter time (s): 4.335 samples/sec: 29.525
g0093:  iteration     6520/10000000 | consumed samples:       834560 | consumed tokens:   1709178880 | elapsed time per iteration (ms): 4368.9 | learning rate: 1.139E-04 | global batch size:   128 | lm loss: 1.230876E+00 | loss scale: 524288.0 | grad norm: 0.650 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.298 | tokens per gpu per second (tgs): 1875.066 | TFLOPs: 15.09 |
g0069: [2024-08-03 02:26:00,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=6530, skipped=3, lr=[0.00011410254506666667, 0.00011410254506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6530 loss: 1.2170 iter time (s): 4.045 samples/sec: 31.643
g0093:  iteration     6530/10000000 | consumed samples:       835840 | consumed tokens:   1711800320 | elapsed time per iteration (ms): 4078.5 | learning rate: 1.141E-04 | global batch size:   128 | lm loss: 1.224268E+00 | loss scale: 524288.0 | grad norm: 0.664 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.384 | tokens per gpu per second (tgs): 2008.579 | TFLOPs: 16.16 |
g0090: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0090: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0093: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0090: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0090: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0069: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0092: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0069: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0069: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0069: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0092: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0092: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0088: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0086: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0086: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0092: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0091: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0088: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0087: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0091: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0091: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0088: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0091: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0086: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0087: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0087: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0087: [2024-08-03 02:26:12,471] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0087: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0090: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0069: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0091: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0093: [2024-08-03 02:26:12,472] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0069: [2024-08-03 02:26:40,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=6540, skipped=3, lr=[0.00011427730773333334, 0.00011427730773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6540 loss: 1.2043 iter time (s): 3.982 samples/sec: 32.149
g0093:  iteration     6540/10000000 | consumed samples:       837120 | consumed tokens:   1714421760 | elapsed time per iteration (ms): 4013.5 | learning rate: 1.143E-04 | global batch size:   128 | lm loss: 1.220077E+00 | loss scale: 1048576.0 | grad norm: 0.576 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.892 | tokens per gpu per second (tgs): 2041.101 | TFLOPs: 16.43 |
g0069: [2024-08-03 02:27:24,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=6550, skipped=3, lr=[0.0001144520704, 0.0001144520704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6550 loss: 1.2204 iter time (s): 4.368 samples/sec: 29.304
g0093:  iteration     6550/10000000 | consumed samples:       838400 | consumed tokens:   1717043200 | elapsed time per iteration (ms): 4400.2 | learning rate: 1.145E-04 | global batch size:   128 | lm loss: 1.219028E+00 | loss scale: 1048576.0 | grad norm: 0.663 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.090 | tokens per gpu per second (tgs): 1861.751 | TFLOPs: 14.98 |
g0069: [2024-08-03 02:28:08,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=6560, skipped=3, lr=[0.00011462683306666667, 0.00011462683306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6560 loss: 1.1934 iter time (s): 4.411 samples/sec: 29.021
g0093:  iteration     6560/10000000 | consumed samples:       839680 | consumed tokens:   1719664640 | elapsed time per iteration (ms): 4442.7 | learning rate: 1.146E-04 | global batch size:   128 | lm loss: 1.218914E+00 | loss scale: 1048576.0 | grad norm: 0.593 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.811 | tokens per gpu per second (tgs): 1843.933 | TFLOPs: 14.84 |
g0069: [2024-08-03 02:28:49,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=6570, skipped=3, lr=[0.00011480159573333334, 0.00011480159573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6570 loss: 1.2604 iter time (s): 4.073 samples/sec: 31.427
g0093:  iteration     6570/10000000 | consumed samples:       840960 | consumed tokens:   1722286080 | elapsed time per iteration (ms): 4105.3 | learning rate: 1.148E-04 | global batch size:   128 | lm loss: 1.228918E+00 | loss scale: 1048576.0 | grad norm: 0.648 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.180 | tokens per gpu per second (tgs): 1995.490 | TFLOPs: 16.06 |
g0069: [2024-08-03 02:29:31,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=6580, skipped=3, lr=[0.0001149763584, 0.0001149763584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6580 loss: 1.1932 iter time (s): 4.106 samples/sec: 31.174
g0093:  iteration     6580/10000000 | consumed samples:       842240 | consumed tokens:   1724907520 | elapsed time per iteration (ms): 4138.5 | learning rate: 1.150E-04 | global batch size:   128 | lm loss: 1.207113E+00 | loss scale: 1048576.0 | grad norm: 0.631 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.929 | tokens per gpu per second (tgs): 1979.438 | TFLOPs: 15.93 |
g0090: [2024-08-03 02:29:58,120] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 6585
g0090: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0090: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 6585
g0090: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 6585
g0090: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0090: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: [2024-08-03 02:29:58,120] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: [2024-08-03 02:29:58,120] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: Grad overflow on iteration 6585
g0086: Grad overflow on iteration 6585
g0069: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: Grad overflow on iteration 6585
g0069: Grad overflow on iteration 6585
g0088: [2024-08-03 02:29:58,120] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: [2024-08-03 02:29:58,120] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 6585
g0091: [2024-08-03 02:29:58,120] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0086: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 6585
g0069: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0088: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0087: Grad overflow on iteration 6585
g0092: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: Grad overflow on iteration 6585
g0088: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: [2024-08-03 02:29:58,120] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 6585
g0088: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: Grad overflow on iteration 6585
g0069: Grad overflow on iteration 6585
g0093: Grad overflow on iteration 6585
g0087: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0093: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0092: Grad overflow on iteration 6585
g0087: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: Grad overflow on iteration 6585
g0087: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: Grad overflow on iteration 6585
g0090: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0091: Grad overflow on iteration 6585
g0069: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: Grad overflow on iteration 6585
g0086: Grad overflow on iteration 6585
g0092: Grad overflow on iteration 6585
g0087: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0086: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0091: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0069: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0091: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0086: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: Grad overflow on iteration 6585
g0092: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0093: Grad overflow on iteration 6585
g0087: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0087: Grad overflow on iteration 6585
g0093: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: Grad overflow on iteration 6585
g0093: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0086: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0087: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0086: Grad overflow on iteration 6585
g0069: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0093: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: Grad overflow on iteration 6585
g0088: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0087: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0092: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0086: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0091: Grad overflow on iteration 6585
g0086: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0088: Grad overflow on iteration 6585
g0092: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0091: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 6585
g0091: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0091: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0091: Grad overflow on iteration 6585
g0069: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0088: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0091: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0091: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0069: [2024-08-03 02:29:58,121] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
g0093: [2024-08-03 02:29:58,121] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0069: [2024-08-03 02:30:15,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=6590, skipped=4, lr=[0.00011515112106666667, 0.00011515112106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6590 loss: 1.2127 iter time (s): 4.408 samples/sec: 29.036
g0093:  iteration     6590/10000000 | consumed samples:       843520 | consumed tokens:   1727528960 | elapsed time per iteration (ms): 4440.4 | learning rate: 1.152E-04 | global batch size:   128 | lm loss: 1.215335E+00 | loss scale: 524288.0 | grad norm: 0.668 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.826 | tokens per gpu per second (tgs): 1844.867 | TFLOPs: 14.85 |
g0069: [2024-08-03 02:30:59,169] [INFO] [logging.py:96:log_dist] [Rank 0] step=6600, skipped=4, lr=[0.00011532588373333334, 0.00011532588373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6600 loss: 1.1860 iter time (s): 4.330 samples/sec: 29.562
g0093:  iteration     6600/10000000 | consumed samples:       844800 | consumed tokens:   1730150400 | elapsed time per iteration (ms): 4362.0 | learning rate: 1.153E-04 | global batch size:   128 | lm loss: 1.231983E+00 | loss scale: 524288.0 | grad norm: 0.612 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.345 | tokens per gpu per second (tgs): 1878.050 | TFLOPs: 15.11 |
g0069: [2024-08-03 02:31:40,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=6610, skipped=4, lr=[0.0001155006464, 0.0001155006464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6610 loss: 1.2127 iter time (s): 4.143 samples/sec: 30.898
g0093:  iteration     6610/10000000 | consumed samples:       846080 | consumed tokens:   1732771840 | elapsed time per iteration (ms): 4174.9 | learning rate: 1.155E-04 | global batch size:   128 | lm loss: 1.219180E+00 | loss scale: 524288.0 | grad norm: 0.542 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.660 | tokens per gpu per second (tgs): 1962.224 | TFLOPs: 15.79 |
g0069: [2024-08-03 02:32:26,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=6620, skipped=4, lr=[0.00011567540906666667, 0.00011567540906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6620 loss: 1.2052 iter time (s): 4.538 samples/sec: 28.207
g0093:  iteration     6620/10000000 | consumed samples:       847360 | consumed tokens:   1735393280 | elapsed time per iteration (ms): 4571.7 | learning rate: 1.157E-04 | global batch size:   128 | lm loss: 1.212966E+00 | loss scale: 524288.0 | grad norm: 0.721 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.998 | tokens per gpu per second (tgs): 1791.890 | TFLOPs: 14.42 |
g0069: [2024-08-03 02:33:08,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=6630, skipped=4, lr=[0.00011585017173333333, 0.00011585017173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6630 loss: 1.2051 iter time (s): 4.132 samples/sec: 30.975
g0093:  iteration     6630/10000000 | consumed samples:       848640 | consumed tokens:   1738014720 | elapsed time per iteration (ms): 4165.5 | learning rate: 1.159E-04 | global batch size:   128 | lm loss: 1.221664E+00 | loss scale: 524288.0 | grad norm: 0.549 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.728 | tokens per gpu per second (tgs): 1966.610 | TFLOPs: 15.83 |
g0069: [2024-08-03 02:33:50,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=6640, skipped=4, lr=[0.00011602493440000001, 0.00011602493440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6640 loss: 1.2615 iter time (s): 4.193 samples/sec: 30.524
g0093:  iteration     6640/10000000 | consumed samples:       849920 | consumed tokens:   1740636160 | elapsed time per iteration (ms): 4225.7 | learning rate: 1.160E-04 | global batch size:   128 | lm loss: 1.217783E+00 | loss scale: 524288.0 | grad norm: 0.695 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.291 | tokens per gpu per second (tgs): 1938.621 | TFLOPs: 15.60 |
g0069: [2024-08-03 02:34:33,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=6650, skipped=4, lr=[0.00011619969706666668, 0.00011619969706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6650 loss: 1.1747 iter time (s): 4.280 samples/sec: 29.903
g0093:  iteration     6650/10000000 | consumed samples:       851200 | consumed tokens:   1743257600 | elapsed time per iteration (ms): 4313.7 | learning rate: 1.162E-04 | global batch size:   128 | lm loss: 1.208812E+00 | loss scale: 524288.0 | grad norm: 0.552 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.673 | tokens per gpu per second (tgs): 1899.074 | TFLOPs: 15.28 |
g0069: [2024-08-03 02:35:14,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=6660, skipped=4, lr=[0.00011637445973333335, 0.00011637445973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6660 loss: 1.2220 iter time (s): 4.008 samples/sec: 31.936
g0093:  iteration     6660/10000000 | consumed samples:       852480 | consumed tokens:   1745879040 | elapsed time per iteration (ms): 4041.6 | learning rate: 1.164E-04 | global batch size:   128 | lm loss: 1.207148E+00 | loss scale: 524288.0 | grad norm: 0.561 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.670 | tokens per gpu per second (tgs): 2026.903 | TFLOPs: 16.31 |
g0069: [2024-08-03 02:35:59,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=6670, skipped=4, lr=[0.00011654922240000001, 0.00011654922240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6670 loss: 1.2112 iter time (s): 4.516 samples/sec: 28.341
g0093:  iteration     6670/10000000 | consumed samples:       853760 | consumed tokens:   1748500480 | elapsed time per iteration (ms): 4548.5 | learning rate: 1.165E-04 | global batch size:   128 | lm loss: 1.212601E+00 | loss scale: 524288.0 | grad norm: 0.582 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.141 | tokens per gpu per second (tgs): 1801.038 | TFLOPs: 14.49 |
g0069: [2024-08-03 02:36:40,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=6680, skipped=4, lr=[0.00011672398506666668, 0.00011672398506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6680 loss: 1.2033 iter time (s): 4.059 samples/sec: 31.535
g0093:  iteration     6680/10000000 | consumed samples:       855040 | consumed tokens:   1751121920 | elapsed time per iteration (ms): 4091.5 | learning rate: 1.167E-04 | global batch size:   128 | lm loss: 1.215848E+00 | loss scale: 524288.0 | grad norm: 0.549 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.285 | tokens per gpu per second (tgs): 2002.215 | TFLOPs: 16.11 |
g0069: [2024-08-03 02:37:20,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=6690, skipped=4, lr=[0.00011689874773333334, 0.00011689874773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6690 loss: 1.2247 iter time (s): 4.001 samples/sec: 31.995
g0093:  iteration     6690/10000000 | consumed samples:       856320 | consumed tokens:   1753743360 | elapsed time per iteration (ms): 4033.0 | learning rate: 1.169E-04 | global batch size:   128 | lm loss: 1.204912E+00 | loss scale: 524288.0 | grad norm: 0.731 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.738 | tokens per gpu per second (tgs): 2031.218 | TFLOPs: 16.35 |
g0069: [2024-08-03 02:38:02,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=6700, skipped=4, lr=[0.00011707351040000001, 0.00011707351040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6700 loss: 1.1784 iter time (s): 4.119 samples/sec: 31.079
g0093:  iteration     6700/10000000 | consumed samples:       857600 | consumed tokens:   1756364800 | elapsed time per iteration (ms): 4150.6 | learning rate: 1.171E-04 | global batch size:   128 | lm loss: 1.211131E+00 | loss scale: 524288.0 | grad norm: 0.690 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.839 | tokens per gpu per second (tgs): 1973.704 | TFLOPs: 15.88 |
g0069: [2024-08-03 02:38:43,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=6710, skipped=4, lr=[0.00011724827306666668, 0.00011724827306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6710 loss: 1.1712 iter time (s): 4.081 samples/sec: 31.368
g0093:  iteration     6710/10000000 | consumed samples:       858880 | consumed tokens:   1758986240 | elapsed time per iteration (ms): 4112.9 | learning rate: 1.172E-04 | global batch size:   128 | lm loss: 1.203010E+00 | loss scale: 524288.0 | grad norm: 0.613 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.122 | tokens per gpu per second (tgs): 1991.781 | TFLOPs: 16.03 |
g0069: [2024-08-03 02:39:26,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=6720, skipped=4, lr=[0.00011742303573333334, 0.00011742303573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6720 loss: 1.1824 iter time (s): 4.230 samples/sec: 30.261
g0093:  iteration     6720/10000000 | consumed samples:       860160 | consumed tokens:   1761607680 | elapsed time per iteration (ms): 4262.0 | learning rate: 1.174E-04 | global batch size:   128 | lm loss: 1.213584E+00 | loss scale: 524288.0 | grad norm: 0.650 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.033 | tokens per gpu per second (tgs): 1922.102 | TFLOPs: 15.47 |
g0069: [2024-08-03 02:40:06,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=6730, skipped=4, lr=[0.00011759779840000001, 0.00011759779840000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6730 loss: 1.2012 iter time (s): 3.996 samples/sec: 32.034
g0093:  iteration     6730/10000000 | consumed samples:       861440 | consumed tokens:   1764229120 | elapsed time per iteration (ms): 4029.5 | learning rate: 1.176E-04 | global batch size:   128 | lm loss: 1.197305E+00 | loss scale: 524288.0 | grad norm: 0.539 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.766 | tokens per gpu per second (tgs): 2033.029 | TFLOPs: 16.36 |
g0069: [2024-08-03 02:40:50,731] [INFO] [logging.py:96:log_dist] [Rank 0] step=6740, skipped=4, lr=[0.00011777256106666667, 0.00011777256106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6740 loss: 1.2034 iter time (s): 4.403 samples/sec: 29.074
g0093:  iteration     6740/10000000 | consumed samples:       862720 | consumed tokens:   1766850560 | elapsed time per iteration (ms): 4435.0 | learning rate: 1.178E-04 | global batch size:   128 | lm loss: 1.212054E+00 | loss scale: 524288.0 | grad norm: 0.546 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.861 | tokens per gpu per second (tgs): 1847.115 | TFLOPs: 14.86 |
g0069: [2024-08-03 02:41:32,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=6750, skipped=4, lr=[0.00011794732373333334, 0.00011794732373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6750 loss: 1.2169 iter time (s): 4.187 samples/sec: 30.571
g0093:  iteration     6750/10000000 | consumed samples:       864000 | consumed tokens:   1769472000 | elapsed time per iteration (ms): 4219.4 | learning rate: 1.179E-04 | global batch size:   128 | lm loss: 1.203055E+00 | loss scale: 524288.0 | grad norm: 0.857 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.336 | tokens per gpu per second (tgs): 1941.518 | TFLOPs: 15.62 |
g0069: [2024-08-03 02:42:16,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=6760, skipped=4, lr=[0.00011812208640000001, 0.00011812208640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6760 loss: 1.2227 iter time (s): 4.346 samples/sec: 29.454
g0093:  iteration     6760/10000000 | consumed samples:       865280 | consumed tokens:   1772093440 | elapsed time per iteration (ms): 4378.2 | learning rate: 1.181E-04 | global batch size:   128 | lm loss: 1.194997E+00 | loss scale: 524288.0 | grad norm: 0.573 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.236 | tokens per gpu per second (tgs): 1871.088 | TFLOPs: 15.06 |
g0069: [2024-08-03 02:42:57,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=6770, skipped=4, lr=[0.00011829684906666667, 0.00011829684906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6770 loss: 1.1914 iter time (s): 4.081 samples/sec: 31.364
g0093:  iteration     6770/10000000 | consumed samples:       866560 | consumed tokens:   1774714880 | elapsed time per iteration (ms): 4113.6 | learning rate: 1.183E-04 | global batch size:   128 | lm loss: 1.209315E+00 | loss scale: 524288.0 | grad norm: 0.693 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.117 | tokens per gpu per second (tgs): 1991.458 | TFLOPs: 16.03 |
g0069: [2024-08-03 02:43:40,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=6780, skipped=4, lr=[0.00011847161173333334, 0.00011847161173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6780 loss: 1.1868 iter time (s): 4.216 samples/sec: 30.358
g0093:  iteration     6780/10000000 | consumed samples:       867840 | consumed tokens:   1777336320 | elapsed time per iteration (ms): 4249.5 | learning rate: 1.185E-04 | global batch size:   128 | lm loss: 1.198695E+00 | loss scale: 524288.0 | grad norm: 0.565 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.121 | tokens per gpu per second (tgs): 1927.760 | TFLOPs: 15.51 |
g0069: [2024-08-03 02:44:23,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=6790, skipped=4, lr=[0.0001186463744, 0.0001186463744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6790 loss: 1.2062 iter time (s): 4.241 samples/sec: 30.179
g0093:  iteration     6790/10000000 | consumed samples:       869120 | consumed tokens:   1779957760 | elapsed time per iteration (ms): 4273.6 | learning rate: 1.186E-04 | global batch size:   128 | lm loss: 1.182956E+00 | loss scale: 524288.0 | grad norm: 0.597 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.951 | tokens per gpu per second (tgs): 1916.874 | TFLOPs: 15.43 |
g0069: [2024-08-03 02:45:06,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=6800, skipped=4, lr=[0.00011882113706666667, 0.00011882113706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6800 loss: 1.1922 iter time (s): 4.330 samples/sec: 29.563
g0093:  iteration     6800/10000000 | consumed samples:       870400 | consumed tokens:   1782579200 | elapsed time per iteration (ms): 4363.1 | learning rate: 1.188E-04 | global batch size:   128 | lm loss: 1.195969E+00 | loss scale: 524288.0 | grad norm: 0.542 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.337 | tokens per gpu per second (tgs): 1877.574 | TFLOPs: 15.11 |
g0069: [2024-08-03 02:45:49,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=6810, skipped=4, lr=[0.00011899589973333335, 0.00011899589973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6810 loss: 1.1892 iter time (s): 4.286 samples/sec: 29.867
g0093:  iteration     6810/10000000 | consumed samples:       871680 | consumed tokens:   1785200640 | elapsed time per iteration (ms): 4317.9 | learning rate: 1.190E-04 | global batch size:   128 | lm loss: 1.192338E+00 | loss scale: 524288.0 | grad norm: 0.495 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.644 | tokens per gpu per second (tgs): 1897.225 | TFLOPs: 15.27 |
g0069: [2024-08-03 02:46:31,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=6820, skipped=4, lr=[0.00011917066240000002, 0.00011917066240000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6820 loss: 1.1892 iter time (s): 4.127 samples/sec: 31.012
g0093:  iteration     6820/10000000 | consumed samples:       872960 | consumed tokens:   1787822080 | elapsed time per iteration (ms): 4160.8 | learning rate: 1.192E-04 | global batch size:   128 | lm loss: 1.204657E+00 | loss scale: 524288.0 | grad norm: 0.628 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.763 | tokens per gpu per second (tgs): 1968.853 | TFLOPs: 15.84 |
g0069: [2024-08-03 02:47:16,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=6830, skipped=4, lr=[0.00011934542506666668, 0.00011934542506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6830 loss: 1.1884 iter time (s): 4.439 samples/sec: 28.838
g0093:  iteration     6830/10000000 | consumed samples:       874240 | consumed tokens:   1790443520 | elapsed time per iteration (ms): 4471.3 | learning rate: 1.193E-04 | global batch size:   128 | lm loss: 1.204674E+00 | loss scale: 524288.0 | grad norm: 0.521 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.627 | tokens per gpu per second (tgs): 1832.145 | TFLOPs: 14.74 |
g0069: [2024-08-03 02:47:57,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=6840, skipped=4, lr=[0.00011952018773333335, 0.00011952018773333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6840 loss: 1.2076 iter time (s): 4.060 samples/sec: 31.523
g0093:  iteration     6840/10000000 | consumed samples:       875520 | consumed tokens:   1793064960 | elapsed time per iteration (ms): 4092.8 | learning rate: 1.195E-04 | global batch size:   128 | lm loss: 1.178663E+00 | loss scale: 524288.0 | grad norm: 0.615 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.274 | tokens per gpu per second (tgs): 2001.540 | TFLOPs: 16.11 |
g0069: [2024-08-03 02:48:38,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=6850, skipped=4, lr=[0.00011969495040000001, 0.00011969495040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6850 loss: 1.1597 iter time (s): 4.153 samples/sec: 30.818
g0093:  iteration     6850/10000000 | consumed samples:       876800 | consumed tokens:   1795686400 | elapsed time per iteration (ms): 4185.8 | learning rate: 1.197E-04 | global batch size:   128 | lm loss: 1.194596E+00 | loss scale: 524288.0 | grad norm: 0.541 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.579 | tokens per gpu per second (tgs): 1957.081 | TFLOPs: 15.75 |
g0069: [2024-08-03 02:49:19,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=6860, skipped=4, lr=[0.00011986971306666668, 0.00011986971306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6860 loss: 1.1902 iter time (s): 4.028 samples/sec: 31.781
g0093:  iteration     6860/10000000 | consumed samples:       878080 | consumed tokens:   1798307840 | elapsed time per iteration (ms): 4059.5 | learning rate: 1.199E-04 | global batch size:   128 | lm loss: 1.182174E+00 | loss scale: 524288.0 | grad norm: 0.510 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.531 | tokens per gpu per second (tgs): 2017.960 | TFLOPs: 16.24 |
g0069: [2024-08-03 02:50:01,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=6870, skipped=4, lr=[0.00012004447573333335, 0.00012004447573333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6870 loss: 1.1997 iter time (s): 4.127 samples/sec: 31.012
g0093:  iteration     6870/10000000 | consumed samples:       879360 | consumed tokens:   1800929280 | elapsed time per iteration (ms): 4160.2 | learning rate: 1.200E-04 | global batch size:   128 | lm loss: 1.197171E+00 | loss scale: 524288.0 | grad norm: 0.636 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.768 | tokens per gpu per second (tgs): 1969.128 | TFLOPs: 15.85 |
g0069: [2024-08-03 02:50:42,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=6880, skipped=4, lr=[0.00012021923840000001, 0.00012021923840000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6880 loss: 1.1846 iter time (s): 4.101 samples/sec: 31.214
g0093:  iteration     6880/10000000 | consumed samples:       880640 | consumed tokens:   1803550720 | elapsed time per iteration (ms): 4133.3 | learning rate: 1.202E-04 | global batch size:   128 | lm loss: 1.194134E+00 | loss scale: 524288.0 | grad norm: 0.597 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.968 | tokens per gpu per second (tgs): 1981.939 | TFLOPs: 15.95 |
g0069: [2024-08-03 02:51:24,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=6890, skipped=4, lr=[0.00012039400106666668, 0.00012039400106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6890 loss: 1.1694 iter time (s): 4.204 samples/sec: 30.449
g0093:  iteration     6890/10000000 | consumed samples:       881920 | consumed tokens:   1806172160 | elapsed time per iteration (ms): 4236.4 | learning rate: 1.204E-04 | global batch size:   128 | lm loss: 1.187830E+00 | loss scale: 524288.0 | grad norm: 0.572 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.214 | tokens per gpu per second (tgs): 1933.713 | TFLOPs: 15.56 |
g0069: [2024-08-03 02:52:08,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=6900, skipped=4, lr=[0.00012056876373333335, 0.00012056876373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6900 loss: 1.1867 iter time (s): 4.326 samples/sec: 29.588
g0093:  iteration     6900/10000000 | consumed samples:       883200 | consumed tokens:   1808793600 | elapsed time per iteration (ms): 4382.4 | learning rate: 1.206E-04 | global batch size:   128 | lm loss: 1.203489E+00 | loss scale: 524288.0 | grad norm: 0.563 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.208 | tokens per gpu per second (tgs): 1869.311 | TFLOPs: 15.04 |
g0069: [2024-08-03 02:52:49,593] [INFO] [logging.py:96:log_dist] [Rank 0] step=6910, skipped=4, lr=[0.00012074352640000001, 0.00012074352640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6910 loss: 1.1736 iter time (s): 4.053 samples/sec: 31.581
g0093:  iteration     6910/10000000 | consumed samples:       884480 | consumed tokens:   1811415040 | elapsed time per iteration (ms): 4088.1 | learning rate: 1.207E-04 | global batch size:   128 | lm loss: 1.182977E+00 | loss scale: 524288.0 | grad norm: 0.698 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.311 | tokens per gpu per second (tgs): 2003.887 | TFLOPs: 16.13 |
g0069: [2024-08-03 02:53:31,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=6920, skipped=4, lr=[0.00012091828906666668, 0.00012091828906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6920 loss: 1.1792 iter time (s): 4.168 samples/sec: 30.707
g0093:  iteration     6920/10000000 | consumed samples:       885760 | consumed tokens:   1814036480 | elapsed time per iteration (ms): 4203.7 | learning rate: 1.209E-04 | global batch size:   128 | lm loss: 1.178308E+00 | loss scale: 524288.0 | grad norm: 0.818 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.449 | tokens per gpu per second (tgs): 1948.749 | TFLOPs: 15.68 |
g0069: [2024-08-03 02:54:13,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=6930, skipped=4, lr=[0.00012109305173333334, 0.00012109305173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6930 loss: 1.1505 iter time (s): 4.156 samples/sec: 30.797
g0093:  iteration     6930/10000000 | consumed samples:       887040 | consumed tokens:   1816657920 | elapsed time per iteration (ms): 4188.9 | learning rate: 1.211E-04 | global batch size:   128 | lm loss: 1.175004E+00 | loss scale: 524288.0 | grad norm: 0.550 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.557 | tokens per gpu per second (tgs): 1955.655 | TFLOPs: 15.74 |
g0069: [2024-08-03 02:54:56,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=6940, skipped=4, lr=[0.00012126781440000001, 0.00012126781440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6940 loss: 1.1976 iter time (s): 4.215 samples/sec: 30.369
g0093:  iteration     6940/10000000 | consumed samples:       888320 | consumed tokens:   1819279360 | elapsed time per iteration (ms): 4248.4 | learning rate: 1.213E-04 | global batch size:   128 | lm loss: 1.192356E+00 | loss scale: 524288.0 | grad norm: 0.603 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.129 | tokens per gpu per second (tgs): 1928.261 | TFLOPs: 15.52 |
g0069: [2024-08-03 02:55:39,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=6950, skipped=4, lr=[0.00012144257706666668, 0.00012144257706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6950 loss: 1.1393 iter time (s): 4.337 samples/sec: 29.515
g0093:  iteration     6950/10000000 | consumed samples:       889600 | consumed tokens:   1821900800 | elapsed time per iteration (ms): 4371.1 | learning rate: 1.214E-04 | global batch size:   128 | lm loss: 1.172771E+00 | loss scale: 524288.0 | grad norm: 0.602 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.283 | tokens per gpu per second (tgs): 1874.117 | TFLOPs: 15.08 |
g0069: [2024-08-03 02:56:20,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=6960, skipped=4, lr=[0.00012161733973333334, 0.00012161733973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6960 loss: 1.1797 iter time (s): 4.048 samples/sec: 31.623
g0093:  iteration     6960/10000000 | consumed samples:       890880 | consumed tokens:   1824522240 | elapsed time per iteration (ms): 4080.1 | learning rate: 1.216E-04 | global batch size:   128 | lm loss: 1.176956E+00 | loss scale: 524288.0 | grad norm: 0.554 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.372 | tokens per gpu per second (tgs): 2007.801 | TFLOPs: 16.16 |
g0069: [2024-08-03 02:57:02,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=6970, skipped=4, lr=[0.00012179210240000001, 0.00012179210240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6970 loss: 1.1498 iter time (s): 4.116 samples/sec: 31.099
g0093:  iteration     6970/10000000 | consumed samples:       892160 | consumed tokens:   1827143680 | elapsed time per iteration (ms): 4150.9 | learning rate: 1.218E-04 | global batch size:   128 | lm loss: 1.174034E+00 | loss scale: 524288.0 | grad norm: 0.634 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.836 | tokens per gpu per second (tgs): 1973.535 | TFLOPs: 15.88 |
g0069: [2024-08-03 02:57:43,481] [INFO] [logging.py:96:log_dist] [Rank 0] step=6980, skipped=4, lr=[0.00012196686506666667, 0.00012196686506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6980 loss: 1.1508 iter time (s): 4.109 samples/sec: 31.154
g0093:  iteration     6980/10000000 | consumed samples:       893440 | consumed tokens:   1829765120 | elapsed time per iteration (ms): 4145.8 | learning rate: 1.220E-04 | global batch size:   128 | lm loss: 1.169201E+00 | loss scale: 524288.0 | grad norm: 0.532 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.874 | tokens per gpu per second (tgs): 1975.958 | TFLOPs: 15.90 |
g0069: [2024-08-03 02:58:22,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=6990, skipped=4, lr=[0.00012214162773333334, 0.00012214162773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 6990 loss: 1.1816 iter time (s): 3.910 samples/sec: 32.733
g0093:  iteration     6990/10000000 | consumed samples:       894720 | consumed tokens:   1832386560 | elapsed time per iteration (ms): 3943.5 | learning rate: 1.221E-04 | global batch size:   128 | lm loss: 1.190551E+00 | loss scale: 524288.0 | grad norm: 0.551 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.458 | tokens per gpu per second (tgs): 2077.321 | TFLOPs: 16.72 |
g0069: [2024-08-03 02:59:03,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=4, lr=[0.0001223163904, 0.0001223163904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7000 loss: 1.1820 iter time (s): 4.056 samples/sec: 31.558
g0093:  iteration     7000/10000000 | consumed samples:       896000 | consumed tokens:   1835008000 | elapsed time per iteration (ms): 4088.3 | learning rate: 1.223E-04 | global batch size:   128 | lm loss: 1.181788E+00 | loss scale: 524288.0 | grad norm: 0.565 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.309 | tokens per gpu per second (tgs): 2003.754 | TFLOPs: 16.12 |
g0093: ------------------------------------------------------------------------------------------------
g0093:  validation loss at iteration 7000 | lm loss value: 1.175367E+00 | lm loss PPL: 3.239333E+00 | 
g0093: ------------------------------------------------------------------------------------------------
g0069: saving checkpoint at iteration    7000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: [2024-08-03 03:05:40,551] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7000 is about to be saved!
g0093: [2024-08-03 03:05:40,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0069: [2024-08-03 03:05:40,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0069: [2024-08-03 03:05:40,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0069: [2024-08-03 03:05:40,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0093: [2024-08-03 03:05:40,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0093: [2024-08-03 03:05:40,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0086: [2024-08-03 03:05:40,558] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0086: [2024-08-03 03:05:40,558] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0086: [2024-08-03 03:05:40,558] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0087: [2024-08-03 03:05:40,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0087: [2024-08-03 03:05:40,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0087: [2024-08-03 03:05:40,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0091: [2024-08-03 03:05:40,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0091: [2024-08-03 03:05:40,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0088: [2024-08-03 03:05:40,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0088: [2024-08-03 03:05:40,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0088: [2024-08-03 03:05:40,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0091: [2024-08-03 03:05:40,560] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0090: [2024-08-03 03:05:40,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0090: [2024-08-03 03:05:40,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0090: [2024-08-03 03:05:40,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0092: [2024-08-03 03:05:40,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0092: [2024-08-03 03:05:40,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0092: [2024-08-03 03:05:40,562] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0093: [2024-08-03 03:05:40,580] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt...
g0087: [2024-08-03 03:05:40,592] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt...
g0088: [2024-08-03 03:05:40,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt...
g0086: [2024-08-03 03:05:40,597] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt...
g0090: [2024-08-03 03:05:40,597] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt...
g0091: [2024-08-03 03:05:40,598] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt...
g0092: [2024-08-03 03:05:40,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt...
g0069: [2024-08-03 03:05:40,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt...
g0086: [2024-08-03 03:05:40,708] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt.
g0091: [2024-08-03 03:05:40,710] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt.
g0087: [2024-08-03 03:05:40,712] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt.
g0090: [2024-08-03 03:05:40,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt.
g0087: [2024-08-03 03:05:40,746] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt...
g0086: [2024-08-03 03:05:40,748] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt...
g0091: [2024-08-03 03:05:40,750] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt...
g0090: [2024-08-03 03:05:40,766] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt...
g0093: [2024-08-03 03:05:40,803] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt.
g0093: [2024-08-03 03:05:40,804] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt...
g0093: [2024-08-03 03:05:40,806] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt.
g0069: [2024-08-03 03:05:40,832] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt.
g0091: [2024-08-03 03:05:40,847] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt.
g0093: [2024-08-03 03:05:40,852] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt...
g0069: [2024-08-03 03:05:40,860] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt...
g0092: [2024-08-03 03:05:40,870] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt.
g0086: [2024-08-03 03:05:40,876] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt.
g0091: [2024-08-03 03:05:40,883] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt...
g0087: [2024-08-03 03:05:40,884] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt.
g0088: [2024-08-03 03:05:40,889] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt.
g0086: [2024-08-03 03:05:40,908] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt...
g0092: [2024-08-03 03:05:40,909] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt...
g0087: [2024-08-03 03:05:40,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt...
g0088: [2024-08-03 03:05:40,923] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt...
g0069: [2024-08-03 03:05:40,992] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt.
g0090: [2024-08-03 03:05:41,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt.
g0092: [2024-08-03 03:05:41,010] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt.
g0087: [2024-08-03 03:05:41,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt.
g0087: [2024-08-03 03:05:41,018] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt...
g0069: [2024-08-03 03:05:41,025] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt...
g0090: [2024-08-03 03:05:41,033] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt...
g0086: [2024-08-03 03:05:41,041] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt.
g0086: [2024-08-03 03:05:41,043] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt...
g0092: [2024-08-03 03:05:41,044] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt...
g0093: [2024-08-03 03:05:41,058] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt.
g0093: [2024-08-03 03:05:41,059] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt...
g0091: [2024-08-03 03:05:41,126] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt.
g0091: [2024-08-03 03:05:41,127] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt...
g0069: [2024-08-03 03:05:41,141] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt.
g0090: [2024-08-03 03:05:41,156] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt.
g0090: [2024-08-03 03:05:41,158] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt...
g0088: [2024-08-03 03:05:41,159] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt.
g0092: [2024-08-03 03:05:41,160] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt.
g0092: [2024-08-03 03:05:41,162] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt...
g0069: [2024-08-03 03:05:41,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt...
g0088: [2024-08-03 03:05:41,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt...
g0088: [2024-08-03 03:05:41,322] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt.
g0088: [2024-08-03 03:05:41,324] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt...
g0069: [2024-08-03 03:05:41,557] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt.
g0069: [2024-08-03 03:05:41,560] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt
g0069: [2024-08-03 03:05:41,560] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0093: [2024-08-03 03:05:42,843] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt.
g0093: [2024-08-03 03:05:42,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0087: [2024-08-03 03:05:43,304] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt.
g0087: [2024-08-03 03:05:43,304] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0086: [2024-08-03 03:05:43,432] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt.
g0086: [2024-08-03 03:05:43,432] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0090: [2024-08-03 03:05:43,495] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt.
g0090: [2024-08-03 03:05:43,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0091: [2024-08-03 03:05:43,526] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt.
g0091: [2024-08-03 03:05:43,527] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0088: [2024-08-03 03:05:43,573] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt.
g0088: [2024-08-03 03:05:43,573] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0092: [2024-08-03 03:05:43,777] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt.
g0092: [2024-08-03 03:05:43,778] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0069: [2024-08-03 03:05:44,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0069: [2024-08-03 03:05:44,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0069:   successfully saved checkpoint at iteration    7000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0069: Checkpoint Save GB: 22.521, GB/Sec: 5.22, Latency(second): 4.312
g0093: (min, max) time across ranks (ms):
g0093:     save-checkpoint ................................: (4312.08, 4312.44)
g0069: [2024-08-03 03:06:28,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=7010, skipped=4, lr=[0.00012249115306666667, 0.00012249115306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7010 loss: 1.1930 iter time (s): 4.306 samples/sec: 29.728
g0093:  iteration     7010/10000000 | consumed samples:       897280 | consumed tokens:   1837629440 | elapsed time per iteration (ms): 44438.6 | learning rate: 1.225E-04 | global batch size:   128 | lm loss: 1.183863E+00 | loss scale: 524288.0 | grad norm: 0.590 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.880 | tokens per gpu per second (tgs): 184.344 | TFLOPs: 1.48 |
g0069: [2024-08-03 03:07:10,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=7020, skipped=4, lr=[0.00012266591573333334, 0.00012266591573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7020 loss: 1.1865 iter time (s): 4.232 samples/sec: 30.248
g0093:  iteration     7020/10000000 | consumed samples:       898560 | consumed tokens:   1840250880 | elapsed time per iteration (ms): 4264.7 | learning rate: 1.227E-04 | global batch size:   128 | lm loss: 1.184914E+00 | loss scale: 524288.0 | grad norm: 0.608 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.014 | tokens per gpu per second (tgs): 1920.906 | TFLOPs: 15.46 |
g0069: [2024-08-03 03:07:53,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=7030, skipped=4, lr=[0.0001228406784, 0.0001228406784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7030 loss: 1.1595 iter time (s): 4.250 samples/sec: 30.120
g0093:  iteration     7030/10000000 | consumed samples:       899840 | consumed tokens:   1842872320 | elapsed time per iteration (ms): 4281.9 | learning rate: 1.228E-04 | global batch size:   128 | lm loss: 1.183787E+00 | loss scale: 524288.0 | grad norm: 0.693 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.894 | tokens per gpu per second (tgs): 1913.190 | TFLOPs: 15.40 |
g0069: [2024-08-03 03:08:35,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=7040, skipped=4, lr=[0.00012301544106666667, 0.00012301544106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7040 loss: 1.1907 iter time (s): 4.183 samples/sec: 30.603
g0093:  iteration     7040/10000000 | consumed samples:       901120 | consumed tokens:   1845493760 | elapsed time per iteration (ms): 4214.9 | learning rate: 1.230E-04 | global batch size:   128 | lm loss: 1.173526E+00 | loss scale: 524288.0 | grad norm: 0.526 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.368 | tokens per gpu per second (tgs): 1943.558 | TFLOPs: 15.64 |
g0069: [2024-08-03 03:09:19,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=7050, skipped=4, lr=[0.00012319020373333334, 0.00012319020373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7050 loss: 1.1719 iter time (s): 4.315 samples/sec: 29.666
g0093:  iteration     7050/10000000 | consumed samples:       902400 | consumed tokens:   1848115200 | elapsed time per iteration (ms): 4347.3 | learning rate: 1.232E-04 | global batch size:   128 | lm loss: 1.170404E+00 | loss scale: 524288.0 | grad norm: 0.627 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.443 | tokens per gpu per second (tgs): 1884.382 | TFLOPs: 15.16 |
g0069: [2024-08-03 03:10:01,222] [INFO] [logging.py:96:log_dist] [Rank 0] step=7060, skipped=4, lr=[0.0001233649664, 0.0001233649664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7060 loss: 1.1731 iter time (s): 4.162 samples/sec: 30.758
g0093:  iteration     7060/10000000 | consumed samples:       903680 | consumed tokens:   1850736640 | elapsed time per iteration (ms): 4194.1 | learning rate: 1.234E-04 | global batch size:   128 | lm loss: 1.179167E+00 | loss scale: 524288.0 | grad norm: 0.632 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.519 | tokens per gpu per second (tgs): 1953.217 | TFLOPs: 15.72 |
g0069: [2024-08-03 03:10:42,037] [INFO] [logging.py:96:log_dist] [Rank 0] step=7070, skipped=4, lr=[0.00012353972906666667, 0.00012353972906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7070 loss: 1.1436 iter time (s): 4.049 samples/sec: 31.616
g0093:  iteration     7070/10000000 | consumed samples:       904960 | consumed tokens:   1853358080 | elapsed time per iteration (ms): 4081.5 | learning rate: 1.235E-04 | global batch size:   128 | lm loss: 1.170249E+00 | loss scale: 524288.0 | grad norm: 0.500 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.361 | tokens per gpu per second (tgs): 2007.093 | TFLOPs: 16.15 |
g0069: [2024-08-03 03:11:24,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=7080, skipped=4, lr=[0.00012371449173333336, 0.00012371449173333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7080 loss: 1.1675 iter time (s): 4.240 samples/sec: 30.189
g0093:  iteration     7080/10000000 | consumed samples:       906240 | consumed tokens:   1855979520 | elapsed time per iteration (ms): 4272.1 | learning rate: 1.237E-04 | global batch size:   128 | lm loss: 1.178434E+00 | loss scale: 524288.0 | grad norm: 0.580 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.962 | tokens per gpu per second (tgs): 1917.560 | TFLOPs: 15.43 |
g0086: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0091: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0088: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0090: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0086: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0090: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0088: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0092: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0092: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0090: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0087: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0088: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0091: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0092: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0088: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0091: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0087: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0091: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0087: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0069: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0091: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0069: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0069: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0088: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0092: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0087: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0069: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0086: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0090: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0092: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0091: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0091: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0086: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0087: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0088: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0093: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 03:11:55,246] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0093: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0093: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0093: [2024-08-03 03:11:55,247] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0093: [2024-08-03 03:11:55,248] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0069: [2024-08-03 03:12:08,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=7090, skipped=4, lr=[0.00012388925440000003, 0.00012388925440000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7090 loss: 1.1884 iter time (s): 4.326 samples/sec: 29.586
g0093:  iteration     7090/10000000 | consumed samples:       907520 | consumed tokens:   1858600960 | elapsed time per iteration (ms): 4359.6 | learning rate: 1.239E-04 | global batch size:   128 | lm loss: 1.171206E+00 | loss scale: 1048576.0 | grad norm: 0.545 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.360 | tokens per gpu per second (tgs): 1879.060 | TFLOPs: 15.12 |
g0069: [2024-08-03 03:12:52,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=7100, skipped=4, lr=[0.0001240640170666667, 0.0001240640170666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7100 loss: 1.1732 iter time (s): 4.402 samples/sec: 29.081
g0093:  iteration     7100/10000000 | consumed samples:       908800 | consumed tokens:   1861222400 | elapsed time per iteration (ms): 4435.1 | learning rate: 1.241E-04 | global batch size:   128 | lm loss: 1.169813E+00 | loss scale: 1048576.0 | grad norm: 0.588 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.861 | tokens per gpu per second (tgs): 1847.101 | TFLOPs: 14.86 |
g0069: [2024-08-03 03:13:35,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=7110, skipped=4, lr=[0.00012423877973333336, 0.00012423877973333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7110 loss: 1.1855 iter time (s): 4.225 samples/sec: 30.296
g0093:  iteration     7110/10000000 | consumed samples:       910080 | consumed tokens:   1863843840 | elapsed time per iteration (ms): 4258.7 | learning rate: 1.242E-04 | global batch size:   128 | lm loss: 1.176588E+00 | loss scale: 1048576.0 | grad norm: 0.568 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.056 | tokens per gpu per second (tgs): 1923.592 | TFLOPs: 15.48 |
g0069: [2024-08-03 03:14:20,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=7120, skipped=4, lr=[0.00012441354240000003, 0.00012441354240000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7120 loss: 1.1596 iter time (s): 4.482 samples/sec: 28.556
g0093:  iteration     7120/10000000 | consumed samples:       911360 | consumed tokens:   1866465280 | elapsed time per iteration (ms): 4515.1 | learning rate: 1.244E-04 | global batch size:   128 | lm loss: 1.170687E+00 | loss scale: 1048576.0 | grad norm: 0.518 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.350 | tokens per gpu per second (tgs): 1814.374 | TFLOPs: 14.60 |
g0069: [2024-08-03 03:15:01,619] [INFO] [logging.py:96:log_dist] [Rank 0] step=7130, skipped=4, lr=[0.0001245883050666667, 0.0001245883050666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7130 loss: 1.1610 iter time (s): 4.085 samples/sec: 31.333
g0093:  iteration     7130/10000000 | consumed samples:       912640 | consumed tokens:   1869086720 | elapsed time per iteration (ms): 4117.6 | learning rate: 1.246E-04 | global batch size:   128 | lm loss: 1.162683E+00 | loss scale: 1048576.0 | grad norm: 0.536 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.086 | tokens per gpu per second (tgs): 1989.492 | TFLOPs: 16.01 |
g0069: [2024-08-03 03:15:42,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=7140, skipped=4, lr=[0.00012476306773333333, 0.00012476306773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7140 loss: 1.1791 iter time (s): 4.061 samples/sec: 31.518
g0093:  iteration     7140/10000000 | consumed samples:       913920 | consumed tokens:   1871708160 | elapsed time per iteration (ms): 4095.3 | learning rate: 1.248E-04 | global batch size:   128 | lm loss: 1.166629E+00 | loss scale: 1048576.0 | grad norm: 0.615 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.255 | tokens per gpu per second (tgs): 2000.321 | TFLOPs: 16.10 |
g0069: [2024-08-03 03:16:26,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=7150, skipped=4, lr=[0.0001249378304, 0.0001249378304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7150 loss: 1.1393 iter time (s): 4.371 samples/sec: 29.285
g0093:  iteration     7150/10000000 | consumed samples:       915200 | consumed tokens:   1874329600 | elapsed time per iteration (ms): 4406.1 | learning rate: 1.249E-04 | global batch size:   128 | lm loss: 1.164812E+00 | loss scale: 1048576.0 | grad norm: 0.538 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.051 | tokens per gpu per second (tgs): 1859.247 | TFLOPs: 14.96 |
g0069: [2024-08-03 03:17:11,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=7160, skipped=4, lr=[0.00012511259306666666, 0.00012511259306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7160 loss: 1.1702 iter time (s): 4.429 samples/sec: 28.898
g0093:  iteration     7160/10000000 | consumed samples:       916480 | consumed tokens:   1876951040 | elapsed time per iteration (ms): 4463.4 | learning rate: 1.251E-04 | global batch size:   128 | lm loss: 1.165131E+00 | loss scale: 1048576.0 | grad norm: 0.554 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.678 | tokens per gpu per second (tgs): 1835.392 | TFLOPs: 14.77 |
g0069: [2024-08-03 03:17:53,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=7170, skipped=4, lr=[0.00012528735573333333, 0.00012528735573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7170 loss: 1.1339 iter time (s): 4.186 samples/sec: 30.579
g0093:  iteration     7170/10000000 | consumed samples:       917760 | consumed tokens:   1879572480 | elapsed time per iteration (ms): 4219.8 | learning rate: 1.253E-04 | global batch size:   128 | lm loss: 1.157389E+00 | loss scale: 1048576.0 | grad norm: 0.524 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.333 | tokens per gpu per second (tgs): 1941.320 | TFLOPs: 15.62 |
g0069: [2024-08-03 03:18:35,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=7180, skipped=4, lr=[0.0001254621184, 0.0001254621184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7180 loss: 1.1497 iter time (s): 4.122 samples/sec: 31.056
g0093:  iteration     7180/10000000 | consumed samples:       919040 | consumed tokens:   1882193920 | elapsed time per iteration (ms): 4154.3 | learning rate: 1.255E-04 | global batch size:   128 | lm loss: 1.159599E+00 | loss scale: 1048576.0 | grad norm: 0.517 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.812 | tokens per gpu per second (tgs): 1971.947 | TFLOPs: 15.87 |
g0093: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 7184
g0092: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: Grad overflow on iteration 7184
g0088: Grad overflow on iteration 7184
g0069: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 7184
g0087: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 7184
g0087: Grad overflow on iteration 7184
g0091: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: Grad overflow on iteration 7184
g0088: Grad overflow on iteration 7184
g0069: Grad overflow on iteration 7184
g0093: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: Grad overflow on iteration 7184
g0091: Grad overflow on iteration 7184
g0069: Grad overflow on iteration 7184
g0087: Grad overflow on iteration 7184
g0069: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0087: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: Grad overflow on iteration 7184
g0069: Grad overflow on iteration 7184
g0090: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: Grad overflow on iteration 7184
g0091: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0093: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0090: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0086: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: Grad overflow on iteration 7184
g0090: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 7184
g0086: Grad overflow on iteration 7184
g0090: Grad overflow on iteration 7184
g0086: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0092: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0090: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0092: Grad overflow on iteration 7184
g0086: Grad overflow on iteration 7184
g0087: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0093: Grad overflow on iteration 7184
g0086: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0090: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0087: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0092: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0086: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0088: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0088: Grad overflow on iteration 7184
g0093: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0088: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0086: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0087: Grad overflow on iteration 7184
g0087: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0093: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0091: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0086: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0093: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:344:_update_scale] 
g0087: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0092: Grad overflow on iteration 7184
g0091: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0092: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0091: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:344:_update_scale] 
g0088: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:344:_update_scale] 
g0091: Grad overflow on iteration 7184
g0088: Grad overflow on iteration 7184
g0091: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0093: Grad overflow on iteration 7184
g0088: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0069: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0086: Grad overflow on iteration 7184
g0093: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0088: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0086: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0092: [2024-08-03 03:18:56,920] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0093: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0092: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0069: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:344:_update_scale] 
g0069: Grad overflow on iteration 7184
g0092: Grad overflow on iteration 7184
g0069: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0092: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0092: [2024-08-03 03:18:56,921] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0069: [2024-08-03 03:18:56,921] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
g0069: [2024-08-03 03:19:18,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=7190, skipped=5, lr=[0.00012563688106666666, 0.00012563688106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7190 loss: 1.1772 iter time (s): 4.298 samples/sec: 29.779
g0093:  iteration     7190/10000000 | consumed samples:       920320 | consumed tokens:   1884815360 | elapsed time per iteration (ms): 4331.9 | learning rate: 1.256E-04 | global batch size:   128 | lm loss: 1.162150E+00 | loss scale: 524288.0 | grad norm: 0.567 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.548 | tokens per gpu per second (tgs): 1891.081 | TFLOPs: 15.22 |
g0069: [2024-08-03 03:19:59,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=7200, skipped=5, lr=[0.00012581164373333333, 0.00012581164373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7200 loss: 1.1671 iter time (s): 4.050 samples/sec: 31.605
g0093:  iteration     7200/10000000 | consumed samples:       921600 | consumed tokens:   1887436800 | elapsed time per iteration (ms): 4083.0 | learning rate: 1.258E-04 | global batch size:   128 | lm loss: 1.162154E+00 | loss scale: 524288.0 | grad norm: 0.540 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.350 | tokens per gpu per second (tgs): 2006.372 | TFLOPs: 16.15 |
g0069: [2024-08-03 03:20:40,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=7210, skipped=5, lr=[0.0001259864064, 0.0001259864064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7210 loss: 1.1427 iter time (s): 4.053 samples/sec: 31.581
g0093:  iteration     7210/10000000 | consumed samples:       922880 | consumed tokens:   1890058240 | elapsed time per iteration (ms): 4086.0 | learning rate: 1.260E-04 | global batch size:   128 | lm loss: 1.168888E+00 | loss scale: 524288.0 | grad norm: 0.554 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.326 | tokens per gpu per second (tgs): 2004.872 | TFLOPs: 16.13 |
g0069: [2024-08-03 03:21:21,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=7220, skipped=5, lr=[0.00012616116906666666, 0.00012616116906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7220 loss: 1.1612 iter time (s): 4.080 samples/sec: 31.369
g0093:  iteration     7220/10000000 | consumed samples:       924160 | consumed tokens:   1892679680 | elapsed time per iteration (ms): 4112.9 | learning rate: 1.262E-04 | global batch size:   128 | lm loss: 1.164894E+00 | loss scale: 524288.0 | grad norm: 0.511 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.121 | tokens per gpu per second (tgs): 1991.774 | TFLOPs: 16.03 |
g0069: [2024-08-03 03:22:04,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=7230, skipped=5, lr=[0.00012633593173333333, 0.00012633593173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7230 loss: 1.1392 iter time (s): 4.315 samples/sec: 29.664
g0093:  iteration     7230/10000000 | consumed samples:       925440 | consumed tokens:   1895301120 | elapsed time per iteration (ms): 4347.9 | learning rate: 1.263E-04 | global batch size:   128 | lm loss: 1.158899E+00 | loss scale: 524288.0 | grad norm: 0.543 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.439 | tokens per gpu per second (tgs): 1884.126 | TFLOPs: 15.16 |
g0069: [2024-08-03 03:22:46,538] [INFO] [logging.py:96:log_dist] [Rank 0] step=7240, skipped=5, lr=[0.0001265106944, 0.0001265106944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7240 loss: 1.1380 iter time (s): 4.159 samples/sec: 30.780
g0093:  iteration     7240/10000000 | consumed samples:       926720 | consumed tokens:   1897922560 | elapsed time per iteration (ms): 4191.1 | learning rate: 1.265E-04 | global batch size:   128 | lm loss: 1.156767E+00 | loss scale: 524288.0 | grad norm: 0.485 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.541 | tokens per gpu per second (tgs): 1954.641 | TFLOPs: 15.73 |
g0069: [2024-08-03 03:23:26,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=7250, skipped=5, lr=[0.00012668545706666666, 0.00012668545706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7250 loss: 1.1655 iter time (s): 4.006 samples/sec: 31.952
g0093:  iteration     7250/10000000 | consumed samples:       928000 | consumed tokens:   1900544000 | elapsed time per iteration (ms): 4038.7 | learning rate: 1.267E-04 | global batch size:   128 | lm loss: 1.148281E+00 | loss scale: 524288.0 | grad norm: 0.505 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.693 | tokens per gpu per second (tgs): 2028.365 | TFLOPs: 16.32 |
g0069: [2024-08-03 03:24:08,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=7260, skipped=5, lr=[0.00012686021973333332, 0.00012686021973333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7260 loss: 1.1349 iter time (s): 4.132 samples/sec: 30.975
g0093:  iteration     7260/10000000 | consumed samples:       929280 | consumed tokens:   1903165440 | elapsed time per iteration (ms): 4165.2 | learning rate: 1.269E-04 | global batch size:   128 | lm loss: 1.155996E+00 | loss scale: 524288.0 | grad norm: 0.598 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.731 | tokens per gpu per second (tgs): 1966.792 | TFLOPs: 15.83 |
g0069: [2024-08-03 03:24:49,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=7270, skipped=5, lr=[0.0001270349824, 0.0001270349824], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7270 loss: 1.1757 iter time (s): 4.028 samples/sec: 31.781
g0093:  iteration     7270/10000000 | consumed samples:       930560 | consumed tokens:   1905786880 | elapsed time per iteration (ms): 4060.4 | learning rate: 1.270E-04 | global batch size:   128 | lm loss: 1.172164E+00 | loss scale: 524288.0 | grad norm: 0.596 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.524 | tokens per gpu per second (tgs): 2017.550 | TFLOPs: 16.24 |
g0069: [2024-08-03 03:25:31,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=7280, skipped=5, lr=[0.00012720974506666666, 0.00012720974506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7280 loss: 1.1700 iter time (s): 4.218 samples/sec: 30.348
g0093:  iteration     7280/10000000 | consumed samples:       931840 | consumed tokens:   1908408320 | elapsed time per iteration (ms): 4250.2 | learning rate: 1.272E-04 | global batch size:   128 | lm loss: 1.157662E+00 | loss scale: 524288.0 | grad norm: 0.601 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.116 | tokens per gpu per second (tgs): 1927.418 | TFLOPs: 15.51 |
g0069: [2024-08-03 03:26:13,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=7290, skipped=5, lr=[0.00012738450773333332, 0.00012738450773333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7290 loss: 1.1820 iter time (s): 4.162 samples/sec: 30.757
g0093:  iteration     7290/10000000 | consumed samples:       933120 | consumed tokens:   1911029760 | elapsed time per iteration (ms): 4194.1 | learning rate: 1.274E-04 | global batch size:   128 | lm loss: 1.162335E+00 | loss scale: 524288.0 | grad norm: 0.604 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.519 | tokens per gpu per second (tgs): 1953.215 | TFLOPs: 15.72 |
g0069: [2024-08-03 03:26:54,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=7300, skipped=5, lr=[0.0001275592704, 0.0001275592704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0069: steps: 7300 loss: 1.1392 iter time (s): 4.066 samples/sec: 31.484
g0093:  iteration     7300/10000000 | consumed samples:       934400 | consumed tokens:   1913651200 | elapsed time per iteration (ms): 4098.3 | learning rate: 1.276E-04 | global batch size:   128 | lm loss: 1.164072E+00 | loss scale: 524288.0 | grad norm: 0.561 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.233 | tokens per gpu per second (tgs): 1998.889 | TFLOPs: 16.09 |
