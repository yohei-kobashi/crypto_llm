
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0108
    HostName g0108
    Port 2222
    StrictHostKeyChecking no

Host g0113
    HostName g0113
    Port 2222
    StrictHostKeyChecking no

Host g0120
    HostName g0120
    Port 2222
    StrictHostKeyChecking no

Host g0121
    HostName g0121
    Port 2222
    StrictHostKeyChecking no

Host g0123
    HostName g0123
    Port 2222
    StrictHostKeyChecking no

Host g0125
    HostName g0125
    Port 2222
    StrictHostKeyChecking no

Host g0126
    HostName g0126
    Port 2222
    StrictHostKeyChecking no

Host g0127
    HostName g0127
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42829706
g0108 slots=4
g0113 slots=4
g0120 slots=4
g0121 slots=4
g0123 slots=4
g0125 slots=4
g0126 slots=4
g0127 slots=4

[2024-08-12 03:27:11,178] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-12 03:27:15,137] [INFO] [runner.py:463:main] Using IP address of 10.1.4.6 for node g0108
[2024-08-12 03:27:15,138] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0108,g0113,g0120,g0121,g0123,g0125,g0126,g0127
[2024-08-12 03:27:15,138] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0108,g0113,g0120,g0121,g0123,g0125,g0126,g0127 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDEwOCI6IFswLCAxLCAyLCAzXSwgImcwMTEzIjogWzAsIDEsIDIsIDNdLCAiZzAxMjAiOiBbMCwgMSwgMiwgM10sICJnMDEyMSI6IFswLCAxLCAyLCAzXSwgImcwMTIzIjogWzAsIDEsIDIsIDNdLCAiZzAxMjUiOiBbMCwgMSwgMiwgM10sICJnMDEyNiI6IFswLCAxLCAyLCAzXSwgImcwMTI3IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.4.6 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '13631488000' --train-samples '6656000' --lr '2.0e-4' --min-lr '1.0e-5' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True' --wandb_tag 'other_gpu'
g0108: [2024-08-12 03:27:18,631] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 03:27:19,985] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 03:27:20,049] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 03:27:20,053] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0127: [2024-08-12 03:27:20,094] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 03:27:20,117] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0126: [2024-08-12 03:27:20,124] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-12 03:27:20,139] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 03:27:20,900] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0108: [2024-08-12 03:27:20,901] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0108: [2024-08-12 03:27:20,901] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0108: [2024-08-12 03:27:20,901] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0108: [2024-08-12 03:27:20,901] [INFO] [launch.py:163:main] dist_world_size=32
g0108: [2024-08-12 03:27:20,901] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0123: [2024-08-12 03:27:22,247] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0123: [2024-08-12 03:27:22,247] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0123: [2024-08-12 03:27:22,247] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0123: [2024-08-12 03:27:22,247] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0123: [2024-08-12 03:27:22,247] [INFO] [launch.py:163:main] dist_world_size=32
g0123: [2024-08-12 03:27:22,247] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0120: [2024-08-12 03:27:22,330] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0120: [2024-08-12 03:27:22,330] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0120: [2024-08-12 03:27:22,330] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0120: [2024-08-12 03:27:22,330] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0120: [2024-08-12 03:27:22,330] [INFO] [launch.py:163:main] dist_world_size=32
g0120: [2024-08-12 03:27:22,330] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0121: [2024-08-12 03:27:22,347] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0121: [2024-08-12 03:27:22,348] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0121: [2024-08-12 03:27:22,348] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0121: [2024-08-12 03:27:22,348] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0121: [2024-08-12 03:27:22,348] [INFO] [launch.py:163:main] dist_world_size=32
g0121: [2024-08-12 03:27:22,348] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0126: [2024-08-12 03:27:22,375] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0126: [2024-08-12 03:27:22,375] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0126: [2024-08-12 03:27:22,376] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0126: [2024-08-12 03:27:22,376] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0126: [2024-08-12 03:27:22,376] [INFO] [launch.py:163:main] dist_world_size=32
g0126: [2024-08-12 03:27:22,376] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0127: [2024-08-12 03:27:22,403] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0127: [2024-08-12 03:27:22,403] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0127: [2024-08-12 03:27:22,403] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0127: [2024-08-12 03:27:22,403] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0127: [2024-08-12 03:27:22,403] [INFO] [launch.py:163:main] dist_world_size=32
g0127: [2024-08-12 03:27:22,403] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0125: [2024-08-12 03:27:22,408] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0125: [2024-08-12 03:27:22,408] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0125: [2024-08-12 03:27:22,408] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0125: [2024-08-12 03:27:22,408] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0125: [2024-08-12 03:27:22,409] [INFO] [launch.py:163:main] dist_world_size=32
g0125: [2024-08-12 03:27:22,409] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0113: [2024-08-12 03:27:22,435] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0113: [2024-08-12 03:27:22,436] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0113: [2024-08-12 03:27:22,436] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0113: [2024-08-12 03:27:22,436] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0113: [2024-08-12 03:27:22,436] [INFO] [launch.py:163:main] dist_world_size=32
g0113: [2024-08-12 03:27:22,436] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0108: [2024-08-12 03:27:24,007] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 03:27:24,008] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 03:27:24,064] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 03:27:24,154] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 03:27:25,385] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 03:27:25,391] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 03:27:25,412] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 03:27:25,412] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 03:27:25,429] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0126: [2024-08-12 03:27:25,452] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 03:27:25,481] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 03:27:25,481] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0126: [2024-08-12 03:27:25,485] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 03:27:25,498] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 03:27:25,511] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 03:27:25,513] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 03:27:25,526] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-12 03:27:25,528] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-12 03:27:25,528] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-12 03:27:25,540] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0126: [2024-08-12 03:27:25,551] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0127: [2024-08-12 03:27:25,555] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0127: [2024-08-12 03:27:25,555] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0127: [2024-08-12 03:27:25,557] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0127: [2024-08-12 03:27:25,557] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 03:27:25,583] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 03:27:25,586] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 03:27:25,586] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 03:27:25,698] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0126: [2024-08-12 03:27:25,737] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-12 03:27:25,776] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 03:27:25,825] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: ninja .................. [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: op name ................ installed .. compatible
g0108: --------------------------------------------------
g0108: ninja .................. [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: op name ................ installed .. compatible
g0108: --------------------------------------------------
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: ninja .................. [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: op name ................ installed .. compatible
g0108: --------------------------------------------------
g0108: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0108: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0108: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0108: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0108: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0108: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: ninja .................. [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: op name ................ installed .. compatible
g0108: --------------------------------------------------
g0108: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0108: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0108: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0108: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0108: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0108: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0108: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0108: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0108: using torch.float32 for parameters ...
g0108: ------------------------ arguments ------------------------
g0108:   accumulate_allreduce_grads_in_fp32 .............. False
g0108:   adam_beta1 ...................................... 0.9
g0108:   adam_beta2 ...................................... 0.95
g0108:   adam_eps ........................................ 1e-08
g0108:   add_bias_linear ................................. False
g0108:   add_position_embedding .......................... False
g0108:   adlr_autoresume ................................. False
g0108:   adlr_autoresume_interval ........................ 1000
g0108:   aml_data_download_path .......................... None
g0108:   apply_layernorm_1p .............................. False
g0108:   apply_query_key_layer_scaling ................... False
g0108:   apply_residual_connection_post_layernorm ........ False
g0108:   async_tensor_model_parallel_allreduce ........... False
g0108:   attention_dropout ............................... 0.0
g0108:   attention_softmax_in_fp32 ....................... False
g0108:   barrier_with_L1_time ............................ True
g0108:   bert_binary_head ................................ True
g0108:   bert_embedder_type .............................. megatron
g0108:   bert_load ....................................... None
g0108:   bf16 ............................................ False
g0108:   bias_dropout_fusion ............................. True
g0108:   bias_gelu_fusion ................................ False
g0108:   biencoder_projection_dim ........................ 0
g0108:   biencoder_shared_query_context_model ............ False
g0108:   block_data_path ................................. None
g0108:   checkpoint_activations .......................... False
g0108:   checkpoint_in_cpu ............................... False
g0108:   checkpoint_num_layers ........................... 1
g0108:   classes_fraction ................................ 1.0
g0108:   clip_grad ....................................... 1.0
g0108:   compression_training ............................ False
g0108:   consumed_train_samples .......................... 0
g0108:   consumed_train_tokens ........................... 0
g0108:   consumed_valid_samples .......................... 0
g0108:   contigious_checkpointing ........................ False
g0108:   cpu_optimizer ................................... False
g0108:   cpu_torch_adam .................................. False
g0108:   create_moe_param_group .......................... False
g0108:   curriculum_learning_legacy ...................... False
g0108:   data_cache_path ................................. None
g0108:   data_efficiency_curriculum_learning ............. False
g0108:   data_impl ....................................... mmap
g0108:   data_parallel_random_init ....................... False
g0108:   data_parallel_size .............................. 4
g0108:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document']
g0108:   data_per_class_fraction ......................... 1.0
g0108:   data_sharding ................................... True
g0108:   dataloader_type ................................. single
g0108:   DDP_impl ........................................ local
g0108:   decoder_num_layers .............................. None
g0108:   decoder_seq_length .............................. None
g0108:   deepscale ....................................... False
g0108:   deepscale_config ................................ None
g0108:   deepspeed ....................................... True
g0108:   deepspeed_activation_checkpointing .............. False
g0108:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0108:   deepspeed_mpi ................................... False
g0108:   dino_bottleneck_size ............................ 256
g0108:   dino_freeze_last_layer .......................... 1
g0108:   dino_head_hidden_size ........................... 2048
g0108:   dino_local_crops_number ......................... 10
g0108:   dino_local_img_size ............................. 96
g0108:   dino_norm_last_layer ............................ False
g0108:   dino_teacher_temp ............................... 0.07
g0108:   dino_warmup_teacher_temp ........................ 0.04
g0108:   dino_warmup_teacher_temp_epochs ................. 30
g0108:   distribute_checkpointed_activations ............. False
g0108:   distribute_saved_activations .................... False
g0108:   distributed_backend ............................. nccl
g0108:   distributed_timeout_minutes ..................... 10
g0108:   ds_fused_adam ................................... False
g0108:   ds_inference .................................... False
g0108:   ds_pipeline_enabled ............................. True
g0108:   ds_sequence_parallel_size ....................... 1
g0108:   embedding_path .................................. None
g0108:   embedding_weights_in_fp32 ....................... False
g0108:   empty_unused_memory_level ....................... 0
g0108:   enable_expert_tensor_parallelism ................ False
g0108:   encoder_num_layers .............................. 22
g0108:   encoder_seq_length .............................. 2048
g0108:   end_weight_decay ................................ 0.1
g0108:   eod_mask_loss ................................... False
g0108:   eval_interval ................................... 1000
g0108:   eval_iters ...................................... 100
g0108:   evidence_data_path .............................. None
g0108:   exit_duration_in_mins ........................... 30000000
g0108:   exit_interval ................................... None
g0108:   exit_on_missing_checkpoint ...................... False
g0108:   exit_signal_handler ............................. False
g0108:   expert_interval ................................. 2
g0108:   ffn_hidden_size ................................. 5632
g0108:   finetune ........................................ False
g0108:   force_ds_sequence_parallel ...................... False
g0108:   fp16 ............................................ False
g0108:   fp16_lm_cross_entropy ........................... False
g0108:   fp32_residual_connection ........................ False
g0108:   fp8_amax_compute_algo ........................... most_recent
g0108:   fp8_amax_history_len ............................ 1
g0108:   fp8_e4m3 ........................................ False
g0108:   fp8_hybrid ...................................... False
g0108:   fp8_interval .................................... 1
g0108:   fp8_margin ...................................... 0
g0108:   fp8_wgrad ....................................... True
g0108:   global_batch_size ............................... 128
g0108:   gradient_accumulation_fusion .................... True
g0108:   head_lr_mult .................................... 1.0
g0108:   hidden_dropout .................................. 0.0
g0108:   hidden_size ..................................... 2048
g0108:   hidden_size_teacher ............................. None
g0108:   hysteresis ...................................... 2
g0108:   ict_head_size ................................... None
g0108:   ict_load ........................................ None
g0108:   img_h ........................................... 224
g0108:   img_w ........................................... 224
g0108:   indexer_batch_size .............................. 128
g0108:   indexer_log_interval ............................ 1000
g0108:   inference ....................................... False
g0108:   inference_batch_times_seqlen_threshold .......... 512
g0108:   init_method_std ................................. 0.013
g0108:   init_method_xavier_uniform ...................... False
g0108:   initial_loss_scale .............................. 4294967296
g0108:   iter_per_epoch .................................. 1250
g0108:   kd .............................................. False
g0108:   kd_alpha_ce ..................................... 1
g0108:   kd_beta_ce ...................................... 1
g0108:   kd_temp ......................................... 1.0
g0108:   kv_channels ..................................... 128
g0108:   layernorm_epsilon ............................... 1e-05
g0108:   lazy_mpu_init ................................... None
g0108:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0108:   load_teacher .................................... None
g0108:   local_rank ...................................... 0
g0108:   log_batch_size_to_tensorboard ................... True
g0108:   log_interval .................................... 10
g0108:   log_learning_rate_to_tensorboard ................ True
g0108:   log_loss_scale_to_tensorboard ................... True
g0108:   log_memory_to_tensorboard ....................... False
g0108:   log_num_zeros_in_grad ........................... False
g0108:   log_optimizer_states_to_tensorboard ............. True
g0108:   log_params_norm ................................. False
g0108:   log_timers_to_tensorboard ....................... True
g0108:   log_validation_ppl_to_tensorboard ............... True
g0108:   log_world_size_to_tensorboard ................... False
g0108:   loss_scale ...................................... None
g0108:   loss_scale_window ............................... 1000
g0108:   lr .............................................. 0.0002
g0108:   lr_decay_iters .................................. None
g0108:   lr_decay_samples ................................ None
g0108:   lr_decay_style .................................. cosine
g0108:   lr_decay_tokens ................................. 300000000000
g0108:   lr_warmup_fraction .............................. None
g0108:   lr_warmup_iters ................................. 0
g0108:   lr_warmup_samples ............................... 0
g0108:   lr_warmup_tokens ................................ 3000000000
g0108:   make_vocab_size_divisible_by .................... 128
g0108:   mask_factor ..................................... 1.0
g0108:   mask_prob ....................................... 0.15
g0108:   mask_type ....................................... random
g0108:   masked_softmax_fusion ........................... True
g0108:   max_position_embeddings ......................... 2048
g0108:   max_tokens_to_oom ............................... 12000
g0108:   mem_efficient_ln ................................ True
g0108:   memory_centric_tiled_linear ..................... False
g0108:   merge_file ...................................... None
g0108:   micro_batch_size ................................ 1
g0108:   min_loss_scale .................................. 1.0
g0108:   min_lr .......................................... 1e-05
g0108:   mlp_type ........................................ standard
g0108:   mmap_warmup ..................................... False
g0108:   moe_eval_capacity_factor ........................ 1.0
g0108:   moe_expert_parallel_size ........................ 1
g0108:   moe_loss_coeff .................................. 0.1
g0108:   moe_min_capacity ................................ 4
g0108:   moe_token_dropping .............................. True
g0108:   moe_train_capacity_factor ....................... 1.0
g0108:   mos ............................................. False
g0108:   no_load_lr_state ................................ False
g0108:   no_load_optim ................................... None
g0108:   no_load_rng ..................................... None
g0108:   no_persist_layer_norm ........................... False
g0108:   no_pipeline_parallel ............................ False
g0108:   no_save_optim ................................... None
g0108:   no_save_rng ..................................... None
g0108:   normalization ................................... rmsnorm
g0108:   num_attention_heads ............................. 16
g0108:   num_attention_heads_teacher ..................... None
g0108:   num_channels .................................... 3
g0108:   num_classes ..................................... 1000
g0108:   num_experts ..................................... [1]
g0108:   num_experts_switch .............................. None
g0108:   num_experts_teacher ............................. [1]
g0108:   num_key_value_heads ............................. 4
g0108:   num_layers ...................................... 22
g0108:   num_layers_per_virtual_pipeline_stage ........... None
g0108:   num_layers_teacher .............................. None
g0108:   num_workers ..................................... 0
g0108:   onnx_safe ....................................... None
g0108:   openai_gelu ..................................... False
g0108:   optimizer ....................................... adam
g0108:   output_bert_embeddings .......................... False
g0108:   overlap_p2p_comm ................................ False
g0108:   override_opt_param_scheduler .................... True
g0108:   params_dtype .................................... torch.float32
g0108:   partition_activations ........................... False
g0108:   patch_dim ....................................... 16
g0108:   perform_initialization .......................... True
g0108:   pipeline_model_parallel_size .................... 8
g0108:   pipeline_model_parallel_split_rank .............. None
g0108:   profile_backward ................................ False
g0108:   query_in_block_prob ............................. 0.1
g0108:   rampup_batch_size ............................... None
g0108:   random_ltd ...................................... False
g0108:   rank ............................................ 0
g0108:   recompute_granularity ........................... None
g0108:   recompute_method ................................ None
g0108:   recompute_num_layers ............................ 1
g0108:   remote_device ................................... none
g0108:   repeated_dataloader ............................. False
g0108:   reset_attention_mask ............................ False
g0108:   reset_iteration ................................. False
g0108:   reset_position_ids .............................. False
g0108:   retriever_report_topk_accuracies ................ []
g0108:   retriever_score_scaling ......................... False
g0108:   retriever_seq_length ............................ 256
g0108:   retro_add_retriever ............................. False
g0108:   retro_cyclic_train_iters ........................ None
g0108:   retro_encoder_attention_dropout ................. 0.1
g0108:   retro_encoder_hidden_dropout .................... 0.1
g0108:   retro_encoder_layers ............................ 2
g0108:   retro_num_neighbors ............................. 2
g0108:   retro_num_retrieved_chunks ...................... 2
g0108:   retro_return_doc_ids ............................ False
g0108:   retro_workdir ................................... None
g0108:   return_data_index ............................... False
g0108:   rotary_percent .................................. 1.0
g0108:   sample_rate ..................................... 1.0
g0108:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0108:   save_interval ................................... 1000
g0108:   scatter_gather_tensors_in_pipeline .............. True
g0108:   scattered_embeddings ............................ False
g0108:   seed ............................................ 1234
g0108:   seq_length ...................................... 2048
g0108:   sequence_parallel ............................... False
g0108:   sgd_momentum .................................... 0.9
g0108:   short_seq_prob .................................. 0.1
g0108:   skip_train ...................................... False
g0108:   split ........................................... 949,50,1
g0108:   split_transformers .............................. False
g0108:   squared_relu .................................... False
g0108:   standalone_embedding_stage ...................... False
g0108:   start_weight_decay .............................. 0.1
g0108:   swiglu .......................................... True
g0108:   swin_backbone_type .............................. tiny
g0108:   synchronize_each_layer .......................... False
g0108:   tensor_model_parallel_size ...................... 1
g0108:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True
g0108:   tensorboard_log_interval ........................ 1
g0108:   tensorboard_queue_size .......................... 1
g0108:   test_data_path .................................. None
g0108:   tf32 ............................................ False
g0108:   tile_factor ..................................... 1
g0108:   timing_log_level ................................ 0
g0108:   timing_log_option ............................... minmax
g0108:   titles_data_path ................................ None
g0108:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
g0108:   tokenizer_type .................................. SentencePieceTokenizer
g0108:   topk ............................................ 1
g0108:   train_data_exact_num_epochs ..................... None
g0108:   train_data_path ................................. None
g0108:   train_desc_path ................................. None
g0108:   train_doc_idx_path .............................. None
g0108:   train_idx_path .................................. None
g0108:   train_iters ..................................... None
g0108:   train_sample_idx_path ........................... None
g0108:   train_samples ................................... 6656000
g0108:   train_shuffle_idx_path .......................... None
g0108:   train_tokens .................................... 13631488000
g0108:   transformer_impl ................................ local
g0108:   transformer_pipeline_model_parallel_size ........ 8
g0108:   universal_checkpoint ............................ False
g0108:   untie_embeddings_and_output_weights ............. True
g0108:   use_checkpoint_args ............................. False
g0108:   use_checkpoint_opt_param_scheduler .............. False
g0108:   use_contiguous_buffers_in_local_ddp ............. True
g0108:   use_cpu_initialization .......................... None
g0108:   use_dataset_only ................................ False
g0108:   use_distributed_optimizer ....................... False
g0108:   use_flash_attn .................................. False
g0108:   use_flash_attn_triton ........................... False
g0108:   use_flash_attn_v1 ............................... False
g0108:   use_flash_attn_v2 ............................... False
g0108:   use_one_sent_docs ............................... False
g0108:   use_pin_memory .................................. False
g0108:   use_ring_exchange_p2p ........................... False
g0108:   use_rotary_position_embeddings .................. True
g0108:   use_tutel ....................................... False
g0108:   use_wandb ....................................... True
g0108:   valid_data_path ................................. None
g0108:   variable_seq_lengths ............................ False
g0108:   virtual_pipeline_model_parallel_size ............ None
g0108:   vision_backbone_type ............................ vit
g0108:   vision_pretraining .............................. False
g0108:   vision_pretraining_type ......................... classify
g0108:   vocab_extra_ids ................................. 0
g0108:   vocab_file ...................................... None
g0108:   vocab_size ...................................... None
g0108:   wandb_entity .................................... yohei-kobashi
g0108:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True
g0108:   wandb_project ................................... encrypted_data_LLM
g0108:   wandb_tag ....................................... other_gpu
g0108:   weight_decay .................................... 0.1
g0108:   weight_decay_incr_style ......................... constant
g0108:   world_size ...................................... 32
g0108:   zero_allgather_bucket_size ...................... 0.0
g0108:   zero_contigious_gradients ....................... False
g0108:   zero_reduce_bucket_size ......................... 0.0
g0108:   zero_reduce_scatter ............................. False
g0108:   zero_stage ...................................... 0
g0108: -------------------- end of arguments ---------------------
g0108: setting number of micro-batches to constant 32
g0108: > building SentencePieceTokenizer tokenizer ...
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: [2024-08-12 03:27:27,739] [INFO] [comm.py:637:init_distributed] cdb=None
g0108: [2024-08-12 03:27:27,740] [INFO] [comm.py:637:init_distributed] cdb=None
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [2024-08-12 03:27:27,743] [INFO] [comm.py:637:init_distributed] cdb=None
g0108:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0108: > initializing torch distributed ...
g0108: [2024-08-12 03:27:27,744] [INFO] [comm.py:637:init_distributed] cdb=None
g0108: [2024-08-12 03:27:27,744] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0108: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op report
g0121: --------------------------------------------------
g0121: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.
g0121: --------------------------------------------------
g0121: JIT compiled ops requires ninja
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op report
g0121: --------------------------------------------------
g0121: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.
g0121: --------------------------------------------------
g0121: JIT compiled ops requires ninja
g0121: ninja .................. [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: op name ................ installed .. compatible
g0121: --------------------------------------------------
g0121: ninja .................. [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: op name ................ installed .. compatible
g0121: --------------------------------------------------
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0123: ninja .................. [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: op name ................ installed .. compatible
g0123: --------------------------------------------------
g0123: ninja .................. [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: op name ................ installed .. compatible
g0123: --------------------------------------------------
g0121: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0121: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0121: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0123: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0123: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0123: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0123: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: DeepSpeed C++/CUDA extension op report
g0125: --------------------------------------------------
g0125: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.
g0125: --------------------------------------------------
g0125: JIT compiled ops requires ninja
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0125: ninja .................. [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: op name ................ installed .. compatible
g0125: --------------------------------------------------
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op report
g0121: --------------------------------------------------
g0121: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.
g0121: --------------------------------------------------
g0121: JIT compiled ops requires ninja
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op report
g0121: --------------------------------------------------
g0121: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.
g0121: --------------------------------------------------
g0121: JIT compiled ops requires ninja
g0121: ninja .................. [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: op name ................ installed .. compatible
g0121: --------------------------------------------------
g0121: ninja .................. [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: op name ................ installed .. compatible
g0121: --------------------------------------------------
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0123: ninja .................. [92m[OKAY][0m
g0123: --------------------------------------------------
g0126: --------------------------------------------------
g0123: op name ................ installed .. compatible
g0126: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0126: --------------------------------------------------
g0126: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0126:       runtime if needed. Op compatibility means that your system
g0126:       meet the required dependencies to JIT install the op.
g0126: --------------------------------------------------
g0126: JIT compiled ops requires ninja
g0126: --------------------------------------------------
g0126: DeepSpeed C++/CUDA extension op report
g0126: --------------------------------------------------
g0126: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0126:       runtime if needed. Op compatibility means that your system
g0126:       meet the required dependencies to JIT install the op.
g0126: --------------------------------------------------
g0126: JIT compiled ops requires ninja
g0126: ninja .................. [92m[OKAY][0m
g0126: --------------------------------------------------
g0126: op name ................ installed .. compatible
g0126: --------------------------------------------------
g0126: ninja .................. [92m[OKAY][0m
g0126: --------------------------------------------------
g0126: op name ................ installed .. compatible
g0126: --------------------------------------------------
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: --------------------------------------------------
g0125: DeepSpeed C++/CUDA extension op report
g0125: --------------------------------------------------
g0125: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.
g0125: --------------------------------------------------
g0125: JIT compiled ops requires ninja
g0125: ninja .................. [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: op name ................ installed .. compatible
g0125: --------------------------------------------------
g0125: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0125: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0125: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: DeepSpeed C++/CUDA extension op report
g0120: --------------------------------------------------
g0120: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.
g0120: --------------------------------------------------
g0120: JIT compiled ops requires ninja
g0120: --------------------------------------------------
g0120: DeepSpeed C++/CUDA extension op report
g0120: --------------------------------------------------
g0120: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.
g0120: --------------------------------------------------
g0120: JIT compiled ops requires ninja
g0121: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0121: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0121: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: ninja .................. [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: op name ................ installed .. compatible
g0120: --------------------------------------------------
g0120: ninja .................. [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: op name ................ installed .. compatible
g0120: --------------------------------------------------
g0125: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0125: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0126: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0126: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0126: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0126: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0126: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0126: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0126: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: DeepSpeed general environment info:
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: DeepSpeed C++/CUDA extension op report
g0125: --------------------------------------------------
g0125: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.
g0125: --------------------------------------------------
g0125: JIT compiled ops requires ninja
g0123: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0123: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0126: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0126: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: ninja .................. [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: op name ................ installed .. compatible
g0125: --------------------------------------------------
g0121: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0126: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0126: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0126: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0126: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0126: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0126: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0126: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0126: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0126: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0126: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0126: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0126: --------------------------------------------------
g0126: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0126: --------------------------------------------------
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0126: DeepSpeed general environment info:
g0126: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0126: torch version .................... 2.0.1+cu118
g0126: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0126: deepspeed info ................... 0.12.4, unknown, unknown
g0126: torch cuda version ............... 11.8
g0126: torch hip version ................ None
g0126: nvcc version ..................... 11.8
g0126: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0126: shared memory (/dev/shm) size .... 188.13 GB
g0126: DeepSpeed general environment info:
g0126: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0126: torch version .................... 2.0.1+cu118
g0126: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0126: deepspeed info ................... 0.12.4, unknown, unknown
g0126: torch cuda version ............... 11.8
g0126: torch hip version ................ None
g0126: nvcc version ..................... 11.8
g0126: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0126: shared memory (/dev/shm) size .... 188.13 GB
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0125: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0125: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0125: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: ninja .................. [92m[OKAY][0m
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0123: --------------------------------------------------
g0123: op name ................ installed .. compatible
g0123: --------------------------------------------------
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0113: ninja .................. [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: op name ................ installed .. compatible
g0113: --------------------------------------------------
g0113: ninja .................. [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: op name ................ installed .. compatible
g0113: --------------------------------------------------
g0125: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0125: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0126: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0126: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0120: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0120: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0120: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0120: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0120: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: DeepSpeed general environment info:
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0120: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0125: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0125: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0126: --------------------------------------------------
g0126: DeepSpeed C++/CUDA extension op report
g0126: --------------------------------------------------
g0126: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0126:       runtime if needed. Op compatibility means that your system
g0126:       meet the required dependencies to JIT install the op.
g0126: --------------------------------------------------
g0126: JIT compiled ops requires ninja
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0126: ninja .................. [92m[OKAY][0m
g0126: --------------------------------------------------
g0126: op name ................ installed .. compatible
g0126: --------------------------------------------------
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0121: [2024-08-12 03:27:29,053] [INFO] [comm.py:637:init_distributed] cdb=None
g0121: [2024-08-12 03:27:29,054] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0125: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0123: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [2024-08-12 03:27:29,054] [INFO] [comm.py:637:init_distributed] cdb=None
g0123: [2024-08-12 03:27:29,054] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0120: DeepSpeed general environment info:
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0120: torch version .................... 2.0.1+cu118
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0120: DeepSpeed general environment info:
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0120: torch version .................... 2.0.1+cu118
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: --------------------------------------------------
g0120: DeepSpeed C++/CUDA extension op report
g0120: --------------------------------------------------
g0120: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.
g0120: --------------------------------------------------
g0120: JIT compiled ops requires ninja
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: ninja .................. [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: op name ................ installed .. compatible
g0120: --------------------------------------------------
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m async_io......  [92m[OKAY][0m...............
g0113:  [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: DeepSpeed general environment info:
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0123: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: DeepSpeed C++/CUDA extension op report
g0120: --------------------------------------------------
g0120: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.
g0120: --------------------------------------------------
g0120: JIT compiled ops requires ninja
g0127: --------------------------------------------------
g0127: DeepSpeed C++/CUDA extension op report
g0127: --------------------------------------------------
g0127: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0127:       runtime if needed. Op compatibility means that your system
g0127:       meet the required dependencies to JIT install the op.
g0127: --------------------------------------------------
g0127: JIT compiled ops requires ninja
g0127: --------------------------------------------------
g0127: DeepSpeed C++/CUDA extension op report
g0127: --------------------------------------------------
g0127: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0127:       runtime if needed. Op compatibility means that your system
g0127:       meet the required dependencies to JIT install the op.
g0127: --------------------------------------------------
g0127: JIT compiled ops requires ninja
g0127: --------------------------------------------------
g0127: DeepSpeed C++/CUDA extension op report
g0127: --------------------------------------------------
g0127: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0127:       runtime if needed. Op compatibility means that your system
g0127:       meet the required dependencies to JIT install the op.
g0127: --------------------------------------------------
g0127: JIT compiled ops requires ninja
g0127: --------------------------------------------------
g0127: DeepSpeed C++/CUDA extension op report
g0127: --------------------------------------------------
g0127: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0127:       runtime if needed. Op compatibility means that your system
g0127:       meet the required dependencies to JIT install the op.
g0127: --------------------------------------------------
g0127: JIT compiled ops requires ninja
g0120: ninja .................. [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: op name ................ installed .. compatible
g0120: --------------------------------------------------
g0127: ninja .................. [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: op name ................ installed .. compatible
g0127: --------------------------------------------------
g0127: ninja .................. [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: op name ................ installed .. compatible
g0127: --------------------------------------------------
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0127: ninja .................. [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: op name ................ installed .. compatible
g0127: --------------------------------------------------
g0127: ninja .................. [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: op name ................ installed .. compatible
g0127: --------------------------------------------------
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0126: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0126: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0126: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [2024-08-12 03:27:29,100] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [2024-08-12 03:27:29,101] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [2024-08-12 03:27:29,106] [INFO] [comm.py:637:init_distributed] cdb=None
g0121: [2024-08-12 03:27:29,106] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: DeepSpeed C++/CUDA extension op report
g0125: --------------------------------------------------
g0125: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.
g0125: --------------------------------------------------
g0125: JIT compiled ops requires ninja
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: ninja .................. [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: op name ................ installed .. compatible
g0125: --------------------------------------------------
g0126: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0126: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0126: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: --------------------------------------------------
g0126: DeepSpeed C++/CUDA extension op report
g0126: --------------------------------------------------
g0126: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0126:       runtime if needed. Op compatibility means that your system
g0126:       meet the required dependencies to JIT install the op.
g0126: --------------------------------------------------
g0126: JIT compiled ops requires ninja
g0126: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0126: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0126: ninja .................. [92m[OKAY][0m
g0126: --------------------------------------------------
g0126: op name ................ installed .. compatible
g0126: --------------------------------------------------
g0126: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0126: --------------------------------------------------
g0126: [2024-08-12 03:27:29,119] [INFO] [comm.py:637:init_distributed] cdb=None
g0120: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0120: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0120: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: [2024-08-12 03:27:29,120] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: DeepSpeed general environment info:
g0126: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0126: torch version .................... 2.0.1+cu118
g0126: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0126: deepspeed info ................... 0.12.4, unknown, unknown
g0126: torch cuda version ............... 11.8
g0126: torch hip version ................ None
g0126: nvcc version ..................... 11.8
g0126: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0126: shared memory (/dev/shm) size .... 188.13 GB
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0120: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0120: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0120: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0127: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0masync_io
g0127:  ...............cpu_adagrad  [92m[YES][0m............  ......[92m[YES][0m  [92m[OKAY][0m......
g0127:  [92m[OKAY][0m
g0127: cpu_lion fused_adam...............  .............[92m[YES][0m ...... [92m[OKAY][0m
g0127:  [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0127: cpu_adam evoformer_attn...............  .........[92m[YES][0m  [93m[NO][0m......  .......[92m[OKAY][0m 
g0127: [93m[NO][0m
g0127: cpu_adagrad fused_lamb............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0127: [92m[OKAY][0m
g0127: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0mfused_lion
g0127:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0127: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0127: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0127: async_ioevoformer_attn  ........................  [93m[NO][0m[92m[YES][0m  .............  [93m[NO][0m[92m[OKAY][0m
g0127: 
g0127: fused_lamb ............. [92m[YES][0m ......fused_adam  [92m[OKAY][0m.............
g0127:  [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_lioncpu_adam  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0127: 
g0127: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0127: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0127: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: ninja .................. [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: op name ................ installed .. compatible
g0113: --------------------------------------------------
g0120: DeepSpeed general environment info:
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0120: torch version .................... 2.0.1+cu118
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0126: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: [2024-08-12 03:27:29,149] [INFO] [comm.py:637:init_distributed] cdb=None
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0127: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0127: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0127: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0127: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0127: cutlass_ops cutlass_ops............  [92m[YES][0m............  ......[92m[YES][0m  [92m[OKAY][0m......
g0127:  [92m[OKAY][0m
g0127: quantizer ..............quantizer  [92m[YES][0m..............  ......[92m[YES][0m  [92m[OKAY][0m......
g0127:  [92m[OKAY][0m
g0127: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: DeepSpeed general environment info:
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0120: torch version .................... 2.0.1+cu118
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0127: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0127: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0127: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0127: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0125: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0125: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0126: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0126: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0127: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0127: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0127: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0127: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0127: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0127: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0127: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0127: ragged_ops ............. [92m[YES][0m[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible 
g0127: ...... [92m[OKAY][0msparse_attn
g0127:  ............ random_ltd[93m[NO][0m  ....................  [92m[YES][0m[93m[NO][0m 
g0127: ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0127: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0127: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0126: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0127: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0127: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0127: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0127: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0127: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0127: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0127: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0126: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0127: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0127: --------------------------------------------------
g0113: ninja .................. [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: op name ................ installed .. compatible
g0113: --------------------------------------------------
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: [2024-08-12 03:27:29,181] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0126: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0126: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0125: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0127: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: DeepSpeed general environment info:
g0127: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0127: torch version .................... 2.0.1+cu118
g0127: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0127: deepspeed info ................... 0.12.4, unknown, unknown
g0127: torch cuda version ............... 11.8
g0127: torch hip version ................ None
g0127: nvcc version ..................... 11.8
g0127: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0127: shared memory (/dev/shm) size .... 188.13 GB
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0126: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [2024-08-12 03:27:29,187] [INFO] [comm.py:637:init_distributed] cdb=None
g0120: [2024-08-12 03:27:29,187] [INFO] [comm.py:637:init_distributed] cdb=None
g0120: [2024-08-12 03:27:29,187] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0126: --------------------------------------------------
g0127: DeepSpeed general environment info:
g0127: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0127: torch version .................... 2.0.1+cu118
g0127: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0127: deepspeed info DeepSpeed general environment info:...................
g0127:  0.12.4, unknown, unknown
g0127: torch install pathtorch cuda version  ..............................  11.8
g0127: ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']torch hip version
g0127:  ................ torch versionNone 
g0127: ....................nvcc version  2.0.1+cu118.....................
g0127:  11.8deepspeed install path
g0127:  ...........deepspeed wheel compiled w.  ......['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed'] 
g0127: torch 2.0, cuda 11.8deepspeed info
g0127:  shared memory (/dev/shm) size...................  ....0.12.4, unknown, unknown 
g0127: 188.13 GBtorch cuda version
g0127:  ............... 11.8
g0127: torch hip version ................ None
g0127: nvcc version ..................... 11.8
g0127: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0127: shared memory (/dev/shm) size .... 188.13 GB
g0127: DeepSpeed general environment info:
g0127: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0127: torch version .................... 2.0.1+cu118
g0127: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0127: deepspeed info ................... 0.12.4, unknown, unknown
g0127: torch cuda version ............... 11.8
g0127: torch hip version ................ None
g0127: nvcc version ..................... 11.8
g0127: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0127: shared memory (/dev/shm) size .... 188.13 GB
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: DeepSpeed general environment info:
g0126: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0126: torch version .................... 2.0.1+cu118
g0126: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0126: deepspeed info ................... 0.12.4, unknown, unknown
g0126: torch cuda version ............... 11.8
g0126: torch hip version ................ None
g0126: nvcc version ..................... 11.8
g0126: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0126: shared memory (/dev/shm) size .... 188.13 GB
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: DeepSpeed general environment info:
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: [2024-08-12 03:27:29,213] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: [2024-08-12 03:27:29,214] [INFO] [comm.py:637:init_distributed] cdb=None
g0127: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0127: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0127: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0127: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: [2024-08-12 03:27:29,233] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0120: [2024-08-12 03:27:29,255] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [2024-08-12 03:27:29,266] [INFO] [comm.py:637:init_distributed] cdb=None
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0126: [2024-08-12 03:27:29,298] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [2024-08-12 03:27:29,304] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [2024-08-12 03:27:29,309] [INFO] [comm.py:637:init_distributed] cdb=None
g0127: [2024-08-12 03:27:29,310] [INFO] [comm.py:637:init_distributed] cdb=None
g0127: [2024-08-12 03:27:29,311] [INFO] [comm.py:637:init_distributed] cdb=None
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [2024-08-12 03:27:29,335] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [2024-08-12 03:27:29,364] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: > setting tensorboard ...
g0127: [2024-08-12 03:27:29,498] [INFO] [comm.py:637:init_distributed] cdb=None
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: > initialized tensor model parallel with size 1
g0108: > initialized pipeline model parallel with size 8
g0108: > setting random seeds to 1234 ...
g0108: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0108: > compiling dataset index builder ...
g0108: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0108: make: Nothing to be done for 'default'.
g0108: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0108: >>> done with dataset index builder. Compilation time: 0.073 seconds
g0108: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0108: > compiling and loading fused kernels ...
g0108: Detected CUDA files, patching ldflags
g0108: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0108: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0108: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0108: ninja: no work to do.
g0108: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0108: Detected CUDA files, patching ldflags
g0108: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0108: Building extension module scaled_masked_softmax_cuda...
g0108: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0108: ninja: no work to do.
g0108: Loading extension module scaled_masked_softmax_cuda...
g0108: Detected CUDA files, patching ldflags
g0108: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0108: Building extension module scaled_softmax_cuda...
g0108: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0108: ninja: no work to do.
g0108: Loading extension module scaled_softmax_cuda...
g0108: >>> done with compiling and loading fused kernels. Compilation time: 4.742 seconds
g0108: time to initialize megatron (seconds): 9.641
g0108: [after megatron is initialized] datetime: 2024-08-12 03:27:36 
g0126: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0120: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0113: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0127: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0125: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0121: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0123: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0108: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0125: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0125: wandb:  $ pip install wandb --upgrade
g0125: wandb: Tracking run with wandb version 0.17.5
g0125: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_032737-e2911f76
g0125: wandb: Run `wandb offline` to turn off syncing.
g0125: wandb: Syncing run g0125.abci.local
g0125: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0125: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/e2911f76
g0121: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0121: wandb:  $ pip install wandb --upgrade
g0121: wandb: Tracking run with wandb version 0.17.5
g0121: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_032737-ye1erx44
g0121: wandb: Run `wandb offline` to turn off syncing.
g0121: wandb: Syncing run g0121.abci.local
g0121: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0121: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/ye1erx44
g0123: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0123: wandb:  $ pip install wandb --upgrade
g0123: wandb: Tracking run with wandb version 0.17.5
g0123: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_032737-eogxiv9u
g0123: wandb: Run `wandb offline` to turn off syncing.
g0123: wandb: Syncing run g0123.abci.local
g0123: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0123: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/eogxiv9u
g0113: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0113: wandb:  $ pip install wandb --upgrade
g0113: wandb: Tracking run with wandb version 0.17.5
g0113: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_032737-ol30fje2
g0113: wandb: Run `wandb offline` to turn off syncing.
g0113: wandb: Syncing run g0113.abci.local
g0113: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0113: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/ol30fje2
g0120: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0120: wandb:  $ pip install wandb --upgrade
g0120: wandb: Tracking run with wandb version 0.17.5
g0120: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_032737-c0dx5mxk
g0120: wandb: Run `wandb offline` to turn off syncing.
g0126: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0126: wandb:  $ pip install wandb --upgrade
g0126: wandb: Tracking run with wandb version 0.17.5
g0126: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_032737-ky6sojt1
g0126: wandb: Run `wandb offline` to turn off syncing.
g0120: wandb: Syncing run g0120.abci.local
g0120: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0120: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/c0dx5mxk
g0126: wandb: Syncing run g0126.abci.local
g0126: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0126: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/ky6sojt1
g0127: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0127: wandb:  $ pip install wandb --upgrade
g0127: wandb: Tracking run with wandb version 0.17.5
g0127: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_032737-rockozpa
g0127: wandb: Run `wandb offline` to turn off syncing.
g0127: wandb: Syncing run g0127.abci.local
g0127: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0127: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/rockozpa
g0108: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0108: wandb:  $ pip install wandb --upgrade
g0108: wandb: Tracking run with wandb version 0.17.5
g0108: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_032737-lklie5rv
g0108: wandb: Run `wandb offline` to turn off syncing.
g0108: wandb: Syncing run g0108.abci.local
g0108: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0108: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/lklie5rv
g0108: building GPT model ...
g0108: [2024-08-12 03:27:38,712] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0108: [2024-08-12 03:27:38,713] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0108: [2024-08-12 03:27:38,713] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 52.99 GB, percent = 14.1%
g0108: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0108: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0108: [2024-08-12 03:27:39,221] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0108: stage=0 layers=5
g0108:      0: _to_float16
g0108:      1: EmbeddingPipe
g0108:      2: ParallelTransformerLayerPipe
g0108:      3: ParallelTransformerLayerPipe
g0108:      4: ParallelTransformerLayerPipe
g0108: stage=1 layers=3
g0108:      5: ParallelTransformerLayerPipe
g0108:      6: ParallelTransformerLayerPipe
g0108:      7: ParallelTransformerLayerPipe
g0108: stage=2 layers=3
g0108:      8: ParallelTransformerLayerPipe
g0108:      9: ParallelTransformerLayerPipe
g0108:     10: ParallelTransformerLayerPipe
g0108: stage=3 layers=3
g0108:     11: ParallelTransformerLayerPipe
g0108:     12: ParallelTransformerLayerPipe
g0108:     13: ParallelTransformerLayerPipe
g0108: stage=4 layers=3
g0108:     14: ParallelTransformerLayerPipe
g0108:     15: ParallelTransformerLayerPipe
g0108:     16: ParallelTransformerLayerPipe
g0108: stage=5 layers=3
g0108:     17: ParallelTransformerLayerPipe
g0108:     18: ParallelTransformerLayerPipe
g0108:     19: ParallelTransformerLayerPipe
g0108: stage=6 layers=3
g0108:     20: ParallelTransformerLayerPipe
g0108:     21: ParallelTransformerLayerPipe
g0108:     22: ParallelTransformerLayerPipe
g0108: stage=7 layers=3
g0108:     23: ParallelTransformerLayerPipe
g0108:     24: MixedFusedRMSNorm
g0108:     25: LMHeadPipe
g0108:   loss: CrossEntropy
g0126:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0127:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0123:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0113:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0120:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0125:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0121:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0108: [2024-08-12 03:27:39,598] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0108: [2024-08-12 03:27:39,599] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0108: [2024-08-12 03:27:39,599] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 53.05 GB, percent = 14.1%
g0108:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0108: setting training iterations to 52000
g0108: > learning rate decay style: cosine
g0108: DeepSpeed is enabled.
g0108: [2024-08-12 03:27:39,601] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0126: [2024-08-12 03:27:39,662] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0126: [2024-08-12 03:27:39,662] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0126: [2024-08-12 03:27:39,662] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0126: [2024-08-12 03:27:39,662] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0120: [2024-08-12 03:27:39,678] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0120: [2024-08-12 03:27:39,678] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0120: [2024-08-12 03:27:39,678] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0120: [2024-08-12 03:27:39,678] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0123: [2024-08-12 03:27:39,701] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0123: [2024-08-12 03:27:39,703] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0123: [2024-08-12 03:27:39,703] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0123: [2024-08-12 03:27:39,703] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 03:27:39,711] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 03:27:39,711] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 03:27:39,711] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 03:27:39,711] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0127: [2024-08-12 03:27:39,722] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0127: [2024-08-12 03:27:39,722] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0127: [2024-08-12 03:27:39,722] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0127: [2024-08-12 03:27:39,722] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 03:27:39,747] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 03:27:39,747] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 03:27:39,747] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 03:27:39,748] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-12 03:27:39,784] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-12 03:27:39,784] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-12 03:27:39,784] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-12 03:27:39,784] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 03:27:39,808] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0108: [2024-08-12 03:27:39,808] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0108: [2024-08-12 03:27:39,809] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0108: [2024-08-12 03:27:39,809] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0108: [2024-08-12 03:27:39,809] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0108: [2024-08-12 03:27:39,830] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0108: [2024-08-12 03:27:39,830] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0108: [2024-08-12 03:27:39,830] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f2d6c0a5ed0>
g0108: [2024-08-12 03:27:39,830] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 03:27:39,830] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 03:27:39,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: [2024-08-12 03:27:39,830] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 03:27:39,831] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0108: [2024-08-12 03:27:39,831] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0108:     "partition_activations": false, 
g0108:     "contiguous_memory_optimization": false, 
g0108:     "cpu_checkpointing": false, 
g0108:     "number_checkpoints": null, 
g0108:     "synchronize_checkpoint_boundary": false, 
g0108:     "profile": false
g0108: }
g0108: [2024-08-12 03:27:39,831] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0108: [2024-08-12 03:27:39,831] [INFO] [config.py:983:print]   amp_enabled .................. False
g0108: [2024-08-12 03:27:39,832] [INFO] [config.py:983:print]   amp_params ................... False
g0108: [2024-08-12 03:27:39,832] [INFO] [config.py:983:print]   autotuning_config ............ {
g0108:     "enabled": false, 
g0108:     "start_step": null, 
g0108:     "end_step": null, 
g0108:     "metric_path": null, 
g0108:     "arg_mappings": null, 
g0108:     "metric": "throughput", 
g0108:     "model_info": null, 
g0108:     "results_dir": "autotuning_results", 
g0108:     "exps_dir": "autotuning_exps", 
g0108:     "overwrite": true, 
g0108:     "fast": true, 
g0108:     "start_profile_step": 3, 
g0108:     "end_profile_step": 5, 
g0108:     "tuner_type": "gridsearch", 
g0108:     "tuner_early_stopping": 5, 
g0108:     "tuner_num_trials": 50, 
g0108:     "model_info_path": null, 
g0108:     "mp_size": 1, 
g0108:     "max_train_batch_size": null, 
g0108:     "min_train_batch_size": 1, 
g0108:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0108:     "min_train_micro_batch_size_per_gpu": 1, 
g0108:     "num_tuning_micro_batch_sizes": 3
g0108: }
g0108: [2024-08-12 03:27:39,832] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0108: [2024-08-12 03:27:39,832] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0108: [2024-08-12 03:27:39,832] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0108: [2024-08-12 03:27:39,833] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0108: [2024-08-12 03:27:39,833] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f2d34d07d50>
g0108: [2024-08-12 03:27:39,833] [INFO] [config.py:983:print]   communication_data_type ...... None
g0108: [2024-08-12 03:27:39,833] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0108: [2024-08-12 03:27:39,833] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0108: [2024-08-12 03:27:39,833] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0108: [2024-08-12 03:27:39,833] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0108: [2024-08-12 03:27:39,833] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0108: [2024-08-12 03:27:39,833] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0108: [2024-08-12 03:27:39,834] [INFO] [config.py:983:print]   disable_allgather ............ False
g0108: [2024-08-12 03:27:39,834] [INFO] [config.py:983:print]   dump_state ................... False
g0108: [2024-08-12 03:27:39,834] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0108: [2024-08-12 03:27:39,834] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0108: [2024-08-12 03:27:39,834] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0108: [2024-08-12 03:27:39,834] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0108: [2024-08-12 03:27:39,834] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0108: [2024-08-12 03:27:39,834] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0108: [2024-08-12 03:27:39,835] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0108: [2024-08-12 03:27:39,835] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0108: [2024-08-12 03:27:39,835] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0108: [2024-08-12 03:27:39,835] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0108: [2024-08-12 03:27:39,835] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0108:     "enabled": false, 
g0108:     "recompute_fwd_factor": 0.0, 
g0108:     "profile_step": 1, 
g0108:     "module_depth": -1, 
g0108:     "top_modules": 1, 
g0108:     "detailed": true, 
g0108:     "output_file": null
g0108: }
g0108: [2024-08-12 03:27:39,835] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0108: [2024-08-12 03:27:39,835] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0108: [2024-08-12 03:27:39,835] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0108: [2024-08-12 03:27:39,835] [INFO] [config.py:983:print]   global_rank .................. 0
g0108: [2024-08-12 03:27:39,836] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0108: [2024-08-12 03:27:39,836] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0108: [2024-08-12 03:27:39,836] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0108: [2024-08-12 03:27:39,836] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0108: [2024-08-12 03:27:39,836] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0108: [2024-08-12 03:27:39,836] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0108: [2024-08-12 03:27:39,836] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0108: [2024-08-12 03:27:39,836] [INFO] [config.py:983:print]   loss_scale ................... 0
g0108: [2024-08-12 03:27:39,837] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0108: [2024-08-12 03:27:39,837] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0108: [2024-08-12 03:27:39,837] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0108: [2024-08-12 03:27:39,837] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0108: [2024-08-12 03:27:39,837] [INFO] [config.py:983:print]   nebula_config ................ {
g0108:     "enabled": false, 
g0108:     "persistent_storage_path": null, 
g0108:     "persistent_time_interval": 100, 
g0108:     "num_of_version_in_retention": 2, 
g0108:     "enable_nebula_load": true, 
g0108:     "load_path": null
g0108: }
g0108: [2024-08-12 03:27:39,837] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0108: [2024-08-12 03:27:39,837] [INFO] [config.py:983:print]   optimizer_name ............... None
g0108: [2024-08-12 03:27:39,837] [INFO] [config.py:983:print]   optimizer_params ............. None
g0108: [2024-08-12 03:27:39,837] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0108: [2024-08-12 03:27:39,838] [INFO] [config.py:983:print]   pld_enabled .................. False
g0108: [2024-08-12 03:27:39,838] [INFO] [config.py:983:print]   pld_params ................... False
g0108: [2024-08-12 03:27:39,838] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0108: [2024-08-12 03:27:39,838] [INFO] [config.py:983:print]   scheduler_name ............... None
g0108: [2024-08-12 03:27:39,838] [INFO] [config.py:983:print]   scheduler_params ............. None
g0108: [2024-08-12 03:27:39,838] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0108: [2024-08-12 03:27:39,838] [INFO] [config.py:983:print]   sparse_attention ............. None
g0108: [2024-08-12 03:27:39,838] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0108: [2024-08-12 03:27:39,839] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0108: [2024-08-12 03:27:39,839] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0108: [2024-08-12 03:27:39,839] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0108: [2024-08-12 03:27:39,839] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0108: [2024-08-12 03:27:39,839] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0108: [2024-08-12 03:27:39,839] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0108: [2024-08-12 03:27:39,839] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0108: [2024-08-12 03:27:39,840] [INFO] [config.py:983:print]   world_size ................... 4
g0108: [2024-08-12 03:27:39,840] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0108: [2024-08-12 03:27:39,840] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0108: [2024-08-12 03:27:39,840] [INFO] [config.py:983:print]   zero_enabled ................. False
g0108: [2024-08-12 03:27:39,840] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0108: [2024-08-12 03:27:39,840] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0108: [2024-08-12 03:27:39,840] [INFO] [config.py:969:print_user_config]   json = {
g0108:     "train_batch_size": 128, 
g0108:     "train_micro_batch_size_per_gpu": 1, 
g0108:     "steps_per_print": 10, 
g0108:     "zero_optimization": {
g0108:         "stage": 0
g0108:     }, 
g0108:     "gradient_clipping": 1.0, 
g0108:     "prescale_gradients": true, 
g0108:     "fp16": {
g0108:         "enabled": true, 
g0108:         "loss_scale": 0, 
g0108:         "loss_scale_window": 500, 
g0108:         "hysteresis": 2, 
g0108:         "min_loss_scale": 1, 
g0108:         "initial_scale_power": 11
g0108:     }, 
g0108:     "wall_clock_breakdown": false
g0108: }
g0108: [2024-08-12 03:27:39,841] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0108: [2024-08-12 03:27:39,841] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 03:27:40,547] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0123: [2024-08-12 03:27:40,547] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0120: [2024-08-12 03:27:40,547] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0126: [2024-08-12 03:27:40,547] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0113: [2024-08-12 03:27:40,548] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0125: [2024-08-12 03:27:40,548] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0127: [2024-08-12 03:27:40,548] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0121: [2024-08-12 03:27:40,548] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0126: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0126: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0126: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0125: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0125: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0127: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0126: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0120: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0120: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0125: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0127: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0120: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 03:27:41,244] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 03:27:41,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 03:27:41,244] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 03:27:41,244] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0120: [2024-08-12 03:27:41,244] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 03:27:41,244] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0127: [2024-08-12 03:27:41,244] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0127: [2024-08-12 03:27:41,244] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 03:27:41,248] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0125: [2024-08-12 03:27:41,256] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0126: [2024-08-12 03:27:43,889] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0126: [2024-08-12 03:27:43,889] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0126: [2024-08-12 03:27:43,889] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0126: [2024-08-12 03:27:43,890] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 03:27:43,890] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 03:27:43,890] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 03:27:43,890] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 03:27:43,890] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 03:27:43,890] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 03:27:43,892] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 03:27:43,893] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 03:27:43,898] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0126: [2024-08-12 03:27:43,898] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0126: [2024-08-12 03:27:43,898] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0126: [2024-08-12 03:27:43,898] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0126: [2024-08-12 03:27:43,898] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0121: [2024-08-12 03:27:43,898] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0120: [2024-08-12 03:27:43,898] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0120: [2024-08-12 03:27:43,898] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0120: [2024-08-12 03:27:43,898] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0121: [2024-08-12 03:27:43,899] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0121: [2024-08-12 03:27:43,901] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0121: [2024-08-12 03:27:43,903] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 03:27:43,908] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0125: [2024-08-12 03:27:43,974] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0125: [2024-08-12 03:27:43,974] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0125: [2024-08-12 03:27:43,974] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0125: [2024-08-12 03:27:43,974] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0125: [2024-08-12 03:27:43,982] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0125: [2024-08-12 03:27:43,982] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0125: [2024-08-12 03:27:43,982] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0125: [2024-08-12 03:27:43,982] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0108: [2024-08-12 03:27:44,209] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 03:27:44,209] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 03:27:44,210] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 03:27:44,210] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 03:27:44,217] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 03:27:44,218] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 03:27:44,219] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 03:27:44,219] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 03:27:44,270] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 03:27:44,270] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 03:27:44,271] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 03:27:44,271] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 03:27:44,278] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0113: [2024-08-12 03:27:44,278] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0113: [2024-08-12 03:27:44,279] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0113: [2024-08-12 03:27:44,279] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0123: [2024-08-12 03:27:44,285] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 03:27:44,285] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 03:27:44,285] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 03:27:44,285] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 03:27:44,292] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0123: [2024-08-12 03:27:44,293] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0123: [2024-08-12 03:27:44,293] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0123: [2024-08-12 03:27:44,293] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0127: [2024-08-12 03:27:44,301] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0127: [2024-08-12 03:27:44,301] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0127: [2024-08-12 03:27:44,301] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0127: [2024-08-12 03:27:44,302] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0127: [2024-08-12 03:27:44,308] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0127: [2024-08-12 03:27:44,310] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0127: [2024-08-12 03:27:44,310] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0127: [2024-08-12 03:27:44,310] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0125: [2024-08-12 03:27:44,872] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0125: [2024-08-12 03:27:44,873] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 03:27:44,877] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0125: [2024-08-12 03:27:44,878] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 03:27:44,914] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0125: [2024-08-12 03:27:44,916] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 03:27:44,917] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0125: [2024-08-12 03:27:44,918] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,003] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0120: [2024-08-12 03:27:45,004] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0120: [2024-08-12 03:27:45,004] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,005] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,026] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0120: [2024-08-12 03:27:45,026] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0120: [2024-08-12 03:27:45,027] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,027] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,249] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,250] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,250] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,250] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,250] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,250] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,250] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,251] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,284] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,284] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,285] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,286] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,298] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,305] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,306] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,306] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,613] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,613] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,613] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,613] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,613] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,613] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,614] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,614] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,626] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,627] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,627] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,627] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,627] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,627] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,628] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,628] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,644] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,644] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,646] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,646] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,658] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,660] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,660] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,661] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,662] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,666] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,666] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,667] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0126: [2024-08-12 03:27:45,668] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0126: [2024-08-12 03:27:45,668] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0126: [2024-08-12 03:27:45,669] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0126: [2024-08-12 03:27:45,669] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0126: [2024-08-12 03:27:45,669] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0126: [2024-08-12 03:27:45,669] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0126: [2024-08-12 03:27:45,669] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0126: [2024-08-12 03:27:45,670] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,681] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,681] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,682] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,683] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0126: [2024-08-12 03:27:45,901] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 03:27:45,902] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0126: [2024-08-12 03:27:45,902] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 03:27:45,902] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 03:27:45,902] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 03:27:45,903] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0126: [2024-08-12 03:27:45,903] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0126: [2024-08-12 03:27:45,903] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,923] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,923] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,923] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,923] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,923] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,924] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,924] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,924] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0126: [2024-08-12 03:27:45,935] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 03:27:45,935] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 03:27:45,939] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 03:27:45,939] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,953] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,953] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,953] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,953] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,953] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,953] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,953] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,954] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,955] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,955] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0126: [2024-08-12 03:27:45,956] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0126: [2024-08-12 03:27:45,956] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,957] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 03:27:45,957] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0126: [2024-08-12 03:27:45,960] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0126: [2024-08-12 03:27:45,960] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,976] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,976] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,977] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 03:27:45,978] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0127: [2024-08-12 03:27:45,984] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0125: [2024-08-12 03:27:45,985] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,985] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0127: [2024-08-12 03:27:45,985] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0127: [2024-08-12 03:27:45,985] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0127: [2024-08-12 03:27:45,986] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0127: [2024-08-12 03:27:45,986] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0127: [2024-08-12 03:27:45,986] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0127: [2024-08-12 03:27:45,986] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0125: [2024-08-12 03:27:45,986] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 03:27:45,986] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0127: [2024-08-12 03:27:45,987] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,091] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0123: [2024-08-12 03:27:46,091] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0123: [2024-08-12 03:27:46,091] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,092] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,092] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0123: [2024-08-12 03:27:46,092] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0123: [2024-08-12 03:27:46,092] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,093] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,216] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0121: [2024-08-12 03:27:46,217] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0121: [2024-08-12 03:27:46,217] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,218] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,219] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0121: [2024-08-12 03:27:46,220] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,222] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0121: [2024-08-12 03:27:46,226] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,258] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,259] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,261] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,261] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,261] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,262] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,265] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 03:27:46,266] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,266] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 03:27:46,267] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,267] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0108: [2024-08-12 03:27:46,268] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 03:27:46,268] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,269] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,272] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 03:27:46,274] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0120: [2024-08-12 03:27:46,288] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 03:27:46,289] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 03:27:46,289] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 03:27:46,289] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 03:27:46,289] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 03:27:46,289] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 03:27:46,289] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 03:27:46,289] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,295] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,300] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,300] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,300] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,300] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,300] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0126: [2024-08-12 03:27:46,300] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0126: [2024-08-12 03:27:46,300] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,300] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,301] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,301] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,306] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,316] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,316] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0120: [2024-08-12 03:27:46,320] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 03:27:46,320] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,321] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,322] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0120: [2024-08-12 03:27:46,322] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 03:27:46,322] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,331] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,331] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,336] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,336] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,352] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0126: [2024-08-12 03:27:46,352] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,356] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0113: [2024-08-12 03:27:46,357] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0126: [2024-08-12 03:27:46,357] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,357] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0126: [2024-08-12 03:27:46,357] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,358] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0113: [2024-08-12 03:27:46,358] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0113: [2024-08-12 03:27:46,358] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,358] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,359] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,378] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 03:27:46,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 03:27:46,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 03:27:46,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 03:27:46,379] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,379] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,379] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,379] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,413] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 03:27:46,413] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 03:27:46,415] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 03:27:46,415] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 03:27:46,434] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,436] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,436] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 03:27:46,437] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,498] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 03:27:46,499] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,504] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 03:27:46,505] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 03:27:46,505] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 03:27:46,505] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,505] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,505] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,539] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,540] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,540] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,540] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,540] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,541] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,540] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,541] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,545] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 03:27:46,550] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 03:27:46,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 03:27:46,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,572] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,572] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,573] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,573] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0108: [2024-08-12 03:27:46,574] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,580] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,584] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 03:27:46,584] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,589] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,589] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,594] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,595] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,612] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,612] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,612] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,613] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,613] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,613] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,613] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,613] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,646] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,646] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,649] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,649] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,668] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,668] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,670] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,671] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0126: [2024-08-12 03:27:46,748] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,749] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0126: [2024-08-12 03:27:46,749] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,750] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0126: [2024-08-12 03:27:46,750] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,750] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,751] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0126: [2024-08-12 03:27:46,751] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0126: [2024-08-12 03:27:46,780] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,780] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,785] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 03:27:46,785] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,906] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,907] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,907] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,907] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,907] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,907] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,908] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,908] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,934] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,934] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,934] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,935] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,935] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,935] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,935] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,936] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,940] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,940] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,941] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,941] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 03:27:46,961] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,961] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,962] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 03:27:46,962] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0121: [2024-08-12 03:27:46,967] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,967] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,968] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 03:27:46,968] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,061] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,061] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,061] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,061] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,061] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,062] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,062] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,062] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,095] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,096] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,117] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,117] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,117] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,118] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0113: [2024-08-12 03:27:47,133] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 03:27:47,133] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 03:27:47,134] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 03:27:47,134] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 03:27:47,134] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 03:27:47,134] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 03:27:47,134] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 03:27:47,135] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,135] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,136] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,136] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,136] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,136] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,136] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,136] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,136] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0113: [2024-08-12 03:27:47,165] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 03:27:47,165] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 03:27:47,167] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 03:27:47,170] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,376] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,376] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,376] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,376] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,377] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,377] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,377] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,377] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,407] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,408] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,411] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,411] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,428] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,428] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,432] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,433] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,825] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,825] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,825] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,826] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,826] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,826] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,827] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,828] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0123: [2024-08-12 03:27:47,850] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 03:27:47,850] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 03:27:47,850] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 03:27:47,850] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 03:27:47,850] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 03:27:47,850] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 03:27:47,850] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 03:27:47,851] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0108: [2024-08-12 03:27:47,859] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,859] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,859] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 03:27:47,859] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0123: [2024-08-12 03:27:47,882] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 03:27:47,882] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 03:27:47,883] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 03:27:47,884] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 03:27:47,904] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 03:27:47,904] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 03:27:47,905] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 03:27:47,906] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,954] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,954] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,954] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,954] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,975] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,975] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,976] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,978] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,989] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,989] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,989] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,989] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,989] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,989] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,989] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,989] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,989] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,990] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,990] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,990] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 03:27:47,990] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,990] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,991] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 03:27:47,991] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0123: [2024-08-12 03:27:48,187] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 03:27:48,187] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 03:27:48,187] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 03:27:48,188] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 03:27:48,188] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 03:27:48,188] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 03:27:48,188] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 03:27:48,188] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 03:27:48,218] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 03:27:48,221] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 03:27:48,221] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 03:27:48,221] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0127: [2024-08-12 03:27:48,270] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 03:27:48,270] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 03:27:48,270] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 03:27:48,271] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 03:27:48,271] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 03:27:48,271] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 03:27:48,271] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 03:27:48,271] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 03:27:48,317] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 03:27:48,317] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 03:27:48,322] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 03:27:48,322] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0108:  > overriding learning rate value to 0.0002
g0108:  > overriding minimum learning rate value to 1e-05
g0108:  > overriding warmup iterations value to 0
g0108:  > overriding warmup tokens value to 3000000000
g0108:  > overriding total number of iterations value to 6656000
g0108:  > overriding decay tokens value to 300000000000
g0108:  > overriding learning rate decay style value to cosine
g0108:  > overriding start weight decay value to 0.1
g0108:  > overriding end weight decay value to 0.1
g0108:  > overriding total number of weight decay iterations value to 6656000
g0108:  > overriding weight decay incr style value to constant
g0108:  checkpoint version 3.0
g0108:   successfully loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase at iteration 42000
g0127: (min, max) time across ranks (ms):
g0127:     load-checkpoint ................................: (8179.78, 8181.18)
g0108: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-12 03:27:49 
g0108: > building train, validation, and test datasets ...
g0108:  > datasets target sizes (minimum size):
g0108:     train:      6656000
g0108:     validation: 678400
g0108:     test:       12800
g0108: > building train, validation, and test datasets for GPT ...
g0108: Single data path provided for train, valid & test
g0108:  > building dataset index ...
g0108:     reading sizes...
g0108:     reading pointers...
g0108:     reading document index...
g0108:     creating numpy buffer of mmap...
g0108:     creating memory view of numpy buffer...
g0108:  > finished creating indexed dataset in 0.030277 seconds
g0108:     number of documents: 250886
g0108:  > dataset split:
g0108:     train:
g0108:      document indices in [0, 238091) total of 238091 documents
g0108:     validation:
g0108:      document indices in [238091, 250635) total of 12544 documents
g0108:     test:
g0108:      document indices in [250635, 250886) total of 251 documents
g0108:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0108:  > last epoch number of samples (457326) is larger than 80% of number of samples per epoch (476821), setting separate_last_epoch to False
g0108:  > elasped time to build and save doc-idx mapping (seconds): 0.133689
g0108:     using:
g0108:      number of documents:       238091
g0108:      number of epochs:          14
g0108:      sequence length:           2048
g0108:      total number of samples:   6675495
g0108:  > elasped time to build and save sample-idx mapping (seconds): 0.195884
g0108:  > building shuffle index with split [0, 6675495) and [6675495, 6675495) ...
g0108:  > elasped time to build and save shuffle-idx mapping (seconds): 0.178760
g0108:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/08ead6f9dd193a5300c72fd3e8930ec7_doc_idx.npy
g0108:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/08ead6f9dd193a5300c72fd3e8930ec7_sample_idx.npy
g0108:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/08ead6f9dd193a5300c72fd3e8930ec7_shuffle_idx.npy
g0108:     loaded indexed file in 0.010 seconds
g0108:     total number of samples: 6675496
g0108:     total number of epochs: 14
g0108:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/d5dc985126ccf4420fa9c4046a4e1bc0_doc_idx.npy
g0108:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/d5dc985126ccf4420fa9c4046a4e1bc0_sample_idx.npy
g0108:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/d5dc985126ccf4420fa9c4046a4e1bc0_shuffle_idx.npy
g0108:     loaded indexed file in 0.091 seconds
g0108:     total number of samples: 692334
g0108:     total number of epochs: 27
g0108:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/7a71560402d99012f7c883a368e28919_doc_idx.npy
g0108:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/7a71560402d99012f7c883a368e28919_sample_idx.npy
g0108:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/7a71560402d99012f7c883a368e28919_shuffle_idx.npy
g0108:     loaded indexed file in 0.026 seconds
g0108:     total number of samples: 12809
g0108:     total number of epochs: 34
g0108: > finished creating GPT datasets ...
g0108: [after dataloaders are built] datetime: 2024-08-12 03:27:51 
g0108: done with setup ...
g0127: (min, max) time across ranks (ms):
g0127:     model-and-optimizer-setup ......................: (10856.32, 10863.94)
g0127:     train/valid/test-data-iterators-setup ..........: (2534.64, 2558.07)
g0108: training ...
g0108: [before the start of training step] datetime: 2024-08-12 03:27:52 
g0108: [2024-08-12 03:28:42,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=42010, skipped=71, lr=[0.00019965900617800823, 0.00019965900617800823], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 42010 loss: 6.8229 iter time (s): 4.998 samples/sec: 25.611
g0127:  iteration    42010/   52000 | consumed samples:      5377280 | consumed tokens:  11012669440 | elapsed time per iteration (ms): 5033.4 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 8.090974E+00 | loss scale: 32.0 | grad norm: 6.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.430 | tokens per gpu per second (tgs): 1627.543 | TFLOPs: 13.10 |
g0108: [Rank 0] (after 42010 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 11650.0 | max reserved: 11650.0
g0126: [Rank 24] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 5054.0 | max reserved: 5054.0
g0120: [Rank 8] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8096.0 | max reserved: 8096.0
g0113: [Rank 4] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 8990.0 | max reserved: 8990.0
g0125: [Rank 20] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5414.0 | max reserved: 5414.0
g0121: [Rank 12] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7202.0 | max reserved: 7202.0
g0123: [Rank 16] (after 42010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6308.0 | max reserved: 6308.0
g0127: [Rank 28] (after 42010 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0108: [2024-08-12 03:29:23,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=42020, skipped=71, lr=[0.00019965878314753572, 0.00019965878314753572], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 42020 loss: 6.4083 iter time (s): 4.016 samples/sec: 31.874
g0127:  iteration    42020/   52000 | consumed samples:      5378560 | consumed tokens:  11015290880 | elapsed time per iteration (ms): 4085.2 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 6.506742E+00 | loss scale: 32.0 | grad norm: 3.819 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.333 | tokens per gpu per second (tgs): 2005.282 | TFLOPs: 16.14 |
g0108: [2024-08-12 03:30:02,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=42030, skipped=71, lr=[0.00019965856004428058, 0.00019965856004428058], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 42030 loss: 6.4869 iter time (s): 3.933 samples/sec: 32.546
g0127:  iteration    42030/   52000 | consumed samples:      5379840 | consumed tokens:  11017912320 | elapsed time per iteration (ms): 3965.5 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 6.407906E+00 | loss scale: 32.0 | grad norm: 2.896 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.279 | tokens per gpu per second (tgs): 2065.838 | TFLOPs: 16.62 |
g0123: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42034
g0123: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42034
g0123: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0123: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0123: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42034
g0123: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42034
g0123: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0125: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42034
g0125: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42034
g0113: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0113: Grad overflow on iteration 42034
g0125: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0126: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42034
g0113: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0121: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42034
g0121: Grad overflow on iteration 42034
g0113: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0125: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0113: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42034
g0120: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0108: Grad overflow on iteration 42034
g0127: Grad overflow on iteration 42034
g0121: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42034
g0120: Grad overflow on iteration 42034
g0120: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0120: Grad overflow on iteration 42034
g0121: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42034
g0120: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0127: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0127: Grad overflow on iteration 42034
g0108: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42034
g0108: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42034
g0120: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0120: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42034
g0120: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42034
g0120: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0127: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0108: Grad overflow on iteration 42034
g0108: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0113: Grad overflow on iteration 42034
g0121: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0121: Grad overflow on iteration 42034
g0108: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0123: [2024-08-12 03:30:23,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0126: Grad overflow on iteration 42034
g0127: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0108: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0127: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:30:23,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0127: Grad overflow on iteration 42034
g0126: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42034
g0108: Grad overflow on iteration 42034
g0126: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0127: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0126: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0126: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42034
g0120: [2024-08-12 03:30:23,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0126: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42034
g0126: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0126: [2024-08-12 03:30:23,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0121: [2024-08-12 03:30:23,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0127: [2024-08-12 03:30:23,581] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42034
g0108: [2024-08-12 03:30:23,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0108: [2024-08-12 03:30:23,581] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32.0, reducing to 16.0
g0113: [2024-08-12 03:30:23,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0127: [2024-08-12 03:30:23,582] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32.0 to 16.0
g0121: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42037
g0121: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0121: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42037
g0121: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42037
g0121: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42037
g0121: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0121: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0126: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42037
g0126: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42037
g0113: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42037
g0121: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0120: Grad overflow on iteration 42037
g0113: Grad overflow on iteration 42037
g0108: Grad overflow on iteration 42037
g0120: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42037
g0108: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0125: Grad overflow on iteration 42037
g0125: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0125: Grad overflow on iteration 42037
g0125: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0113: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0120: Grad overflow on iteration 42037
g0108: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42037
g0126: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0127: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42037
g0123: Grad overflow on iteration 42037
g0127: Grad overflow on iteration 42037
g0108: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42037
g0127: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:30:36,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0126: Grad overflow on iteration 42037
g0126: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42037
g0123: Grad overflow on iteration 42037
g0126: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0123: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0125: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0123: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:30:36,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0123: Grad overflow on iteration 42037
g0127: Grad overflow on iteration 42037
g0125: Grad overflow on iteration 42037
g0125: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0127: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0125: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0113: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0127: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0123: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0120: Grad overflow on iteration 42037
g0123: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0113: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42037
g0125: Grad overflow on iteration 42037
g0127: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0123: Grad overflow on iteration 42037
g0120: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0127: Grad overflow on iteration 42037
g0113: Grad overflow on iteration 42037
g0127: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0125: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0120: Grad overflow on iteration 42037
g0123: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0113: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0120: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0113: [2024-08-12 03:30:36,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0120: [2024-08-12 03:30:36,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0108: [2024-08-12 03:30:36,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0127: [2024-08-12 03:30:36,281] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 16.0 to 8.0
g0108: [2024-08-12 03:30:36,281] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16.0, reducing to 8.0
g0108: [2024-08-12 03:30:44,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=42040, skipped=73, lr=[0.000199658336868243, 0.000199658336868243], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 42040 loss: 6.4668 iter time (s): 4.131 samples/sec: 30.987
g0127:  iteration    42040/   52000 | consumed samples:      5381120 | consumed tokens:  11020533760 | elapsed time per iteration (ms): 4163.4 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 8.0 | grad norm: 2.821 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.744 | tokens per gpu per second (tgs): 1967.616 | TFLOPs: 15.83 |
g0121: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42040
g0121: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0121: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42040
g0121: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0126: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42040
g0126: Grad overflow on iteration 42040
g0108: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0108: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0108: Grad overflow on iteration 42040
g0126: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42040
g0126: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0125: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42040
g0125: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42040
g0125: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0125: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0125: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42040
g0123: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42040
g0123: Grad overflow on iteration 42040
g0126: Grad overflow on iteration 42040
g0127: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0113: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0123: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0125: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0123: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42040
g0127: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0123: Grad overflow on iteration 42040
g0127: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0113: Grad overflow on iteration 42040
g0127: Grad overflow on iteration 42040
g0120: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42040
g0127: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0120: Grad overflow on iteration 42040
g0123: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0120: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42040
g0125: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0121: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0113: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42040
g0113: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0125: Grad overflow on iteration 42040
g0123: Grad overflow on iteration 42040
g0120: [2024-08-12 03:30:48,823] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0113: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0120: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0120: Grad overflow on iteration 42040
g0126: Grad overflow on iteration 42040
g0120: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0113: Grad overflow on iteration 42040
g0123: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42040
g0121: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42040
g0127: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42040
g0120: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42040
g0127: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0125: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0126: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0108: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42040
g0113: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0123: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0120: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0121: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0127: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42040
g0108: [2024-08-12 03:30:48,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42040
g0108: [2024-08-12 03:30:48,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0108: [2024-08-12 03:30:48,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0108: [2024-08-12 03:30:48,825] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8.0, reducing to 4.0
g0127: [2024-08-12 03:30:48,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 8.0 to 4.0
g0125: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42041
g0113: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42041
g0113: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42041
g0113: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0125: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42041
g0127: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42041
g0127: Grad overflow on iteration 42041
g0127: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: Grad overflow on iteration 42041
g0123: Grad overflow on iteration 42041
g0121: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42041
g0123: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0127: Grad overflow on iteration 42041
g0121: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0121: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42041
g0125: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42041
g0127: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0127: Grad overflow on iteration 42041
g0125: Grad overflow on iteration 42041
g0108: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42041
g0126: Grad overflow on iteration 42041
g0123: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42041
g0126: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42041
g0108: Grad overflow on iteration 42041
g0126: Grad overflow on iteration 42041
g0108: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0123: Grad overflow on iteration 42041
g0120: Grad overflow on iteration 42041
g0125: Grad overflow on iteration 42041
g0121: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0123: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0123: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42041
g0123: Grad overflow on iteration 42041
g0113: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0123: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0127: Grad overflow on iteration 42041
g0127: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0126: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0108: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: Grad overflow on iteration 42041
g0113: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0113: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42041
g0113: Grad overflow on iteration 42041
g0108: Grad overflow on iteration 42041
g0108: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0125: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0126: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0108: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0126: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: Grad overflow on iteration 42041
g0120: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0121: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0126: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0108: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0113: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0108: Grad overflow on iteration 42041
g0108: [2024-08-12 03:30:53,050] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0108: [2024-08-12 03:30:53,050] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4.0, reducing to 2.0
g0127: [2024-08-12 03:30:53,051] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 4.0 to 2.0
g0120: [2024-08-12 03:31:01,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42043
g0125: [2024-08-12 03:31:01,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42043
g0121: [2024-08-12 03:31:01,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42043
g0120: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42043
g0120: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42043
g0120: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0120: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0120: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0121: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42043
g0125: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:01,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:01,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42043
g0121: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42043
g0126: Grad overflow on iteration 42043
g0121: Grad overflow on iteration 42043
g0108: [2024-08-12 03:31:01,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0123: Grad overflow on iteration 42043
g0121: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0113: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0108: Grad overflow on iteration 42043
g0121: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0127: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42043
g0113: Grad overflow on iteration 42043
g0126: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0126: Grad overflow on iteration 42043
g0113: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42043
g0123: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42043
g0123: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0126: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0123: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0126: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0125: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42043
g0125: Grad overflow on iteration 42043
g0126: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0126: Grad overflow on iteration 42043
g0113: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0125: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0126: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0127: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42043
g0127: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42043
g0123: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0127: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0127: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0113: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0113: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42043
g0125: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42043
g0120: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42043
g0120: Grad overflow on iteration 42043
g0108: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42043
g0120: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0126: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42043
g0113: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0125: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0108: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0108: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0126: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0108: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0127: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0123: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42043
g0121: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42043
g0127: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42043
g0121: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0127: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0123: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0108: [2024-08-12 03:31:01,349] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 2.0, reducing to 1.0
g0108: [2024-08-12 03:31:01,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42043
g0108: [2024-08-12 03:31:01,349] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 2.0 to 1.0
g0125: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42045
g0125: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0121: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42045
g0121: Grad overflow on iteration 42045
g0121: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0120: Grad overflow on iteration 42045
g0120: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0121: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42045
g0120: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42045
g0120: Grad overflow on iteration 42045
g0113: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42045
g0113: Grad overflow on iteration 42045
g0113: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0113: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42045
g0120: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0125: Grad overflow on iteration 42045
g0123: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0127: Grad overflow on iteration 42045
g0123: Grad overflow on iteration 42045
g0127: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0126: Grad overflow on iteration 42045
g0108: Grad overflow on iteration 42045
g0108: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42045
g0123: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0123: Grad overflow on iteration 42045
g0123: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0120: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42045
g0120: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0123: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42045
g0113: Grad overflow on iteration 42045
g0123: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0113: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0126: Grad overflow on iteration 42045
g0126: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0113: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0126: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0121: Grad overflow on iteration 42045
g0121: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0121: Grad overflow on iteration 42045
g0123: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0125: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42045
g0125: Grad overflow on iteration 42045
g0125: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0126: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0127: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42045
g0108: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42045
g0108: Grad overflow on iteration 42045
g0123: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0126: Grad overflow on iteration 42045
g0126: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0127: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0121: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0127: Grad overflow on iteration 42045
g0127: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0127: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42045
g0127: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0120: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: [2024-08-12 03:31:09,320] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42045
g0108: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0108: [2024-08-12 03:31:09,321] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1.0, reducing to 1
g0127: [2024-08-12 03:31:09,321] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1.0 to 1
g0125: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42046
g0125: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42046
g0121: Grad overflow on iteration 42046
g0121: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42046
g0113: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42046
g0113: Grad overflow on iteration 42046
g0125: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42046
g0113: Grad overflow on iteration 42046
g0126: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42046
g0113: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42046
g0113: Grad overflow on iteration 42046
g0108: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42046
g0126: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42046
g0120: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42046
g0127: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42046
g0120: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42046
g0126: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42046
g0123: Grad overflow on iteration 42046
g0121: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42046
g0123: Grad overflow on iteration 42046
g0125: Grad overflow on iteration 42046
g0123: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42046
g0121: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42046
g0125: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42046
g0127: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42046
g0123: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42046
g0113: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42046
g0127: Grad overflow on iteration 42046
g0120: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42046
g0113: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42046
g0108: [2024-08-12 03:31:13,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:13,557] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42046
g0127: [2024-08-12 03:31:13,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:13,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:13,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:13,558] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42046
g0108: [2024-08-12 03:31:13,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:13,558] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:31:13,558] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42047
g0123: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42047
g0125: Grad overflow on iteration 42047
g0123: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42047
g0125: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42047
g0121: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42047
g0125: Grad overflow on iteration 42047
g0121: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42047
g0126: Grad overflow on iteration 42047
g0113: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42047
g0121: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42047
g0126: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42047
g0126: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42047
g0127: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42047
g0113: Grad overflow on iteration 42047
g0123: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42047
g0120: Grad overflow on iteration 42047
g0108: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42047
g0120: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42047
g0125: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42047
g0108: Grad overflow on iteration 42047
g0120: Grad overflow on iteration 42047
g0120: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42047
g0120: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42047
g0127: Grad overflow on iteration 42047
g0125: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42047
g0113: Grad overflow on iteration 42047
g0126: Grad overflow on iteration 42047
g0120: Grad overflow on iteration 42047
g0120: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42047
g0113: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42047
g0108: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42047
g0108: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:17,943] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:17,944] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:17,944] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:17,944] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0113: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42048
g0113: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42048
g0108: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42048
g0125: Grad overflow on iteration 42048
g0108: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42048
g0121: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42048
g0121: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42048
g0108: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42048
g0125: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42048
g0120: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42048
g0127: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42048
g0125: Grad overflow on iteration 42048
g0125: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42048
g0126: Grad overflow on iteration 42048
g0123: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42048
g0123: Grad overflow on iteration 42048
g0127: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42048
g0123: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42048
g0123: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42048
g0120: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42048
g0120: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42048
g0127: Grad overflow on iteration 42048
g0120: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42048
g0126: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42048
g0126: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:21,986] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42048
g0121: Grad overflow on iteration 42048
g0125: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42048
g0125: Grad overflow on iteration 42048
g0120: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:21,987] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0125: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42048
g0126: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42048
g0126: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42048
g0127: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42048
g0108: [2024-08-12 03:31:21,987] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42049
g0113: Grad overflow on iteration 42049
g0125: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42049
g0125: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42049
g0125: Grad overflow on iteration 42049
g0113: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42049
g0125: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42049
g0113: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:26,185] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42049
g0127: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:26,185] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42049
g0121: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42049
g0126: Grad overflow on iteration 42049
g0108: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42049
g0113: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42049
g0123: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42049
g0126: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42049
g0125: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42049
g0127: Grad overflow on iteration 42049
g0125: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42049
g0123: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42049
g0123: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42049
g0123: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42049
g0127: Grad overflow on iteration 42049
g0123: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42049
g0121: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42049
g0127: Grad overflow on iteration 42049
g0126: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42049
g0121: Grad overflow on iteration 42049
g0126: Grad overflow on iteration 42049
g0113: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42049
g0121: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42049
g0126: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42049
g0121: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42049
g0121: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:26,186] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:31:26,186] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:26,187] [INFO] [logging.py:96:log_dist] [Rank 0] step=42050, skipped=81, lr=[0.00019965822525293088, 0.00019965822525293088], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 42050 loss: nan iter time (s): 4.130 samples/sec: 30.991
g0127:  iteration    42050/   52000 | consumed samples:      5382400 | consumed tokens:  11023155200 | elapsed time per iteration (ms): 4162.8 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 3.043 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.749 | tokens per gpu per second (tgs): 1967.926 | TFLOPs: 15.84 |
g0113: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42050
g0113: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42050
g0125: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42050
g0125: Grad overflow on iteration 42050
g0121: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42050
g0120: Grad overflow on iteration 42050
g0125: Grad overflow on iteration 42050
g0120: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42050
g0127: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42050
g0123: Grad overflow on iteration 42050
g0120: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42050
g0123: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42050
g0113: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42050
g0108: Grad overflow on iteration 42050
g0108: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42050
g0123: Grad overflow on iteration 42050
g0108: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42050
g0108: Grad overflow on iteration 42050
g0121: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42050
g0126: Grad overflow on iteration 42050
g0108: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42050
g0126: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42050
g0126: Grad overflow on iteration 42050
g0127: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42050
g0126: Grad overflow on iteration 42050
g0125: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:30,466] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42050
g0121: Grad overflow on iteration 42050
g0125: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42050
g0125: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42050
g0121: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42050
g0126: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42050
g0127: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42050
g0126: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:30,467] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:31:30,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42051
g0125: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42051
g0125: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42051
g0121: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42051
g0121: Grad overflow on iteration 42051
g0113: Grad overflow on iteration 42051
g0121: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42051
g0125: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42051
g0113: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42051
g0125: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42051
g0121: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42051
g0113: Grad overflow on iteration 42051
g0113: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42051
g0121: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42051
g0127: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42051
g0127: Grad overflow on iteration 42051
g0127: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42051
g0108: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42051
g0125: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42051
g0123: Grad overflow on iteration 42051
g0127: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42051
g0127: Grad overflow on iteration 42051
g0123: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42051
g0123: Grad overflow on iteration 42051
g0126: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42051
g0120: Grad overflow on iteration 42051
g0126: Grad overflow on iteration 42051
g0120: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42051
g0123: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42051
g0126: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42051
g0120: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42051
g0123: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42051
g0120: [2024-08-12 03:31:34,465] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:34,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:34,466] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:34,466] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0125: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42052
g0125: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42052
g0113: Grad overflow on iteration 42052
g0125: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42052
g0113: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42052
g0125: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42052
g0113: Grad overflow on iteration 42052
g0113: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42052
g0113: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42052
g0121: Grad overflow on iteration 42052
g0120: Grad overflow on iteration 42052
g0123: Grad overflow on iteration 42052
g0126: Grad overflow on iteration 42052
g0126: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42052
g0126: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42052
g0123: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42052
g0120: Grad overflow on iteration 42052
g0123: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42052
g0127: Grad overflow on iteration 42052
g0120: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42052
g0120: Grad overflow on iteration 42052
g0121: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42052
g0126: Grad overflow on iteration 42052
g0121: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42052
g0108: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42052
g0108: Grad overflow on iteration 42052
g0127: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42052
g0108: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42052
g0126: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42052
g0126: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42052
g0127: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42052
g0108: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42052
g0120: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:38,531] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:38,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:38,532] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0123: [2024-08-12 03:31:38,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:38,532] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:42,591] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:42,591] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42053
g0113: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:42,591] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42053
g0120: Grad overflow on iteration 42053
g0125: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:42,591] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42053
g0127: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42053
g0126: [2024-08-12 03:31:42,591] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42053
g0123: [2024-08-12 03:31:42,591] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:42,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42053
g0123: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42053
g0123: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:42,591] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42053
g0120: [2024-08-12 03:31:42,591] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42053
g0113: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:42,591] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:42,591] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42053
g0121: Grad overflow on iteration 42053
g0126: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42053
g0123: Grad overflow on iteration 42053
g0127: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42053
g0120: Grad overflow on iteration 42053
g0127: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42053
g0127: Grad overflow on iteration 42053
g0108: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42053
g0113: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42053
g0126: Grad overflow on iteration 42053
g0120: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42053
g0120: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42053
g0121: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42053
g0121: Grad overflow on iteration 42053
g0120: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42053
g0108: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42053
g0108: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42053
g0127: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42053
g0125: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42053
g0121: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42053
g0121: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42053
g0121: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:42,592] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:42,593] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:42,593] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42054
g0113: Grad overflow on iteration 42054
g0125: Grad overflow on iteration 42054
g0108: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42054
g0120: Grad overflow on iteration 42054
g0125: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42054
g0113: Grad overflow on iteration 42054
g0120: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42054
g0121: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42054
g0120: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42054
g0121: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42054
g0121: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42054
g0113: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42054
g0113: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42054
g0126: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42054
g0123: Grad overflow on iteration 42054
g0121: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42054
g0123: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:46,902] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42054
g0120: Grad overflow on iteration 42054
g0126: Grad overflow on iteration 42054
g0123: Grad overflow on iteration 42054
g0121: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42054
g0121: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42054
g0125: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42054
g0125: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42054
g0123: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42054
g0127: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42054
g0126: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42054
g0127: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42054
g0126: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42054
g0108: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42054
g0108: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42054
g0123: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:46,903] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:46,904] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:46,904] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42055
g0113: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42055
g0113: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42055
g0113: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42055
g0113: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42055
g0125: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42055
g0125: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42055
g0120: Grad overflow on iteration 42055
g0120: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42055
g0123: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42055
g0123: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42055
g0126: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42055
g0121: Grad overflow on iteration 42055
g0120: Grad overflow on iteration 42055
g0126: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42055
g0125: Grad overflow on iteration 42055
g0127: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42055
g0125: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42055
g0125: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42055
g0108: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42055
g0125: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42055
g0125: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42055
g0125: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42055
g0123: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42055
g0126: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42055
g0123: Grad overflow on iteration 42055
g0108: Grad overflow on iteration 42055
g0121: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42055
g0120: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42055
g0120: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42055
g0127: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42055
g0127: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:51,083] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0123: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42055
g0120: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:51,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:51,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:51,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:51,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42056
g0120: Grad overflow on iteration 42056
g0108: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42056
g0113: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42056
g0108: Grad overflow on iteration 42056
g0113: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42056
g0125: Grad overflow on iteration 42056
g0127: Grad overflow on iteration 42056
g0126: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42056
g0113: Grad overflow on iteration 42056
g0121: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42056
g0127: Grad overflow on iteration 42056
g0121: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42056
g0120: Grad overflow on iteration 42056
g0126: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42056
g0127: Grad overflow on iteration 42056
g0121: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42056
g0125: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42056
g0108: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42056
g0121: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42056
g0113: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42056
g0126: [2024-08-12 03:31:55,196] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42056
g0125: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42056
g0120: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42056
g0120: Grad overflow on iteration 42056
g0123: Grad overflow on iteration 42056
g0125: Grad overflow on iteration 42056
g0123: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42056
g0108: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42056
g0126: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42056
g0127: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42056
g0113: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42056
g0123: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:55,197] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:55,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:55,198] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:55,198] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0113: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42057
g0113: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42057
g0113: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42057
g0113: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42057
g0123: Grad overflow on iteration 42057
g0123: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42057
g0121: Grad overflow on iteration 42057
g0125: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42057
g0125: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42057
g0125: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42057
g0127: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42057
g0121: Grad overflow on iteration 42057
g0127: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42057
g0127: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42057
g0126: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42057
g0108: Grad overflow on iteration 42057
g0127: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42057
g0120: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42057
g0120: Grad overflow on iteration 42057
g0120: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42057
g0126: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42057
g0125: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42057
g0121: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42057
g0123: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42057
g0123: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42057
g0126: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42057
g0108: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42057
g0125: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42057
g0120: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42057
g0108: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42057
g0126: Grad overflow on iteration 42057
g0108: Grad overflow on iteration 42057
g0120: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:31:59,452] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:31:59,453] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:31:59,453] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42058
g0108: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42058
g0121: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42058
g0121: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42058
g0121: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42058
g0121: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42058
g0123: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42058
g0120: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42058
g0127: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42058
g0121: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42058
g0123: Grad overflow on iteration 42058
g0113: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42058
g0123: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42058
g0123: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42058
g0126: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42058
g0120: Grad overflow on iteration 42058
g0125: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42058
g0120: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42058
g0126: Grad overflow on iteration 42058
g0125: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42058
g0120: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42058
g0120: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42058
g0126: Grad overflow on iteration 42058
g0127: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42058
g0126: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42058
g0126: Grad overflow on iteration 42058
g0125: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42058
g0113: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42058
g0125: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:03,467] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42058
g0123: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42058
g0127: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42058
g0108: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42058
g0108: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:03,468] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:03,468] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0125: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42059
g0126: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42059
g0113: Grad overflow on iteration 42059
g0125: Grad overflow on iteration 42059
g0108: Grad overflow on iteration 42059
g0123: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42059
g0108: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42059
g0123: Grad overflow on iteration 42059
g0113: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42059
g0123: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42059
g0113: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42059
g0108: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42059
g0121: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42059
g0121: Grad overflow on iteration 42059
g0126: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42059
g0120: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42059
g0126: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42059
g0125: Grad overflow on iteration 42059
g0126: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42059
g0127: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42059
g0126: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42059
g0127: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42059
g0126: Grad overflow on iteration 42059
g0126: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42059
g0108: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42059
g0121: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42059
g0125: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42059
g0120: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42059
g0108: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42059
g0127: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42059
g0127: Grad overflow on iteration 42059
g0123: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:06,952] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0120: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42059
g0121: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:06,952] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:06,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=42060, skipped=91, lr=[0.00019965822525293088, 0.00019965822525293088], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 42060 loss: nan iter time (s): 4.044 samples/sec: 31.652
g0127:  iteration    42060/   52000 | consumed samples:      5383680 | consumed tokens:  11025776640 | elapsed time per iteration (ms): 4076.5 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 3.043 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.399 | tokens per gpu per second (tgs): 2009.548 | TFLOPs: 16.17 |
g0113: [2024-08-12 03:32:10,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:10,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42060
g0113: Grad overflow on iteration 42060
g0125: Grad overflow on iteration 42060
g0108: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:10,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42060
g0108: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42060
g0108: Grad overflow on iteration 42060
g0121: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42060
g0127: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42060
g0108: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42060
g0108: Grad overflow on iteration 42060
g0127: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:10,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42060
g0121: Grad overflow on iteration 42060
g0120: Grad overflow on iteration 42060
g0125: Grad overflow on iteration 42060
g0121: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42060
g0120: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42060
g0121: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:10,824] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42060
g0113: Grad overflow on iteration 42060
g0125: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42060
g0126: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42060
g0126: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42060
g0120: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42060
g0121: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42060
g0120: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42060
g0108: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42060
g0108: Grad overflow on iteration 42060
g0113: Grad overflow on iteration 42060
g0123: Grad overflow on iteration 42060
g0123: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42060
g0123: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42060
g0123: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42060
g0121: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42060
g0113: [2024-08-12 03:32:10,825] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:10,825] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:32:10,826] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42061
g0113: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42061
g0113: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42061
g0113: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42061
g0125: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42061
g0125: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42061
g0125: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42061
g0125: Grad overflow on iteration 42061
g0108: Grad overflow on iteration 42061
g0127: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42061
g0123: Grad overflow on iteration 42061
g0127: Grad overflow on iteration 42061
g0121: Grad overflow on iteration 42061
g0121: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42061
g0120: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42061
g0121: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42061
g0126: Grad overflow on iteration 42061
g0120: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42061
g0126: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42061
g0126: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42061
g0126: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42061
g0120: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42061
g0120: Grad overflow on iteration 42061
g0108: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42061
g0127: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42061
g0121: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42061
g0123: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42061
g0123: [2024-08-12 03:32:14,773] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42061
g0125: Grad overflow on iteration 42061
g0123: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42061
g0127: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:14,774] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0123: Grad overflow on iteration 42061
g0123: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42061
g0108: [2024-08-12 03:32:14,774] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42062
g0121: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42062
g0123: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42062
g0108: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42062
g0125: Grad overflow on iteration 42062
g0125: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42062
g0121: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42062
g0108: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42062
g0108: Grad overflow on iteration 42062
g0113: Grad overflow on iteration 42062
g0108: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42062
g0113: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42062
g0127: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42062
g0113: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42062
g0127: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42062
g0125: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42062
g0126: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:18,759] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42062
g0125: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42062
g0123: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42062
g0123: Grad overflow on iteration 42062
g0123: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42062
g0127: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42062
g0113: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42062
g0125: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42062
g0123: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42062
g0123: Grad overflow on iteration 42062
g0113: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42062
g0127: Grad overflow on iteration 42062
g0126: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42062
g0123: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42062
g0120: Grad overflow on iteration 42062
g0121: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:18,760] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42062
g0108: [2024-08-12 03:32:18,761] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 03:32:18,761] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42063
g0108: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42063
g0123: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42063
g0125: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42063
g0125: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42063
g0123: Grad overflow on iteration 42063
g0123: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42063
g0125: Grad overflow on iteration 42063
g0120: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42063
g0126: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42063
g0113: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42063
g0120: Grad overflow on iteration 42063
g0121: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42063
g0121: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42063
g0113: Grad overflow on iteration 42063
g0125: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42063
g0113: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42063
g0126: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42063
g0125: Grad overflow on iteration 42063
g0113: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42063
g0113: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42063
g0125: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42063
g0126: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42063
g0126: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:22,929] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42063
g0120: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42063
g0108: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42063
g0123: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42063
g0121: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42063
g0125: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42063
g0126: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42063
g0120: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42063
g0123: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:22,930] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42063
g0127: [2024-08-12 03:32:22,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:22,931] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:22,931] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0121: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42064
g0121: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42064
g0121: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42064
g0125: Grad overflow on iteration 42064
g0125: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42064
g0125: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42064
g0113: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42064
g0125: Grad overflow on iteration 42064
g0125: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42064
g0125: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42064
g0108: Grad overflow on iteration 42064
g0120: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42064
g0127: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42064
g0120: Grad overflow on iteration 42064
g0108: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42064
g0120: Grad overflow on iteration 42064
g0108: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42064
g0108: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42064
g0123: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42064
g0123: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42064
g0113: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42064
g0120: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42064
g0126: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42064
g0120: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42064
g0113: Grad overflow on iteration 42064
g0123: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42064
g0126: Grad overflow on iteration 42064
g0127: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42064
g0126: Grad overflow on iteration 42064
g0126: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42064
g0113: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42064
g0120: [2024-08-12 03:32:27,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:27,083] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42064
g0108: [2024-08-12 03:32:27,083] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:32:27,084] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42065
g0113: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42065
g0125: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42065
g0121: Grad overflow on iteration 42065
g0120: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42065
g0125: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42065
g0121: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42065
g0121: Grad overflow on iteration 42065
g0125: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42065
g0121: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42065
g0120: Grad overflow on iteration 42065
g0123: Grad overflow on iteration 42065
g0113: Grad overflow on iteration 42065
g0126: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42065
g0123: Grad overflow on iteration 42065
g0120: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42065
g0113: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42065
g0123: Grad overflow on iteration 42065
g0126: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42065
g0113: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42065
g0120: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42065
g0123: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42065
g0120: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42065
g0121: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42065
g0108: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42065
g0121: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42065
g0121: Grad overflow on iteration 42065
g0126: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42065
g0127: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42065
g0127: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42065
g0108: Grad overflow on iteration 42065
g0127: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42065
g0108: [2024-08-12 03:32:30,983] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:30,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:30,984] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:32:30,984] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:35,177] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42066
g0120: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42066
g0120: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42066
g0120: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42066
g0123: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42066
g0120: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42066
g0123: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42066
g0123: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42066
g0123: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42066
g0121: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42066
g0123: Grad overflow on iteration 42066
g0121: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42066
g0113: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42066
g0126: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42066
g0113: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42066
g0113: Grad overflow on iteration 42066
g0125: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42066
g0113: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42066
g0121: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42066
g0121: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42066
g0125: Grad overflow on iteration 42066
g0121: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42066
g0108: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42066
g0121: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42066
g0127: Grad overflow on iteration 42066
g0108: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42066
g0113: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42066
g0125: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:35,178] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42066
g0126: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42066
g0126: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42066
g0127: Grad overflow on iteration 42066
g0127: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42066
g0127: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:35,178] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42067
g0126: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42067
g0126: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42067
g0120: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42067
g0121: Grad overflow on iteration 42067
g0113: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42067
g0125: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42067
g0120: Grad overflow on iteration 42067
g0121: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42067
g0125: Grad overflow on iteration 42067
g0113: Grad overflow on iteration 42067
g0125: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42067
g0108: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42067
g0108: Grad overflow on iteration 42067
g0121: Grad overflow on iteration 42067
g0108: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42067
g0126: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42067
g0113: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42067
g0113: Grad overflow on iteration 42067
g0125: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42067
g0125: Grad overflow on iteration 42067
g0127: Grad overflow on iteration 42067
g0127: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42067
g0125: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42067
g0127: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42067
g0125: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42067
g0123: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42067
g0123: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42067
g0120: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42067
g0108: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42067
g0108: Grad overflow on iteration 42067
g0123: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42067
g0108: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:39,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:39,398] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0113: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:39,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42068
g0126: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42068
g0126: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42068
g0125: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42068
g0113: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42068
g0126: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42068
g0113: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42068
g0125: Grad overflow on iteration 42068
g0113: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42068
g0123: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42068
g0126: Grad overflow on iteration 42068
g0121: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42068
g0121: Grad overflow on iteration 42068
g0127: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42068
g0127: Grad overflow on iteration 42068
g0123: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42068
g0123: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42068
g0120: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42068
g0113: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42068
g0113: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42068
g0127: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42068
g0120: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42068
g0125: Grad overflow on iteration 42068
g0113: Grad overflow on iteration 42068
g0120: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42068
g0120: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42068
g0127: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42068
g0113: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42068
g0123: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42068
g0127: Grad overflow on iteration 42068
g0108: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42068
g0127: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42068
g0123: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:43,383] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:43,384] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0126: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42069
g0126: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42069
g0127: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42069
g0127: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42069
g0126: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42069
g0120: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42069
g0125: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42069
g0127: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42069
g0125: Grad overflow on iteration 42069
g0121: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42069
g0125: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42069
g0125: Grad overflow on iteration 42069
g0113: Grad overflow on iteration 42069
g0125: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42069
g0120: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42069
g0108: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42069
g0121: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42069
g0125: Grad overflow on iteration 42069
g0126: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42069
g0126: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42069
g0126: Grad overflow on iteration 42069
g0113: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42069
g0121: Grad overflow on iteration 42069
g0120: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42069
g0120: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42069
g0123: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42069
g0108: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42069
g0113: Grad overflow on iteration 42069
g0123: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42069
g0123: Grad overflow on iteration 42069
g0127: Grad overflow on iteration 42069
g0127: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42069
g0121: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:47,726] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:47,727] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:47,727] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:47,727] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:47,727] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 03:32:47,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=42070, skipped=101, lr=[0.00019965822525293088, 0.00019965822525293088], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 42070 loss: nan iter time (s): 4.044 samples/sec: 31.648
g0127:  iteration    42070/   52000 | consumed samples:      5384960 | consumed tokens:  11028398080 | elapsed time per iteration (ms): 4077.5 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 3.043 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.392 | tokens per gpu per second (tgs): 2009.091 | TFLOPs: 16.17 |
g0120: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42070
g0120: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42070
g0121: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42070
g0121: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42070
g0108: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42070
g0121: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42070
g0127: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42070
g0108: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42070
g0121: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42070
g0120: Grad overflow on iteration 42070
g0126: Grad overflow on iteration 42070
g0120: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42070
g0123: Grad overflow on iteration 42070
g0123: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42070
g0123: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42070
g0108: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42070
g0125: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42070
g0123: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42070
g0127: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42070
g0125: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42070
g0125: Grad overflow on iteration 42070
g0126: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42070
g0125: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42070
g0126: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42070
g0113: [2024-08-12 03:32:51,935] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42070
g0125: Grad overflow on iteration 42070
g0127: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42070
g0113: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42070
g0113: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42070
g0113: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42070
g0108: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42070
g0108: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42070
g0126: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:51,936] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:51,937] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:51,937] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0125: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42071
g0120: [2024-08-12 03:32:56,099] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42071
g0123: Grad overflow on iteration 42071
g0126: [2024-08-12 03:32:56,099] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42071
g0125: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42071
g0123: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42071
g0120: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42071
g0126: Grad overflow on iteration 42071
g0108: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42071
g0123: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42071
g0108: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42071
g0125: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42071
g0121: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42071
g0121: Grad overflow on iteration 42071
g0108: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42071
g0121: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42071
g0121: Grad overflow on iteration 42071
g0123: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42071
g0108: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42071
g0113: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42071
g0126: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42071
g0108: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42071
g0121: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42071
g0127: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42071
g0125: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42071
g0121: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42071
g0113: Grad overflow on iteration 42071
g0125: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42071
g0113: Grad overflow on iteration 42071
g0127: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42071
g0127: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42071
g0120: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42071
g0113: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:32:56,100] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:56,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:32:56,101] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:32:56,101] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0125: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42072
g0125: Grad overflow on iteration 42072
g0127: Grad overflow on iteration 42072
g0108: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42072
g0127: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42072
g0127: Grad overflow on iteration 42072
g0121: Grad overflow on iteration 42072
g0123: Grad overflow on iteration 42072
g0125: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42072
g0125: Grad overflow on iteration 42072
g0113: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42072
g0125: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42072
g0113: Grad overflow on iteration 42072
g0108: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42072
g0125: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42072
g0113: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42072
g0127: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42072
g0125: Grad overflow on iteration 42072
g0108: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42072
g0113: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42072
g0108: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42072
g0121: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42072
g0121: Grad overflow on iteration 42072
g0121: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42072
g0121: Grad overflow on iteration 42072
g0108: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42072
g0126: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42072
g0126: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42072
g0123: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42072
g0123: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42072
g0126: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42072
g0120: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42072
g0123: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:00,279] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:00,279] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:33:00,280] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42073
g0121: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42073
g0121: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42073
g0126: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42073
g0126: Grad overflow on iteration 42073
g0126: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42073
g0123: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42073
g0121: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42073
g0123: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42073
g0113: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42073
g0113: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42073
g0123: Grad overflow on iteration 42073
g0113: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42073
g0108: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42073
g0108: Grad overflow on iteration 42073
g0125: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42073
g0108: Grad overflow on iteration 42073
g0120: Grad overflow on iteration 42073
g0108: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42073
g0127: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42073
g0127: Grad overflow on iteration 42073
g0127: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42073
g0127: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42073
g0127: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42073
g0120: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42073
g0113: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42073
g0121: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42073
g0125: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42073
g0123: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42073
g0120: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42073
g0126: [2024-08-12 03:33:04,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:04,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42073
g0125: [2024-08-12 03:33:04,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:04,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:04,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:04,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:04,875] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42073
g0120: [2024-08-12 03:33:04,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:04,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:04,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42074
g0108: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42074
g0108: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42074
g0120: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42074
g0120: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42074
g0120: Grad overflow on iteration 42074
g0113: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42074
g0123: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42074
g0120: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42074
g0120: Grad overflow on iteration 42074
g0123: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42074
g0120: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42074
g0113: Grad overflow on iteration 42074
g0126: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42074
g0113: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42074
g0121: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42074
g0121: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42074
g0121: Grad overflow on iteration 42074
g0123: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42074
g0126: Grad overflow on iteration 42074
g0113: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42074
g0126: Grad overflow on iteration 42074
g0121: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42074
g0125: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42074
g0108: Grad overflow on iteration 42074
g0125: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42074
g0108: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42074
g0126: Grad overflow on iteration 42074
g0108: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42074
g0127: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42074
g0126: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:08,973] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42074
g0127: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:08,973] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42074
g0127: [2024-08-12 03:33:08,974] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42076
g0120: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42076
g0120: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42076
g0120: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42076
g0120: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42076
g0125: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42076
g0123: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42076
g0123: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42076
g0123: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42076
g0123: Grad overflow on iteration 42076
g0125: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42076
g0123: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42076
g0113: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42076
g0108: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42076
g0108: Grad overflow on iteration 42076
g0108: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42076
g0126: Grad overflow on iteration 42076
g0108: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42076
g0120: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42076
g0123: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42076
g0126: Grad overflow on iteration 42076
g0113: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42076
g0121: Grad overflow on iteration 42076
g0108: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42076
g0108: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42076
g0126: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42076
g0113: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42076
g0126: Grad overflow on iteration 42076
g0126: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42076
g0127: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42076
g0127: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42076
g0121: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:17,635] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42076
g0108: [2024-08-12 03:33:17,636] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:17,636] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:33:17,636] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42077
g0108: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42077
g0108: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42077
g0120: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42077
g0120: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42077
g0120: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42077
g0120: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42077
g0113: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42077
g0125: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42077
g0125: Grad overflow on iteration 42077
g0113: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42077
g0127: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42077
g0127: Grad overflow on iteration 42077
g0127: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42077
g0125: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42077
g0127: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42077
g0125: Grad overflow on iteration 42077
g0123: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42077
g0113: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42077
g0121: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42077
g0123: Grad overflow on iteration 42077
g0121: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42077
g0125: Grad overflow on iteration 42077
g0113: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42077
g0123: Grad overflow on iteration 42077
g0127: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42077
g0125: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42077
g0121: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42077
g0126: Grad overflow on iteration 42077
g0121: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42077
g0123: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:21,914] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42077
g0127: Grad overflow on iteration 42077
g0108: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:21,915] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:33:21,915] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42078
g0108: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42078
g0120: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42078
g0108: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42078
g0123: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42078
g0123: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42078
g0125: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42078
g0125: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42078
g0125: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42078
g0125: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42078
g0120: Grad overflow on iteration 42078
g0126: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42078
g0121: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42078
g0126: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42078
g0125: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42078
g0121: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42078
g0121: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42078
g0113: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42078
g0120: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42078
g0120: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42078
g0113: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42078
g0127: Grad overflow on iteration 42078
g0127: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42078
g0127: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42078
g0121: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42078
g0121: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42078
g0113: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42078
g0127: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42078
g0127: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42078
g0113: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42078
g0108: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42078
g0108: [2024-08-12 03:33:26,134] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42078
g0108: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:26,135] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:26,135] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42079
g0120: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42079
g0126: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42079
g0120: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42079
g0126: Grad overflow on iteration 42079
g0120: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42079
g0126: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42079
g0126: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42079
g0120: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42079
g0120: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42079
g0125: Grad overflow on iteration 42079
g0123: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42079
g0125: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42079
g0113: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42079
g0123: Grad overflow on iteration 42079
g0113: Grad overflow on iteration 42079
g0127: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42079
g0125: Grad overflow on iteration 42079
g0113: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42079
g0123: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42079
g0127: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42079
g0127: Grad overflow on iteration 42079
g0123: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42079
g0123: Grad overflow on iteration 42079
g0127: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42079
g0121: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42079
g0113: Grad overflow on iteration 42079
g0121: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42079
g0121: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42079
g0113: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42079
g0108: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42079
g0108: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42079
g0108: [2024-08-12 03:33:29,651] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 03:33:29,651] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:29,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=42080, skipped=110, lr=[0.00019965753283187183, 0.00019965753283187183], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 42080 loss: nan iter time (s): 4.160 samples/sec: 30.772
g0127:  iteration    42080/   52000 | consumed samples:      5386240 | consumed tokens:  11031019520 | elapsed time per iteration (ms): 4192.4 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 3.068 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.532 | tokens per gpu per second (tgs): 1954.021 | TFLOPs: 15.72 |
g0126: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42080
g0126: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42080
g0125: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42080
g0126: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42080
g0126: Grad overflow on iteration 42080
g0120: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42080
g0108: Grad overflow on iteration 42080
g0127: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42080
g0108: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42080
g0123: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42080
g0127: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42080
g0127: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42080
g0120: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42080
g0108: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42080
g0120: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42080
g0125: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42080
g0125: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42080
g0125: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42080
g0125: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42080
g0126: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42080
g0108: Grad overflow on iteration 42080
g0123: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42080
g0125: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42080
g0108: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42080
g0113: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42080
g0113: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42080
g0125: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42080
g0113: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42080
g0123: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42080
g0123: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42080
g0121: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42080
g0121: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42080
g0113: [2024-08-12 03:33:33,876] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:33,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:33,877] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:33:33,877] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:38,070] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42081
g0108: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42081
g0123: Grad overflow on iteration 42081
g0113: [2024-08-12 03:33:38,070] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:38,070] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:38,070] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42081
g0126: Grad overflow on iteration 42081
g0113: Grad overflow on iteration 42081
g0126: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42081
g0123: Grad overflow on iteration 42081
g0121: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42081
g0127: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42081
g0120: Grad overflow on iteration 42081
g0123: Grad overflow on iteration 42081
g0120: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42081
g0121: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42081
g0121: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:38,070] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42081
g0113: Grad overflow on iteration 42081
g0125: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42081
g0125: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42081
g0126: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42081
g0113: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42081
g0125: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42081
g0126: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42081
g0127: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42081
g0127: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42081
g0127: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42081
g0108: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42081
g0108: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42081
g0120: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42081
g0113: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42081
g0121: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42081
g0123: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:38,072] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42081
g0127: [2024-08-12 03:33:38,071] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42081
g0108: [2024-08-12 03:33:38,072] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:38,072] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:38,073] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0121: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42082
g0121: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42082
g0108: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42082
g0108: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42082
g0126: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42082
g0126: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42082
g0108: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42082
g0127: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42082
g0126: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42082
g0127: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42082
g0120: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42082
g0113: Grad overflow on iteration 42082
g0121: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42082
g0113: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42082
g0113: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42082
g0126: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42082
g0120: Grad overflow on iteration 42082
g0120: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42082
g0113: Grad overflow on iteration 42082
g0120: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42082
g0127: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42082
g0127: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42082
g0127: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42082
g0125: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42082
g0125: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42082
g0125: [2024-08-12 03:33:42,138] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42082
g0125: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42082
g0113: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42082
g0113: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42082
g0120: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42082
g0120: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42082
g0123: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:42,139] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42082
g0108: [2024-08-12 03:33:42,139] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:33:42,140] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42083
g0120: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42083
g0121: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42083
g0113: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42083
g0121: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42083
g0120: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42083
g0125: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42083
g0125: Grad overflow on iteration 42083
g0125: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42083
g0113: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42083
g0120: Grad overflow on iteration 42083
g0126: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42083
g0126: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42083
g0126: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42083
g0125: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42083
g0125: Grad overflow on iteration 42083
g0121: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42083
g0108: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42083
g0127: Grad overflow on iteration 42083
g0121: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42083
g0125: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42083
g0108: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42083
g0113: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42083
g0127: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42083
g0108: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42083
g0127: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42083
g0123: Grad overflow on iteration 42083
g0108: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42083
g0123: Grad overflow on iteration 42083
g0123: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42083
g0113: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:46,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42083
g0121: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42083
g0108: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:46,348] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:33:46,348] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42084
g0120: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42084
g0125: Grad overflow on iteration 42084
g0113: Grad overflow on iteration 42084
g0120: Grad overflow on iteration 42084
g0123: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42084
g0123: Grad overflow on iteration 42084
g0120: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42084
g0120: Grad overflow on iteration 42084
g0123: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42084
g0120: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42084
g0121: Grad overflow on iteration 42084
g0121: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42084
g0121: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42084
g0125: Grad overflow on iteration 42084
g0113: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42084
g0113: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42084
g0113: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42084
g0113: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42084
g0121: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42084
g0125: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42084
g0125: Grad overflow on iteration 42084
g0125: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42084
g0125: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42084
g0108: Grad overflow on iteration 42084
g0125: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42084
g0121: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:50,609] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42084
g0126: Grad overflow on iteration 42084
g0127: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42084
g0123: [2024-08-12 03:33:50,609] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42084
g0127: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42084
g0127: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42084
g0126: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:50,608] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:50,609] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:50,609] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0126: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42085
g0126: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42085
g0126: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42085
g0120: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42085
g0120: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42085
g0120: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42085
g0126: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42085
g0123: Grad overflow on iteration 42085
g0120: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42085
g0125: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42085
g0125: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42085
g0125: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42085
g0120: Grad overflow on iteration 42085
g0121: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42085
g0108: Grad overflow on iteration 42085
g0127: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42085
g0127: Grad overflow on iteration 42085
g0127: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42085
g0127: Grad overflow on iteration 42085
g0121: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42085
g0120: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42085
g0125: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42085
g0127: Grad overflow on iteration 42085
g0127: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42085
g0121: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42085
g0127: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42085
g0108: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42085
g0108: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42085
g0113: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42085
g0113: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42085
g0113: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42085
g0113: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42085
g0121: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:54,819] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:54,820] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:54,820] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:54,820] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:54,820] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0113: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42086
g0113: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42086
g0113: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42086
g0123: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42086
g0120: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42086
g0121: Grad overflow on iteration 42086
g0123: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42086
g0121: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42086
g0108: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42086
g0108: Grad overflow on iteration 42086
g0108: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42086
g0108: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42086
g0120: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42086
g0120: Grad overflow on iteration 42086
g0113: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42086
g0126: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42086
g0121: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42086
g0121: Grad overflow on iteration 42086
g0127: Grad overflow on iteration 42086
g0121: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42086
g0127: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:33:58,816] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42086
g0120: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42086
g0126: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: Grad overflow on iteration 42086
g0123: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42086
g0120: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42086
g0113: [2024-08-12 03:33:58,816] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42086
g0125: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42086
g0123: Grad overflow on iteration 42086
g0127: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42086
g0123: [2024-08-12 03:33:58,816] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42086
g0125: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42086
g0126: [2024-08-12 03:33:58,815] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:58,816] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:58,816] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42086
g0108: [2024-08-12 03:33:58,816] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:33:58,816] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0120: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42087
g0120: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42087
g0120: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42087
g0120: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42087
g0113: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42087
g0113: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42087
g0113: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42087
g0126: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42087
g0108: Grad overflow on iteration 42087
g0120: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42087
g0125: Grad overflow on iteration 42087
g0113: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42087
g0113: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42087
g0127: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: Grad overflow on iteration 42087
g0121: Grad overflow on iteration 42087
g0108: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42087
g0108: Grad overflow on iteration 42087
g0121: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42087
g0127: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42087
g0121: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42087
g0123: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42087
g0121: Grad overflow on iteration 42087
g0120: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42087
g0126: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42087
g0123: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42087
g0127: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: Grad overflow on iteration 42087
g0121: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42087
g0126: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: Grad overflow on iteration 42087
g0108: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42087
g0123: Grad overflow on iteration 42087
g0127: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42087
g0123: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: Grad overflow on iteration 42087
g0123: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:02,909] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0127: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:02,909] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42088
g0126: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42088
g0126: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42088
g0126: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42088
g0123: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42088
g0123: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42088
g0123: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42088
g0113: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42088
g0120: Grad overflow on iteration 42088
g0127: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42088
g0120: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42088
g0125: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42088
g0113: Grad overflow on iteration 42088
g0121: Grad overflow on iteration 42088
g0127: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42088
g0121: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42088
g0113: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42088
g0108: Grad overflow on iteration 42088
g0113: Grad overflow on iteration 42088
g0125: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42088
g0113: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42088
g0120: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42088
g0125: Grad overflow on iteration 42088
g0125: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42088
g0125: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42088
g0108: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42088
g0121: [2024-08-12 03:34:07,041] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42088
g0108: Grad overflow on iteration 42088
g0108: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42088
g0121: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42088
g0121: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42088
g0126: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42088
g0127: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:07,042] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:07,043] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0108: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42089
g0108: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42089
g0108: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0108: Grad overflow on iteration 42089
g0108: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42089
g0123: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42089
g0123: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42089
g0123: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42089
g0126: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42089
g0126: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42089
g0108: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 42089
g0108: Grad overflow on iteration 42089
g0126: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42089
g0126: Grad overflow on iteration 42089
g0126: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0126: Grad overflow on iteration 42089
g0127: Grad overflow on iteration 42089
g0126: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42089
g0127: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: Grad overflow on iteration 42089
g0121: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42089
g0113: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0127: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:34:11,158] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42089
g0121: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0121: Grad overflow on iteration 42089
g0121: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: Grad overflow on iteration 42089
g0125: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42089
g0120: Grad overflow on iteration 42089
g0113: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0121: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42089
g0120: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42089
g0113: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0120: Grad overflow on iteration 42089
g0120: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0120: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42089
g0126: Grad overflow on iteration 42089
g0125: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: Grad overflow on iteration 42089
g0125: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0113: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 42089
g0113: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0127: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: Grad overflow on iteration 42089
g0108: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0125: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0126: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0123: [2024-08-12 03:34:11,159] [INFO] [fused_optimizer.py:344:_update_scale] 
g0123: Grad overflow on iteration 42089
g0108: [2024-08-12 03:34:11,159] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1, reducing to 1
g0123: [2024-08-12 03:34:11,160] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1 to 1
g0108: [2024-08-12 03:34:11,160] [INFO] [logging.py:96:log_dist] [Rank 0] step=42090, skipped=120, lr=[0.00019965753283187183, 0.00019965753283187183], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: steps: 42090 loss: nan iter time (s): 4.119 samples/sec: 31.079
g0127:  iteration    42090/   52000 | consumed samples:      5387520 | consumed tokens:  11033640960 | elapsed time per iteration (ms): 4150.8 | learning rate: 1.997E-04 | global batch size:   128 | loss scale: 1.0 | grad norm: 3.068 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.837 | tokens per gpu per second (tgs): 1973.585 | TFLOPs: 15.88 |
