
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0108
    HostName g0108
    Port 2222
    StrictHostKeyChecking no

Host g0113
    HostName g0113
    Port 2222
    StrictHostKeyChecking no

Host g0120
    HostName g0120
    Port 2222
    StrictHostKeyChecking no

Host g0121
    HostName g0121
    Port 2222
    StrictHostKeyChecking no

Host g0123
    HostName g0123
    Port 2222
    StrictHostKeyChecking no

Host g0125
    HostName g0125
    Port 2222
    StrictHostKeyChecking no

Host g0126
    HostName g0126
    Port 2222
    StrictHostKeyChecking no

Host g0127
    HostName g0127
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42829704
g0108 slots=4
g0113 slots=4
g0120 slots=4
g0121 slots=4
g0123 slots=4
g0125 slots=4
g0126 slots=4
g0127 slots=4

[2024-08-12 02:52:26,711] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-12 02:52:30,410] [INFO] [runner.py:463:main] Using IP address of 10.1.4.6 for node g0108
[2024-08-12 02:52:30,411] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0108,g0113,g0120,g0121,g0123,g0125,g0126,g0127
[2024-08-12 02:52:30,411] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0108,g0113,g0120,g0121,g0123,g0125,g0126,g0127 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDEwOCI6IFswLCAxLCAyLCAzXSwgImcwMTEzIjogWzAsIDEsIDIsIDNdLCAiZzAxMjAiOiBbMCwgMSwgMiwgM10sICJnMDEyMSI6IFswLCAxLCAyLCAzXSwgImcwMTIzIjogWzAsIDEsIDIsIDNdLCAiZzAxMjUiOiBbMCwgMSwgMiwgM10sICJnMDEyNiI6IFswLCAxLCAyLCAzXSwgImcwMTI3IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.4.6 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '13631488000' --train-samples '6656000' --train-data-exact-num-epochs '1' --lr '2.0e-4' --min-lr '1.0e-5' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True' --wandb_tag 'other_gpu'
g0108: [2024-08-12 02:52:33,836] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 02:52:33,891] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 02:52:33,919] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-12 02:52:33,920] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 02:52:33,923] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 02:52:33,926] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0127: [2024-08-12 02:52:33,926] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0126: [2024-08-12 02:52:34,006] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 02:52:36,043] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0120: [2024-08-12 02:52:36,043] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0120: [2024-08-12 02:52:36,043] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0120: [2024-08-12 02:52:36,043] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0120: [2024-08-12 02:52:36,043] [INFO] [launch.py:163:main] dist_world_size=32
g0120: [2024-08-12 02:52:36,043] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0108: [2024-08-12 02:52:36,055] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0108: [2024-08-12 02:52:36,055] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0108: [2024-08-12 02:52:36,055] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0108: [2024-08-12 02:52:36,055] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0108: [2024-08-12 02:52:36,055] [INFO] [launch.py:163:main] dist_world_size=32
g0108: [2024-08-12 02:52:36,055] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0121: [2024-08-12 02:52:36,123] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0121: [2024-08-12 02:52:36,123] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0121: [2024-08-12 02:52:36,123] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0121: [2024-08-12 02:52:36,123] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0121: [2024-08-12 02:52:36,123] [INFO] [launch.py:163:main] dist_world_size=32
g0121: [2024-08-12 02:52:36,123] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0113: [2024-08-12 02:52:36,129] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0113: [2024-08-12 02:52:36,129] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0113: [2024-08-12 02:52:36,129] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0113: [2024-08-12 02:52:36,129] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0113: [2024-08-12 02:52:36,130] [INFO] [launch.py:163:main] dist_world_size=32
g0113: [2024-08-12 02:52:36,130] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0127: [2024-08-12 02:52:36,166] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0127: [2024-08-12 02:52:36,166] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0127: [2024-08-12 02:52:36,166] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0127: [2024-08-12 02:52:36,166] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0127: [2024-08-12 02:52:36,166] [INFO] [launch.py:163:main] dist_world_size=32
g0127: [2024-08-12 02:52:36,166] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0125: [2024-08-12 02:52:36,176] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0125: [2024-08-12 02:52:36,176] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0125: [2024-08-12 02:52:36,176] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0123: [2024-08-12 02:52:36,176] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0123: [2024-08-12 02:52:36,176] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0123: [2024-08-12 02:52:36,176] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0123: [2024-08-12 02:52:36,176] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0125: [2024-08-12 02:52:36,176] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0123: [2024-08-12 02:52:36,176] [INFO] [launch.py:163:main] dist_world_size=32
g0123: [2024-08-12 02:52:36,176] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0125: [2024-08-12 02:52:36,176] [INFO] [launch.py:163:main] dist_world_size=32
g0125: [2024-08-12 02:52:36,176] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0126: [2024-08-12 02:52:36,251] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0126: [2024-08-12 02:52:36,251] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0108': [0, 1, 2, 3], 'g0113': [0, 1, 2, 3], 'g0120': [0, 1, 2, 3], 'g0121': [0, 1, 2, 3], 'g0123': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0126': [0, 1, 2, 3], 'g0127': [0, 1, 2, 3]}
g0126: [2024-08-12 02:52:36,251] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0126: [2024-08-12 02:52:36,251] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0108': [0, 1, 2, 3], 'g0113': [4, 5, 6, 7], 'g0120': [8, 9, 10, 11], 'g0121': [12, 13, 14, 15], 'g0123': [16, 17, 18, 19], 'g0125': [20, 21, 22, 23], 'g0126': [24, 25, 26, 27], 'g0127': [28, 29, 30, 31]})
g0126: [2024-08-12 02:52:36,251] [INFO] [launch.py:163:main] dist_world_size=32
g0126: [2024-08-12 02:52:36,251] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0120: [2024-08-12 02:52:39,131] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 02:52:39,131] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 02:52:39,134] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 02:52:39,154] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 02:52:39,159] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 02:52:39,176] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 02:52:39,214] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 02:52:39,217] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 02:52:39,251] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 02:52:39,257] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0127: [2024-08-12 02:52:39,274] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 02:52:39,279] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 02:52:39,279] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 02:52:39,290] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0108: [2024-08-12 02:52:39,291] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 02:52:39,300] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0121: [2024-08-12 02:52:39,306] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0127: [2024-08-12 02:52:39,312] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0126: [2024-08-12 02:52:39,322] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 02:52:39,326] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0126: [2024-08-12 02:52:39,326] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-12 02:52:39,350] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-12 02:52:39,350] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0127: [2024-08-12 02:52:39,369] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0126: [2024-08-12 02:52:39,403] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-12 02:52:39,404] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0126: [2024-08-12 02:52:39,404] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: [2024-08-12 02:52:39,407] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0127: [2024-08-12 02:52:39,408] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-12 02:52:39,436] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0123: [2024-08-12 02:52:39,486] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-12 02:52:39,490] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0120: --------------------------------------------------
g0120: DeepSpeed C++/CUDA extension op report
g0120: --------------------------------------------------
g0120: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.
g0120: --------------------------------------------------
g0120: JIT compiled ops requires ninja
g0120: --------------------------------------------------
g0120: DeepSpeed C++/CUDA extension op report
g0120: --------------------------------------------------
g0120: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.
g0120: --------------------------------------------------
g0120: JIT compiled ops requires ninja
g0120: ninja .................. [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: op name ................ installed .. compatible
g0120: --------------------------------------------------
g0120: ninja .................. [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: op name ................ installed .. compatible
g0120: --------------------------------------------------
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op report
g0121: --------------------------------------------------
g0121: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.
g0121: --------------------------------------------------
g0121: JIT compiled ops requires ninja
g0121: ninja .................. [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: op name ................ installed .. compatible
g0121: --------------------------------------------------
g0120: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0120: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0120: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0120: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0120: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op report
g0121: --------------------------------------------------
g0121: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.
g0121: --------------------------------------------------
g0121: JIT compiled ops requires ninja
g0121: ninja .................. [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: op name ................ installed .. compatible
g0121: --------------------------------------------------
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: ninja .................. [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: op name ................ installed .. compatible
g0108: --------------------------------------------------
g0108: ninja .................. [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: op name ................ installed .. compatible
g0108: --------------------------------------------------
g0120: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: DeepSpeed C++/CUDA extension op report
g0127: --------------------------------------------------
g0127: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0127:       runtime if needed. Op compatibility means that your system
g0127:       meet the required dependencies to JIT install the op.
g0127: --------------------------------------------------
g0127: JIT compiled ops requires ninja
g0127: ninja .................. [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: op name ................ installed .. compatible
g0127: --------------------------------------------------
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: DeepSpeed general environment info:
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0120: torch version .................... 2.0.1+cu118
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0120: DeepSpeed general environment info:
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0120: torch version .................... 2.0.1+cu118
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0126: --------------------------------------------------
g0126: DeepSpeed C++/CUDA extension op report
g0126: --------------------------------------------------
g0126: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0126:       runtime if needed. Op compatibility means that your system
g0126:       meet the required dependencies to JIT install the op.
g0126: --------------------------------------------------
g0126: JIT compiled ops requires ninja
g0126: ninja .................. [92m[OKAY][0m
g0126: --------------------------------------------------
g0126: op name ................ installed .. compatible
g0126: --------------------------------------------------
g0121: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0121: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0121: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0108: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0108: async_iofused_lamb  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0108: 
g0108: fused_adam ............. fused_lion[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0108: ...... [92m[OKAY][0m
g0108: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0108: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0108: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0108: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0127: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0127: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0127: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0127: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0123: ninja .................. [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: op name ................ installed .. compatible
g0123: --------------------------------------------------
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0127: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0126: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0126: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0127: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: ninja .................. [92m[OKAY][0m
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: op name ................ installed .. compatible
g0123: --------------------------------------------------
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0126: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0127: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0127: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0127: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: --------------------------------------------------
g0120: DeepSpeed C++/CUDA extension op report
g0120: --------------------------------------------------
g0120: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.
g0120: --------------------------------------------------
g0120: JIT compiled ops requires ninja
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0126: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: ninja .................. [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: op name ................ installed .. compatible
g0120: --------------------------------------------------
g0127: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0127: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0126: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0127: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0127: --------------------------------------------------
g0126: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0126: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0126: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0127: DeepSpeed general environment info:
g0127: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0127: torch version .................... 2.0.1+cu118
g0127: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0127: deepspeed info ................... 0.12.4, unknown, unknown
g0127: torch cuda version ............... 11.8
g0127: torch hip version ................ None
g0127: nvcc version ..................... 11.8
g0127: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0127: shared memory (/dev/shm) size .... 188.13 GB
g0126: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0126: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0126: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0126: --------------------------------------------------
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0126: DeepSpeed general environment info:
g0126: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0126: torch version .................... 2.0.1+cu118
g0126: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0126: deepspeed info ................... 0.12.4, unknown, unknown
g0126: torch cuda version ............... 11.8
g0126: torch hip version ................ None
g0126: nvcc version ..................... 11.8
g0126: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0126: shared memory (/dev/shm) size .... 188.13 GB
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op report
g0121: --------------------------------------------------
g0121: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.
g0121: --------------------------------------------------
g0121: JIT compiled ops requires ninja
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0121: ninja .................. [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: op name ................ installed .. compatible
g0121: --------------------------------------------------
g0123: ninja .................. [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: op name ................ installed .. compatible
g0123: --------------------------------------------------
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: --------------------------------------------------
g0108: DeepSpeed C++/CUDA extension op report
g0108: --------------------------------------------------
g0108: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0108:       runtime if needed. Op compatibility means that your system
g0108:       meet the required dependencies to JIT install the op.
g0108: --------------------------------------------------
g0108: JIT compiled ops requires ninja
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: ninja .................. [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: op name ................ installed .. compatible
g0108: --------------------------------------------------
g0108: ninja .................. [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: op name ................ installed .. compatible
g0108: --------------------------------------------------
g0127: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0126: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0123: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0120: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0120: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0120: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0120: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0120: --------------------------------------------------
g0120: DeepSpeed C++/CUDA extension op report
g0120: --------------------------------------------------
g0120: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0120:       runtime if needed. Op compatibility means that your system
g0120:       meet the required dependencies to JIT install the op.
g0120: --------------------------------------------------
g0120: JIT compiled ops requires ninja
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ninja .................. [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: op name ................ installed .. compatible
g0113: --------------------------------------------------
g0113: ninja .................. [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: op name ................ installed .. compatible
g0113: --------------------------------------------------
g0120: ninja .................. [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: op name ................ installed .. compatible
g0120: --------------------------------------------------
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0123: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0123: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0123: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0120: [2024-08-12 02:52:42,738] [INFO] [comm.py:637:init_distributed] cdb=None
g0121: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0121: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [2024-08-12 02:52:42,739] [INFO] [comm.py:637:init_distributed] cdb=None
g0120: DeepSpeed general environment info:
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0120: torch version .................... 2.0.1+cu118
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0121: DeepSpeed C++/CUDA extension op report
g0121: --------------------------------------------------
g0121: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0121:       runtime if needed. Op compatibility means that your system
g0121:       meet the required dependencies to JIT install the op.
g0121: --------------------------------------------------
g0121: JIT compiled ops requires ninja
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0108: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: ninja .................. [92m[OKAY][0m
g0108: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: --------------------------------------------------
g0121: op name ................ installed .. compatible
g0108: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0121: --------------------------------------------------
g0108: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0108: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0108: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: [2024-08-12 02:52:42,753] [INFO] [comm.py:637:init_distributed] cdb=None
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0108: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0123: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0123: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0121: [2024-08-12 02:52:42,766] [INFO] [comm.py:637:init_distributed] cdb=None
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0108: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0108: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0108: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: --------------------------------------------------
g0125: DeepSpeed C++/CUDA extension op report
g0125: --------------------------------------------------
g0125: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.
g0125: --------------------------------------------------
g0125: JIT compiled ops requires ninja
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0108: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0108: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0125: ninja .................. [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: op name ................ installed .. compatible
g0125: --------------------------------------------------
g0126: --------------------------------------------------
g0126: DeepSpeed C++/CUDA extension op report
g0126: --------------------------------------------------
g0126: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0126:       runtime if needed. Op compatibility means that your system
g0126:       meet the required dependencies to JIT install the op.
g0126: --------------------------------------------------
g0126: JIT compiled ops requires ninja
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0108: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108: --------------------------------------------------
g0120: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0120: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0120: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: ninja .................. [92m[OKAY][0m
g0126: --------------------------------------------------
g0126: op name ................ installed .. compatible
g0126: --------------------------------------------------
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0120: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0108: DeepSpeed general environment info:
g0108: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0108: torch version .................... 2.0.1+cu118
g0108: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0108: deepspeed info ................... 0.12.4, unknown, unknown
g0108: torch cuda version ............... 11.8
g0108: torch hip version ................ None
g0108: nvcc version ..................... 11.8
g0108: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0108: shared memory (/dev/shm) size .... 188.13 GB
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: [2024-08-12 02:52:42,784] [INFO] [comm.py:637:init_distributed] cdb=None
g0120: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: [2024-08-12 02:52:42,786] [INFO] [comm.py:637:init_distributed] cdb=None
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0120: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0120: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0120: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0120: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0120: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0120: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0120: --------------------------------------------------
g0126: [2024-08-12 02:52:42,800] [INFO] [comm.py:637:init_distributed] cdb=None
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: DeepSpeed general environment info:
g0120: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0120: torch version .................... 2.0.1+cu118
g0120: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0120: deepspeed info ................... 0.12.4, unknown, unknown
g0120: torch cuda version ............... 11.8
g0120: torch hip version ................ None
g0120: nvcc version ..................... 11.8
g0120: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0120: shared memory (/dev/shm) size .... 188.13 GB
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0121: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0121: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0121: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0108: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0121: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0121: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0108: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0108: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0108: using torch.float32 for parameters ...
g0108: ------------------------ arguments ------------------------
g0108:   accumulate_allreduce_grads_in_fp32 .............. False
g0108:   adam_beta1 ...................................... 0.9
g0108:   adam_beta2 ...................................... 0.95
g0108:   adam_eps ........................................ 1e-08
g0108:   add_bias_linear ................................. False
g0108:   add_position_embedding .......................... False
g0108:   adlr_autoresume ................................. False
g0108:   adlr_autoresume_interval ........................ 1000
g0108:   aml_data_download_path .......................... None
g0108:   apply_layernorm_1p .............................. False
g0108:   apply_query_key_layer_scaling ................... False
g0108:   apply_residual_connection_post_layernorm ........ False
g0108:   async_tensor_model_parallel_allreduce ........... False
g0108:   attention_dropout ............................... 0.0
g0108:   attention_softmax_in_fp32 ....................... False
g0108:   barrier_with_L1_time ............................ True
g0108:   bert_binary_head ................................ True
g0108:   bert_embedder_type .............................. megatron
g0108:   bert_load ....................................... None
g0108:   bf16 ............................................ False
g0108:   bias_dropout_fusion ............................. True
g0108:   bias_gelu_fusion ................................ False
g0108:   biencoder_projection_dim ........................ 0
g0108:   biencoder_shared_query_context_model ............ False
g0108:   block_data_path ................................. None
g0108:   checkpoint_activations .......................... False
g0108:   checkpoint_in_cpu ............................... False
g0108:   checkpoint_num_layers ........................... 1
g0108:   classes_fraction ................................ 1.0
g0108:   clip_grad ....................................... 1.0
g0108:   compression_training ............................ False
g0108:   consumed_train_samples .......................... 0
g0108:   consumed_train_tokens ........................... 0
g0108:   consumed_valid_samples .......................... 0
g0108:   contigious_checkpointing ........................ False
g0108:   cpu_optimizer ................................... False
g0108:   cpu_torch_adam .................................. False
g0108:   create_moe_param_group .......................... False
g0108:   curriculum_learning_legacy ...................... False
g0108:   data_cache_path ................................. None
g0108:   data_efficiency_curriculum_learning ............. False
g0108:   data_impl ....................................... mmap
g0108:   data_parallel_random_init ....................... False
g0108:   data_parallel_size .............................. 4
g0108:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document']
g0108:   data_per_class_fraction ......................... 1.0
g0108:   data_sharding ................................... True
g0108:   dataloader_type ................................. single
g0108:   DDP_impl ........................................ local
g0108:   decoder_num_layers .............................. None
g0108:   decoder_seq_length .............................. None
g0108:   deepscale ....................................... False
g0108:   deepscale_config ................................ None
g0108:   deepspeed ....................................... True
g0108:   deepspeed_activation_checkpointing .............. False
g0108:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0108:   deepspeed_mpi ................................... False
g0108:   dino_bottleneck_size ............................ 256
g0108:   dino_freeze_last_layer .......................... 1
g0108:   dino_head_hidden_size ........................... 2048
g0108:   dino_local_crops_number ......................... 10
g0108:   dino_local_img_size ............................. 96
g0108:   dino_norm_last_layer ............................ False
g0108:   dino_teacher_temp ............................... 0.07
g0108:   dino_warmup_teacher_temp ........................ 0.04
g0108:   dino_warmup_teacher_temp_epochs ................. 30
g0108:   distribute_checkpointed_activations ............. False
g0108:   distribute_saved_activations .................... False
g0108:   distributed_backend ............................. nccl
g0108:   distributed_timeout_minutes ..................... 10
g0108:   ds_fused_adam ................................... False
g0108:   ds_inference .................................... False
g0108:   ds_pipeline_enabled ............................. True
g0108:   ds_sequence_parallel_size ....................... 1
g0108:   embedding_path .................................. None
g0108:   embedding_weights_in_fp32 ....................... False
g0108:   empty_unused_memory_level ....................... 0
g0108:   enable_expert_tensor_parallelism ................ False
g0108:   encoder_num_layers .............................. 22
g0108:   encoder_seq_length .............................. 2048
g0108:   end_weight_decay ................................ 0.1
g0108:   eod_mask_loss ................................... False
g0108:   eval_interval ................................... 1000
g0108:   eval_iters ...................................... 100
g0108:   evidence_data_path .............................. None
g0108:   exit_duration_in_mins ........................... 30000000
g0108:   exit_interval ................................... None
g0108:   exit_on_missing_checkpoint ...................... False
g0108:   exit_signal_handler ............................. False
g0108:   expert_interval ................................. 2
g0108:   ffn_hidden_size ................................. 5632
g0108:   finetune ........................................ False
g0108:   force_ds_sequence_parallel ...................... False
g0108:   fp16 ............................................ False
g0108:   fp16_lm_cross_entropy ........................... False
g0108:   fp32_residual_connection ........................ False
g0108:   fp8_amax_compute_algo ........................... most_recent
g0108:   fp8_amax_history_len ............................ 1
g0108:   fp8_e4m3 ........................................ False
g0108:   fp8_hybrid ...................................... False
g0108:   fp8_interval .................................... 1
g0108:   fp8_margin ...................................... 0
g0108:   fp8_wgrad ....................................... True
g0108:   global_batch_size ............................... 128
g0108:   gradient_accumulation_fusion .................... True
g0108:   head_lr_mult .................................... 1.0
g0108:   hidden_dropout .................................. 0.0
g0108:   hidden_size ..................................... 2048
g0108:   hidden_size_teacher ............................. None
g0108:   hysteresis ...................................... 2
g0108:   ict_head_size ................................... None
g0108:   ict_load ........................................ None
g0108:   img_h ........................................... 224
g0108:   img_w ........................................... 224
g0108:   indexer_batch_size .............................. 128
g0108:   indexer_log_interval ............................ 1000
g0108:   inference ....................................... False
g0108:   inference_batch_times_seqlen_threshold .......... 512
g0108:   init_method_std ................................. 0.013
g0108:   init_method_xavier_uniform ...................... False
g0108:   initial_loss_scale .............................. 4294967296
g0108:   iter_per_epoch .................................. 1250
g0108:   kd .............................................. False
g0108:   kd_alpha_ce ..................................... 1
g0108:   kd_beta_ce ...................................... 1
g0108:   kd_temp ......................................... 1.0
g0108:   kv_channels ..................................... 128
g0108:   layernorm_epsilon ............................... 1e-05
g0108:   lazy_mpu_init ................................... None
g0108:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0108:   load_teacher .................................... None
g0108:   local_rank ...................................... 0
g0108:   log_batch_size_to_tensorboard ................... True
g0108:   log_interval .................................... 10
g0108:   log_learning_rate_to_tensorboard ................ True
g0108:   log_loss_scale_to_tensorboard ................... True
g0108:   log_memory_to_tensorboard ....................... False
g0108:   log_num_zeros_in_grad ........................... False
g0108:   log_optimizer_states_to_tensorboard ............. True
g0108:   log_params_norm ................................. False
g0108:   log_timers_to_tensorboard ....................... True
g0108:   log_validation_ppl_to_tensorboard ............... True
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0108:   log_world_size_to_tensorboard ................... False
g0108:   loss_scale ...................................... None
g0113: --------------------------------------------------
g0108:   loss_scale_window ............................... 1000
g0108:   lr .............................................. 0.0002
g0108:   lr_decay_iters .................................. None
g0108:   lr_decay_samples ................................ None
g0108:   lr_decay_style .................................. cosine
g0108:   lr_decay_tokens ................................. 300000000000
g0108:   lr_warmup_fraction .............................. None
g0108:   lr_warmup_iters ................................. 0
g0108:   lr_warmup_samples ............................... 0
g0108:   lr_warmup_tokens ................................ 3000000000
g0108:   make_vocab_size_divisible_by .................... 128
g0108:   mask_factor ..................................... 1.0
g0108:   mask_prob ....................................... 0.15
g0108:   mask_type ....................................... random
g0108:   masked_softmax_fusion ........................... True
g0108:   max_position_embeddings ......................... 2048
g0108:   max_tokens_to_oom ............................... 12000
g0108:   mem_efficient_ln ................................ True
g0108:   memory_centric_tiled_linear ..................... False
g0108:   merge_file ...................................... None
g0108:   micro_batch_size ................................ 1
g0108:   min_loss_scale .................................. 1.0
g0108:   min_lr .......................................... 1e-05
g0108:   mlp_type ........................................ standard
g0108:   mmap_warmup ..................................... False
g0108:   moe_eval_capacity_factor ........................ 1.0
g0108:   moe_expert_parallel_size ........................ 1
g0108:   moe_loss_coeff .................................. 0.1
g0108:   moe_min_capacity ................................ 4
g0108:   moe_token_dropping .............................. True
g0108:   moe_train_capacity_factor ....................... 1.0
g0108:   mos ............................................. False
g0108:   no_load_lr_state ................................ False
g0108:   no_load_optim ................................... None
g0108:   no_load_rng ..................................... None
g0108:   no_persist_layer_norm ........................... False
g0108:   no_pipeline_parallel ............................ False
g0108:   no_save_optim ................................... None
g0108:   no_save_rng ..................................... None
g0108:   normalization ................................... rmsnorm
g0108:   num_attention_heads ............................. 16
g0108:   num_attention_heads_teacher ..................... None
g0108:   num_channels .................................... 3
g0108:   num_classes ..................................... 1000
g0108:   num_experts ..................................... [1]
g0108:   num_experts_switch .............................. None
g0108:   num_experts_teacher ............................. [1]
g0108:   num_key_value_heads ............................. 4
g0108:   num_layers ...................................... 22
g0108:   num_layers_per_virtual_pipeline_stage ........... None
g0108:   num_layers_teacher .............................. None
g0108:   num_workers ..................................... 0
g0108:   onnx_safe ....................................... None
g0108:   openai_gelu ..................................... False
g0108:   optimizer ....................................... adam
g0108:   output_bert_embeddings .......................... False
g0108:   overlap_p2p_comm ................................ False
g0108:   override_opt_param_scheduler .................... True
g0108:   params_dtype .................................... torch.float32
g0108:   partition_activations ........................... False
g0108:   patch_dim ....................................... 16
g0108:   perform_initialization .......................... True
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0108:   pipeline_model_parallel_size .................... 8
g0108:   pipeline_model_parallel_split_rank .............. None
g0108:   profile_backward ................................ False
g0108:   query_in_block_prob ............................. 0.1
g0108:   rampup_batch_size ............................... None
g0108:   random_ltd ...................................... False
g0108:   rank ............................................ 0
g0108:   recompute_granularity ........................... None
g0108:   recompute_method ................................ None
g0108:   recompute_num_layers ............................ 1
g0108:   remote_device ................................... none
g0108:   repeated_dataloader ............................. False
g0108:   reset_attention_mask ............................ False
g0108:   reset_iteration ................................. False
g0108:   reset_position_ids .............................. False
g0108:   retriever_report_topk_accuracies ................ []
g0108:   retriever_score_scaling ......................... False
g0108:   retriever_seq_length ............................ 256
g0108:   retro_add_retriever ............................. False
g0108:   retro_cyclic_train_iters ........................ None
g0108:   retro_encoder_attention_dropout ................. 0.1
g0108:   retro_encoder_hidden_dropout .................... 0.1
g0108:   retro_encoder_layers ............................ 2
g0108:   retro_num_neighbors ............................. 2
g0108:   retro_num_retrieved_chunks ...................... 2
g0108:   retro_return_doc_ids ............................ False
g0108:   retro_workdir ................................... None
g0108:   return_data_index ............................... False
g0108:   rotary_percent .................................. 1.0
g0108:   sample_rate ..................................... 1.0
g0108:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0108:   save_interval ................................... 1000
g0108:   scatter_gather_tensors_in_pipeline .............. True
g0108:   scattered_embeddings ............................ False
g0108:   seed ............................................ 1234
g0108:   seq_length ...................................... 2048
g0108:   sequence_parallel ............................... False
g0108:   sgd_momentum .................................... 0.9
g0108:   short_seq_prob .................................. 0.1
g0108:   skip_train ...................................... False
g0108:   split ........................................... 949,50,1
g0108:   split_transformers .............................. False
g0108:   squared_relu .................................... False
g0108:   standalone_embedding_stage ...................... False
g0108:   start_weight_decay .............................. 0.1
g0108:   swiglu .......................................... True
g0108:   swin_backbone_type .............................. tiny
g0108:   synchronize_each_layer .......................... False
g0108:   tensor_model_parallel_size ...................... 1
g0108:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True
g0108:   tensorboard_log_interval ........................ 1
g0108:   tensorboard_queue_size .......................... 1
g0108:   test_data_path .................................. None
g0108:   tf32 ............................................ False
g0108:   tile_factor ..................................... 1
g0108:   timing_log_level ................................ 0
g0108:   timing_log_option ............................... minmax
g0108:   titles_data_path ................................ None
g0108:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model
g0108:   tokenizer_type .................................. SentencePieceTokenizer
g0108:   topk ............................................ 1
g0108:   train_data_exact_num_epochs ..................... 1
g0108:   train_data_path ................................. None
g0108:   train_desc_path ................................. None
g0108:   train_doc_idx_path .............................. None
g0108:   train_idx_path .................................. None
g0108:   train_iters ..................................... None
g0108:   train_sample_idx_path ........................... None
g0108:   train_samples ................................... 6656000
g0108:   train_shuffle_idx_path .......................... None
g0108:   train_tokens .................................... 13631488000
g0108:   transformer_impl ................................ local
g0108:   transformer_pipeline_model_parallel_size ........ 8
g0108:   universal_checkpoint ............................ False
g0108:   untie_embeddings_and_output_weights ............. True
g0108:   use_checkpoint_args ............................. False
g0108:   use_checkpoint_opt_param_scheduler .............. False
g0108:   use_contiguous_buffers_in_local_ddp ............. True
g0108:   use_cpu_initialization .......................... None
g0108:   use_dataset_only ................................ False
g0108:   use_distributed_optimizer ....................... False
g0108:   use_flash_attn .................................. False
g0108:   use_flash_attn_triton ........................... False
g0108:   use_flash_attn_v1 ............................... False
g0108:   use_flash_attn_v2 ............................... False
g0108:   use_one_sent_docs ............................... False
g0108:   use_pin_memory .................................. False
g0108:   use_ring_exchange_p2p ........................... False
g0108:   use_rotary_position_embeddings .................. True
g0108:   use_tutel ....................................... False
g0108:   use_wandb ....................................... True
g0108:   valid_data_path ................................. None
g0108:   variable_seq_lengths ............................ False
g0108:   virtual_pipeline_model_parallel_size ............ None
g0108:   vision_backbone_type ............................ vit
g0108:   vision_pretraining .............................. False
g0108:   vision_pretraining_type ......................... classify
g0108:   vocab_extra_ids ................................. 0
g0108:   vocab_file ...................................... None
g0108:   vocab_size ...................................... None
g0108:   wandb_entity .................................... yohei-kobashi
g0108:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True
g0108:   wandb_project ................................... encrypted_data_LLM
g0108:   wandb_tag ....................................... other_gpu
g0108:   weight_decay .................................... 0.1
g0108:   weight_decay_incr_style ......................... constant
g0108:   world_size ...................................... 32
g0108:   zero_allgather_bucket_size ...................... 0.0
g0108:   zero_contigious_gradients ....................... False
g0108:   zero_reduce_bucket_size ......................... 0.0
g0108:   zero_reduce_scatter ............................. False
g0108:   zero_stage ...................................... 0
g0108: -------------------- end of arguments ---------------------
g0108: setting number of micro-batches to constant 32
g0108: > building SentencePieceTokenizer tokenizer ...
g0121: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0127: --------------------------------------------------
g0127: DeepSpeed C++/CUDA extension op report
g0127: --------------------------------------------------
g0127: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0127:       runtime if needed. Op compatibility means that your system
g0127:       meet the required dependencies to JIT install the op.
g0127: --------------------------------------------------
g0127: JIT compiled ops requires ninja
g0121: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0121: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0121: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0127: ninja .................. [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: op name ................ installed .. compatible
g0127: --------------------------------------------------
g0127: --------------------------------------------------
g0127: DeepSpeed C++/CUDA extension op report
g0127: --------------------------------------------------
g0127: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0127:       runtime if needed. Op compatibility means that your system
g0127:       meet the required dependencies to JIT install the op.
g0127: --------------------------------------------------
g0127: JIT compiled ops requires ninja
g0121: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0121: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0121: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0127: ninja .................. [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: op name ................ installed .. compatible
g0127: --------------------------------------------------
g0127: --------------------------------------------------
g0127: DeepSpeed C++/CUDA extension op report
g0127: --------------------------------------------------
g0127: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0127:       runtime if needed. Op compatibility means that your system
g0127:       meet the required dependencies to JIT install the op.
g0127: --------------------------------------------------
g0127: JIT compiled ops requires ninja
g0120: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0121: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0121: --------------------------------------------------
g0126: --------------------------------------------------
g0126: DeepSpeed C++/CUDA extension op report
g0126: --------------------------------------------------
g0126: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0126:       runtime if needed. Op compatibility means that your system
g0126:       meet the required dependencies to JIT install the op.
g0126: --------------------------------------------------
g0126: JIT compiled ops requires ninja
g0127: ninja .................. [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: op name ................ installed .. compatible
g0127: --------------------------------------------------
g0126: --------------------------------------------------
g0126: DeepSpeed C++/CUDA extension op report
g0126: --------------------------------------------------
g0126: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0126:       runtime if needed. Op compatibility means that your system
g0126:       meet the required dependencies to JIT install the op.
g0126: --------------------------------------------------
g0126: JIT compiled ops requires ninja
g0126: ninja .................. [92m[OKAY][0m
g0126: --------------------------------------------------
g0126: op name ................ installed .. compatible
g0126: --------------------------------------------------
g0125: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0125: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0125: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: DeepSpeed general environment info:
g0121: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0121: torch version .................... 2.0.1+cu118
g0121: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0121: deepspeed info ................... 0.12.4, unknown, unknown
g0121: torch cuda version ............... 11.8
g0121: torch hip version ................ None
g0121: nvcc version ..................... 11.8
g0121: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0121: shared memory (/dev/shm) size .... 188.13 GB
g0125: --------------------------------------------------
g0125: DeepSpeed C++/CUDA extension op report
g0125: --------------------------------------------------
g0125: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.
g0125: --------------------------------------------------
g0125: JIT compiled ops requires ninja
g0126: ninja .................. [92m[OKAY][0m
g0126: --------------------------------------------------
g0126: op name ................ installed .. compatible
g0126: --------------------------------------------------
g0125: ninja .................. [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: op name ................ installed .. compatible
g0125: --------------------------------------------------
g0126: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: DeepSpeed C++/CUDA extension op report
g0126: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0123:       runtime if needed. Op compatibility means that your system
g0123:       meet the required dependencies to JIT install the op.
g0123: --------------------------------------------------
g0123: JIT compiled ops requires ninja
g0126: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0126: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0126: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0123: ninja .................. [92m[OKAY][0m
g0123: --------------------------------------------------
g0123: op name ................ installed .. compatible
g0123: --------------------------------------------------
g0126: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0126: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: --------------------------------------------------
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: DeepSpeed C++/CUDA extension op report
g0125: --------------------------------------------------
g0125: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.
g0125: --------------------------------------------------
g0125: JIT compiled ops requires ninja
g0125: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: ninja[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0 
g0125: .................. [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: op name ................ installed .. compatible
g0125: --------------------------------------------------
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0125: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0126: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0126: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0126: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0120: [2024-08-12 02:52:42,849] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0126: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0126: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0121: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0126: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0126: --------------------------------------------------
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0126: DeepSpeed general environment info:
g0126: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0126: torch version .................... 2.0.1+cu118
g0126: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0126: deepspeed info ................... 0.12.4, unknown, unknown
g0126: torch cuda version ............... 11.8
g0126: torch hip version ................ None
g0126: nvcc version ..................... 11.8
g0126: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0126: shared memory (/dev/shm) size .... 188.13 GB
g0113: ninja .................. [92m[OKAY][0m
g0113: --------------------------------------------------
g0125: DeepSpeed general environment info:
g0113: op name ................ installed .. compatible
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: --------------------------------------------------
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0123: [2024-08-12 02:52:42,870] [INFO] [comm.py:637:init_distributed] cdb=None
g0123: [2024-08-12 02:52:42,871] [INFO] [comm.py:637:init_distributed] cdb=None
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0127: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0127: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0121: [2024-08-12 02:52:42,875] [INFO] [comm.py:637:init_distributed] cdb=None
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: DeepSpeed C++/CUDA extension op report
g0125: --------------------------------------------------
g0125: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.
g0125: --------------------------------------------------
g0125: JIT compiled ops requires ninja
g0125: ninja .................. [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: op name ................ installed .. compatible
g0125: --------------------------------------------------
g0126: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0126: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0126: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0127: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0127: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0127: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0127: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0126: evoformer_attn ......... [93m[NO][0m ....... **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****[93m[NO][0m
g0126: 
g0126: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0123: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0123: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0126: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0127: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0127: inference_core_ops ..... [92m[YES][0m ragged_device_ops......  [92m[OKAY][0m......
g0127:  [92m[YES][0m ...... [92m[OKAY][0m
g0123: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0125: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0125: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [2024-08-12 02:52:42,890] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0127: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: cutlass_ops ragged_ops............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0127: [92m[OKAY][0m
g0127: quantizer random_ltd..............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0127: [92m[OKAY][0m
g0127: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0127: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0127: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0126: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0108: [2024-08-12 02:52:42,894] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0126: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0126: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0108: > initializing torch distributed ...
g0108: [2024-08-12 02:52:42,895] [INFO] [comm.py:637:init_distributed] cdb=None
g0108: [2024-08-12 02:52:42,895] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0126: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0126: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0126: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0127: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0127: spatial_inferenceragged_device_ops  ............  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0127: 
g0127: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0126: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0126: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0123: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0123: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0123: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0123: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0126: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0126: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0125: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0125: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0125: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0126: --------------------------------------------------
g0127: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0127: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0127: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0126: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0126: --------------------------------------------------
g0123: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0123: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0123: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0127: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0127: --------------------------------------------------
g0127: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0127: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0127: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0125: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0126: DeepSpeed general environment info:
g0126: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0126: torch version .................... 2.0.1+cu118
g0126: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0126: deepspeed info ................... 0.12.4, unknown, unknown
g0126: torch cuda version ............... 11.8
g0126: torch hip version ................ None
g0126: nvcc version ..................... 11.8
g0126: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0126: shared memory (/dev/shm) size .... 188.13 GB
g0125: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0127: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0127: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0127: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0126: DeepSpeed general environment info:
g0126: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0126: torch version .................... 2.0.1+cu118
g0126: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0126: deepspeed info ................... 0.12.4, unknown, unknown
g0126: torch cuda version ............... 11.8
g0126: torch hip version ................ None
g0126: nvcc version ..................... 11.8
g0126: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0126: shared memory (/dev/shm) size .... 188.13 GB
g0123: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0123: --------------------------------------------------
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0127: DeepSpeed general environment info:
g0127: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0127: torch version .................... 2.0.1+cu118
g0127: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0127: deepspeed info ................... 0.12.4, unknown, unknown
g0127: spatial_inferencetorch cuda version  .....................  [92m[YES][0m11.8 
g0127: ......torch hip version  [92m[OKAY][0m................
g0127:  None
g0127: transformernvcc version  .................................  [92m[YES][0m11.8 
g0127: ...... [92m[OKAY][0m
g0127: deepspeed wheel compiled w. stochastic_transformer......  .torch 2.0, cuda 11.8 
g0127: [92m[YES][0m shared memory (/dev/shm) size......  ....[92m[OKAY][0m 
g0127: 188.13 GB
g0120: [2024-08-12 02:52:42,908] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0127: --------------------------------------------------
g0123: DeepSpeed general environment info:
g0123: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0123: torch version .................... 2.0.1+cu118
g0123: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0123: deepspeed info ................... 0.12.4, unknown, unknown
g0123: torch cuda version ............... 11.8
g0123: torch hip version ................ None
g0123: nvcc version ..................... 11.8
g0123: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0123: shared memory (/dev/shm) size .... 188.13 GB
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0127: --------------------------------------------------
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ninja .................. [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: op name ................ installed .. compatible
g0113: --------------------------------------------------
g0127: DeepSpeed general environment info:
g0127: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0127: torch version .................... 2.0.1+cu118
g0127: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0127: deepspeed info ................... 0.12.4, unknown, unknown
g0127: torch cuda version ............... 11.8
g0127: torch hip version ................ None
g0127: nvcc version ..................... 11.8
g0127: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0127: shared memory (/dev/shm) size .... 188.13 GB
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: DeepSpeed general environment info:
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0127: DeepSpeed general environment info:
g0127: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0127: torch version .................... 2.0.1+cu118
g0127: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0127: deepspeed info ................... 0.12.4, unknown, unknown
g0127: torch cuda version ............... 11.8
g0127: torch hip version ................ None
g0127: nvcc version ..................... 11.8
g0127: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0127: shared memory (/dev/shm) size .... 188.13 GB
g0125: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0125: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0126: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0126: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: DeepSpeed general environment info:
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0123: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0121: [2024-08-12 02:52:42,934] [INFO] [comm.py:637:init_distributed] cdb=None
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0125: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0125: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0125: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0113: [2024-08-12 02:52:42,941] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [2024-08-12 02:52:42,941] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0127: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0125: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0127: > setting tensorboard ...
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: DeepSpeed general environment info:
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0127: [2024-08-12 02:52:42,965] [INFO] [comm.py:637:init_distributed] cdb=None
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: [2024-08-12 02:52:42,969] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [2024-08-12 02:52:42,978] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0126: [2024-08-12 02:52:43,014] [INFO] [comm.py:637:init_distributed] cdb=None
g0123: [2024-08-12 02:52:43,016] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: [2024-08-12 02:52:43,016] [INFO] [comm.py:637:init_distributed] cdb=None
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [2024-08-12 02:52:43,023] [INFO] [comm.py:637:init_distributed] cdb=None
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [2024-08-12 02:52:43,028] [INFO] [comm.py:637:init_distributed] cdb=None
g0127: [2024-08-12 02:52:43,029] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [2024-08-12 02:52:43,032] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [2024-08-12 02:52:43,042] [INFO] [comm.py:637:init_distributed] cdb=None
g0127: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [2024-08-12 02:52:43,047] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [2024-08-12 02:52:43,076] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [2024-08-12 02:52:43,097] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0126: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0120: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0121: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0123: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0108-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0108: > initialized tensor model parallel with size 1
g0108: > initialized pipeline model parallel with size 8
g0108: > setting random seeds to 1234 ...
g0108: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0108: > compiling dataset index builder ...
g0108: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0108: make: Nothing to be done for 'default'.
g0108: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0108: >>> done with dataset index builder. Compilation time: 0.072 seconds
g0108: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0108: > compiling and loading fused kernels ...
g0108: Detected CUDA files, patching ldflags
g0108: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0108: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0108: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0108: ninja: no work to do.
g0108: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0108: Detected CUDA files, patching ldflags
g0108: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0108: Building extension module scaled_masked_softmax_cuda...
g0108: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0108: ninja: no work to do.
g0108: Loading extension module scaled_masked_softmax_cuda...
g0108: Detected CUDA files, patching ldflags
g0108: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0108: Building extension module scaled_softmax_cuda...
g0108: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0108: ninja: no work to do.
g0108: Loading extension module scaled_softmax_cuda...
g0108: >>> done with compiling and loading fused kernels. Compilation time: 4.357 seconds
g0108: time to initialize megatron (seconds): 8.622
g0108: [after megatron is initialized] datetime: 2024-08-12 02:52:50 
g0113: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0121: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0125: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0123: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0120: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0108: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0126: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0127: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0125: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0125: wandb:  $ pip install wandb --upgrade
g0125: wandb: Tracking run with wandb version 0.17.5
g0125: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_025251-v3ned57j
g0125: wandb: Run `wandb offline` to turn off syncing.
g0123: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0123: wandb:  $ pip install wandb --upgrade
g0123: wandb: Tracking run with wandb version 0.17.5
g0123: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_025251-yxjxr3em
g0123: wandb: Run `wandb offline` to turn off syncing.
g0121: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0121: wandb:  $ pip install wandb --upgrade
g0121: wandb: Tracking run with wandb version 0.17.5
g0121: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_025251-5eotzivi
g0121: wandb: Run `wandb offline` to turn off syncing.
g0125: wandb: Syncing run g0125.abci.local
g0125: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0125: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/v3ned57j
g0123: wandb: Syncing run g0123.abci.local
g0123: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0123: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/yxjxr3em
g0121: wandb: Syncing run g0121.abci.local
g0121: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0121: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/5eotzivi
g0108: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0108: wandb:  $ pip install wandb --upgrade
g0108: wandb: Tracking run with wandb version 0.17.5
g0108: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_025251-kpkoyycx
g0108: wandb: Run `wandb offline` to turn off syncing.
g0108: wandb: Syncing run g0108.abci.local
g0108: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0108: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/kpkoyycx
g0113: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0113: wandb:  $ pip install wandb --upgrade
g0113: wandb: Tracking run with wandb version 0.17.5
g0113: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_025251-qt5h1uyr
g0113: wandb: Run `wandb offline` to turn off syncing.
g0113: wandb: Syncing run g0113.abci.local
g0113: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0113: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/qt5h1uyr
g0120: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0120: wandb:  $ pip install wandb --upgrade
g0120: wandb: Tracking run with wandb version 0.17.5
g0120: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_025251-3pik48wh
g0120: wandb: Run `wandb offline` to turn off syncing.
g0120: wandb: Syncing run g0120.abci.local
g0120: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0120: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/3pik48wh
g0126: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0126: wandb:  $ pip install wandb --upgrade
g0126: wandb: Tracking run with wandb version 0.17.5
g0126: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_025251-hpcffw4g
g0126: wandb: Run `wandb offline` to turn off syncing.
g0126: wandb: Syncing run g0126.abci.local
g0126: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0126: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/hpcffw4g
g0127: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0127: wandb:  $ pip install wandb --upgrade
g0127: wandb: Tracking run with wandb version 0.17.5
g0127: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240812_025251-h1uamsbw
g0127: wandb: Run `wandb offline` to turn off syncing.
g0127: wandb: Syncing run g0127.abci.local
g0127: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0127: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/h1uamsbw
g0108: building GPT model ...
g0108: [2024-08-12 02:52:52,839] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0108: [2024-08-12 02:52:52,841] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0108: [2024-08-12 02:52:52,841] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 53.49 GB, percent = 14.2%
g0108: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0108: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0108: [2024-08-12 02:52:53,364] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0108: stage=0 layers=5
g0108:      0: _to_float16
g0108:      1: EmbeddingPipe
g0108:      2: ParallelTransformerLayerPipe
g0108:      3: ParallelTransformerLayerPipe
g0108:      4: ParallelTransformerLayerPipe
g0108: stage=1 layers=3
g0108:      5: ParallelTransformerLayerPipe
g0108:      6: ParallelTransformerLayerPipe
g0108:      7: ParallelTransformerLayerPipe
g0108: stage=2 layers=3
g0108:      8: ParallelTransformerLayerPipe
g0108:      9: ParallelTransformerLayerPipe
g0108:     10: ParallelTransformerLayerPipe
g0108: stage=3 layers=3
g0108:     11: ParallelTransformerLayerPipe
g0108:     12: ParallelTransformerLayerPipe
g0108:     13: ParallelTransformerLayerPipe
g0108: stage=4 layers=3
g0108:     14: ParallelTransformerLayerPipe
g0108:     15: ParallelTransformerLayerPipe
g0108:     16: ParallelTransformerLayerPipe
g0108: stage=5 layers=3
g0108:     17: ParallelTransformerLayerPipe
g0108:     18: ParallelTransformerLayerPipe
g0108:     19: ParallelTransformerLayerPipe
g0108: stage=6 layers=3
g0108:     20: ParallelTransformerLayerPipe
g0108:     21: ParallelTransformerLayerPipe
g0108:     22: ParallelTransformerLayerPipe
g0108: stage=7 layers=3
g0108:     23: ParallelTransformerLayerPipe
g0108:     24: MixedFusedRMSNorm
g0108:     25: LMHeadPipe
g0108:   loss: CrossEntropy
g0121:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0126:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0127:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0125:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0123:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0120:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0113:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0108: [2024-08-12 02:52:53,800] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0108: [2024-08-12 02:52:53,801] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0108: [2024-08-12 02:52:53,801] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 53.55 GB, percent = 14.2%
g0108:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0108: setting training iterations to 52000
g0108: > learning rate decay style: cosine
g0108: DeepSpeed is enabled.
g0108: [2024-08-12 02:52:53,803] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0120: [2024-08-12 02:52:53,820] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0120: [2024-08-12 02:52:53,822] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0120: [2024-08-12 02:52:53,823] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0120: [2024-08-12 02:52:53,825] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0126: [2024-08-12 02:52:53,833] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0126: [2024-08-12 02:52:53,833] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0126: [2024-08-12 02:52:53,834] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0126: [2024-08-12 02:52:53,834] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 02:52:53,836] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 02:52:53,836] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 02:52:53,836] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0121: [2024-08-12 02:52:53,836] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 02:52:53,857] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 02:52:53,857] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 02:52:53,858] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-12 02:52:53,859] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0123: [2024-08-12 02:52:53,866] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0123: [2024-08-12 02:52:53,866] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0123: [2024-08-12 02:52:53,866] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0123: [2024-08-12 02:52:53,867] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-12 02:52:53,867] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-12 02:52:53,868] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-12 02:52:53,868] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-12 02:52:53,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0127: [2024-08-12 02:52:53,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0127: [2024-08-12 02:52:53,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0127: [2024-08-12 02:52:53,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0127: [2024-08-12 02:52:53,871] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 02:52:53,992] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0108: [2024-08-12 02:52:53,992] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0108: [2024-08-12 02:52:53,993] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0108: [2024-08-12 02:52:53,993] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0108: [2024-08-12 02:52:53,993] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0108: [2024-08-12 02:52:54,008] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 02:52:54,008] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 02:52:54,008] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 02:52:54,008] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0108: [2024-08-12 02:52:54,008] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0108: [2024-08-12 02:52:54,009] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f7bc813af50>
g0108: [2024-08-12 02:52:54,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0108: [2024-08-12 02:52:54,009] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0108: [2024-08-12 02:52:54,009] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0108:     "partition_activations": false, 
g0108:     "contiguous_memory_optimization": false, 
g0108:     "cpu_checkpointing": false, 
g0108:     "number_checkpoints": null, 
g0108:     "synchronize_checkpoint_boundary": false, 
g0108:     "profile": false
g0108: }
g0108: [2024-08-12 02:52:54,010] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0108: [2024-08-12 02:52:54,010] [INFO] [config.py:983:print]   amp_enabled .................. False
g0108: [2024-08-12 02:52:54,010] [INFO] [config.py:983:print]   amp_params ................... False
g0108: [2024-08-12 02:52:54,010] [INFO] [config.py:983:print]   autotuning_config ............ {
g0108:     "enabled": false, 
g0108:     "start_step": null, 
g0108:     "end_step": null, 
g0108:     "metric_path": null, 
g0108:     "arg_mappings": null, 
g0108:     "metric": "throughput", 
g0108:     "model_info": null, 
g0108:     "results_dir": "autotuning_results", 
g0108:     "exps_dir": "autotuning_exps", 
g0108:     "overwrite": true, 
g0108:     "fast": true, 
g0108:     "start_profile_step": 3, 
g0108:     "end_profile_step": 5, 
g0108:     "tuner_type": "gridsearch", 
g0108:     "tuner_early_stopping": 5, 
g0108:     "tuner_num_trials": 50, 
g0108:     "model_info_path": null, 
g0108:     "mp_size": 1, 
g0108:     "max_train_batch_size": null, 
g0108:     "min_train_batch_size": 1, 
g0108:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0108:     "min_train_micro_batch_size_per_gpu": 1, 
g0108:     "num_tuning_micro_batch_sizes": 3
g0108: }
g0108: [2024-08-12 02:52:54,010] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0108: [2024-08-12 02:52:54,011] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0108: [2024-08-12 02:52:54,011] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0108: [2024-08-12 02:52:54,011] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0108: [2024-08-12 02:52:54,011] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f7bc8176490>
g0108: [2024-08-12 02:52:54,011] [INFO] [config.py:983:print]   communication_data_type ...... None
g0108: [2024-08-12 02:52:54,011] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0108: [2024-08-12 02:52:54,011] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0108: [2024-08-12 02:52:54,011] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0108: [2024-08-12 02:52:54,012] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0108: [2024-08-12 02:52:54,012] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0108: [2024-08-12 02:52:54,012] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0108: [2024-08-12 02:52:54,012] [INFO] [config.py:983:print]   disable_allgather ............ False
g0108: [2024-08-12 02:52:54,012] [INFO] [config.py:983:print]   dump_state ................... False
g0108: [2024-08-12 02:52:54,012] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0108: [2024-08-12 02:52:54,012] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0108: [2024-08-12 02:52:54,012] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0108: [2024-08-12 02:52:54,013] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0108: [2024-08-12 02:52:54,013] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0108: [2024-08-12 02:52:54,013] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0108: [2024-08-12 02:52:54,013] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0108: [2024-08-12 02:52:54,013] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0108: [2024-08-12 02:52:54,013] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0108: [2024-08-12 02:52:54,013] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0108: [2024-08-12 02:52:54,013] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0108:     "enabled": false, 
g0108:     "recompute_fwd_factor": 0.0, 
g0108:     "profile_step": 1, 
g0108:     "module_depth": -1, 
g0108:     "top_modules": 1, 
g0108:     "detailed": true, 
g0108:     "output_file": null
g0108: }
g0108: [2024-08-12 02:52:54,013] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0108: [2024-08-12 02:52:54,014] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0108: [2024-08-12 02:52:54,014] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0108: [2024-08-12 02:52:54,014] [INFO] [config.py:983:print]   global_rank .................. 0
g0108: [2024-08-12 02:52:54,014] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0108: [2024-08-12 02:52:54,014] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0108: [2024-08-12 02:52:54,014] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0108: [2024-08-12 02:52:54,014] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0108: [2024-08-12 02:52:54,014] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0108: [2024-08-12 02:52:54,015] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0108: [2024-08-12 02:52:54,015] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0108: [2024-08-12 02:52:54,015] [INFO] [config.py:983:print]   loss_scale ................... 0
g0108: [2024-08-12 02:52:54,015] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0108: [2024-08-12 02:52:54,015] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0108: [2024-08-12 02:52:54,015] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0108: [2024-08-12 02:52:54,015] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0108: [2024-08-12 02:52:54,015] [INFO] [config.py:983:print]   nebula_config ................ {
g0108:     "enabled": false, 
g0108:     "persistent_storage_path": null, 
g0108:     "persistent_time_interval": 100, 
g0108:     "num_of_version_in_retention": 2, 
g0108:     "enable_nebula_load": true, 
g0108:     "load_path": null
g0108: }
g0108: [2024-08-12 02:52:54,016] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0108: [2024-08-12 02:52:54,016] [INFO] [config.py:983:print]   optimizer_name ............... None
g0108: [2024-08-12 02:52:54,016] [INFO] [config.py:983:print]   optimizer_params ............. None
g0108: [2024-08-12 02:52:54,016] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0108: [2024-08-12 02:52:54,016] [INFO] [config.py:983:print]   pld_enabled .................. False
g0108: [2024-08-12 02:52:54,016] [INFO] [config.py:983:print]   pld_params ................... False
g0108: [2024-08-12 02:52:54,016] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0108: [2024-08-12 02:52:54,017] [INFO] [config.py:983:print]   scheduler_name ............... None
g0108: [2024-08-12 02:52:54,017] [INFO] [config.py:983:print]   scheduler_params ............. None
g0108: [2024-08-12 02:52:54,017] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0108: [2024-08-12 02:52:54,017] [INFO] [config.py:983:print]   sparse_attention ............. None
g0108: [2024-08-12 02:52:54,017] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0108: [2024-08-12 02:52:54,017] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0108: [2024-08-12 02:52:54,017] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0108: [2024-08-12 02:52:54,017] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0108: [2024-08-12 02:52:54,018] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0108: [2024-08-12 02:52:54,018] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0108: [2024-08-12 02:52:54,018] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0108: [2024-08-12 02:52:54,018] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0108: [2024-08-12 02:52:54,018] [INFO] [config.py:983:print]   world_size ................... 4
g0108: [2024-08-12 02:52:54,018] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0108: [2024-08-12 02:52:54,018] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0108: [2024-08-12 02:52:54,018] [INFO] [config.py:983:print]   zero_enabled ................. False
g0108: [2024-08-12 02:52:54,019] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0108: [2024-08-12 02:52:54,019] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0108: [2024-08-12 02:52:54,019] [INFO] [config.py:969:print_user_config]   json = {
g0108:     "train_batch_size": 128, 
g0108:     "train_micro_batch_size_per_gpu": 1, 
g0108:     "steps_per_print": 10, 
g0108:     "zero_optimization": {
g0108:         "stage": 0
g0108:     }, 
g0108:     "gradient_clipping": 1.0, 
g0108:     "prescale_gradients": true, 
g0108:     "fp16": {
g0108:         "enabled": true, 
g0108:         "loss_scale": 0, 
g0108:         "loss_scale_window": 500, 
g0108:         "hysteresis": 2, 
g0108:         "min_loss_scale": 1, 
g0108:         "initial_scale_power": 11
g0108:     }, 
g0108:     "wall_clock_breakdown": false
g0108: }
g0108: [2024-08-12 02:52:54,019] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0108: [2024-08-12 02:52:54,019] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0108: [2024-08-12 02:52:54,735] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0123: [2024-08-12 02:52:54,735] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0120: [2024-08-12 02:52:54,735] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0126: [2024-08-12 02:52:54,735] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0127: [2024-08-12 02:52:54,736] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0113: [2024-08-12 02:52:54,736] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0125: [2024-08-12 02:52:54,736] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0121: [2024-08-12 02:52:54,736] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0120: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0120: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0127: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0127: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0127: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0127: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0120: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 02:52:55,451] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0120: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0125: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0126: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0126: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0126: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0126: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0125: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0125: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0121: [2024-08-12 02:52:55,452] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0125: [2024-08-12 02:52:55,462] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0113: [2024-08-12 02:52:58,654] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 02:52:58,655] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 02:52:58,656] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0126: [2024-08-12 02:52:58,656] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0126: [2024-08-12 02:52:58,656] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 02:52:58,657] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0126: [2024-08-12 02:52:58,657] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0126: [2024-08-12 02:52:58,657] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 02:52:58,662] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0113: [2024-08-12 02:52:58,662] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0108: [2024-08-12 02:52:58,662] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 02:52:58,663] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 02:52:58,663] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 02:52:58,663] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 02:52:58,663] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 02:52:58,663] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 02:52:58,663] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0123: [2024-08-12 02:52:58,663] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0113: [2024-08-12 02:52:58,664] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0113: [2024-08-12 02:52:58,664] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt...
g0126: [2024-08-12 02:52:58,664] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0126: [2024-08-12 02:52:58,664] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0126: [2024-08-12 02:52:58,664] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0126: [2024-08-12 02:52:58,665] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt...
g0108: [2024-08-12 02:52:58,670] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 02:52:58,670] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 02:52:58,671] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0108: [2024-08-12 02:52:58,671] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt...
g0123: [2024-08-12 02:52:58,671] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0123: [2024-08-12 02:52:58,672] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0123: [2024-08-12 02:52:58,672] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0123: [2024-08-12 02:52:58,672] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt...
g0120: [2024-08-12 02:52:58,677] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 02:52:58,677] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 02:52:58,677] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 02:52:58,678] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0120: [2024-08-12 02:52:58,684] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0120: [2024-08-12 02:52:58,685] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0120: [2024-08-12 02:52:58,686] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0120: [2024-08-12 02:52:58,686] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt...
g0125: [2024-08-12 02:52:58,876] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0125: [2024-08-12 02:52:58,876] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0125: [2024-08-12 02:52:58,876] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0125: [2024-08-12 02:52:58,876] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 02:52:58,878] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 02:52:58,878] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 02:52:58,878] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0121: [2024-08-12 02:52:58,879] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0125: [2024-08-12 02:52:58,883] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0125: [2024-08-12 02:52:58,884] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0125: [2024-08-12 02:52:58,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0125: [2024-08-12 02:52:58,885] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt...
g0121: [2024-08-12 02:52:58,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0121: [2024-08-12 02:52:58,886] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0121: [2024-08-12 02:52:58,887] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0121: [2024-08-12 02:52:58,887] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt...
g0127: [2024-08-12 02:52:59,093] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0127: [2024-08-12 02:52:59,093] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0127: [2024-08-12 02:52:59,093] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0127: [2024-08-12 02:52:59,093] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0127: [2024-08-12 02:52:59,100] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0127: [2024-08-12 02:52:59,100] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0127: [2024-08-12 02:52:59,101] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0127: [2024-08-12 02:52:59,101] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt...
g0113: [2024-08-12 02:52:59,242] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0113: [2024-08-12 02:52:59,242] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0113: [2024-08-12 02:52:59,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,243] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,249] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0113: [2024-08-12 02:52:59,250] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,250] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_01_model_states.pt.
g0113: [2024-08-12 02:52:59,250] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,284] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,284] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,284] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,285] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,286] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,286] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,287] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,287] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,321] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,322] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,322] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,322] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_05-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,343] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,343] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,343] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,343] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,380] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,380] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,383] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,383] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,384] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,384] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,410] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,410] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,418] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,418] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_06-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,431] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,432] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,439] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,439] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,464] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,464] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,465] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,465] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,477] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,477] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,477] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,477] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt...
g0113: [2024-08-12 02:52:59,495] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,495] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,512] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0113: [2024-08-12 02:52:59,512] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_07-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,415] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0120: [2024-08-12 02:53:00,416] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,416] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0120: [2024-08-12 02:53:00,416] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0120: [2024-08-12 02:53:00,416] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_02_model_states.pt.
g0120: [2024-08-12 02:53:00,417] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,417] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,418] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,455] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,455] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,455] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,456] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,456] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,456] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,456] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,456] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,490] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,490] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,492] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,492] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_08-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,506] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,509] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0126: [2024-08-12 02:53:00,509] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0126: [2024-08-12 02:53:00,509] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0126: [2024-08-12 02:53:00,509] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,510] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,510] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,511] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,512] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,513] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,515] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_06_model_states.pt.
g0126: [2024-08-12 02:53:00,517] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,542] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,543] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,547] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,547] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,548] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,548] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,548] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,549] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,550] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,551] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,552] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,552] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,552] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,552] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,556] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,572] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,581] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,582] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,582] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,584] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,585] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,586] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,586] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_09-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,590] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_20-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,598] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,602] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,603] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,603] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,606] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,607] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,613] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,640] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,641] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,641] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,641] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,642] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,642] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,652] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,653] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,674] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,674] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,674] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0126: [2024-08-12 02:53:00,685] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_21-model_00-model_states.pt.
g0123: [2024-08-12 02:53:00,685] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0123: [2024-08-12 02:53:00,685] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0123: [2024-08-12 02:53:00,685] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0123: [2024-08-12 02:53:00,685] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_04_model_states.pt.
g0123: [2024-08-12 02:53:00,686] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 02:53:00,686] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 02:53:00,686] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 02:53:00,687] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,687] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,696] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,696] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0126: [2024-08-12 02:53:00,706] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0125: [2024-08-12 02:53:00,747] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0125: [2024-08-12 02:53:00,748] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0125: [2024-08-12 02:53:00,748] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 02:53:00,749] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 02:53:00,749] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0125: [2024-08-12 02:53:00,750] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_05_model_states.pt.
g0125: [2024-08-12 02:53:00,750] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 02:53:00,750] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0127: [2024-08-12 02:53:00,806] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0127: [2024-08-12 02:53:00,806] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0127: [2024-08-12 02:53:00,806] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0127: [2024-08-12 02:53:00,807] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0127: [2024-08-12 02:53:00,807] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0127: [2024-08-12 02:53:00,807] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_07_model_states.pt.
g0127: [2024-08-12 02:53:00,807] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0127: [2024-08-12 02:53:00,808] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,874] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,875] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,875] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,875] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,875] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,875] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,875] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,876] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt...
g0120: [2024-08-12 02:53:00,906] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,906] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,908] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0120: [2024-08-12 02:53:00,909] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_10-model_00-model_states.pt.
g0123: [2024-08-12 02:53:00,973] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 02:53:00,974] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 02:53:00,974] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 02:53:00,974] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 02:53:00,974] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 02:53:00,975] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 02:53:00,975] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0123: [2024-08-12 02:53:00,975] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt...
g0125: [2024-08-12 02:53:00,995] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 02:53:00,995] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 02:53:00,995] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 02:53:00,996] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 02:53:00,996] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 02:53:00,996] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 02:53:00,996] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0125: [2024-08-12 02:53:00,996] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt...
g0123: [2024-08-12 02:53:01,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 02:53:01,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 02:53:01,010] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0123: [2024-08-12 02:53:01,010] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_14-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,028] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,028] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0123: [2024-08-12 02:53:01,031] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 02:53:01,031] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 02:53:01,031] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 02:53:01,032] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,031] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,032] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_17-model_00-model_states.pt.
g0126: [2024-08-12 02:53:01,039] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 02:53:01,039] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 02:53:01,039] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 02:53:01,039] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0126: [2024-08-12 02:53:01,039] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0126: [2024-08-12 02:53:01,039] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0126: [2024-08-12 02:53:01,039] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 02:53:01,041] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,049] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,049] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,052] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,053] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0126: [2024-08-12 02:53:01,071] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 02:53:01,071] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 02:53:01,071] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0126: [2024-08-12 02:53:01,073] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_22-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,091] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,091] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,091] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,091] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,091] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,091] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,092] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,092] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,124] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,124] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,126] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,126] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_18-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,145] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,145] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,147] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,147] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,178] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,178] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,179] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,179] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,180] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,181] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,182] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,182] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,205] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0121: [2024-08-12 02:53:01,206] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0121: [2024-08-12 02:53:01,206] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0121: [2024-08-12 02:53:01,207] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,207] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_03_model_states.pt.
g0121: [2024-08-12 02:53:01,207] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,207] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,207] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0125: [2024-08-12 02:53:01,210] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,210] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,215] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0125: [2024-08-12 02:53:01,217] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_19-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,231] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 02:53:01,231] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 02:53:01,231] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 02:53:01,231] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/mp_rank_00_model_states.pt.
g0108: [2024-08-12 02:53:01,231] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,232] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,232] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,232] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,247] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,247] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,247] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,248] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,248] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,249] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,249] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,249] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,278] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,278] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,279] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,279] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,281] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,281] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,282] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,282] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,282] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,283] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,286] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,287] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_11-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,304] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,304] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,307] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,308] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,323] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,323] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,332] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,333] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_01-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,343] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,344] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,344] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,344] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,345] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,345] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,346] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,346] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,351] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,351] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,362] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,362] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,377] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,377] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,378] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,378] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_12-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,398] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,398] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,399] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,400] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,435] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,435] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,436] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,436] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,437] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,437] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,437] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,437] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt...
g0121: [2024-08-12 02:53:01,468] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,468] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,472] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0121: [2024-08-12 02:53:01,472] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_13-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,689] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,689] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,690] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,690] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,690] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,690] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,690] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,691] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,722] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,722] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,727] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,727] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_02-model_00-model_states.pt.
g0108: [2024-08-12 02:53:01,743] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,743] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,748] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 02:53:01,748] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,047] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,047] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,047] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,047] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,048] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,048] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,048] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,048] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,077] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,077] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,081] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,082] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_03-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,098] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,098] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,103] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,103] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,133] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,133] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,135] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,135] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,138] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,138] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,138] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,138] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt...
g0108: [2024-08-12 02:53:02,167] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,169] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,170] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0108: [2024-08-12 02:53:02,170] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_04-model_00-model_states.pt.
g0123: [2024-08-12 02:53:02,870] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 02:53:02,870] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,071] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,071] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,071] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,071] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,071] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,071] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt...
g0108:  > overriding learning rate value to 0.0002
g0108:  > overriding minimum learning rate value to 1e-05
g0108:  > overriding warmup iterations value to 0
g0108:  > overriding warmup tokens value to 3000000000
g0108:  > overriding total number of iterations value to 6656000
g0108:  > overriding decay tokens value to 300000000000
g0108:  > overriding learning rate decay style value to cosine
g0108:  > overriding start weight decay value to 0.1
g0108:  > overriding end weight decay value to 0.1
g0108:  > overriding total number of weight decay iterations value to 6656000
g0108:  > overriding weight decay incr style value to constant
g0108:  checkpoint version 3.0
g0127: [2024-08-12 02:53:03,612] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 02:53:03,612] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 02:53:03,612] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 02:53:03,612] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0127: [2024-08-12 02:53:03,612] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 02:53:03,613] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0127: [2024-08-12 02:53:03,613] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0127: [2024-08-12 02:53:03,616] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,852] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,852] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,852] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,852] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_15-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,873] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,873] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,873] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,874] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,909] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,909] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,909] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,909] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,912] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,912] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,912] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,912] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt...
g0123: [2024-08-12 02:53:03,941] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,941] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,948] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0123: [2024-08-12 02:53:03,948] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_16-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,451] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,451] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,451] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,452] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_23-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,472] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,472] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,473] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,474] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,474] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_24-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,474] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,474] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,475] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,475] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,880] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,880] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,880] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,881] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,881] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,881] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,881] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,882] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt...
g0127: [2024-08-12 02:53:04,928] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,928] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,932] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0127: [2024-08-12 02:53:04,932] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step42000/layer_25-model_00-model_states.pt.
g0108:   successfully loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase at iteration 42000
g0127: (min, max) time across ranks (ms):
g0127:     load-checkpoint ................................: (10330.70, 10331.08)
g0108: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-12 02:53:05 
g0108: > building train, validation, and test datasets ...
g0108:  > datasets target sizes (minimum size):
g0108:     train:      6656000
g0108:     validation: 678400
g0108:     test:       12800
g0108: > building train, validation, and test datasets for GPT ...
g0108: Single data path provided for train, valid & test
g0108:  > building dataset index ...
g0108:     reading sizes...
g0108:     reading pointers...
g0108:     reading document index...
g0108:     creating numpy buffer of mmap...
g0108:     creating memory view of numpy buffer...
g0108:  > finished creating indexed dataset in 0.033130 seconds
g0108:     number of documents: 250886
g0108:  > dataset split:
g0108:     train:
g0108:      document indices in [0, 238091) total of 238091 documents
g0108:     validation:
g0108:      document indices in [238091, 250635) total of 12544 documents
g0108:     test:
g0108:      document indices in [250635, 250886) total of 251 documents
g0108:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0108:  > only one epoch required, setting separate_last_epoch to False
g0108:  > elasped time to build and save doc-idx mapping (seconds): 0.031934
g0108:     using:
g0108:      number of documents:       238091
g0108:      number of epochs:          1
g0108:      sequence length:           2048
g0108:      total number of samples:   476821
g0108:  > elasped time to build and save sample-idx mapping (seconds): 0.013061
g0108:  > building shuffle index with split [0, 476821) and [476821, 476821) ...
g0108:  > elasped time to build and save shuffle-idx mapping (seconds): 0.012386
g0108:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/1dbbad17180c302cc83fc4f6f3155ddf_doc_idx.npy
g0108:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/1dbbad17180c302cc83fc4f6f3155ddf_sample_idx.npy
g0108:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/1dbbad17180c302cc83fc4f6f3155ddf_shuffle_idx.npy
g0108:     loaded indexed file in 0.008 seconds
g0108:     total number of samples: 476822
g0108:     total number of epochs: 1
g0108:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0108:  > last epoch number of samples (11709) is smaller than 80% of number of samples per epoch (25641), setting separate_last_epoch to True
g0108:  > elasped time to build and save doc-idx mapping (seconds): 0.013638
g0108:     using:
g0108:      number of documents:       12544
g0108:      number of epochs:          27
g0108:      sequence length:           2048
g0108:      total number of samples:   692333
g0108:  > elasped time to build and save sample-idx mapping (seconds): 0.013303
g0108:  > building shuffle index with split [0, 666691) and [666691, 692333) ...
g0108:  > elasped time to build and save shuffle-idx mapping (seconds): 0.019784
g0108:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/d5dc985126ccf4420fa9c4046a4e1bc0_doc_idx.npy
g0108:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/d5dc985126ccf4420fa9c4046a4e1bc0_sample_idx.npy
g0108:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/d5dc985126ccf4420fa9c4046a4e1bc0_shuffle_idx.npy
g0108:     loaded indexed file in 0.007 seconds
g0108:     total number of samples: 692334
g0108:     total number of epochs: 27
g0108:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0108:  > last epoch number of samples (369) is larger than 80% of number of samples per epoch (376), setting separate_last_epoch to False
g0108:  > elasped time to build and save doc-idx mapping (seconds): 0.001475
g0108:     using:
g0108:      number of documents:       251
g0108:      number of epochs:          34
g0108:      sequence length:           2048
g0108:      total number of samples:   12808
g0108:  > elasped time to build and save sample-idx mapping (seconds): 0.001930
g0108:  > building shuffle index with split [0, 12808) and [12808, 12808) ...
g0108:  > elasped time to build and save shuffle-idx mapping (seconds): 0.001644
g0108:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/7a71560402d99012f7c883a368e28919_doc_idx.npy
g0108:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/7a71560402d99012f7c883a368e28919_sample_idx.npy
g0108: Traceback (most recent call last):
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0108: Traceback (most recent call last):
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0108:     pretrain(train_valid_test_datasets_provider,
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0108: Traceback (most recent call last):
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0108:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/7a71560402d99012f7c883a368e28919_shuffle_idx.npy
g0108:     pretrain(train_valid_test_datasets_provider,
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0108:     loaded indexed file in 0.003 seconds
g0108:     pretrain(train_valid_test_datasets_provider,
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0108:     total number of samples: 12809
g0108:     total number of epochs: 34
g0108:         = build_train_valid_test_data_iterators(= build_train_valid_test_data_iterators(
g0108: 
g0108: > finished creating GPT datasets ...
g0108:       ^^^^^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108: ^^^^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0108: ^
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0108:     = build_train_valid_test_data_iterators(
g0108:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0108:     build_train_valid_test_data_loaders(
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0108:     build_train_valid_test_data_loaders(
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0108:     build_train_valid_test_data_loaders(
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0108:     train_dataloader = build_pretraining_data_loader(
g0108:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0108:     train_dataloader = build_pretraining_data_loader(
g0108:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0108:     train_dataloader = build_pretraining_data_loader(
g0108:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0108:         batch_sampler = MegatronPretrainingSampler(batch_sampler = MegatronPretrainingSampler(
g0108: 
g0108:            batch_sampler = MegatronPretrainingSampler( 
g0108:                                               ^  ^ ^^ ^^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108: ^
g0108: ^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0108: ^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0108: 
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0108: Traceback (most recent call last):
g0108:         assert self.consumed_samples < self.total_samples, \assert self.consumed_samples < self.total_samples, \
g0108: 
g0108:                      ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108: ^^
g0108: AssertionErrorAssertionError: : no samples left to consume: 5376000, 476821no samples left to consume: 5376000, 476821
g0108: 
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0108:     pretrain(train_valid_test_datasets_provider,
g0108:     assert self.consumed_samples < self.total_samples, \
g0108:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108: AssertionError: no samples left to consume: 5376000, 476821
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0108:     = build_train_valid_test_data_iterators(
g0108:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0108:     build_train_valid_test_data_loaders(
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0108:     train_dataloader = build_pretraining_data_loader(
g0108:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0108:     batch_sampler = MegatronPretrainingSampler(
g0108:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0108:     assert self.consumed_samples < self.total_samples, \
g0108:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0108: AssertionError: no samples left to consume: 5376000, 476821
g0113: Traceback (most recent call last):
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0126: Traceback (most recent call last):
g0126: Traceback (most recent call last):
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0121: Traceback (most recent call last):
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0121: Traceback (most recent call last):
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0126: Traceback (most recent call last):
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0113: Traceback (most recent call last):
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0113: Traceback (most recent call last):
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0113:     pretrain(train_valid_test_datasets_provider,
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0121: Traceback (most recent call last):
g0120: Traceback (most recent call last):
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0120: Traceback (most recent call last):
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0113:     pretrain(train_valid_test_datasets_provider,
g0121:     pretrain(train_valid_test_datasets_provider,
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0120: Traceback (most recent call last):
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0121:     pretrain(train_valid_test_datasets_provider,
g0120:     pretrain(train_valid_test_datasets_provider,
g0126:     pretrain(train_valid_test_datasets_provider,
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0113:     pretrain(train_valid_test_datasets_provider,
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0120:     pretrain(train_valid_test_datasets_provider,
g0126:     pretrain(train_valid_test_datasets_provider,
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0121:     pretrain(train_valid_test_datasets_provider,
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0125: Traceback (most recent call last):
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0126:     pretrain(train_valid_test_datasets_provider,
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0120:     pretrain(train_valid_test_datasets_provider,
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0125: Traceback (most recent call last):
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0125: Traceback (most recent call last):
g0125:     pretrain(train_valid_test_datasets_provider,
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0113:     = build_train_valid_test_data_iterators(    
g0113: = build_train_valid_test_data_iterators(
g0113:     = build_train_valid_test_data_iterators(
g0121:     = build_train_valid_test_data_iterators(
g0120:         = build_train_valid_test_data_iterators(= build_train_valid_test_data_iterators(    
g0120: 
g0120: = build_train_valid_test_data_iterators(
g0125:     pretrain(train_valid_test_datasets_provider,
g0120:       ^^^^^^^^^^^^^^^^^ ^ ^  ^ ^  ^   ^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0120: ^^^^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0113:       ^^^ ^ ^ ^ ^  ^  ^^^ ^^ ^^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113: ^^^^^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0113: ^^^^^^
g0113: ^^^^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0120: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:       ^^^^^^^^^^^^^^^^^^^^^^    ^    ^= build_train_valid_test_data_iterators(^= build_train_valid_test_data_iterators(^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0120: 
g0121: ^
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0121: ^^^^^^^^^^^
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0121:             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121: 
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0126:             = build_train_valid_test_data_iterators(= build_train_valid_test_data_iterators(= build_train_valid_test_data_iterators(
g0126: 
g0126: 
g0125:     pretrain(train_valid_test_datasets_provider,
g0125:         = build_train_valid_test_data_iterators(= build_train_valid_test_data_iterators(
g0125: 
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0121:     build_train_valid_test_data_loaders(
g0125:       ^^^^^^^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0125: ^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0120:     build_train_valid_test_data_loaders(
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0125: ^^^^^^
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0113:     build_train_valid_test_data_loaders(
g0125:     = build_train_valid_test_data_iterators(
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0125:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:     build_train_valid_test_data_loaders(
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0126:       ^^^ ^ ^  ^  ^  ^  ^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0126: ^^^^^^^^^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0126: ^^^^
g0126: ^
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0120:     build_train_valid_test_data_loaders(
g0120:       File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0120: build_train_valid_test_data_loaders(
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0121:     train_dataloader = build_pretraining_data_loader(
g0113:     build_train_valid_test_data_loaders(
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0125:     build_train_valid_test_data_loaders(
g0120:     train_dataloader = build_pretraining_data_loader(
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0125:     build_train_valid_test_data_loaders(
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0121:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:       File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0121: build_train_valid_test_data_loaders(
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0125:     build_train_valid_test_data_loaders(
g0120:                        ^^^^^^^^^^^^^^^    ^train_dataloader = build_pretraining_data_loader(^
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0120: ^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ 
g0120:           File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0120:     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0120:         train_dataloader = build_pretraining_data_loader(batch_sampler = MegatronPretrainingSampler(
g0121:     build_train_valid_test_data_loaders(
g0120: 
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0121:     batch_sampler = MegatronPretrainingSampler(
g0121:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0120:                                         batch_sampler = MegatronPretrainingSampler(  
g0125:     train_dataloader = build_pretraining_data_loader(
g0113:     train_dataloader = build_pretraining_data_loader(
g0120:    ^ ^  ^ ^^ ^^ ^^ ^^ ^^ ^^ ^^ ^ ^^ ^^ ^^ ^^ ^^ ^ ^^ ^^ ^^ ^^ ^^^^^^^^^^^^^^^^^^^^^^^
g0120: ^^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0120: ^^^
g0120: ^^^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0120: ^^^^^^
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0113:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0125:                        ^^^^^    ^train_dataloader = build_pretraining_data_loader(^
g0125: ^^^^^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^ ^ 
g0113: Traceback (most recent call last):
g0125:      ^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0120: Traceback (most recent call last):
g0125: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0126:         build_train_valid_test_data_loaders(build_train_valid_test_data_loaders(
g0126: 
g0113:     train_dataloader = build_pretraining_data_loader(
g0125:     train_dataloader = build_pretraining_data_loader(
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0121: Traceback (most recent call last):
g0121:     train_dataloader = build_pretraining_data_loader(
g0113:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0125:                        ^^^^^^^^^^^^^^^^^^^^^Traceback (most recent call last):
g0125: ^^^^^^^^^
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0113:     train_dataloader = build_pretraining_data_loader(
g0126:     build_train_valid_test_data_loaders(
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0121:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0121:     assert self.consumed_samples < self.total_samples, \
g0113:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0121:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121: AssertionError: no samples left to consume: 5376000, 476821
g0120:         batch_sampler = MegatronPretrainingSampler(assert self.consumed_samples < self.total_samples, \
g0120: 
g0125:         batch_sampler = MegatronPretrainingSampler(batch_sampler = MegatronPretrainingSampler(
g0125: 
g0113:     batch_sampler = MegatronPretrainingSampler(
g0113:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0126: Traceback (most recent call last):
g0120:                     ^^^^^^^^^^^^^^^^^ ^ ^ ^ ^ ^^ ^ ^ ^ ^ 
g0125:                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0125: 
g0120:  ^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0120: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0120: AssertionError: no samples left to consume: 5376000, 476821
g0120:     assert self.consumed_samples < self.total_samples, \
g0125:     batch_sampler = MegatronPretrainingSampler(
g0121:     train_dataloader = build_pretraining_data_loader(
g0121:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0113:     batch_sampler = MegatronPretrainingSampler(
g0113:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0120:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0125:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0120: AssertionError: no samples left to consume: 5376000, 476821  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0120:     pretrain(train_valid_test_datasets_provider,
g0120: 
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0125:     pretrain(train_valid_test_datasets_provider,
g0113:     batch_sampler = MegatronPretrainingSampler(
g0126:     train_dataloader = build_pretraining_data_loader(
g0126:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0126:     train_dataloader = build_pretraining_data_loader(
g0121:     batch_sampler = MegatronPretrainingSampler(
g0121:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0120:     assert self.consumed_samples < self.total_samples, \
g0113:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0113:     pretrain(train_valid_test_datasets_provider,
g0126:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0113:     assert self.consumed_samples < self.total_samples, \
g0121:     batch_sampler = MegatronPretrainingSampler(
g0120:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0120:     = build_train_valid_test_data_iterators(
g0120:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0125:     = build_train_valid_test_data_iterators(
g0125:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0120: AssertionError: no samples left to consume: 5376000, 476821
g0113:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113: AssertionError: no samples left to consume: 5376000, 476821
g0113:     assert self.consumed_samples < self.total_samples, \
g0126:     train_dataloader = build_pretraining_data_loader(
g0125:     assert self.consumed_samples < self.total_samples, \
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0113:     = build_train_valid_test_data_iterators(
g0121:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0126:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:     assert self.consumed_samples < self.total_samples, \
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0125:     assert self.consumed_samples < self.total_samples, \
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0120:     build_train_valid_test_data_loaders(
g0113:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113: AssertionError: no samples left to consume: 5376000, 476821
g0113:     assert self.consumed_samples < self.total_samples, \
g0126:     batch_sampler = MegatronPretrainingSampler(
g0121:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0125:                 File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0125:     build_train_valid_test_data_loaders(
g0126:            batch_sampler = MegatronPretrainingSampler( 
g0126:                       ^ ^ ^ ^ ^ ^ ^ ^   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0126:     pretrain(train_valid_test_datasets_provider,
g0126: ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0126: ^^^^^^^^^^
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0125:   ^^ ^^^ ^^^ ^^^ ^^^ ^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0125: ^^^AssertionError^: no samples left to consume: 5376000, 476821^
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0120:     train_dataloader = build_pretraining_data_loader(
g0120:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0113:     build_train_valid_test_data_loaders(
g0113: AssertionError: no samples left to consume: 5376000, 476821
g0125: ^^^^^^^    ^^assert self.consumed_samples < self.total_samples, \^
g0126:     batch_sampler = MegatronPretrainingSampler(
g0125: ^^^^^^^^^^^^^^^^^^^
g0126:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0121: AssertionError: no samples left to consume: 5376000, 476821
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0121:     pretrain(train_valid_test_datasets_provider,
g0125:     train_dataloader = build_pretraining_data_loader(
g0125:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:     assert self.consumed_samples < self.total_samples, \
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0126:     = build_train_valid_test_data_iterators(
g0126:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0113:     train_dataloader = build_pretraining_data_loader(
g0125:         AssertionError  :  no samples left to consume: 5376000, 476821^
g0113:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121: AssertionError: no samples left to consume: 5376000, 476821
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0121:     = build_train_valid_test_data_iterators(
g0121:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0120:     batch_sampler = MegatronPretrainingSampler(
g0120:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0126:         assert self.consumed_samples < self.total_samples, \assert self.consumed_samples < self.total_samples, \
g0126: 
g0125: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0125:     batch_sampler = MegatronPretrainingSampler(
g0125:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0126:     build_train_valid_test_data_loaders(
g0125: AssertionError: no samples left to consume: 5376000, 476821
g0120:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0120:     assert self.consumed_samples < self.total_samples, \
g0120:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0113:     batch_sampler = MegatronPretrainingSampler(
g0113:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0121:     build_train_valid_test_data_loaders(
g0125:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0125:     assert self.consumed_samples < self.total_samples, \
g0125:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0126:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0126:     train_dataloader = build_pretraining_data_loader(
g0126:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0113:     assert self.consumed_samples < self.total_samples, \
g0126: ^^^^^^^^^^^^^^^
g0113:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0120: AssertionError: no samples left to consume: 5376000, 476821
g0126: 
g0126:     assert self.consumed_samples < self.total_samples, \
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0121:     train_dataloader = build_pretraining_data_loader(
g0121:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0125: AssertionError: no samples left to consume: 5376000, 476821
g0126: AssertionError: no samples left to consume: 5376000, 476821
g0126: AssertionError: no samples left to consume: 5376000, 476821
g0126:            ^^^^^^^^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0126:     batch_sampler = MegatronPretrainingSampler(
g0126:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0113: AssertionError: no samples left to consume: 5376000, 476821
g0126: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0126: AssertionError: no samples left to consume: 5376000, 476821
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0121:     batch_sampler = MegatronPretrainingSampler(
g0121:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0126:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0126:     assert self.consumed_samples < self.total_samples, \
g0126:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0121:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0121:     assert self.consumed_samples < self.total_samples, \
g0121:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0126: AssertionError: no samples left to consume: 5376000, 476821
g0121: AssertionError: no samples left to consume: 5376000, 476821
g0123: Traceback (most recent call last):
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0123: Traceback (most recent call last):
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0123:     pretrain(train_valid_test_datasets_provider,
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0123:     pretrain(train_valid_test_datasets_provider,
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0123:     = build_train_valid_test_data_iterators(
g0123:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0123:     = build_train_valid_test_data_iterators(
g0123:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0123:     build_train_valid_test_data_loaders(
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0123:     train_dataloader = build_pretraining_data_loader(
g0123:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0123:     build_train_valid_test_data_loaders(
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0123:     batch_sampler = MegatronPretrainingSampler(
g0123:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0123:     train_dataloader = build_pretraining_data_loader(
g0123:                        ^^Traceback (most recent call last):
g0123: ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0123:     assert self.consumed_samples < self.total_samples, \
g0123:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123: AssertionError    : batch_sampler = MegatronPretrainingSampler(no samples left to consume: 5376000, 476821
g0123: 
g0123:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0123:     assert self.consumed_samples < self.total_samples, \
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0123:     pretrain(train_valid_test_datasets_provider,
g0123:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123: AssertionError: no samples left to consume: 5376000, 476821
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0123:     = build_train_valid_test_data_iterators(
g0123:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0123:     build_train_valid_test_data_loaders(
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0123:     train_dataloader = build_pretraining_data_loader(
g0123:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0123:     batch_sampler = MegatronPretrainingSampler(
g0123:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0123:     assert self.consumed_samples < self.total_samples, \
g0123:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123: AssertionError: no samples left to consume: 5376000, 476821
g0123: Traceback (most recent call last):
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0123:     pretrain(train_valid_test_datasets_provider,
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0123:     = build_train_valid_test_data_iterators(
g0123:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0123:     build_train_valid_test_data_loaders(
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0123:     train_dataloader = build_pretraining_data_loader(
g0123:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0123:     batch_sampler = MegatronPretrainingSampler(
g0123:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0123:     assert self.consumed_samples < self.total_samples, \
g0123:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0123: AssertionError: no samples left to consume: 5376000, 476821
g0127: Traceback (most recent call last):
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0127: Traceback (most recent call last):
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0127: Traceback (most recent call last):
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0127:     pretrain(train_valid_test_datasets_provider,
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0127:     pretrain(train_valid_test_datasets_provider,
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0127:     pretrain(train_valid_test_datasets_provider,
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0127:     = build_train_valid_test_data_iterators(
g0127:     = build_train_valid_test_data_iterators(
g0127:             = build_train_valid_test_data_iterators(   
g0127: ^ ^^^^^^^^^^^ ^ ^^ ^^ ^^ ^^^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127: 
g0127: ^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0127: ^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0127: ^^^^^^
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0127:     build_train_valid_test_data_loaders(
g0127:     build_train_valid_test_data_loaders(
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0127:     build_train_valid_test_data_loaders(
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0127:     train_dataloader = build_pretraining_data_loader(
g0127:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0127:     train_dataloader = build_pretraining_data_loader(
g0127:       Traceback (most recent call last):
g0127:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0127:         batch_sampler = MegatronPretrainingSampler(train_dataloader = build_pretraining_data_loader(
g0127: 
g0127:                                        ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127: ^^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0127: 
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0127:     batch_sampler = MegatronPretrainingSampler(
g0127:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0127:     assert self.consumed_samples < self.total_samples, \    
g0127: batch_sampler = MegatronPretrainingSampler(
g0127:                     ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127: ^^^^^^^  File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0127: ^^^^^^^^^^^^^^^^^^^
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py", line 356, in <module>
g0127:     pretrain(train_valid_test_datasets_provider,
g0127: AssertionError: no samples left to consume: 5376000, 476821
g0127:     assert self.consumed_samples < self.total_samples, \
g0127:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127: AssertionError: no samples left to consume: 5376000, 476821
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 195, in pretrain
g0127:     = build_train_valid_test_data_iterators(
g0127:       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127:     assert self.consumed_samples < self.total_samples, \
g0127:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1572, in build_train_valid_test_data_iterators
g0127:     build_train_valid_test_data_loaders(
g0127: AssertionError: no samples left to consume: 5376000, 476821
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/training.py", line 1532, in build_train_valid_test_data_loaders
g0127:     train_dataloader = build_pretraining_data_loader(
g0127:                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 24, in build_pretraining_data_loader
g0127:     batch_sampler = MegatronPretrainingSampler(
g0127:                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127:   File "/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data/data_samplers.py", line 68, in __init__
g0127:     assert self.consumed_samples < self.total_samples, \
g0127:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
g0127: AssertionError: no samples left to consume: 5376000, 476821
g0120: [2024-08-12 02:53:09,052] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3921399
g0108: [2024-08-12 02:53:09,063] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4093306
g0121: [2024-08-12 02:53:09,133] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1183346
g0113: [2024-08-12 02:53:09,138] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1363593
g0127: [2024-08-12 02:53:09,176] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2919118
g0123: [2024-08-12 02:53:09,184] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2772795
g0125: [2024-08-12 02:53:09,185] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1596187
g0126: [2024-08-12 02:53:09,260] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 152894
g0108: [2024-08-12 02:53:09,428] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4093307
g0108: [2024-08-12 02:53:09,443] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4093308
g0108: [2024-08-12 02:53:09,458] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 4093309
g0108: [2024-08-12 02:53:09,458] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--train-data-exact-num-epochs', '1', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True', '--log-optimizer-states-to-tensorboard', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 1
g0120: [2024-08-12 02:53:09,496] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3921400
g0120: [2024-08-12 02:53:09,514] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3921401
g0120: [2024-08-12 02:53:09,530] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 3921402
g0120: [2024-08-12 02:53:09,530] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--train-data-exact-num-epochs', '1', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True', '--log-optimizer-states-to-tensorboard', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 1
g0121: [2024-08-12 02:53:09,553] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1183347
g0121: [2024-08-12 02:53:09,568] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1183348
g0127: [2024-08-12 02:53:09,569] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2919119
g0121: [2024-08-12 02:53:09,582] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1183349
g0121: [2024-08-12 02:53:09,582] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--train-data-exact-num-epochs', '1', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True', '--log-optimizer-states-to-tensorboard', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 1
g0127: [2024-08-12 02:53:09,587] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2919120
g0127: [2024-08-12 02:53:09,587] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2919121
g0125: [2024-08-12 02:53:09,590] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1596188
g0125: [2024-08-12 02:53:09,590] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1596189
g0127: [2024-08-12 02:53:09,603] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--train-data-exact-num-epochs', '1', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True', '--log-optimizer-states-to-tensorboard', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 1
g0125: [2024-08-12 02:53:09,607] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1596190
g0125: [2024-08-12 02:53:09,621] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--train-data-exact-num-epochs', '1', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True', '--log-optimizer-states-to-tensorboard', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 1
g0113: [2024-08-12 02:53:09,623] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1363594
g0123: [2024-08-12 02:53:09,629] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2772796
g0123: [2024-08-12 02:53:09,629] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2772797
g0113: [2024-08-12 02:53:09,642] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1363595
g0123: [2024-08-12 02:53:09,646] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 2772798
g0113: [2024-08-12 02:53:09,658] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 1363596
g0113: [2024-08-12 02:53:09,659] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--train-data-exact-num-epochs', '1', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True', '--log-optimizer-states-to-tensorboard', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 1
g0123: [2024-08-12 02:53:09,662] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--train-data-exact-num-epochs', '1', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True', '--log-optimizer-states-to-tensorboard', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 1
g0126: [2024-08-12 02:53:09,705] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 152895
g0126: [2024-08-12 02:53:09,721] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 152896
g0126: [2024-08-12 02:53:09,735] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 152897
g0126: [2024-08-12 02:53:09,735] [ERROR] [launch.py:321:sigkill_handler] ['/home/acf16449gb/crypto_llm/train/.venv_train/bin/python3', '-u', '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py', '--local_rank=3', '--override-opt_param-scheduler', '--optimizer', 'adam', '--adam-beta1', '0.9', '--adam-beta2', '0.95', '--tensor-model-parallel-size', '1', '--init-method-std', '0.013', '--lr-decay-tokens', '300000000000', '--lr-warmup-tokens', '3000000000', '--micro-batch-size', '1', '--exit-duration-in-mins', '30000000', '--global-batch-size', '128', '--num-layers', '22', '--hidden-size', '2048', '--ffn-hidden-size', '5632', '--num-attention-heads', '16', '--num-key-value-heads', '4', '--no-query-key-layer-scaling', '--attention-dropout', '0', '--hidden-dropout', '0', '--use-rotary-position-embeddings', '--untie-embeddings-and-output-weights', '--swiglu', '--normalization', 'rmsnorm', '--disable-bias-linear', '--seq-length', '2048', '--max-position-embeddings', '2048', '--train-tokens', '13631488000', '--train-samples', '6656000', '--train-data-exact-num-epochs', '1', '--lr', '2.0e-4', '--min-lr', '1.0e-5', '--lr-decay-style', 'cosine', '--split', '949,50,1', '--log-interval', '10', '--eval-interval', '1000', '--eval-iters', '100', '--save-interval', '1000', '--weight-decay', '0.1', '--clip-grad', '1.0', '--hysteresis', '2', '--num-workers', '0', '--seed', '1234', '--load', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--save', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase', '--no-async-tensor-model-parallel-allreduce', '--tensorboard-queue-size', '1', '--log-timers-to-tensorboard', '--log-batch-size-to-tensorboard', '--log-validation-ppl-to-tensorboard', '--tensorboard-dir', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000000_1234_True', '--log-optimizer-states-to-tensorboard', '--tokenizer-type', 'SentencePieceTokenizer', '--tokenizer-model', '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000000_1234_True.model', '--data-path', '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000000_1234_True_no_encryption_text_document', '--data-impl', 'mmap', '--deepspeed', '--deepspeed_config', '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json', '--zero-stage', '0', '--pipeline-model-parallel-size', '8', '--use_wandb', '--wandb_entity', 'yohei-kobashi', '--wandb_project', 'encrypted_data_LLM', '--wandb_group', 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000000_1234_True', '--wandb_tag', 'other_gpu'] exits with return code = 1
pdsh@g0108: g0108: ssh exited with exit code 1
pdsh@g0108: g0121: ssh exited with exit code 1
pdsh@g0108: g0123: ssh exited with exit code 1
pdsh@g0108: g0125: ssh exited with exit code 1
pdsh@g0108: g0120: ssh exited with exit code 1
pdsh@g0108: g0127: ssh exited with exit code 1
pdsh@g0108: g0113: ssh exited with exit code 1
pdsh@g0108: g0126: ssh exited with exit code 1
