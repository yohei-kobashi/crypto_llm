
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0113
    HostName g0113
    Port 2222
    StrictHostKeyChecking no

Host g0125
    HostName g0125
    Port 2222
    StrictHostKeyChecking no

Host g0128
    HostName g0128
    Port 2222
    StrictHostKeyChecking no

Host g0129
    HostName g0129
    Port 2222
    StrictHostKeyChecking no

Host g0130
    HostName g0130
    Port 2222
    StrictHostKeyChecking no

Host g0131
    HostName g0131
    Port 2222
    StrictHostKeyChecking no

Host g0132
    HostName g0132
    Port 2222
    StrictHostKeyChecking no

Host g0133
    HostName g0133
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000100_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000100_1234_True_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000100_1234_True_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42771608
g0113 slots=4
g0125 slots=4
g0128 slots=4
g0129 slots=4
g0130 slots=4
g0131 slots=4
g0132 slots=4
g0133 slots=4

[2024-08-02 18:07:10,423] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-02 18:07:16,383] [INFO] [runner.py:463:main] Using IP address of 10.1.4.11 for node g0113
[2024-08-02 18:07:16,385] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0113,g0125,g0128,g0129,g0130,g0131,g0132,g0133
[2024-08-02 18:07:16,386] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0113,g0125,g0128,g0129,g0130,g0131,g0132,g0133 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDExMyI6IFswLCAxLCAyLCAzXSwgImcwMTI1IjogWzAsIDEsIDIsIDNdLCAiZzAxMjgiOiBbMCwgMSwgMiwgM10sICJnMDEyOSI6IFswLCAxLCAyLCAzXSwgImcwMTMwIjogWzAsIDEsIDIsIDNdLCAiZzAxMzEiOiBbMCwgMSwgMiwgM10sICJnMDEzMiI6IFswLCAxLCAyLCAzXSwgImcwMTMzIjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.4.11 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '2621440000000' --train-samples '1280000000' --train-data-exact-num-epochs '1' --lr '2.0e-4' --min-lr '1.0e-5' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000100_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000100_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000100_1234_True_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_000100_1234_True' --wandb_tag 'other_gpu'
g0113: [2024-08-02 18:07:19,829] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-02 18:07:22,089] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0113: [2024-08-02 18:07:22,089] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0113': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0128': [0, 1, 2, 3], 'g0129': [0, 1, 2, 3], 'g0130': [0, 1, 2, 3], 'g0131': [0, 1, 2, 3], 'g0132': [0, 1, 2, 3], 'g0133': [0, 1, 2, 3]}
g0113: [2024-08-02 18:07:22,089] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0113: [2024-08-02 18:07:22,089] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0113': [0, 1, 2, 3], 'g0125': [4, 5, 6, 7], 'g0128': [8, 9, 10, 11], 'g0129': [12, 13, 14, 15], 'g0130': [16, 17, 18, 19], 'g0131': [20, 21, 22, 23], 'g0132': [24, 25, 26, 27], 'g0133': [28, 29, 30, 31]})
g0113: [2024-08-02 18:07:22,089] [INFO] [launch.py:163:main] dist_world_size=32
g0113: [2024-08-02 18:07:22,089] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0113: [2024-08-02 18:07:25,235] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-02 18:07:25,235] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-02 18:07:25,341] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: [2024-08-02 18:07:25,388] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0130: [2024-08-02 18:07:26,846] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-02 18:07:26,863] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0129: [2024-08-02 18:07:27,036] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0128: [2024-08-02 18:07:27,037] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0132: [2024-08-02 18:07:27,057] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0133: [2024-08-02 18:07:27,306] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0131: [2024-08-02 18:07:27,327] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0113: --------------------------------------------------
g0113: DeepSpeed C++/CUDA extension op report
g0113: --------------------------------------------------
g0113: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0113:       runtime if needed. Op compatibility means that your system
g0113:       meet the required dependencies to JIT install the op.
g0113: --------------------------------------------------
g0113: JIT compiled ops requires ninja
g0113: ninjaninja  ....................................  [92m[OKAY][0m[92m[OKAY][0mninja
g0113: 
g0113:  ----------------------------------------------------------------------------------------------------..................
g0113: 
g0113:  op nameop name[92m[OKAY][0m  
g0113: ................................  --------------------------------------------------installedinstalled
g0113:   ....op name   compatiblecompatible................
g0113: 
g0113:  ----------------------------------------------------------------------------------------------------installed
g0113: ninja
g0113:   ....................  compatible[92m[OKAY][0m
g0113: 
g0113: --------------------------------------------------
g0113: --------------------------------------------------
g0113: op name ................ installed .. compatible
g0113: --------------------------------------------------
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0masync_io ......  ...............[92m[OKAY][0m 
g0113: [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attnfused_adam  ...................... [93m[NO][0m  [92m[YES][0m.......  ......[93m[NO][0m 
g0113: [92m[OKAY][0m
g0113: fused_lamb .............cpu_adam  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m......
g0113:  [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m fused_lion......  .............[92m[OKAY][0m 
g0113: [92m[YES][0m ......cpu_lion  [92m[OKAY][0m...............
g0113:  [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cpu_lion ............... [92m[YES][0masync_io ......  ...............[92m[OKAY][0m 
g0113: [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attnfused_adam  ......................  [93m[NO][0m[92m[YES][0m  .............  [93m[NO][0m[92m[OKAY][0m
g0113: 
g0113: fused_lamb cpu_adam.............  ...............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0113: [92m[OKAY][0m
g0113: cpu_adagrad ............ [92m[YES][0m ......fused_lion  [92m[OKAY][0m.............
g0113:  [92m[YES][0m ......cpu_lion  [92m[OKAY][0m...............
g0113:  [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0113: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0113: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0113: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: cutlass_opsquantizer  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0113: 
g0113: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: ragged_device_ops ...... ragged_device_ops[92m[YES][0m  ............  [92m[YES][0m[92m[OKAY][0m 
g0113: ...... [92m[OKAY][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0113: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0113: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0113: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0113: --------------------------------------------------
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0113: DeepSpeed general environment info:
g0113: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0113: torch version .................... 2.0.1+cu118
g0113: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0113: deepspeed info ................... 0.12.4, unknown, unknown
g0113: torch cuda version ............... 11.8
g0113: torch hip version ................ None
g0113: nvcc version ..................... 11.8
g0113: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0113: shared memory (/dev/shm) size .... 188.13 GB
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0113: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0113: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0113: using torch.float32 for parameters ...
g0113: ------------------------ arguments ------------------------
g0113:   accumulate_allreduce_grads_in_fp32 .............. False
g0113:   adam_beta1 ...................................... 0.9
g0113:   adam_beta2 ...................................... 0.95
g0113:   adam_eps ........................................ 1e-08
g0113:   add_bias_linear ................................. False
g0113:   add_position_embedding .......................... False
g0113:   adlr_autoresume ................................. False
g0113:   adlr_autoresume_interval ........................ 1000
g0113:   aml_data_download_path .......................... None
g0113:   apply_layernorm_1p .............................. False
g0113:   apply_query_key_layer_scaling ................... False
g0113:   apply_residual_connection_post_layernorm ........ False
g0113:   async_tensor_model_parallel_allreduce ........... False
g0113:   attention_dropout ............................... 0.0
g0113:   attention_softmax_in_fp32 ....................... False
g0113:   barrier_with_L1_time ............................ True
g0113:   bert_binary_head ................................ True
g0113:   bert_embedder_type .............................. megatron
g0113:   bert_load ....................................... None
g0113:   bf16 ............................................ False
g0113:   bias_dropout_fusion ............................. True
g0113:   bias_gelu_fusion ................................ False
g0113:   biencoder_projection_dim ........................ 0
g0113:   biencoder_shared_query_context_model ............ False
g0113:   block_data_path ................................. None
g0113:   checkpoint_activations .......................... False
g0113:   checkpoint_in_cpu ............................... False
g0113:   checkpoint_num_layers ........................... 1
g0113:   classes_fraction ................................ 1.0
g0113:   clip_grad ....................................... 1.0
g0113:   compression_training ............................ False
g0113:   consumed_train_samples .......................... 0
g0113:   consumed_train_tokens ........................... 0
g0113:   consumed_valid_samples .......................... 0
g0113:   contigious_checkpointing ........................ False
g0113:   cpu_optimizer ................................... False
g0113:   cpu_torch_adam .................................. False
g0113:   create_moe_param_group .......................... False
g0113:   curriculum_learning_legacy ...................... False
g0113:   data_cache_path ................................. None
g0113:   data_efficiency_curriculum_learning ............. False
g0113:   data_impl ....................................... mmap
g0113:   data_parallel_random_init ....................... False
g0113:   data_parallel_size .............................. 4
g0113:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_000100_1234_True_text_document']
g0113:   data_per_class_fraction ......................... 1.0
g0113:   data_sharding ................................... True
g0113:   dataloader_type ................................. single
g0113:   DDP_impl ........................................ local
g0113:   decoder_num_layers .............................. None
g0113:   decoder_seq_length .............................. None
g0113:   deepscale ....................................... False
g0113:   deepscale_config ................................ None
g0113:   deepspeed ....................................... True
g0113:   deepspeed_activation_checkpointing .............. False
g0113:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0113:   deepspeed_mpi ................................... False
g0113:   dino_bottleneck_size ............................ 256
g0113:   dino_freeze_last_layer .......................... 1
g0113:   dino_head_hidden_size ........................... 2048
g0113:   dino_local_crops_number ......................... 10
g0113:   dino_local_img_size ............................. 96
g0113:   dino_norm_last_layer ............................ False
g0113:   dino_teacher_temp ............................... 0.07
g0113:   dino_warmup_teacher_temp ........................ 0.04
g0113:   dino_warmup_teacher_temp_epochs ................. 30
g0113:   distribute_checkpointed_activations ............. False
g0113:   distribute_saved_activations .................... False
g0113:   distributed_backend ............................. nccl
g0113:   distributed_timeout_minutes ..................... 10
g0113:   ds_fused_adam ................................... False
g0113:   ds_inference .................................... False
g0113:   ds_pipeline_enabled ............................. True
g0113:   ds_sequence_parallel_size ....................... 1
g0113:   embedding_path .................................. None
g0113:   embedding_weights_in_fp32 ....................... False
g0113:   empty_unused_memory_level ....................... 0
g0113:   enable_expert_tensor_parallelism ................ False
g0113:   encoder_num_layers .............................. 22
g0113:   encoder_seq_length .............................. 2048
g0113:   end_weight_decay ................................ 0.1
g0113:   eod_mask_loss ................................... False
g0113:   eval_interval ................................... 1000
g0113:   eval_iters ...................................... 100
g0113:   evidence_data_path .............................. None
g0113:   exit_duration_in_mins ........................... 30000000
g0113:   exit_interval ................................... None
g0113:   exit_on_missing_checkpoint ...................... False
g0113:   exit_signal_handler ............................. False
g0113:   expert_interval ................................. 2
g0113:   ffn_hidden_size ................................. 5632
g0113:   finetune ........................................ False
g0113:   force_ds_sequence_parallel ...................... False
g0113:   fp16 ............................................ False
g0113:   fp16_lm_cross_entropy ........................... False
g0113:   fp32_residual_connection ........................ False
g0113:   fp8_amax_compute_algo ........................... most_recent
g0113:   fp8_amax_history_len ............................ 1
g0113:   fp8_e4m3 ........................................ False
g0113:   fp8_hybrid ...................................... False
g0113:   fp8_interval .................................... 1
g0113:   fp8_margin ...................................... 0
g0113:   fp8_wgrad ....................................... True
g0113:   global_batch_size ............................... 128
g0113:   gradient_accumulation_fusion .................... True
g0113:   head_lr_mult .................................... 1.0
g0113:   hidden_dropout .................................. 0.0
g0113:   hidden_size ..................................... 2048
g0113:   hidden_size_teacher ............................. None
g0113:   hysteresis ...................................... 2
g0113:   ict_head_size ................................... None
g0113:   ict_load ........................................ None
g0113:   img_h ........................................... 224
g0113:   img_w ........................................... 224
g0113:   indexer_batch_size .............................. 128
g0113:   indexer_log_interval ............................ 1000
g0113:   inference ....................................... False
g0113:   inference_batch_times_seqlen_threshold .......... 512
g0113:   init_method_std ................................. 0.013
g0113:   init_method_xavier_uniform ...................... False
g0113:   initial_loss_scale .............................. 4294967296
g0113:   iter_per_epoch .................................. 1250
g0113:   kd .............................................. False
g0113:   kd_alpha_ce ..................................... 1
g0113:   kd_beta_ce ...................................... 1
g0113:   kd_temp ......................................... 1.0
g0113:   kv_channels ..................................... 128
g0113:   layernorm_epsilon ............................... 1e-05
g0113:   lazy_mpu_init ................................... None
g0113:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113:   load_teacher .................................... None
g0113:   local_rank ...................................... 0
g0113:   log_batch_size_to_tensorboard ................... True
g0113:   log_interval .................................... 10
g0113:   log_learning_rate_to_tensorboard ................ True
g0113:   log_loss_scale_to_tensorboard ................... True
g0113:   log_memory_to_tensorboard ....................... False
g0113:   log_num_zeros_in_grad ........................... False
g0113:   log_optimizer_states_to_tensorboard ............. True
g0113:   log_params_norm ................................. False
g0113:   log_timers_to_tensorboard ....................... True
g0113:   log_validation_ppl_to_tensorboard ............... True
g0113:   log_world_size_to_tensorboard ................... False
g0113:   loss_scale ...................................... None
g0113:   loss_scale_window ............................... 1000
g0113:   lr .............................................. 0.0002
g0113:   lr_decay_iters .................................. None
g0113:   lr_decay_samples ................................ None
g0113:   lr_decay_style .................................. cosine
g0113:   lr_decay_tokens ................................. 300000000000
g0113:   lr_warmup_fraction .............................. None
g0113:   lr_warmup_iters ................................. 0
g0113:   lr_warmup_samples ............................... 0
g0113:   lr_warmup_tokens ................................ 3000000000
g0113:   make_vocab_size_divisible_by .................... 128
g0113:   mask_factor ..................................... 1.0
g0113:   mask_prob ....................................... 0.15
g0113:   mask_type ....................................... random
g0113:   masked_softmax_fusion ........................... True
g0113:   max_position_embeddings ......................... 2048
g0113:   max_tokens_to_oom ............................... 12000
g0113:   mem_efficient_ln ................................ True
g0113:   memory_centric_tiled_linear ..................... False
g0113:   merge_file ...................................... None
g0113:   micro_batch_size ................................ 1
g0113:   min_loss_scale .................................. 1.0
g0113:   min_lr .......................................... 1e-05
g0113:   mlp_type ........................................ standard
g0113:   mmap_warmup ..................................... False
g0113:   moe_eval_capacity_factor ........................ 1.0
g0113:   moe_expert_parallel_size ........................ 1
g0113:   moe_loss_coeff .................................. 0.1
g0113:   moe_min_capacity ................................ 4
g0113:   moe_token_dropping .............................. True
g0113:   moe_train_capacity_factor ....................... 1.0
g0113:   mos ............................................. False
g0113:   no_load_lr_state ................................ False
g0113:   no_load_optim ................................... None
g0113:   no_load_rng ..................................... None
g0113:   no_persist_layer_norm ........................... False
g0113:   no_pipeline_parallel ............................ False
g0113:   no_save_optim ................................... None
g0113:   no_save_rng ..................................... None
g0113:   normalization ................................... rmsnorm
g0113:   num_attention_heads ............................. 16
g0113:   num_attention_heads_teacher ..................... None
g0113:   num_channels .................................... 3
g0113:   num_classes ..................................... 1000
g0113:   num_experts ..................................... [1]
g0113:   num_experts_switch .............................. None
g0113:   num_experts_teacher ............................. [1]
g0113:   num_key_value_heads ............................. 4
g0113:   num_layers ...................................... 22
g0113:   num_layers_per_virtual_pipeline_stage ........... None
g0113:   num_layers_teacher .............................. None
g0113:   num_workers ..................................... 0
g0113:   onnx_safe ....................................... None
g0113:   openai_gelu ..................................... False
g0113:   optimizer ....................................... adam
g0113:   output_bert_embeddings .......................... False
g0113:   overlap_p2p_comm ................................ False
g0113:   override_opt_param_scheduler .................... True
g0113:   params_dtype .................................... torch.float32
g0113:   partition_activations ........................... False
g0113:   patch_dim ....................................... 16
g0113:   perform_initialization .......................... True
g0113:   pipeline_model_parallel_size .................... 8
g0113:   pipeline_model_parallel_split_rank .............. None
g0113:   profile_backward ................................ False
g0113:   query_in_block_prob ............................. 0.1
g0113:   rampup_batch_size ............................... None
g0113:   random_ltd ...................................... False
g0113:   rank ............................................ 0
g0113:   recompute_granularity ........................... None
g0113:   recompute_method ................................ None
g0113:   recompute_num_layers ............................ 1
g0113:   remote_device ................................... none
g0113:   repeated_dataloader ............................. False
g0113:   reset_attention_mask ............................ False
g0113:   reset_iteration ................................. False
g0113:   reset_position_ids .............................. False
g0113:   retriever_report_topk_accuracies ................ []
g0113:   retriever_score_scaling ......................... False
g0113:   retriever_seq_length ............................ 256
g0113:   retro_add_retriever ............................. False
g0113:   retro_cyclic_train_iters ........................ None
g0113:   retro_encoder_attention_dropout ................. 0.1
g0113:   retro_encoder_hidden_dropout .................... 0.1
g0113:   retro_encoder_layers ............................ 2
g0113:   retro_num_neighbors ............................. 2
g0113:   retro_num_retrieved_chunks ...................... 2
g0113:   retro_return_doc_ids ............................ False
g0113:   retro_workdir ................................... None
g0113:   return_data_index ............................... False
g0113:   rotary_percent .................................. 1.0
g0113:   sample_rate ..................................... 1.0
g0113:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113:   save_interval ................................... 1000
g0113:   scatter_gather_tensors_in_pipeline .............. True
g0113:   scattered_embeddings ............................ False
g0113:   seed ............................................ 1234
g0113:   seq_length ...................................... 2048
g0113:   sequence_parallel ............................... False
g0113:   sgd_momentum .................................... 0.9
g0113:   short_seq_prob .................................. 0.1
g0113:   skip_train ...................................... False
g0113:   split ........................................... 949,50,1
g0113:   split_transformers .............................. False
g0113:   squared_relu .................................... False
g0113:   standalone_embedding_stage ...................... False
g0113:   start_weight_decay .............................. 0.1
g0113:   swiglu .......................................... True
g0113:   swin_backbone_type .............................. tiny
g0113:   synchronize_each_layer .......................... False
g0113:   tensor_model_parallel_size ...................... 1
g0113:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_000100_1234_True
g0113:   tensorboard_log_interval ........................ 1
g0113:   tensorboard_queue_size .......................... 1
g0113:   test_data_path .................................. None
g0113:   tf32 ............................................ False
g0113:   tile_factor ..................................... 1
g0113:   timing_log_level ................................ 0
g0113:   timing_log_option ............................... minmax
g0113:   titles_data_path ................................ None
g0113:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_000100_1234_True.model
g0113:   tokenizer_type .................................. SentencePieceTokenizer
g0113:   topk ............................................ 1
g0113:   train_data_exact_num_epochs ..................... 1
g0113:   train_data_path ................................. None
g0113:   train_desc_path ................................. None
g0113:   train_doc_idx_path .............................. None
g0113:   train_idx_path .................................. None
g0113:   train_iters ..................................... None
g0113:   train_sample_idx_path ........................... None
g0113:   train_samples ................................... 1280000000
g0113:   train_shuffle_idx_path .......................... None
g0113:   train_tokens .................................... 2621440000000
g0113:   transformer_impl ................................ local
g0113:   transformer_pipeline_model_parallel_size ........ 8
g0113:   universal_checkpoint ............................ False
g0113:   untie_embeddings_and_output_weights ............. True
g0113:   use_checkpoint_args ............................. False
g0113:   use_checkpoint_opt_param_scheduler .............. False
g0113:   use_contiguous_buffers_in_local_ddp ............. True
g0113:   use_cpu_initialization .......................... None
g0113:   use_dataset_only ................................ False
g0113:   use_distributed_optimizer ....................... False
g0113:   use_flash_attn .................................. False
g0113:   use_flash_attn_triton ........................... False
g0113:   use_flash_attn_v1 ............................... False
g0113:   use_flash_attn_v2 ............................... False
g0113:   use_one_sent_docs ............................... False
g0113:   use_pin_memory .................................. False
g0113:   use_ring_exchange_p2p ........................... False
g0113:   use_rotary_position_embeddings .................. True
g0113:   use_tutel ....................................... False
g0113:   use_wandb ....................................... True
g0113:   valid_data_path ................................. None
g0113:   variable_seq_lengths ............................ False
g0113:   virtual_pipeline_model_parallel_size ............ None
g0113:   vision_backbone_type ............................ vit
g0113:   vision_pretraining .............................. False
g0113:   vision_pretraining_type ......................... classify
g0113:   vocab_extra_ids ................................. 0
g0113:   vocab_file ...................................... None
g0113:   vocab_size ...................................... None
g0113:   wandb_entity .................................... yohei-kobashi
g0113:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_000100_1234_True
g0113:   wandb_project ................................... encrypted_data_LLM
g0113:   wandb_tag ....................................... other_gpu
g0113:   weight_decay .................................... 0.1
g0113:   weight_decay_incr_style ......................... constant
g0113:   world_size ...................................... 32
g0113:   zero_allgather_bucket_size ...................... 0.0
g0113:   zero_contigious_gradients ....................... False
g0113:   zero_reduce_bucket_size ......................... 0.0
g0113:   zero_reduce_scatter ............................. False
g0113:   zero_stage ...................................... 0
g0113: -------------------- end of arguments ---------------------
g0113: setting number of micro-batches to constant 32
g0113: > building SentencePieceTokenizer tokenizer ...
g0113:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0113: > initializing torch distributed ...
g0113: [2024-08-02 18:07:29,910] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [2024-08-02 18:07:29,910] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0113: [2024-08-02 18:07:29,910] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [2024-08-02 18:07:29,910] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [2024-08-02 18:07:29,911] [INFO] [comm.py:637:init_distributed] cdb=None
g0113: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0130: [2024-08-02 18:07:30,807] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0130: [2024-08-02 18:07:30,807] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0113': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0128': [0, 1, 2, 3], 'g0129': [0, 1, 2, 3], 'g0130': [0, 1, 2, 3], 'g0131': [0, 1, 2, 3], 'g0132': [0, 1, 2, 3], 'g0133': [0, 1, 2, 3]}
g0130: [2024-08-02 18:07:30,807] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0130: [2024-08-02 18:07:30,807] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0113': [0, 1, 2, 3], 'g0125': [4, 5, 6, 7], 'g0128': [8, 9, 10, 11], 'g0129': [12, 13, 14, 15], 'g0130': [16, 17, 18, 19], 'g0131': [20, 21, 22, 23], 'g0132': [24, 25, 26, 27], 'g0133': [28, 29, 30, 31]})
g0130: [2024-08-02 18:07:30,807] [INFO] [launch.py:163:main] dist_world_size=32
g0130: [2024-08-02 18:07:30,807] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0125: [2024-08-02 18:07:30,941] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0125: [2024-08-02 18:07:30,941] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0113': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0128': [0, 1, 2, 3], 'g0129': [0, 1, 2, 3], 'g0130': [0, 1, 2, 3], 'g0131': [0, 1, 2, 3], 'g0132': [0, 1, 2, 3], 'g0133': [0, 1, 2, 3]}
g0125: [2024-08-02 18:07:30,941] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0125: [2024-08-02 18:07:30,941] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0113': [0, 1, 2, 3], 'g0125': [4, 5, 6, 7], 'g0128': [8, 9, 10, 11], 'g0129': [12, 13, 14, 15], 'g0130': [16, 17, 18, 19], 'g0131': [20, 21, 22, 23], 'g0132': [24, 25, 26, 27], 'g0133': [28, 29, 30, 31]})
g0125: [2024-08-02 18:07:30,941] [INFO] [launch.py:163:main] dist_world_size=32
g0125: [2024-08-02 18:07:30,941] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0128: [2024-08-02 18:07:31,053] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0128: [2024-08-02 18:07:31,053] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0113': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0128': [0, 1, 2, 3], 'g0129': [0, 1, 2, 3], 'g0130': [0, 1, 2, 3], 'g0131': [0, 1, 2, 3], 'g0132': [0, 1, 2, 3], 'g0133': [0, 1, 2, 3]}
g0128: [2024-08-02 18:07:31,053] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0128: [2024-08-02 18:07:31,053] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0113': [0, 1, 2, 3], 'g0125': [4, 5, 6, 7], 'g0128': [8, 9, 10, 11], 'g0129': [12, 13, 14, 15], 'g0130': [16, 17, 18, 19], 'g0131': [20, 21, 22, 23], 'g0132': [24, 25, 26, 27], 'g0133': [28, 29, 30, 31]})
g0128: [2024-08-02 18:07:31,053] [INFO] [launch.py:163:main] dist_world_size=32
g0128: [2024-08-02 18:07:31,053] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0132: [2024-08-02 18:07:31,088] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0132: [2024-08-02 18:07:31,089] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0113': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0128': [0, 1, 2, 3], 'g0129': [0, 1, 2, 3], 'g0130': [0, 1, 2, 3], 'g0131': [0, 1, 2, 3], 'g0132': [0, 1, 2, 3], 'g0133': [0, 1, 2, 3]}
g0132: [2024-08-02 18:07:31,089] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0132: [2024-08-02 18:07:31,089] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0113': [0, 1, 2, 3], 'g0125': [4, 5, 6, 7], 'g0128': [8, 9, 10, 11], 'g0129': [12, 13, 14, 15], 'g0130': [16, 17, 18, 19], 'g0131': [20, 21, 22, 23], 'g0132': [24, 25, 26, 27], 'g0133': [28, 29, 30, 31]})
g0132: [2024-08-02 18:07:31,089] [INFO] [launch.py:163:main] dist_world_size=32
g0132: [2024-08-02 18:07:31,089] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0129: [2024-08-02 18:07:31,124] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0129: [2024-08-02 18:07:31,124] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0113': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0128': [0, 1, 2, 3], 'g0129': [0, 1, 2, 3], 'g0130': [0, 1, 2, 3], 'g0131': [0, 1, 2, 3], 'g0132': [0, 1, 2, 3], 'g0133': [0, 1, 2, 3]}
g0129: [2024-08-02 18:07:31,124] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0129: [2024-08-02 18:07:31,124] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0113': [0, 1, 2, 3], 'g0125': [4, 5, 6, 7], 'g0128': [8, 9, 10, 11], 'g0129': [12, 13, 14, 15], 'g0130': [16, 17, 18, 19], 'g0131': [20, 21, 22, 23], 'g0132': [24, 25, 26, 27], 'g0133': [28, 29, 30, 31]})
g0129: [2024-08-02 18:07:31,124] [INFO] [launch.py:163:main] dist_world_size=32
g0129: [2024-08-02 18:07:31,124] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0133: [2024-08-02 18:07:31,199] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0133: [2024-08-02 18:07:31,199] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0113': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0128': [0, 1, 2, 3], 'g0129': [0, 1, 2, 3], 'g0130': [0, 1, 2, 3], 'g0131': [0, 1, 2, 3], 'g0132': [0, 1, 2, 3], 'g0133': [0, 1, 2, 3]}
g0133: [2024-08-02 18:07:31,199] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0133: [2024-08-02 18:07:31,199] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0113': [0, 1, 2, 3], 'g0125': [4, 5, 6, 7], 'g0128': [8, 9, 10, 11], 'g0129': [12, 13, 14, 15], 'g0130': [16, 17, 18, 19], 'g0131': [20, 21, 22, 23], 'g0132': [24, 25, 26, 27], 'g0133': [28, 29, 30, 31]})
g0133: [2024-08-02 18:07:31,200] [INFO] [launch.py:163:main] dist_world_size=32
g0133: [2024-08-02 18:07:31,200] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0131: [2024-08-02 18:07:31,314] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0131: [2024-08-02 18:07:31,314] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0113': [0, 1, 2, 3], 'g0125': [0, 1, 2, 3], 'g0128': [0, 1, 2, 3], 'g0129': [0, 1, 2, 3], 'g0130': [0, 1, 2, 3], 'g0131': [0, 1, 2, 3], 'g0132': [0, 1, 2, 3], 'g0133': [0, 1, 2, 3]}
g0131: [2024-08-02 18:07:31,314] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0131: [2024-08-02 18:07:31,314] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0113': [0, 1, 2, 3], 'g0125': [4, 5, 6, 7], 'g0128': [8, 9, 10, 11], 'g0129': [12, 13, 14, 15], 'g0130': [16, 17, 18, 19], 'g0131': [20, 21, 22, 23], 'g0132': [24, 25, 26, 27], 'g0133': [28, 29, 30, 31]})
g0131: [2024-08-02 18:07:31,314] [INFO] [launch.py:163:main] dist_world_size=32
g0131: [2024-08-02 18:07:31,314] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0130: [2024-08-02 18:07:33,861] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0130: [2024-08-02 18:07:33,861] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0130: [2024-08-02 18:07:33,877] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-02 18:07:34,008] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-02 18:07:34,008] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-02 18:07:34,049] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0130: [2024-08-02 18:07:34,101] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0128: [2024-08-02 18:07:34,112] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0128: [2024-08-02 18:07:34,112] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0125: [2024-08-02 18:07:34,143] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0129: [2024-08-02 18:07:34,175] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0129: [2024-08-02 18:07:34,175] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0132: [2024-08-02 18:07:34,179] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0132: [2024-08-02 18:07:34,179] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0128: [2024-08-02 18:07:34,202] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0129: [2024-08-02 18:07:34,251] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0129: [2024-08-02 18:07:34,251] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0128: [2024-08-02 18:07:34,262] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0133: [2024-08-02 18:07:34,266] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0133: [2024-08-02 18:07:34,266] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0132: [2024-08-02 18:07:34,272] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0132: [2024-08-02 18:07:34,343] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0133: [2024-08-02 18:07:34,359] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0133: [2024-08-02 18:07:34,386] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0131: [2024-08-02 18:07:34,430] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0131: [2024-08-02 18:07:34,430] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0131: [2024-08-02 18:07:34,431] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0131: [2024-08-02 18:07:34,606] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0130: --------------------------------------------------
g0130: DeepSpeed C++/CUDA extension op report
g0130: --------------------------------------------------
g0130: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0130:       runtime if needed. Op compatibility means that your system
g0130:       meet the required dependencies to JIT install the op.
g0130: --------------------------------------------------
g0130: JIT compiled ops requires ninja
g0130: --------------------------------------------------
g0130: DeepSpeed C++/CUDA extension op report
g0130: --------------------------------------------------
g0130: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0130:       runtime if needed. Op compatibility means that your system
g0130:       meet the required dependencies to JIT install the op.
g0130: --------------------------------------------------
g0130: JIT compiled ops requires ninja
g0130: --------------------------------------------------
g0130: DeepSpeed C++/CUDA extension op report
g0130: --------------------------------------------------
g0130: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0130:       runtime if needed. Op compatibility means that your system
g0130:       meet the required dependencies to JIT install the op.
g0130: --------------------------------------------------
g0130: JIT compiled ops requires ninja
g0130: --------------------------------------------------
g0130: DeepSpeed C++/CUDA extension op report
g0130: --------------------------------------------------
g0130: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0130:       runtime if needed. Op compatibility means that your system
g0130:       meet the required dependencies to JIT install the op.
g0130: --------------------------------------------------
g0130: JIT compiled ops requires ninja
g0130: ninjaninja  ....................................ninjaninja   [92m[OKAY][0m..................[92m[OKAY][0m 
g0130:  ..................
g0130: [92m[OKAY][0m-------------------------------------------------- 
g0130: --------------------------------------------------
g0130: [92m[OKAY][0m
g0130: --------------------------------------------------
g0130: op name
g0130: op name --------------------------------------------------................op name 
g0130:   ................installed................  op name installed.. installed  ................ ..compatible .. 
g0130: installed compatible --------------------------------------------------compatible
g0130: ..
g0130: 
g0130:  --------------------------------------------------compatible
g0130: --------------------------------------------------
g0130: 
g0130: --------------------------------------------------
g0130: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0130: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0130: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0130: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0130: async_io ...............[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0130: [92m[YES][0m ......evoformer_attn  [92m[OKAY][0m.........
g0130:  [93m[NO][0m ....... [93m[NO][0m
g0130: fused_adam fused_lamb.............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0130: [92m[OKAY][0m
g0130: cpu_adam async_io...............  [92m[YES][0m...............  ......[92m[YES][0m fused_lion [92m[OKAY][0m ......
g0130: .............  [92m[OKAY][0m[92m[YES][0mcpu_adagrad
g0130:   ..................  [92m[OKAY][0m[92m[YES][0m
g0130:  ...... [92m[OKAY][0mfused_adam
g0130:  ............. cpu_lion[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0130: ...... [92m[OKAY][0m
g0130: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0130: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0130: cpu_adagradevoformer_attn  .....................  async_io[92m[YES][0m[93m[NO][0m   ............................   [92m[OKAY][0m[93m[NO][0m[92m[YES][0m
g0130: 
g0130:  ...... cpu_lionfused_lamb[92m[OKAY][0m  ...............
g0130: .............  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0mfused_adam
g0130: 
g0130:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: fused_lion .............[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_adam 
g0130:  [92m[YES][0m............... evoformer_attn ...... [92m[YES][0m ......... [92m[OKAY][0m ......
g0130: [93m[NO][0m  [92m[OKAY][0m.......
g0130:  [93m[NO][0m
g0130: cpu_adagrad ............fused_lamb  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0130:  [92m[OKAY][0m
g0130: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0mfused_lion
g0130:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0130: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0130: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0130: inference_core_ops ..... [92m[YES][0minference_core_ops  ...........  [92m[OKAY][0m[92m[YES][0m
g0130:  ...... [92m[OKAY][0m
g0130: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0130: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0130: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0130: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0130: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0130: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0130: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0130: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0130: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0130: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0130: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0130: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0130: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0130: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0130: ragged_ops ............. [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible[92m[YES][0m
g0130:  ...... sparse_attn[92m[OKAY][0m 
g0130: ............ [93m[NO][0m random_ltd.......  .............[93m[NO][0mragged_ops 
g0130:  [92m[YES][0m.............  ......[92m[YES][0m  [92m[OKAY][0m......
g0130:  [92m[OKAY][0m
g0130: random_ltd ............. [92m[YES][0m ......[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0 
g0130: [92m[OKAY][0m
g0130: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0130: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0130: sparse_attn[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible 
g0130: ............ [93m[NO][0msparse_attn  ...................  [93m[NO][0m[93m[NO][0m
g0130:  ....... [93m[NO][0m
g0130: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0130: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0130: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0130: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0130: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0130: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0130: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0130: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0130: stochastic_transformer .spatial_inference  [92m[YES][0m......  ......[92m[YES][0m  [92m[OKAY][0m......
g0130:  [92m[OKAY][0m
g0130: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0130: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0130: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0130: --------------------------------------------------
g0130: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0130: --------------------------------------------------
g0130: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0130: --------------------------------------------------
g0130: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0130: --------------------------------------------------
g0130: DeepSpeed general environment info:
g0130: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0130: torch version .................... 2.0.1+cu118
g0130: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0130: deepspeed info ................... 0.12.4, unknown, unknown
g0130: torch cuda version ............... 11.8
g0130: torch hip version ................ None
g0130: nvcc version ..................... 11.8
g0130: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0130: shared memory (/dev/shm) size .... 188.13 GB
g0130: DeepSpeed general environment info:
g0130: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0130: torch version .................... 2.0.1+cu118
g0130: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0130: deepspeed info ................... 0.12.4, unknown, unknown
g0130: torch cuda version ............... 11.8
g0130: torch hip version ................ None
g0130: nvcc version ..................... 11.8
g0130: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0130: shared memory (/dev/shm) size .... 188.13 GB
g0130: DeepSpeed general environment info:
g0130: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0130: torch version .................... 2.0.1+cu118
g0130: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0130: deepspeed info ................... 0.12.4, unknown, unknown
g0130: torch cuda version ............... 11.8
g0130: torch hip version ................ None
g0130: nvcc version ..................... 11.8
g0130: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0130: shared memory (/dev/shm) size .... 188.13 GB
g0130: DeepSpeed general environment info:
g0130: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0130: torch version .................... 2.0.1+cu118
g0130: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0130: deepspeed info ................... 0.12.4, unknown, unknown
g0130: torch cuda version ............... 11.8
g0130: torch hip version ................ None
g0130: nvcc version ..................... 11.8
g0130: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0130: shared memory (/dev/shm) size .... 188.13 GB
g0125: ----------------------------------------------------------------------------------------------------
g0125: 
g0125: --------------------------------------------------DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0125: 
g0125: 
g0125: ----------------------------------------------------------------------------------------------------DeepSpeed C++/CUDA extension op report
g0125: 
g0125: 
g0125: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.--------------------------------------------------
g0125: 
g0125: 
g0125: ----------------------------------------------------------------------------------------------------
g0125: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.
g0125: 
g0125: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0125: --------------------------------------------------
g0125: 
g0125: JIT compiled ops requires ninja
g0125: --------------------------------------------------
g0125: DeepSpeed C++/CUDA extension op report
g0125: --------------------------------------------------
g0125: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0125:       runtime if needed. Op compatibility means that your system
g0125:       meet the required dependencies to JIT install the op.
g0125: --------------------------------------------------
g0125: JIT compiled ops requires ninja
g0128: --------------------------------------------------
g0128: DeepSpeed C++/CUDA extension op report
g0128: --------------------------------------------------
g0128: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0128:       runtime if needed. Op compatibility means that your system
g0128:       meet the required dependencies to JIT install the op.
g0128: --------------------------------------------------
g0128: JIT compiled ops requires ninja
g0128: ----------------------------------------------------------------------------------------------------
g0128: 
g0128: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0128: 
g0128: ----------------------------------------------------------------------------------------------------
g0128: 
g0128: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0128:       runtime if needed. Op compatibility means that your system
g0128:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0128:       runtime if needed. Op compatibility means that your system
g0128:       meet the required dependencies to JIT install the op.
g0128: 
g0128: ----------------------------------------------------------------------------------------------------
g0128: 
g0128: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0128: 
g0128: --------------------------------------------------
g0128: DeepSpeed C++/CUDA extension op report
g0128: --------------------------------------------------
g0128: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0128:       runtime if needed. Op compatibility means that your system
g0128:       meet the required dependencies to JIT install the op.
g0128: --------------------------------------------------
g0128: JIT compiled ops requires ninja
g0125: ninjaninja  ninja..................ninja..................    [92m[OKAY][0m..................[92m[OKAY][0m..................
g0125:  
g0125:  [92m[OKAY][0m--------------------------------------------------
g0125: [92m[OKAY][0m--------------------------------------------------
g0125: 
g0125: 
g0125: --------------------------------------------------op name
g0125: -------------------------------------------------- op name
g0125: op name ................ ................op name ................   installedinstalled................installed    ....installed..    compatiblecompatible..compatible
g0125: 
g0125:  
g0125: ----------------------------------------------------------------------------------------------------compatible--------------------------------------------------
g0125: 
g0125: 
g0125: 
g0125: --------------------------------------------------
g0128: ninjaninja ninjaninja..................    ..................[92m[OKAY][0m.................. ..................
g0128: [92m[OKAY][0m  
g0128: --------------------------------------------------[92m[OKAY][0m[92m[OKAY][0m
g0128: 
g0128: --------------------------------------------------
g0128: 
g0128: op name--------------------------------------------------op name -------------------------------------------------- 
g0128: ................
g0128: ................  op nameinstalledinstalledop name   ....................    ................installedcompatiblecompatible 
g0128:  
g0128: ..--------------------------------------------------installed --------------------------------------------------
g0128:  compatible
g0128: ..
g0128:  --------------------------------------------------compatible
g0128: 
g0128: --------------------------------------------------
g0130: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0130: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0130: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0130: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0125: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0125: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_adam ............. async_io[92m[YES][0m  .....................  [92m[OKAY][0m[92m[YES][0m
g0125:  ...... [92m[OKAY][0mcpu_adam
g0125:  ............... [92m[YES][0m ...... fused_adam[92m[OKAY][0m 
g0125: ............. [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0m[92m[YES][0m
g0125:  ...... [92m[OKAY][0mcpu_adam
g0125:  ............... [92m[YES][0mcpu_lion  .....................  [92m[OKAY][0m[92m[YES][0m
g0125:  ...... cpu_adagrad[92m[OKAY][0m 
g0125: ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_lion
g0125:  ............... evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0125: ....... [93m[NO][0m
g0125: fused_lambasync_io[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0125: .............  evoformer_attn...............[92m[YES][0m   .........[92m[YES][0m......   [93m[NO][0m......[92m[OKAY][0m  
g0125: .......[92m[OKAY][0m 
g0125: [93m[NO][0m
g0125: fused_lamb .............fused_adam fused_lion [92m[YES][0m ............. ............. ...... [92m[YES][0m [92m[YES][0m [92m[OKAY][0m ......
g0125: ......  [92m[OKAY][0m[92m[OKAY][0m
g0125: 
g0125: cpu_adam fused_lion...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0125: [92m[OKAY][0m
g0125: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0125: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0125: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0128: async_io ...............fused_adam  [92m[YES][0m.............  [92m[YES][0m...... ......  [92m[OKAY][0m[92m[OKAY][0m
g0128: 
g0128: cpu_adam ............... [92m[YES][0m ......fused_adam  [92m[OKAY][0m
g0128: ............. cpu_adagrad[92m[YES][0m  ..................  [92m[YES][0m [92m[OKAY][0m......
g0128:  [92m[OKAY][0m
g0128: cpu_lioncpu_adam  ..............................  [92m[YES][0m [92m[YES][0m......  [92m[OKAY][0m......
g0128:  [92m[OKAY][0m
g0128: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_adagrad
g0128:  ............evoformer_attn  .........[92m[YES][0m  [93m[NO][0m...... .......  [92m[OKAY][0m[93m[NO][0m
g0128: 
g0128: fused_lamb .............cpu_lion  [92m[YES][0m .....................  [92m[OKAY][0m[92m[YES][0m
g0128:  ...... [92m[OKAY][0m
g0128: fused_lion async_io.............  [92m[YES][0m...............  ...... [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m async_io
g0128: 
g0128: ......  ...............[92m[OKAY][0mevoformer_attn 
g0128:  [92m[YES][0m ...............  [92m[OKAY][0m[93m[NO][0mfused_adam
g0128:   .................... [92m[YES][0m  fused_adam......[93m[NO][0m  
g0128: .............[92m[OKAY][0m 
g0128: [92m[YES][0m fused_lamb......cpu_adam   ............................[92m[OKAY][0m [92m[YES][0m
g0128:  ...... cpu_adam[92m[OKAY][0m 
g0128:  [92m[YES][0m............... cpu_adagrad ...... [92m[YES][0m............   ......[92m[OKAY][0m [92m[YES][0m[92m[OKAY][0m 
g0128: ......
g0128:  [92m[OKAY][0m
g0128: cpu_adagrad ............cpu_lion  [92m[YES][0m............... fused_lion ......[92m[YES][0m   .............[92m[OKAY][0m......  
g0128: [92m[YES][0m[92m[OKAY][0m 
g0128: cpu_lion...... ...............  [92m[OKAY][0m[92m[YES][0m 
g0128: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......
g0128:  [92m[OKAY][0m
g0128: evoformer_attn ......... [93m[NO][0m ....... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m[NO][0m
g0128: 
g0128: evoformer_attn fused_lamb.........  .............[93m[NO][0m  [92m[YES][0m.......  ......[93m[NO][0m 
g0128: [92m[OKAY][0m
g0128: fused_lamb ............. [92m[YES][0m ......fused_lion  [92m[OKAY][0m.............
g0128:  [92m[YES][0m ...... [92m[OKAY][0m
g0128: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: inference_core_ops .....inference_core_ops [92m[YES][0m  ...........  [92m[YES][0m[92m[OKAY][0m ......
g0125:  [92m[OKAY][0m
g0125: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0128: 
g0128: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0128: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0128: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0128: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0128: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0128: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0128: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0125: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0125: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0125: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatibleragged_ops
g0125:  ............. sparse_attn[92m[YES][0m  ..................  [93m[NO][0m[92m[OKAY][0m 
g0125: ....... [93m[NO][0mrandom_ltd
g0125:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0125: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0125: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0125: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0128: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0128: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0125: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0125: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0128: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0128: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0125: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0125: --------------------------------------------------
g0128: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0128: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0128: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0128: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0128: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0128: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: DeepSpeed general environment info:
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0128: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0128: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0128: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0128: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0128: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0128: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0128: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0125: DeepSpeed general environment info:
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0125: DeepSpeed general environment info:
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0125: DeepSpeed general environment info:
g0125: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0125: torch version .................... 2.0.1+cu118
g0125: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0125: deepspeed info ................... 0.12.4, unknown, unknown
g0125: torch cuda version ............... 11.8
g0125: torch hip version ................ None
g0125: nvcc version ..................... 11.8
g0125: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0125: shared memory (/dev/shm) size .... 188.13 GB
g0128: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0128: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0128: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0128: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0128: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0128: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0128: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0128: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0128: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0128: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0128: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0128: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0128: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0128: --------------------------------------------------
g0130: [2024-08-02 18:07:38,448] [INFO] [comm.py:637:init_distributed] cdb=None
g0128: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0128: --------------------------------------------------
g0129: --------------------------------------------------
g0129: DeepSpeed C++/CUDA extension op report
g0129: --------------------------------------------------
g0129: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0129:       runtime if needed. Op compatibility means that your system
g0129:       meet the required dependencies to JIT install the op.
g0129: --------------------------------------------------
g0129: JIT compiled ops requires ninja
g0129: --------------------------------------------------
g0129: DeepSpeed C++/CUDA extension op report
g0129: --------------------------------------------------
g0129: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0129:       runtime if needed. Op compatibility means that your system
g0129:       meet the required dependencies to JIT install the op.
g0129: --------------------------------------------------
g0129: JIT compiled ops requires ninja
g0129: --------------------------------------------------
g0129: DeepSpeed C++/CUDA extension op report
g0129: --------------------------------------------------
g0129: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0129:       runtime if needed. Op compatibility means that your system
g0129:       meet the required dependencies to JIT install the op.
g0129: --------------------------------------------------
g0129: JIT compiled ops requires ninja
g0130: [2024-08-02 18:07:38,450] [INFO] [comm.py:637:init_distributed] cdb=None
g0130: [2024-08-02 18:07:38,450] [INFO] [comm.py:637:init_distributed] cdb=None
g0130: [2024-08-02 18:07:38,450] [INFO] [comm.py:637:init_distributed] cdb=None
g0129: --------------------------------------------------
g0129: DeepSpeed C++/CUDA extension op report
g0129: --------------------------------------------------
g0129: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0129:       runtime if needed. Op compatibility means that your system
g0129:       meet the required dependencies to JIT install the op.
g0129: --------------------------------------------------
g0129: JIT compiled ops requires ninja
g0128: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0128: --------------------------------------------------
g0128: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0128: --------------------------------------------------
g0128: DeepSpeed general environment info:
g0128: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0128: torch version .................... 2.0.1+cu118
g0128: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0128: deepspeed info ................... 0.12.4, unknown, unknown
g0128: torch cuda version ............... 11.8
g0128: torch hip version ................ None
g0128: nvcc version ..................... 11.8
g0128: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0128: shared memory (/dev/shm) size .... 188.13 GB
g0128: DeepSpeed general environment info:
g0128: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0128: torch version .................... 2.0.1+cu118
g0128: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0128: deepspeed info ................... 0.12.4, unknown, unknown
g0128: torch cuda version ............... 11.8
g0128: torch hip version ................ None
g0128: nvcc version ..................... 11.8
g0128: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0128: shared memory (/dev/shm) size .... 188.13 GB
g0130: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0130: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0130: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0130: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0129: ninjaninjaninja  ninja....................................    ..................[92m[OKAY][0m[92m[OKAY][0m.................. 
g0129: 
g0129:  [92m[OKAY][0m[92m[OKAY][0m
g0129: ----------------------------------------------------------------------------------------------------
g0129: 
g0129: 
g0129: ----------------------------------------------------------------------------------------------------
g0129: op nameop name
g0129:   op name................................ op name  ................ installedinstalled ................  installed .... installed  .. compatible ..
g0129: compatible compatible
g0129: --------------------------------------------------compatible
g0129: 
g0129: 
g0129: ----------------------------------------------------------------------------------------------------
g0129: --------------------------------------------------
g0129: 
g0130: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0130: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0130: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0130: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0128: DeepSpeed general environment info:
g0128: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0128: torch version .................... 2.0.1+cu118
g0128: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0128: deepspeed info ................... 0.12.4, unknown, unknown
g0128: torch cuda version ............... 11.8
g0128: torch hip version ................ None
g0128: nvcc version ..................... 11.8
g0128: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0128: shared memory (/dev/shm) size .... 188.13 GB
g0128: DeepSpeed general environment info:
g0128: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0128: torch version .................... 2.0.1+cu118
g0128: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0128: deepspeed info ................... 0.12.4, unknown, unknown
g0128: torch cuda version ............... 11.8
g0128: torch hip version ................ None
g0128: nvcc version ..................... 11.8
g0128: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0128: shared memory (/dev/shm) size .... 188.13 GB
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0125: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0132: --------------------------------------------------
g0132: DeepSpeed C++/CUDA extension op report
g0132: --------------------------------------------------
g0132: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0132:       runtime if needed. Op compatibility means that your system
g0132:       meet the required dependencies to JIT install the op.
g0132: --------------------------------------------------
g0132: JIT compiled ops requires ninja
g0132: --------------------------------------------------
g0132: DeepSpeed C++/CUDA extension op report
g0132: --------------------------------------------------
g0132: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0132:       runtime if needed. Op compatibility means that your system
g0132:       meet the required dependencies to JIT install the op.
g0132: --------------------------------------------------
g0132: JIT compiled ops requires ninja
g0132: --------------------------------------------------
g0132: DeepSpeed C++/CUDA extension op report
g0132: --------------------------------------------------
g0132: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0132:       runtime if needed. Op compatibility means that your system
g0132:       meet the required dependencies to JIT install the op.
g0132: --------------------------------------------------
g0132: JIT compiled ops requires ninja
g0132: --------------------------------------------------
g0132: DeepSpeed C++/CUDA extension op report
g0132: --------------------------------------------------
g0132: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0132:       runtime if needed. Op compatibility means that your system
g0132:       meet the required dependencies to JIT install the op.
g0132: --------------------------------------------------
g0132: JIT compiled ops requires ninja
g0128: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0128: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0128: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0128: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0132: ninjaninjaninja   ......................................................  [92m[OKAY][0m [92m[OKAY][0m
g0132: [92m[OKAY][0m
g0132: 
g0132: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0132: 
g0132: 
g0132: op nameop nameop name   ................................................   installedinstalledinstalled   ......   compatiblecompatiblecompatible
g0132: 
g0132: 
g0132: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0132: 
g0132: 
g0132: ninja .................. [92m[OKAY][0m
g0132: --------------------------------------------------
g0132: op name ................ installed .. compatible
g0132: --------------------------------------------------
g0129: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0129: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0129: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0129: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0129: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0129: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0129: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0129: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0129: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0129: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0129: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0129: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0129: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0129: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0129: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0masync_io
g0129:  ...............cpu_lion  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m...... 
g0129: [92m[OKAY][0m
g0129: fused_adam ............. [92m[YES][0m ...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[OKAY][0m
g0129: 
g0129: evoformer_attn .........cpu_adam  [93m[NO][0m...............  .......[92m[YES][0m  [93m[NO][0m......
g0129:  [92m[OKAY][0m
g0129: fused_lamb .............cpu_adagrad  [92m[YES][0m............  ......[92m[YES][0m  [92m[OKAY][0m......
g0129:  [92m[OKAY][0m
g0129: cpu_lion ............... fused_lion[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0129: ...... [92m[OKAY][0m
g0129: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0129: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0129: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0129: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0129: 
g0129: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [2024-08-02 18:07:38,565] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: [2024-08-02 18:07:38,566] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: [2024-08-02 18:07:38,566] [INFO] [comm.py:637:init_distributed] cdb=None
g0125: [2024-08-02 18:07:38,566] [INFO] [comm.py:637:init_distributed] cdb=None
g0129: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0129: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0129: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0129: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0129: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0129: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0129: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0129: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0125: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0129: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0129: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0129: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0129: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0129: ragged_ops ............. [92m[YES][0m ......[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible 
g0129: [92m[OKAY][0m
g0129: sparse_attn ............random_ltd  [93m[NO][0m.............  .......[92m[YES][0m  [93m[NO][0m
g0129: ...... [92m[OKAY][0m
g0129: ragged_ops ............. [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0[92m[YES][0m
g0129:  ...... [92m[OKAY][0m
g0129: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0129: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0129: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0129: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0129: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0133: --------------------------------------------------
g0133: DeepSpeed C++/CUDA extension op report
g0133: --------------------------------------------------
g0133: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0133:       runtime if needed. Op compatibility means that your system
g0133:       meet the required dependencies to JIT install the op.
g0133: --------------------------------------------------
g0133: JIT compiled ops requires ninja
g0133: --------------------------------------------------
g0133: DeepSpeed C++/CUDA extension op report
g0133: --------------------------------------------------
g0133: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0133:       runtime if needed. Op compatibility means that your system
g0133:       meet the required dependencies to JIT install the op.
g0133: 
g0133: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0133: 
g0133: JIT compiled ops requires ninja--------------------------------------------------
g0133: 
g0133: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0133:       runtime if needed. Op compatibility means that your system
g0133:       meet the required dependencies to JIT install the op.
g0133: --------------------------------------------------
g0133: JIT compiled ops requires ninja
g0133: --------------------------------------------------
g0133: DeepSpeed C++/CUDA extension op report
g0133: --------------------------------------------------
g0133: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0133:       runtime if needed. Op compatibility means that your system
g0133:       meet the required dependencies to JIT install the op.
g0133: --------------------------------------------------
g0133: JIT compiled ops requires ninja
g0129: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0129: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0129: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0128: [2024-08-02 18:07:38,580] [INFO] [comm.py:637:init_distributed] cdb=None
g0129: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0129: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0129: stochastic_transformer . [92m[YES][0m spatial_inference......  [92m[OKAY][0m......
g0129:  [92m[YES][0m ...... [92m[OKAY][0m
g0129: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0129: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0129: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0129: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0129: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0129: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0129: --------------------------------------------------
g0133: ninjaninjaninja   ninja......................................................    [92m[OKAY][0m[92m[OKAY][0m[92m[OKAY][0m..................
g0133: 
g0133: 
g0133:  [92m[OKAY][0m------------------------------------------------------------------------------------------------------------------------------------------------------
g0133: 
g0133: 
g0133: 
g0133: --------------------------------------------------op nameop nameop name
g0133:    ................................op name................    installedinstalled................installed    ..installed....    compatible..compatiblecompatible
g0133:  
g0133: 
g0133: --------------------------------------------------compatible----------------------------------------------------------------------------------------------------
g0133: 
g0133: 
g0133: 
g0133: --------------------------------------------------
g0128: [2024-08-02 18:07:38,585] [INFO] [comm.py:637:init_distributed] cdb=None
g0129: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0129: --------------------------------------------------
g0129: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0129: --------------------------------------------------
g0129: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0129: --------------------------------------------------
g0128: [2024-08-02 18:07:38,587] [INFO] [comm.py:637:init_distributed] cdb=None
g0128: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0128: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0129: DeepSpeed general environment info:
g0129: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0129: torch version .................... 2.0.1+cu118
g0129: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0129: deepspeed info ................... 0.12.4, unknown, unknown
g0129: torch cuda version ............... 11.8
g0129: torch hip version ................ None
g0129: nvcc version ..................... 11.8
g0129: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0129: shared memory (/dev/shm) size .... 188.13 GB
g0128: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0128: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0128: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0129: DeepSpeed general environment info:
g0129: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0129: torch version .................... 2.0.1+cu118
g0129: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0129: deepspeed info ................... 0.12.4, unknown, unknown
g0129: torch cuda version ............... 11.8
g0129: torch hip version ................ None
g0129: nvcc version ..................... 11.8
g0129: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0129: shared memory (/dev/shm) size .... 188.13 GB
g0129: DeepSpeed general environment info:
g0129: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0129: torch version .................... 2.0.1+cu118
g0129: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0129: deepspeed info ................... 0.12.4, unknown, unknown
g0129: torch cuda version ............... 11.8
g0129: torch hip version ................ None
g0129: nvcc version ..................... 11.8
g0129: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0129: shared memory (/dev/shm) size .... 188.13 GB
g0129: DeepSpeed general environment info:
g0129: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0129: torch version .................... 2.0.1+cu118
g0129: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0129: deepspeed info ................... 0.12.4, unknown, unknown
g0129: torch cuda version ............... 11.8
g0129: torch hip version ................ None
g0129: nvcc version ..................... 11.8
g0129: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0129: shared memory (/dev/shm) size .... 188.13 GB
g0128: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0128: [2024-08-02 18:07:38,599] [INFO] [comm.py:637:init_distributed] cdb=None
g0128: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0128: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0129: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0129: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0129: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0129: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0133: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0133: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0133: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0133: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0133: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0133: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0133: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0133: async_io ............... [92m[YES][0m fused_adam......  .............[92m[OKAY][0m 
g0133: [92m[YES][0m ...... [92m[OKAY][0m
g0133: fused_adam cpu_adam.............  ...............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0133: [92m[OKAY][0m
g0133: cpu_adamcpu_adagrad  ...........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0133: 
g0133: cpu_adagradcpu_lion  ...........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0133: 
g0133: cpu_lion ............... [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0133: [92m[OKAY][0m
g0133: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0133: 
g0133: fused_lambevoformer_attn  ......................  [92m[YES][0m[93m[NO][0m  .............  [92m[OKAY][0m[93m[NO][0m
g0133: 
g0133: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0mfused_lamb
g0133:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0133: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0133: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0133: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0133: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0133: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0133: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: --------------------------------------------------
g0131: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0131: 
g0131: --------------------------------------------------DeepSpeed C++/CUDA extension op report
g0131: 
g0131: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0131:       runtime if needed. Op compatibility means that your system
g0131:       meet the required dependencies to JIT install the op.--------------------------------------------------
g0131: 
g0131: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0131:       runtime if needed. Op compatibility means that your system
g0131:       meet the required dependencies to JIT install the op.
g0131: 
g0131: JIT compiled ops requires ninja--------------------------------------------------
g0131: 
g0131: JIT compiled ops requires ninja
g0131: --------------------------------------------------
g0131: DeepSpeed C++/CUDA extension op report
g0131: --------------------------------------------------
g0131: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0131:       runtime if needed. Op compatibility means that your system
g0131:       meet the required dependencies to JIT install the op.
g0131: --------------------------------------------------
g0131: JIT compiled ops requires ninja
g0133: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0133: inference_core_opsinference_core_ops  ..........  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0133: 
g0133: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0131: --------------------------------------------------
g0131: DeepSpeed C++/CUDA extension op report
g0131: --------------------------------------------------
g0131: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0131:       runtime if needed. Op compatibility means that your system
g0131:       meet the required dependencies to JIT install the op.
g0131: --------------------------------------------------
g0131: JIT compiled ops requires ninja
g0131: ninjaninjaninjaninja    ........................................................................   [92m[OKAY][0m [92m[OKAY][0m[92m[OKAY][0m
g0131: [92m[OKAY][0m
g0131: 
g0131: 
g0131: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0131: --------------------------------------------------
g0131: 
g0131: 
g0131: op nameop nameop name   op name................................................    ................installedinstalledinstalled    installed......    ..compatiblecompatiblecompatible 
g0131: 
g0131: 
g0131: compatible----------------------------------------------------------------------------------------------------
g0131: --------------------------------------------------
g0131: 
g0131: 
g0131: --------------------------------------------------
g0133: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0133: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0133: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0133: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0133: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0133: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0133: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0133: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0133: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0133: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0133: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0133: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0133: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0133: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0133: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0133: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0133: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0133: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0133: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0133: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0133: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0133: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0133: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0133: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0133: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0133: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0133: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0133: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0133: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0133: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0133: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0133: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0132: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0132: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0132: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0132: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0132: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0132: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0132: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0132: async_io ............... [92m[YES][0m ...... [92m[OKAY][0mfused_adam
g0132:  ............. async_io[92m[YES][0m  ......fused_adam ............... [92m[OKAY][0m .............
g0132:  [92m[YES][0m[92m[YES][0m  cpu_adam............   ...............[92m[OKAY][0m[92m[OKAY][0m 
g0132: [92m[YES][0m
g0132:  ...... cpu_adam[92m[OKAY][0m 
g0132: ............... [92m[YES][0mcpu_adagrad fused_adam ...... ............  .............[92m[OKAY][0m[92m[YES][0m
g0132:   ......[92m[YES][0m  cpu_adagrad[92m[OKAY][0m...... 
g0132: ............  [92m[OKAY][0mcpu_lion[92m[YES][0m
g0132:   .....................  [92m[YES][0m[92m[OKAY][0m 
g0132: cpu_adam......  [92m[OKAY][0mcpu_lion...............
g0132:   ...............[92m[YES][0m [92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0132: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0132: 
g0132: evoformer_attn cpu_adagrad.........[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH  
g0132: [93m[NO][0m............ evoformer_attn .......  [92m[YES][0m.........[93m[NO][0m 
g0132:  [93m[NO][0m...... ....... fused_lamb  [92m[OKAY][0m[93m[NO][0m.............
g0132: 
g0132:  [92m[YES][0m ......fused_lamb  cpu_lion[92m[OKAY][0m.............
g0132:   ...............[92m[YES][0m  ......[92m[YES][0m [92m[OKAY][0m 
g0132: ...... fused_lion[92m[OKAY][0m .............
g0132:  [92m[YES][0m fused_lion......  .............[92m[OKAY][0m 
g0132: [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0132: [92m[OKAY][0m
g0132: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0132: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0133: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0133: --------------------------------------------------
g0133: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0133: --------------------------------------------------
g0133: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0133: --------------------------------------------------
g0133: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0133: --------------------------------------------------
g0133: DeepSpeed general environment info:
g0133: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0133: torch version .................... 2.0.1+cu118
g0133: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0133: deepspeed info ................... 0.12.4, unknown, unknown
g0133: torch cuda version ............... 11.8
g0133: torch hip version ................ None
g0133: nvcc version ..................... 11.8
g0133: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0133: shared memory (/dev/shm) size .... 188.13 GB
g0133: DeepSpeed general environment info:
g0133: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0133: torch version .................... 2.0.1+cu118
g0133: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0133: deepspeed info ................... 0.12.4, unknown, unknown
g0133: torch cuda version ............... 11.8
g0133: torch hip version ................ None
g0133: nvcc version ..................... 11.8
g0133: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0133: shared memory (/dev/shm) size .... 188.13 GB
g0133: DeepSpeed general environment info:
g0133: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0133: torch version .................... 2.0.1+cu118
g0133: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0133: deepspeed info ................... 0.12.4, unknown, unknown
g0133: torch cuda version ............... 11.8
g0133: torch hip version ................ None
g0133: nvcc version ..................... 11.8
g0133: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0133: shared memory (/dev/shm) size .... 188.13 GB
g0133: DeepSpeed general environment info:
g0133: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0133: torch version .................... 2.0.1+cu118
g0133: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0133: deepspeed info ................... 0.12.4, unknown, unknown
g0133: torch cuda version ............... 11.8
g0133: torch hip version ................ None
g0133: nvcc version ..................... 11.8
g0133: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0133: shared memory (/dev/shm) size .... 188.13 GB
g0129: [2024-08-02 18:07:38,715] [INFO] [comm.py:637:init_distributed] cdb=None
g0129: [2024-08-02 18:07:38,716] [INFO] [comm.py:637:init_distributed] cdb=None
g0129: [2024-08-02 18:07:38,718] [INFO] [comm.py:637:init_distributed] cdb=None
g0132: inference_core_opsinference_core_ops .....  .....[92m[YES][0m  ......[92m[YES][0m  [92m[OKAY][0m
g0132: ...... [92m[OKAY][0m
g0132: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0132: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0129: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0129: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0129: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0129: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0129: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0129: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0129: [2024-08-02 18:07:38,725] [INFO] [comm.py:637:init_distributed] cdb=None
g0132: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0132: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0132: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0132: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0132: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0129: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0132: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0132: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0132: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0132: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0129: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0132: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0132: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0132: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0132: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0132: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0132: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0132: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0132: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0132: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0132: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0132: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0132: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0132: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0132: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0132: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0132: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0132: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0132: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0133: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0133: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0133: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0133: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0132: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0132: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0132: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0132: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0132: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0132: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0132: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0132: --------------------------------------------------
g0132: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0132: --------------------------------------------------
g0132: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0132: --------------------------------------------------
g0132: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0132: --------------------------------------------------
g0132: DeepSpeed general environment info:
g0132: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0132: torch version .................... 2.0.1+cu118
g0132: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0132: deepspeed info ................... 0.12.4, unknown, unknown
g0132: torch cuda version ............... 11.8
g0132: torch hip version ................ None
g0132: nvcc version ..................... 11.8
g0132: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0132: shared memory (/dev/shm) size .... 188.13 GB
g0132: DeepSpeed general environment info:
g0132: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0132: torch version .................... 2.0.1+cu118
g0132: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0132: deepspeed info ................... 0.12.4, unknown, unknown
g0132: torch cuda version ............... 11.8
g0132: torch hip version ................ None
g0132: nvcc version ..................... 11.8
g0132: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0132: shared memory (/dev/shm) size .... 188.13 GB
g0132: DeepSpeed general environment info:
g0132: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0132: torch version .................... 2.0.1+cu118
g0132: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0132: deepspeed info ................... 0.12.4, unknown, unknown
g0132: torch cuda version ............... 11.8
g0132: torch hip version ................ None
g0132: nvcc version ..................... 11.8
g0132: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0132: shared memory (/dev/shm) size .... 188.13 GB
g0132: DeepSpeed general environment info:
g0132: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0132: torch version .................... 2.0.1+cu118
g0132: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0132: deepspeed info ................... 0.12.4, unknown, unknown
g0132: torch cuda version ............... 11.8
g0132: torch hip version ................ None
g0132: nvcc version ..................... 11.8
g0132: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0132: shared memory (/dev/shm) size .... 188.13 GB
g0131: async_io ............... [92m[YES][0m ......async_io [92m[OKAY][0m 
g0131: ............... [92m[YES][0m ...... [92m[OKAY][0m
g0131: fused_adam ............. [92m[YES][0m ...... fused_adam[92m[OKAY][0m 
g0131: ............. [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0131: [92m[YES][0m ...... cpu_adam[92m[OKAY][0m 
g0131: ............... [92m[YES][0m cpu_adagrad......  ............[92m[OKAY][0m 
g0131: [92m[YES][0m ...... cpu_adagrad[92m[OKAY][0m 
g0131: ............ [92m[YES][0m cpu_lion......  [92m[OKAY][0m
g0131: ............... cpu_lion[92m[YES][0m  .....................  [92m[YES][0m[92m[OKAY][0m 
g0131: ...... [92m[OKAY][0m
g0131: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0131: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHevoformer_attn
g0131:  ......... evoformer_attn[93m[NO][0m  ................  [93m[NO][0m[93m[NO][0m 
g0131: ....... [93m[NO][0mfused_lamb
g0131:  ............. fused_lamb[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0131: ......async_io [92m[OKAY][0m 
g0131: ............... [92m[YES][0m ......fused_lion  [92m[OKAY][0m............. 
g0131: fused_lion[92m[YES][0m  ...................  [92m[YES][0m[92m[OKAY][0m 
g0131: ...... [92m[OKAY][0m
g0131: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0131: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0131: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0131: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0131: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0131: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0131: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0131: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0131: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0131: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0131: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0131: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: inference_core_ops .....inference_core_ops [92m[YES][0m  ...........  [92m[YES][0m[92m[OKAY][0m ......
g0131:  [92m[OKAY][0m
g0131: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0131: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0131: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0131: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0131: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0131: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0131: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0132: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0131: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0131: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0132: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0132: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0132: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0131: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0131: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0131: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0mragged_ops
g0131:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0random_ltd
g0131:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0131: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0131: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatiblesparse_attn
g0131:  ............ sparse_attn[93m[NO][0m  ...................  [93m[NO][0m[93m[NO][0m
g0131:  ....... [93m[NO][0m
g0131: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0131: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0131: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0131: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0131: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0131: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0131: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0131: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0131: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0131: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0131: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0131: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0131: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0131: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0131: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0131: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0131: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0131: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0131: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0131: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0131: --------------------------------------------------
g0131: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0131: --------------------------------------------------
g0131: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0131: --------------------------------------------------
g0131: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0131: --------------------------------------------------
g0131: DeepSpeed general environment info:
g0131: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0131: torch version .................... 2.0.1+cu118
g0131: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0131: deepspeed info ................... 0.12.4, unknown, unknown
g0131: torch cuda version ............... 11.8
g0131: torch hip version ................ None
g0131: nvcc version ..................... 11.8
g0131: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0131: shared memory (/dev/shm) size .... 188.13 GB
g0131: DeepSpeed general environment info:
g0131: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0131: torch version .................... 2.0.1+cu118
g0131: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0131: deepspeed info ................... 0.12.4, unknown, unknown
g0131: torch cuda version ............... 11.8
g0131: torch hip version ................ None
g0131: nvcc version ..................... 11.8
g0131: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0131: shared memory (/dev/shm) size .... 188.13 GB
g0131: DeepSpeed general environment info:
g0131: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0131: torch version .................... 2.0.1+cu118
g0131: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0131: deepspeed info ................... 0.12.4, unknown, unknown
g0131: torch cuda version ............... 11.8
g0131: torch hip version ................ None
g0131: nvcc version ..................... 11.8
g0131: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0131: shared memory (/dev/shm) size .... 188.13 GB
g0131: DeepSpeed general environment info:
g0131: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0131: torch version .................... 2.0.1+cu118
g0131: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0131: deepspeed info ................... 0.12.4, unknown, unknown
g0131: torch cuda version ............... 11.8
g0131: torch hip version ................ None
g0131: nvcc version ..................... 11.8
g0131: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0131: shared memory (/dev/shm) size .... 188.13 GB
g0133: [2024-08-02 18:07:38,835] [INFO] [comm.py:637:init_distributed] cdb=None
g0133: [2024-08-02 18:07:38,836] [INFO] [comm.py:637:init_distributed] cdb=None
g0133: [2024-08-02 18:07:38,839] [INFO] [comm.py:637:init_distributed] cdb=None
g0133: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0133: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0133: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0133: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0133: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0133: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0131: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0131: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0131: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0131: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0132: [2024-08-02 18:07:38,881] [INFO] [comm.py:637:init_distributed] cdb=None
g0132: [2024-08-02 18:07:38,886] [INFO] [comm.py:637:init_distributed] cdb=None
g0132: [2024-08-02 18:07:38,886] [INFO] [comm.py:637:init_distributed] cdb=None
g0132: [2024-08-02 18:07:38,887] [INFO] [comm.py:637:init_distributed] cdb=None
g0132: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0132: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0132: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0132: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0132: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0132: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0132: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0132: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0131: [2024-08-02 18:07:38,942] [INFO] [comm.py:637:init_distributed] cdb=None
g0131: [2024-08-02 18:07:38,942] [INFO] [comm.py:637:init_distributed] cdb=None
g0131: [2024-08-02 18:07:38,942] [INFO] [comm.py:637:init_distributed] cdb=None
g0131: [2024-08-02 18:07:38,944] [INFO] [comm.py:637:init_distributed] cdb=None
g0131: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0131: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0131: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0131: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0131: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0131: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0131: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0131: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0133: > setting tensorboard ...
g0133: [2024-08-02 18:07:39,224] [INFO] [comm.py:637:init_distributed] cdb=None
g0133: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0133: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0113-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0113: > initialized tensor model parallel with size 1
g0113: > initialized pipeline model parallel with size 8
g0113: > setting random seeds to 1234 ...
g0113: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0113: > compiling dataset index builder ...
g0113: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0113: make: Nothing to be done for 'default'.
g0113: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0113: >>> done with dataset index builder. Compilation time: 0.081 seconds
g0113: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0113: > compiling and loading fused kernels ...
g0113: Detected CUDA files, patching ldflags
g0113: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0113: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0113: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0113: ninja: no work to do.
g0113: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0113: Detected CUDA files, patching ldflags
g0113: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0113: Building extension module scaled_masked_softmax_cuda...
g0113: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0113: ninja: no work to do.
g0113: Loading extension module scaled_masked_softmax_cuda...
g0113: Detected CUDA files, patching ldflags
g0113: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0113: Building extension module scaled_softmax_cuda...
g0113: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0113: ninja: no work to do.
g0113: Loading extension module scaled_softmax_cuda...
g0113: >>> done with compiling and loading fused kernels. Compilation time: 8.209 seconds
g0113: time to initialize megatron (seconds): 22.191
g0113: [after megatron is initialized] datetime: 2024-08-02 18:07:50 
g0128: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0125: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0131: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0129: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0113: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0133: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0130: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0132: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0125: wandb: Tracking run with wandb version 0.17.5
g0125: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180751-yyf9ufuh
g0125: wandb: Run `wandb offline` to turn off syncing.
g0125: wandb: Syncing run g0125.abci.local
g0125: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0125: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/yyf9ufuh
g0133: wandb: Tracking run with wandb version 0.17.5
g0133: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180751-utlx77s2
g0133: wandb: Run `wandb offline` to turn off syncing.
g0133: wandb: Syncing run g0133.abci.local
g0133: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0133: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/utlx77s2
g0131: wandb: Tracking run with wandb version 0.17.5
g0131: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180751-01a4whuo
g0131: wandb: Run `wandb offline` to turn off syncing.
g0131: wandb: Syncing run g0131.abci.local
g0131: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0131: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/01a4whuo
g0113: wandb: Tracking run with wandb version 0.17.5
g0113: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180751-386i8k92
g0113: wandb: Run `wandb offline` to turn off syncing.
g0113: wandb: Syncing run g0113.abci.local
g0113: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0113: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/386i8k92
g0132: wandb: Tracking run with wandb version 0.17.5
g0132: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180751-0zjicm6o
g0132: wandb: Run `wandb offline` to turn off syncing.
g0132: wandb: Syncing run g0132.abci.local
g0132: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0132: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/0zjicm6o
g0130: wandb: Tracking run with wandb version 0.17.5
g0130: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180751-9dhyo9td
g0130: wandb: Run `wandb offline` to turn off syncing.
g0130: wandb: Syncing run g0130.abci.local
g0130: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0130: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/9dhyo9td
g0129: wandb: Tracking run with wandb version 0.17.5
g0129: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180751-eeig2dt1
g0129: wandb: Run `wandb offline` to turn off syncing.
g0129: wandb: Syncing run g0129.abci.local
g0129: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0129: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/eeig2dt1
g0128: wandb: Tracking run with wandb version 0.17.5
g0128: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240802_180751-t0m5njr0
g0128: wandb: Run `wandb offline` to turn off syncing.
g0128: wandb: Syncing run g0128.abci.local
g0128: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0128: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/t0m5njr0
g0113: building GPT model ...
g0113: [2024-08-02 18:07:52,783] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0113: [2024-08-02 18:07:52,784] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0113: [2024-08-02 18:07:52,784] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 43.46 GB, percent = 11.5%
g0113: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0113: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0113: [2024-08-02 18:07:53,294] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0113: stage=0 layers=5
g0113:      0: _to_float16
g0113:      1: EmbeddingPipe
g0113:      2: ParallelTransformerLayerPipe
g0113:      3: ParallelTransformerLayerPipe
g0113:      4: ParallelTransformerLayerPipe
g0113: stage=1 layers=3
g0113:      5: ParallelTransformerLayerPipe
g0113:      6: ParallelTransformerLayerPipe
g0113:      7: ParallelTransformerLayerPipe
g0113: stage=2 layers=3
g0113:      8: ParallelTransformerLayerPipe
g0113:      9: ParallelTransformerLayerPipe
g0113:     10: ParallelTransformerLayerPipe
g0113: stage=3 layers=3
g0113:     11: ParallelTransformerLayerPipe
g0113:     12: ParallelTransformerLayerPipe
g0113:     13: ParallelTransformerLayerPipe
g0113: stage=4 layers=3
g0113:     14: ParallelTransformerLayerPipe
g0113:     15: ParallelTransformerLayerPipe
g0113:     16: ParallelTransformerLayerPipe
g0113: stage=5 layers=3
g0113:     17: ParallelTransformerLayerPipe
g0113:     18: ParallelTransformerLayerPipe
g0113:     19: ParallelTransformerLayerPipe
g0113: stage=6 layers=3
g0113:     20: ParallelTransformerLayerPipe
g0113:     21: ParallelTransformerLayerPipe
g0113:     22: ParallelTransformerLayerPipe
g0113: stage=7 layers=3
g0113:     23: ParallelTransformerLayerPipe
g0113:     24: MixedFusedRMSNorm
g0113:     25: LMHeadPipe
g0113:   loss: CrossEntropy
g0129:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0130:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0131:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0132:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0133:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0128:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0125:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0113: [2024-08-02 18:07:53,860] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0113: [2024-08-02 18:07:53,861] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0113: [2024-08-02 18:07:53,861] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 43.52 GB, percent = 11.6%
g0113:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0113: setting training iterations to 10000000
g0113: > learning rate decay style: cosine
g0113: DeepSpeed is enabled.
g0113: [2024-08-02 18:07:53,863] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0129: [2024-08-02 18:07:53,888] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0129: [2024-08-02 18:07:53,888] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0129: [2024-08-02 18:07:53,888] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0129: [2024-08-02 18:07:53,889] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0130: [2024-08-02 18:07:53,917] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0130: [2024-08-02 18:07:53,917] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0130: [2024-08-02 18:07:53,917] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0130: [2024-08-02 18:07:53,917] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0131: [2024-08-02 18:07:53,926] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0131: [2024-08-02 18:07:53,926] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0131: [2024-08-02 18:07:53,926] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0131: [2024-08-02 18:07:53,927] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0128: [2024-08-02 18:07:53,927] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0128: [2024-08-02 18:07:53,927] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0128: [2024-08-02 18:07:53,927] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0128: [2024-08-02 18:07:53,927] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0133: [2024-08-02 18:07:53,932] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0133: [2024-08-02 18:07:53,932] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0133: [2024-08-02 18:07:53,933] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0133: [2024-08-02 18:07:53,933] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0132: [2024-08-02 18:07:53,946] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0132: [2024-08-02 18:07:53,946] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0132: [2024-08-02 18:07:53,946] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0132: [2024-08-02 18:07:53,946] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-02 18:07:53,994] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-02 18:07:53,995] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-02 18:07:53,995] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0125: [2024-08-02 18:07:53,995] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-02 18:07:54,073] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0113: [2024-08-02 18:07:54,074] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0113: [2024-08-02 18:07:54,074] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0113: [2024-08-02 18:07:54,075] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0113: [2024-08-02 18:07:54,075] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0113: [2024-08-02 18:07:54,105] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0113: [2024-08-02 18:07:54,105] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-02 18:07:54,106] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0113: [2024-08-02 18:07:54,106] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-02 18:07:54,106] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7fe51e1651d0>
g0113: [2024-08-02 18:07:54,106] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-02 18:07:54,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: [2024-08-02 18:07:54,106] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0113: [2024-08-02 18:07:54,107] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0113:     "partition_activations": false, 
g0113:     "contiguous_memory_optimization": false, 
g0113:     "cpu_checkpointing": false, 
g0113:     "number_checkpoints": null, 
g0113:     "synchronize_checkpoint_boundary": false, 
g0113:     "profile": false
g0113: }
g0113: [2024-08-02 18:07:54,107] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0113: [2024-08-02 18:07:54,107] [INFO] [config.py:983:print]   amp_enabled .................. False
g0113: [2024-08-02 18:07:54,107] [INFO] [config.py:983:print]   amp_params ................... False
g0113: [2024-08-02 18:07:54,107] [INFO] [config.py:983:print]   autotuning_config ............ {
g0113:     "enabled": false, 
g0113:     "start_step": null, 
g0113:     "end_step": null, 
g0113:     "metric_path": null, 
g0113:     "arg_mappings": null, 
g0113:     "metric": "throughput", 
g0113:     "model_info": null, 
g0113:     "results_dir": "autotuning_results", 
g0113:     "exps_dir": "autotuning_exps", 
g0113:     "overwrite": true, 
g0113:     "fast": true, 
g0113:     "start_profile_step": 3, 
g0113:     "end_profile_step": 5, 
g0113:     "tuner_type": "gridsearch", 
g0113:     "tuner_early_stopping": 5, 
g0113:     "tuner_num_trials": 50, 
g0113:     "model_info_path": null, 
g0113:     "mp_size": 1, 
g0113:     "max_train_batch_size": null, 
g0113:     "min_train_batch_size": 1, 
g0113:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0113:     "min_train_micro_batch_size_per_gpu": 1, 
g0113:     "num_tuning_micro_batch_sizes": 3
g0113: }
g0113: [2024-08-02 18:07:54,107] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0113: [2024-08-02 18:07:54,108] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0113: [2024-08-02 18:07:54,108] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0113: [2024-08-02 18:07:54,108] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0113: [2024-08-02 18:07:54,108] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fe51e166d50>
g0113: [2024-08-02 18:07:54,108] [INFO] [config.py:983:print]   communication_data_type ...... None
g0113: [2024-08-02 18:07:54,108] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0113: [2024-08-02 18:07:54,108] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0113: [2024-08-02 18:07:54,109] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0113: [2024-08-02 18:07:54,109] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0113: [2024-08-02 18:07:54,109] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0113: [2024-08-02 18:07:54,109] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0113: [2024-08-02 18:07:54,109] [INFO] [config.py:983:print]   disable_allgather ............ False
g0113: [2024-08-02 18:07:54,109] [INFO] [config.py:983:print]   dump_state ................... False
g0113: [2024-08-02 18:07:54,109] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0113: [2024-08-02 18:07:54,109] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0113: [2024-08-02 18:07:54,109] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0113: [2024-08-02 18:07:54,110] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0113: [2024-08-02 18:07:54,110] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0113: [2024-08-02 18:07:54,110] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0113: [2024-08-02 18:07:54,110] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0113: [2024-08-02 18:07:54,110] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0113: [2024-08-02 18:07:54,110] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0113: [2024-08-02 18:07:54,110] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0113: [2024-08-02 18:07:54,110] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0113:     "enabled": false, 
g0113:     "recompute_fwd_factor": 0.0, 
g0113:     "profile_step": 1, 
g0113:     "module_depth": -1, 
g0113:     "top_modules": 1, 
g0113:     "detailed": true, 
g0113:     "output_file": null
g0113: }
g0113: [2024-08-02 18:07:54,111] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0113: [2024-08-02 18:07:54,111] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0113: [2024-08-02 18:07:54,111] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0113: [2024-08-02 18:07:54,111] [INFO] [config.py:983:print]   global_rank .................. 0
g0113: [2024-08-02 18:07:54,111] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0113: [2024-08-02 18:07:54,111] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0113: [2024-08-02 18:07:54,111] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0113: [2024-08-02 18:07:54,111] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0113: [2024-08-02 18:07:54,112] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0113: [2024-08-02 18:07:54,112] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0113: [2024-08-02 18:07:54,112] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0113: [2024-08-02 18:07:54,112] [INFO] [config.py:983:print]   loss_scale ................... 0
g0113: [2024-08-02 18:07:54,112] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0113: [2024-08-02 18:07:54,112] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0113: [2024-08-02 18:07:54,112] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0113: [2024-08-02 18:07:54,112] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0113: [2024-08-02 18:07:54,112] [INFO] [config.py:983:print]   nebula_config ................ {
g0113:     "enabled": false, 
g0113:     "persistent_storage_path": null, 
g0113:     "persistent_time_interval": 100, 
g0113:     "num_of_version_in_retention": 2, 
g0113:     "enable_nebula_load": true, 
g0113:     "load_path": null
g0113: }
g0113: [2024-08-02 18:07:54,113] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0113: [2024-08-02 18:07:54,113] [INFO] [config.py:983:print]   optimizer_name ............... None
g0113: [2024-08-02 18:07:54,113] [INFO] [config.py:983:print]   optimizer_params ............. None
g0113: [2024-08-02 18:07:54,113] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0113: [2024-08-02 18:07:54,113] [INFO] [config.py:983:print]   pld_enabled .................. False
g0113: [2024-08-02 18:07:54,113] [INFO] [config.py:983:print]   pld_params ................... False
g0113: [2024-08-02 18:07:54,113] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0113: [2024-08-02 18:07:54,114] [INFO] [config.py:983:print]   scheduler_name ............... None
g0113: [2024-08-02 18:07:54,114] [INFO] [config.py:983:print]   scheduler_params ............. None
g0113: [2024-08-02 18:07:54,114] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0113: [2024-08-02 18:07:54,114] [INFO] [config.py:983:print]   sparse_attention ............. None
g0113: [2024-08-02 18:07:54,114] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0113: [2024-08-02 18:07:54,114] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0113: [2024-08-02 18:07:54,114] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0113: [2024-08-02 18:07:54,114] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0113: [2024-08-02 18:07:54,114] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0113: [2024-08-02 18:07:54,115] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0113: [2024-08-02 18:07:54,115] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0113: [2024-08-02 18:07:54,115] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0113: [2024-08-02 18:07:54,115] [INFO] [config.py:983:print]   world_size ................... 4
g0113: [2024-08-02 18:07:54,115] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0113: [2024-08-02 18:07:54,115] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0113: [2024-08-02 18:07:54,115] [INFO] [config.py:983:print]   zero_enabled ................. False
g0113: [2024-08-02 18:07:54,115] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0113: [2024-08-02 18:07:54,116] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0113: [2024-08-02 18:07:54,116] [INFO] [config.py:969:print_user_config]   json = {
g0113:     "train_batch_size": 128, 
g0113:     "train_micro_batch_size_per_gpu": 1, 
g0113:     "steps_per_print": 10, 
g0113:     "zero_optimization": {
g0113:         "stage": 0
g0113:     }, 
g0113:     "gradient_clipping": 1.0, 
g0113:     "prescale_gradients": true, 
g0113:     "fp16": {
g0113:         "enabled": true, 
g0113:         "loss_scale": 0, 
g0113:         "loss_scale_window": 500, 
g0113:         "hysteresis": 2, 
g0113:         "min_loss_scale": 1, 
g0113:         "initial_scale_power": 11
g0113:     }, 
g0113:     "wall_clock_breakdown": false
g0113: }
g0113: [2024-08-02 18:07:54,116] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0113: [2024-08-02 18:07:54,116] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0113: [2024-08-02 18:07:54,829] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0130: [2024-08-02 18:07:54,830] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0128: [2024-08-02 18:07:54,830] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0132: [2024-08-02 18:07:54,830] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0131: [2024-08-02 18:07:54,830] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0129: [2024-08-02 18:07:54,831] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0133: [2024-08-02 18:07:54,831] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0125: [2024-08-02 18:07:54,831] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0132: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0113: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0113: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0132: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0113: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0113: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0132: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0133: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0133: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0131: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0131: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0133: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0131: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0133: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0130: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0132: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0130: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0128: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0128: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0131: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0130: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0130: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0125: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0128: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0125: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0129: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0128: [2024-08-02 18:07:55,520] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0125: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0129: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0125: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0129: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0113: WARNING: could not find the metadata file /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase 
g0129: [2024-08-02 18:07:55,521] [WARNING] [engine.py:2703:load_checkpoint] Unable to find latest file at /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/latest, if trying to load latest checkpoint please ensure this file exists or pass an explicit checkpoint tag when loading a checkpoint.
g0113:     will not load any checkpoints and will start from random
g0133: (min, max) time across ranks (ms):
g0133:     load-checkpoint ................................: (2.61, 3.16)
g0113: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-02 18:07:55 
g0113: > building train, validation, and test datasets ...
g0113:  > datasets target sizes (minimum size):
g0113:     train:      1280000000
g0113:     validation: 128012800
g0113:     test:       12800
g0113: > building train, validation, and test datasets for GPT ...
g0113: Single data path provided for train, valid & test
g0113:  > building dataset index ...
g0113:     reading sizes...
g0113:     reading pointers...
g0113:     reading document index...
g0113:     creating numpy buffer of mmap...
g0113:     creating memory view of numpy buffer...
g0113:  > finished creating indexed dataset in 0.030276 seconds
g0113:     number of documents: 2237032
g0113:  > dataset split:
g0113:     train:
g0113:      document indices in [0, 2122943) total of 2122943 documents
g0113:     validation:
g0113:      document indices in [2122943, 2234795) total of 111852 documents
g0113:     test:
g0113:      document indices in [2234795, 2237032) total of 2237 documents
g0113:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0113:  > only one epoch required, setting separate_last_epoch to False
g0113:  > elasped time to build and save doc-idx mapping (seconds): 0.080403
g0113:     using:
g0113:      number of documents:       2122943
g0113:      number of epochs:          1
g0113:      sequence length:           2048
g0113:      total number of samples:   7350897
g0113:  > elasped time to build and save sample-idx mapping (seconds): 0.180754
g0113:  > building shuffle index with split [0, 7350897) and [7350897, 7350897) ...
g0113:  > elasped time to build and save shuffle-idx mapping (seconds): 0.222950
g0113:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/9b38ebe0050576708d9bc079bd720cfd_doc_idx.npy
g0113:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/9b38ebe0050576708d9bc079bd720cfd_sample_idx.npy
g0113:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/9b38ebe0050576708d9bc079bd720cfd_shuffle_idx.npy
g0113:     loaded indexed file in 0.134 seconds
g0113:     total number of samples: 7350898
g0113:     total number of epochs: 1
g0113:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0113:  > last epoch number of samples (119543) is smaller than 80% of number of samples per epoch (385220), setting separate_last_epoch to True
g0113:  > elasped time to build and save doc-idx mapping (seconds): 2.676661
g0113:     using:
g0113:      number of documents:       111852
g0113:      number of epochs:          333
g0113:      sequence length:           2048
g0113:      total number of samples:   128278478
g0113:  > elasped time to build and save sample-idx mapping (seconds): 2.776443
g0113:  > building shuffle index with split [0, 127893257) and [127893257, 128278478) ...
g0113:  > elasped time to build and save shuffle-idx mapping (seconds): 7.632546
g0113:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/a0dc3f68f9f381bfdcfad0b5fa70d290_doc_idx.npy
g0113:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/a0dc3f68f9f381bfdcfad0b5fa70d290_sample_idx.npy
g0113:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/a0dc3f68f9f381bfdcfad0b5fa70d290_shuffle_idx.npy
g0113:     loaded indexed file in 0.070 seconds
g0113:     total number of samples: 128278479
g0113:     total number of epochs: 333
g0113:  > WARNING: could not find index map files, building the indices on rank 0 ...
g0113:  > last epoch number of samples (2796) is smaller than 80% of number of samples per epoch (5002), setting separate_last_epoch to True
g0113:  > elasped time to build and save doc-idx mapping (seconds): 0.001480
g0113:     using:
g0113:      number of documents:       2237
g0113:      number of epochs:          3
g0113:      sequence length:           2048
g0113:      total number of samples:   15006
g0113:  > elasped time to build and save sample-idx mapping (seconds): 0.001641
g0113:  > building shuffle index with split [0, 10004) and [10004, 15006) ...
g0113:  > elasped time to build and save shuffle-idx mapping (seconds): 0.001652
g0113:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/bf50b63d979b46dde966811bacbffbc0_doc_idx.npy
g0113:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/bf50b63d979b46dde966811bacbffbc0_sample_idx.npy
g0113:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/bf50b63d979b46dde966811bacbffbc0_shuffle_idx.npy
g0113:     loaded indexed file in 0.006 seconds
g0113:     total number of samples: 15007
g0113:     total number of epochs: 3
g0113: > finished creating GPT datasets ...
g0113: [after dataloaders are built] datetime: 2024-08-02 18:08:15 
g0113: done with setup ...
g0113: training ...
g0133: (min, max) time across ranks (ms):
g0133:     model-and-optimizer-setup ......................: (2911.39, 2918.54)
g0133:     train/valid/test-data-iterators-setup ..........: (19852.97, 19856.72)
g0113: [before the start of training step] datetime: 2024-08-02 18:08:15 
g0113: [2024-08-02 18:09:07,058] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[1.5728640000000002e-07, 1.5728640000000002e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 10 loss: 10.5467 iter time (s): 5.119 samples/sec: 25.005
g0133:  iteration       10/10000000 | consumed samples:         1280 | consumed tokens:      2621440 | elapsed time per iteration (ms): 5155.1 | learning rate: 1.573E-07 | global batch size:   128 | lm loss: 1.054915E+01 | loss scale: 2048.0 | grad norm: 7.152 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.830 | tokens per gpu per second (tgs): 1589.116 | TFLOPs: 12.79 |
g0133: [Rank 28] (after 10 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0129: [Rank 12] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7198.0 | max reserved: 7198.0
g0132: [Rank 24] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 4662.0 | max reserved: 4662.0
g0131: [Rank 20] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5410.0 | max reserved: 5410.0
g0113: [Rank 0] (after 10 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 11650.0 | max reserved: 11650.0
g0128: [Rank 8] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8092.0 | max reserved: 8092.0
g0130: [Rank 16] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6304.0 | max reserved: 6304.0
g0125: [Rank 4] (after 10 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 8986.0 | max reserved: 8986.0
g0113: [2024-08-02 18:09:48,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[3.3204906666666666e-07, 3.3204906666666666e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 20 loss: 10.4443 iter time (s): 3.942 samples/sec: 32.470
g0133:  iteration       20/10000000 | consumed samples:         2560 | consumed tokens:      5242880 | elapsed time per iteration (ms): 4109.7 | learning rate: 3.320E-07 | global batch size:   128 | lm loss: 1.049276E+01 | loss scale: 2048.0 | grad norm: 7.767 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.146 | tokens per gpu per second (tgs): 1993.351 | TFLOPs: 16.04 |
g0113: [2024-08-02 18:10:28,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[5.068117333333334e-07, 5.068117333333334e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 30 loss: 10.2232 iter time (s): 4.013 samples/sec: 31.897
g0133:  iteration       30/10000000 | consumed samples:         3840 | consumed tokens:      7864320 | elapsed time per iteration (ms): 4045.8 | learning rate: 5.068E-07 | global batch size:   128 | lm loss: 1.033738E+01 | loss scale: 2048.0 | grad norm: 10.146 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.638 | tokens per gpu per second (tgs): 2024.812 | TFLOPs: 16.29 |
g0113: [2024-08-02 18:11:09,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[6.815744000000001e-07, 6.815744000000001e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 40 loss: 9.8628 iter time (s): 4.044 samples/sec: 31.649
g0133:  iteration       40/10000000 | consumed samples:         5120 | consumed tokens:     10485760 | elapsed time per iteration (ms): 4077.0 | learning rate: 6.816E-07 | global batch size:   128 | lm loss: 1.002588E+01 | loss scale: 2048.0 | grad norm: 6.840 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.396 | tokens per gpu per second (tgs): 2009.337 | TFLOPs: 16.17 |
g0113: [2024-08-02 18:11:50,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[8.563370666666667e-07, 8.563370666666667e-07], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 50 loss: 9.6243 iter time (s): 4.066 samples/sec: 31.482
g0133:  iteration       50/10000000 | consumed samples:         6400 | consumed tokens:     13107200 | elapsed time per iteration (ms): 4098.6 | learning rate: 8.563E-07 | global batch size:   128 | lm loss: 9.723010E+00 | loss scale: 2048.0 | grad norm: 3.471 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.230 | tokens per gpu per second (tgs): 1998.739 | TFLOPs: 16.08 |
g0113: [2024-08-02 18:12:31,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[1.0310997333333332e-06, 1.0310997333333332e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 60 loss: 9.4499 iter time (s): 4.090 samples/sec: 31.294
g0133:  iteration       60/10000000 | consumed samples:         7680 | consumed tokens:     15728640 | elapsed time per iteration (ms): 4123.0 | learning rate: 1.031E-06 | global batch size:   128 | lm loss: 9.517480E+00 | loss scale: 2048.0 | grad norm: 2.468 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.045 | tokens per gpu per second (tgs): 1986.894 | TFLOPs: 15.99 |
g0113: [2024-08-02 18:13:12,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.2058624000000002e-06, 1.2058624000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 70 loss: 9.3465 iter time (s): 4.082 samples/sec: 31.354
g0133:  iteration       70/10000000 | consumed samples:         8960 | consumed tokens:     18350080 | elapsed time per iteration (ms): 4115.4 | learning rate: 1.206E-06 | global batch size:   128 | lm loss: 9.396178E+00 | loss scale: 2048.0 | grad norm: 2.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.102 | tokens per gpu per second (tgs): 1990.558 | TFLOPs: 16.02 |
g0113: [2024-08-02 18:13:54,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[1.3806250666666669e-06, 1.3806250666666669e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 80 loss: 9.2992 iter time (s): 4.119 samples/sec: 31.075
g0133:  iteration       80/10000000 | consumed samples:        10240 | consumed tokens:     20971520 | elapsed time per iteration (ms): 4152.1 | learning rate: 1.381E-06 | global batch size:   128 | lm loss: 9.328206E+00 | loss scale: 2048.0 | grad norm: 1.746 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.828 | tokens per gpu per second (tgs): 1972.981 | TFLOPs: 15.88 |
g0113: [2024-08-02 18:14:35,507] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[1.5553877333333333e-06, 1.5553877333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 90 loss: 9.2403 iter time (s): 4.091 samples/sec: 31.292
g0133:  iteration       90/10000000 | consumed samples:        11520 | consumed tokens:     23592960 | elapsed time per iteration (ms): 4123.2 | learning rate: 1.555E-06 | global batch size:   128 | lm loss: 9.267982E+00 | loss scale: 2048.0 | grad norm: 1.595 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.044 | tokens per gpu per second (tgs): 1986.787 | TFLOPs: 15.99 |
g0113: [2024-08-02 18:15:16,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=100, skipped=0, lr=[1.7301504e-06, 1.7301504e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 100 loss: 9.1922 iter time (s): 4.085 samples/sec: 31.336
g0133:  iteration      100/10000000 | consumed samples:        12800 | consumed tokens:     26214400 | elapsed time per iteration (ms): 4117.4 | learning rate: 1.730E-06 | global batch size:   128 | lm loss: 9.214659E+00 | loss scale: 2048.0 | grad norm: 1.678 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.087 | tokens per gpu per second (tgs): 1989.583 | TFLOPs: 16.01 |
g0113: [2024-08-02 18:15:58,359] [INFO] [logging.py:96:log_dist] [Rank 0] step=110, skipped=0, lr=[1.9049130666666667e-06, 1.9049130666666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 110 loss: 9.1097 iter time (s): 4.135 samples/sec: 30.956
g0133:  iteration      110/10000000 | consumed samples:        14080 | consumed tokens:     28835840 | elapsed time per iteration (ms): 4167.7 | learning rate: 1.905E-06 | global batch size:   128 | lm loss: 9.153337E+00 | loss scale: 2048.0 | grad norm: 2.152 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.712 | tokens per gpu per second (tgs): 1965.600 | TFLOPs: 15.82 |
g0113: [2024-08-02 18:16:40,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=120, skipped=0, lr=[2.0796757333333334e-06, 2.0796757333333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 120 loss: 9.0353 iter time (s): 4.160 samples/sec: 30.766
g0133:  iteration      120/10000000 | consumed samples:        15360 | consumed tokens:     31457280 | elapsed time per iteration (ms): 4193.0 | learning rate: 2.080E-06 | global batch size:   128 | lm loss: 9.084669E+00 | loss scale: 2048.0 | grad norm: 1.928 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.527 | tokens per gpu per second (tgs): 1953.714 | TFLOPs: 15.72 |
g0113: [2024-08-02 18:17:21,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=130, skipped=0, lr=[2.2544384e-06, 2.2544384e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 130 loss: 8.9701 iter time (s): 4.070 samples/sec: 31.453
g0133:  iteration      130/10000000 | consumed samples:        16640 | consumed tokens:     34078720 | elapsed time per iteration (ms): 4102.4 | learning rate: 2.254E-06 | global batch size:   128 | lm loss: 8.999748E+00 | loss scale: 2048.0 | grad norm: 2.399 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.201 | tokens per gpu per second (tgs): 1996.894 | TFLOPs: 16.07 |
g0113: [2024-08-02 18:18:01,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=140, skipped=0, lr=[2.429201066666667e-06, 2.429201066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 140 loss: 8.8588 iter time (s): 4.026 samples/sec: 31.791
g0133:  iteration      140/10000000 | consumed samples:        17920 | consumed tokens:     36700160 | elapsed time per iteration (ms): 4059.2 | learning rate: 2.429E-06 | global batch size:   128 | lm loss: 8.900467E+00 | loss scale: 2048.0 | grad norm: 2.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.533 | tokens per gpu per second (tgs): 2018.132 | TFLOPs: 16.24 |
g0113: [2024-08-02 18:18:40,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=150, skipped=0, lr=[2.6039637333333333e-06, 2.6039637333333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 150 loss: 8.7452 iter time (s): 3.867 samples/sec: 33.099
g0133:  iteration      150/10000000 | consumed samples:        19200 | consumed tokens:     39321600 | elapsed time per iteration (ms): 3900.0 | learning rate: 2.604E-06 | global batch size:   128 | lm loss: 8.793676E+00 | loss scale: 2048.0 | grad norm: 3.847 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.820 | tokens per gpu per second (tgs): 2100.503 | TFLOPs: 16.90 |
g0113: [2024-08-02 18:19:20,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=160, skipped=0, lr=[2.7787264000000002e-06, 2.7787264000000002e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 160 loss: 8.6330 iter time (s): 3.918 samples/sec: 32.672
g0133:  iteration      160/10000000 | consumed samples:        20480 | consumed tokens:     41943040 | elapsed time per iteration (ms): 3951.0 | learning rate: 2.779E-06 | global batch size:   128 | lm loss: 8.683035E+00 | loss scale: 2048.0 | grad norm: 4.087 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.397 | tokens per gpu per second (tgs): 2073.406 | TFLOPs: 16.69 |
g0113: [2024-08-02 18:20:00,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=170, skipped=0, lr=[2.953489066666667e-06, 2.953489066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 170 loss: 8.5560 iter time (s): 3.952 samples/sec: 32.388
g0133:  iteration      170/10000000 | consumed samples:        21760 | consumed tokens:     44564480 | elapsed time per iteration (ms): 3985.2 | learning rate: 2.953E-06 | global batch size:   128 | lm loss: 8.586268E+00 | loss scale: 2048.0 | grad norm: 3.360 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.119 | tokens per gpu per second (tgs): 2055.588 | TFLOPs: 16.54 |
g0113: [2024-08-02 18:20:40,366] [INFO] [logging.py:96:log_dist] [Rank 0] step=180, skipped=0, lr=[3.128251733333333e-06, 3.128251733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 180 loss: 8.4385 iter time (s): 3.977 samples/sec: 32.188
g0133:  iteration      180/10000000 | consumed samples:        23040 | consumed tokens:     47185920 | elapsed time per iteration (ms): 4009.7 | learning rate: 3.128E-06 | global batch size:   128 | lm loss: 8.491016E+00 | loss scale: 2048.0 | grad norm: 3.671 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.922 | tokens per gpu per second (tgs): 2043.032 | TFLOPs: 16.44 |
g0113: [2024-08-02 18:21:21,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=190, skipped=0, lr=[3.3030144e-06, 3.3030144e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 190 loss: 8.3479 iter time (s): 4.054 samples/sec: 31.571
g0133:  iteration      190/10000000 | consumed samples:        24320 | consumed tokens:     49807360 | elapsed time per iteration (ms): 4089.7 | learning rate: 3.303E-06 | global batch size:   128 | lm loss: 8.391228E+00 | loss scale: 2048.0 | grad norm: 4.823 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.298 | tokens per gpu per second (tgs): 2003.100 | TFLOPs: 16.12 |
g0113: [2024-08-02 18:22:02,444] [INFO] [logging.py:96:log_dist] [Rank 0] step=200, skipped=0, lr=[3.477777066666667e-06, 3.477777066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 200 loss: 8.2561 iter time (s): 4.085 samples/sec: 31.331
g0133:  iteration      200/10000000 | consumed samples:        25600 | consumed tokens:     52428800 | elapsed time per iteration (ms): 4118.2 | learning rate: 3.478E-06 | global batch size:   128 | lm loss: 8.293751E+00 | loss scale: 2048.0 | grad norm: 3.519 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.082 | tokens per gpu per second (tgs): 1989.235 | TFLOPs: 16.01 |
g0113: [2024-08-02 18:22:44,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=210, skipped=0, lr=[3.6525397333333335e-06, 3.6525397333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 210 loss: 8.1298 iter time (s): 4.134 samples/sec: 30.963
g0133:  iteration      210/10000000 | consumed samples:        26880 | consumed tokens:     55050240 | elapsed time per iteration (ms): 4166.6 | learning rate: 3.653E-06 | global batch size:   128 | lm loss: 8.183330E+00 | loss scale: 2048.0 | grad norm: 3.752 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.721 | tokens per gpu per second (tgs): 1966.131 | TFLOPs: 15.82 |
g0113: [2024-08-02 18:23:25,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=220, skipped=0, lr=[3.8273024e-06, 3.8273024e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 220 loss: 8.0224 iter time (s): 4.113 samples/sec: 31.120
g0133:  iteration      220/10000000 | consumed samples:        28160 | consumed tokens:     57671680 | elapsed time per iteration (ms): 4145.6 | learning rate: 3.827E-06 | global batch size:   128 | lm loss: 8.068420E+00 | loss scale: 2048.0 | grad norm: 4.653 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.876 | tokens per gpu per second (tgs): 1976.056 | TFLOPs: 15.90 |
g0113: [2024-08-02 18:24:06,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=230, skipped=0, lr=[4.002065066666667e-06, 4.002065066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 230 loss: 7.9267 iter time (s): 4.058 samples/sec: 31.541
g0133:  iteration      230/10000000 | consumed samples:        29440 | consumed tokens:     60293120 | elapsed time per iteration (ms): 4091.1 | learning rate: 4.002E-06 | global batch size:   128 | lm loss: 7.972000E+00 | loss scale: 2048.0 | grad norm: 5.518 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.288 | tokens per gpu per second (tgs): 2002.411 | TFLOPs: 16.11 |
g0113: [2024-08-02 18:24:47,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=240, skipped=0, lr=[4.176827733333334e-06, 4.176827733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 240 loss: 7.8123 iter time (s): 4.046 samples/sec: 31.634
g0133:  iteration      240/10000000 | consumed samples:        30720 | consumed tokens:     62914560 | elapsed time per iteration (ms): 4079.9 | learning rate: 4.177E-06 | global batch size:   128 | lm loss: 7.867007E+00 | loss scale: 2048.0 | grad norm: 4.446 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.374 | tokens per gpu per second (tgs): 2007.906 | TFLOPs: 16.16 |
g0113: [2024-08-02 18:25:30,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=250, skipped=0, lr=[4.351590400000001e-06, 4.351590400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 250 loss: 7.6937 iter time (s): 4.247 samples/sec: 30.141
g0133:  iteration      250/10000000 | consumed samples:        32000 | consumed tokens:     65536000 | elapsed time per iteration (ms): 4281.6 | learning rate: 4.352E-06 | global batch size:   128 | lm loss: 7.768362E+00 | loss scale: 2048.0 | grad norm: 3.600 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.896 | tokens per gpu per second (tgs): 1913.315 | TFLOPs: 15.40 |
g0113: [2024-08-02 18:26:12,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=260, skipped=0, lr=[4.526353066666667e-06, 4.526353066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 260 loss: 7.5994 iter time (s): 4.233 samples/sec: 30.242
g0133:  iteration      260/10000000 | consumed samples:        33280 | consumed tokens:     68157440 | elapsed time per iteration (ms): 4265.0 | learning rate: 4.526E-06 | global batch size:   128 | lm loss: 7.660445E+00 | loss scale: 2048.0 | grad norm: 3.809 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.011 | tokens per gpu per second (tgs): 1920.729 | TFLOPs: 15.46 |
g0113: [2024-08-02 18:26:53,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=270, skipped=0, lr=[4.701115733333334e-06, 4.701115733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 270 loss: 7.5272 iter time (s): 4.072 samples/sec: 31.436
g0133:  iteration      270/10000000 | consumed samples:        34560 | consumed tokens:     70778880 | elapsed time per iteration (ms): 4104.3 | learning rate: 4.701E-06 | global batch size:   128 | lm loss: 7.561131E+00 | loss scale: 2048.0 | grad norm: 3.504 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.187 | tokens per gpu per second (tgs): 1995.978 | TFLOPs: 16.06 |
g0113: [2024-08-02 18:27:33,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=280, skipped=0, lr=[4.875878400000001e-06, 4.875878400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 280 loss: 7.4011 iter time (s): 3.936 samples/sec: 32.517
g0133:  iteration      280/10000000 | consumed samples:        35840 | consumed tokens:     73400320 | elapsed time per iteration (ms): 3969.2 | learning rate: 4.876E-06 | global batch size:   128 | lm loss: 7.461424E+00 | loss scale: 2048.0 | grad norm: 3.831 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.248 | tokens per gpu per second (tgs): 2063.877 | TFLOPs: 16.61 |
g0113: [2024-08-02 18:28:12,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=290, skipped=0, lr=[5.050641066666667e-06, 5.050641066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 290 loss: 7.3282 iter time (s): 3.909 samples/sec: 32.744
g0133:  iteration      290/10000000 | consumed samples:        37120 | consumed tokens:     76021760 | elapsed time per iteration (ms): 3942.1 | learning rate: 5.051E-06 | global batch size:   128 | lm loss: 7.366922E+00 | loss scale: 2048.0 | grad norm: 3.851 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.470 | tokens per gpu per second (tgs): 2078.096 | TFLOPs: 16.72 |
g0113: [2024-08-02 18:28:55,393] [INFO] [logging.py:96:log_dist] [Rank 0] step=300, skipped=0, lr=[5.225403733333334e-06, 5.225403733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 300 loss: 7.2295 iter time (s): 4.217 samples/sec: 30.355
g0133:  iteration      300/10000000 | consumed samples:        38400 | consumed tokens:     78643200 | elapsed time per iteration (ms): 4249.5 | learning rate: 5.225E-06 | global batch size:   128 | lm loss: 7.278764E+00 | loss scale: 2048.0 | grad norm: 4.937 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.121 | tokens per gpu per second (tgs): 1927.744 | TFLOPs: 15.51 |
g0113: [2024-08-02 18:29:36,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=310, skipped=0, lr=[5.4001664e-06, 5.4001664e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 310 loss: 7.1476 iter time (s): 4.064 samples/sec: 31.493
g0133:  iteration      310/10000000 | consumed samples:        39680 | consumed tokens:     81264640 | elapsed time per iteration (ms): 4097.3 | learning rate: 5.400E-06 | global batch size:   128 | lm loss: 7.192795E+00 | loss scale: 2048.0 | grad norm: 4.484 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.240 | tokens per gpu per second (tgs): 1999.359 | TFLOPs: 16.09 |
g0113: [2024-08-02 18:30:18,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=320, skipped=0, lr=[5.574929066666667e-06, 5.574929066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 320 loss: 7.0600 iter time (s): 4.131 samples/sec: 30.985
g0133:  iteration      320/10000000 | consumed samples:        40960 | consumed tokens:     83886080 | elapsed time per iteration (ms): 4163.8 | learning rate: 5.575E-06 | global batch size:   128 | lm loss: 7.106913E+00 | loss scale: 2048.0 | grad norm: 3.385 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.741 | tokens per gpu per second (tgs): 1967.426 | TFLOPs: 15.83 |
g0113: [2024-08-02 18:30:59,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=330, skipped=0, lr=[5.7496917333333335e-06, 5.7496917333333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 330 loss: 7.0029 iter time (s): 4.105 samples/sec: 31.179
g0133:  iteration      330/10000000 | consumed samples:        42240 | consumed tokens:     86507520 | elapsed time per iteration (ms): 4138.4 | learning rate: 5.750E-06 | global batch size:   128 | lm loss: 7.024649E+00 | loss scale: 2048.0 | grad norm: 4.614 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.930 | tokens per gpu per second (tgs): 1979.501 | TFLOPs: 15.93 |
g0113: [2024-08-02 18:31:41,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=340, skipped=0, lr=[5.9244543999999995e-06, 5.9244543999999995e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 340 loss: 6.9093 iter time (s): 4.218 samples/sec: 30.347
g0133:  iteration      340/10000000 | consumed samples:        43520 | consumed tokens:     89128960 | elapsed time per iteration (ms): 4250.6 | learning rate: 5.924E-06 | global batch size:   128 | lm loss: 6.944342E+00 | loss scale: 2048.0 | grad norm: 2.990 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.114 | tokens per gpu per second (tgs): 1927.276 | TFLOPs: 15.51 |
g0113: [2024-08-02 18:32:23,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=350, skipped=0, lr=[6.0992170666666664e-06, 6.0992170666666664e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 350 loss: 6.8527 iter time (s): 4.142 samples/sec: 30.902
g0133:  iteration      350/10000000 | consumed samples:        44800 | consumed tokens:     91750400 | elapsed time per iteration (ms): 4174.7 | learning rate: 6.099E-06 | global batch size:   128 | lm loss: 6.870718E+00 | loss scale: 2048.0 | grad norm: 3.337 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.661 | tokens per gpu per second (tgs): 1962.273 | TFLOPs: 15.79 |
g0113: [2024-08-02 18:33:05,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=360, skipped=0, lr=[6.273979733333333e-06, 6.273979733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 360 loss: 6.7585 iter time (s): 4.110 samples/sec: 31.142
g0133:  iteration      360/10000000 | consumed samples:        46080 | consumed tokens:     94371840 | elapsed time per iteration (ms): 4147.3 | learning rate: 6.274E-06 | global batch size:   128 | lm loss: 6.801155E+00 | loss scale: 2048.0 | grad norm: 3.560 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.863 | tokens per gpu per second (tgs): 1975.238 | TFLOPs: 15.90 |
g0113: [2024-08-02 18:33:47,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=370, skipped=0, lr=[6.4487424e-06, 6.4487424e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 370 loss: 6.6938 iter time (s): 4.157 samples/sec: 30.788
g0133:  iteration      370/10000000 | consumed samples:        47360 | consumed tokens:     96993280 | elapsed time per iteration (ms): 4190.2 | learning rate: 6.449E-06 | global batch size:   128 | lm loss: 6.733307E+00 | loss scale: 2048.0 | grad norm: 2.708 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.547 | tokens per gpu per second (tgs): 1955.022 | TFLOPs: 15.73 |
g0113: [2024-08-02 18:34:28,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=380, skipped=0, lr=[6.623505066666667e-06, 6.623505066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 380 loss: 6.6387 iter time (s): 4.164 samples/sec: 30.742
g0133:  iteration      380/10000000 | consumed samples:        48640 | consumed tokens:     99614720 | elapsed time per iteration (ms): 4196.3 | learning rate: 6.624E-06 | global batch size:   128 | lm loss: 6.669949E+00 | loss scale: 2048.0 | grad norm: 3.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.503 | tokens per gpu per second (tgs): 1952.173 | TFLOPs: 15.71 |
g0113: [2024-08-02 18:35:10,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=390, skipped=0, lr=[6.798267733333334e-06, 6.798267733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 390 loss: 6.5973 iter time (s): 4.161 samples/sec: 30.765
g0133:  iteration      390/10000000 | consumed samples:        49920 | consumed tokens:    102236160 | elapsed time per iteration (ms): 4193.5 | learning rate: 6.798E-06 | global batch size:   128 | lm loss: 6.616832E+00 | loss scale: 2048.0 | grad norm: 3.496 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.523 | tokens per gpu per second (tgs): 1953.488 | TFLOPs: 15.72 |
g0113: [2024-08-02 18:35:51,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=400, skipped=0, lr=[6.973030400000001e-06, 6.973030400000001e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 400 loss: 6.5159 iter time (s): 4.074 samples/sec: 31.418
g0133:  iteration      400/10000000 | consumed samples:        51200 | consumed tokens:    104857600 | elapsed time per iteration (ms): 4107.1 | learning rate: 6.973E-06 | global batch size:   128 | lm loss: 6.559223E+00 | loss scale: 2048.0 | grad norm: 3.026 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.166 | tokens per gpu per second (tgs): 1994.608 | TFLOPs: 16.05 |
g0113: [2024-08-02 18:36:34,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=410, skipped=0, lr=[7.147793066666666e-06, 7.147793066666666e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 410 loss: 6.4902 iter time (s): 4.189 samples/sec: 30.557
g0133:  iteration      410/10000000 | consumed samples:        52480 | consumed tokens:    107479040 | elapsed time per iteration (ms): 4222.3 | learning rate: 7.148E-06 | global batch size:   128 | lm loss: 6.517249E+00 | loss scale: 2048.0 | grad norm: 3.517 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.315 | tokens per gpu per second (tgs): 1940.186 | TFLOPs: 15.61 |
g0113: [2024-08-02 18:37:15,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=420, skipped=0, lr=[7.322555733333333e-06, 7.322555733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 420 loss: 6.4402 iter time (s): 4.099 samples/sec: 31.224
g0133:  iteration      420/10000000 | consumed samples:        53760 | consumed tokens:    110100480 | elapsed time per iteration (ms): 4132.1 | learning rate: 7.323E-06 | global batch size:   128 | lm loss: 6.456753E+00 | loss scale: 2048.0 | grad norm: 2.610 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.977 | tokens per gpu per second (tgs): 1982.505 | TFLOPs: 15.95 |
g0113: [2024-08-02 18:37:56,134] [INFO] [logging.py:96:log_dist] [Rank 0] step=430, skipped=0, lr=[7.4973184e-06, 7.4973184e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 430 loss: 6.3854 iter time (s): 4.027 samples/sec: 31.784
g0133:  iteration      430/10000000 | consumed samples:        55040 | consumed tokens:    112721920 | elapsed time per iteration (ms): 4062.2 | learning rate: 7.497E-06 | global batch size:   128 | lm loss: 6.409256E+00 | loss scale: 2048.0 | grad norm: 5.370 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.510 | tokens per gpu per second (tgs): 2016.637 | TFLOPs: 16.23 |
g0113: [2024-08-02 18:38:38,053] [INFO] [logging.py:96:log_dist] [Rank 0] step=440, skipped=0, lr=[7.672081066666667e-06, 7.672081066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 440 loss: 6.3285 iter time (s): 4.157 samples/sec: 30.791
g0133:  iteration      440/10000000 | consumed samples:        56320 | consumed tokens:    115343360 | elapsed time per iteration (ms): 4189.7 | learning rate: 7.672E-06 | global batch size:   128 | lm loss: 6.356676E+00 | loss scale: 2048.0 | grad norm: 4.804 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.551 | tokens per gpu per second (tgs): 1955.269 | TFLOPs: 15.73 |
g0113: [2024-08-02 18:39:20,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=450, skipped=0, lr=[7.846843733333333e-06, 7.846843733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 450 loss: 6.2571 iter time (s): 4.171 samples/sec: 30.691
g0133:  iteration      450/10000000 | consumed samples:        57600 | consumed tokens:    117964800 | elapsed time per iteration (ms): 4203.7 | learning rate: 7.847E-06 | global batch size:   128 | lm loss: 6.295108E+00 | loss scale: 2048.0 | grad norm: 4.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.449 | tokens per gpu per second (tgs): 1948.761 | TFLOPs: 15.68 |
g0113: [2024-08-02 18:40:01,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=460, skipped=0, lr=[8.0216064e-06, 8.0216064e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 460 loss: 6.1910 iter time (s): 4.093 samples/sec: 31.276
g0133:  iteration      460/10000000 | consumed samples:        58880 | consumed tokens:    120586240 | elapsed time per iteration (ms): 4125.3 | learning rate: 8.022E-06 | global batch size:   128 | lm loss: 6.231426E+00 | loss scale: 2048.0 | grad norm: 3.543 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.028 | tokens per gpu per second (tgs): 1985.774 | TFLOPs: 15.98 |
g0113: [2024-08-02 18:40:43,479] [INFO] [logging.py:96:log_dist] [Rank 0] step=470, skipped=0, lr=[8.196369066666667e-06, 8.196369066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 470 loss: 6.1139 iter time (s): 4.181 samples/sec: 30.618
g0133:  iteration      470/10000000 | consumed samples:        60160 | consumed tokens:    123207680 | elapsed time per iteration (ms): 4213.5 | learning rate: 8.196E-06 | global batch size:   128 | lm loss: 6.160037E+00 | loss scale: 2048.0 | grad norm: 4.558 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.378 | tokens per gpu per second (tgs): 1944.222 | TFLOPs: 15.65 |
g0113: [2024-08-02 18:41:26,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=480, skipped=0, lr=[8.371131733333335e-06, 8.371131733333335e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 480 loss: 6.0021 iter time (s): 4.240 samples/sec: 30.188
g0133:  iteration      480/10000000 | consumed samples:        61440 | consumed tokens:    125829120 | elapsed time per iteration (ms): 4273.2 | learning rate: 8.371E-06 | global batch size:   128 | lm loss: 6.066080E+00 | loss scale: 2048.0 | grad norm: 4.246 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.954 | tokens per gpu per second (tgs): 1917.067 | TFLOPs: 15.43 |
g0113: [2024-08-02 18:42:08,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=490, skipped=0, lr=[8.5458944e-06, 8.5458944e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 490 loss: 5.9154 iter time (s): 4.183 samples/sec: 30.602
g0133:  iteration      490/10000000 | consumed samples:        62720 | consumed tokens:    128450560 | elapsed time per iteration (ms): 4216.3 | learning rate: 8.546E-06 | global batch size:   128 | lm loss: 5.973899E+00 | loss scale: 2048.0 | grad norm: 3.867 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.359 | tokens per gpu per second (tgs): 1942.953 | TFLOPs: 15.64 |
g0113: [2024-08-02 18:42:50,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=500, skipped=0, lr=[8.720657066666667e-06, 8.720657066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 500 loss: 5.8061 iter time (s): 4.150 samples/sec: 30.845
g0133:  iteration      500/10000000 | consumed samples:        64000 | consumed tokens:    131072000 | elapsed time per iteration (ms): 4182.3 | learning rate: 8.721E-06 | global batch size:   128 | lm loss: 5.861047E+00 | loss scale: 2048.0 | grad norm: 4.785 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.605 | tokens per gpu per second (tgs): 1958.725 | TFLOPs: 15.76 |
g0128: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0128: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0128: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0129: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0129: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0125: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0125: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0113: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0113: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0131: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0131: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0113: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0125: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0131: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0130: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0129: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0130: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0133: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0132: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0129: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0132: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0130: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0132: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0132: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0130: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0133: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0133: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0133: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0133: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0133: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0130: [2024-08-02 18:42:54,481] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0113: [2024-08-02 18:42:54,480] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 2048 to 4096
g0113: [2024-08-02 18:43:32,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=510, skipped=0, lr=[8.895419733333333e-06, 8.895419733333333e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 510 loss: 5.6917 iter time (s): 4.192 samples/sec: 30.532
g0133:  iteration      510/10000000 | consumed samples:        65280 | consumed tokens:    133693440 | elapsed time per iteration (ms): 4225.0 | learning rate: 8.895E-06 | global batch size:   128 | lm loss: 5.749839E+00 | loss scale: 4096.0 | grad norm: 5.708 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.296 | tokens per gpu per second (tgs): 1938.935 | TFLOPs: 15.60 |
g0113: [2024-08-02 18:44:14,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=520, skipped=0, lr=[9.0701824e-06, 9.0701824e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 520 loss: 5.6000 iter time (s): 4.166 samples/sec: 30.722
g0133:  iteration      520/10000000 | consumed samples:        66560 | consumed tokens:    136314880 | elapsed time per iteration (ms): 4201.0 | learning rate: 9.070E-06 | global batch size:   128 | lm loss: 5.652853E+00 | loss scale: 4096.0 | grad norm: 5.761 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.469 | tokens per gpu per second (tgs): 1949.990 | TFLOPs: 15.69 |
g0113: [2024-08-02 18:44:55,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=530, skipped=0, lr=[9.244945066666667e-06, 9.244945066666667e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 530 loss: 5.4851 iter time (s): 4.062 samples/sec: 31.515
g0133:  iteration      530/10000000 | consumed samples:        67840 | consumed tokens:    138936320 | elapsed time per iteration (ms): 4094.3 | learning rate: 9.245E-06 | global batch size:   128 | lm loss: 5.545784E+00 | loss scale: 4096.0 | grad norm: 4.486 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.263 | tokens per gpu per second (tgs): 2000.809 | TFLOPs: 16.10 |
g0113: [2024-08-02 18:45:37,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=540, skipped=0, lr=[9.419707733333334e-06, 9.419707733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 540 loss: 5.3655 iter time (s): 4.209 samples/sec: 30.411
g0133:  iteration      540/10000000 | consumed samples:        69120 | consumed tokens:    141557760 | elapsed time per iteration (ms): 4241.6 | learning rate: 9.420E-06 | global batch size:   128 | lm loss: 5.435464E+00 | loss scale: 4096.0 | grad norm: 5.614 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.177 | tokens per gpu per second (tgs): 1931.343 | TFLOPs: 15.54 |
g0113: [2024-08-02 18:46:19,990] [INFO] [logging.py:96:log_dist] [Rank 0] step=550, skipped=0, lr=[9.5944704e-06, 9.5944704e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 550 loss: 5.2651 iter time (s): 4.184 samples/sec: 30.590
g0133:  iteration      550/10000000 | consumed samples:        70400 | consumed tokens:    144179200 | elapsed time per iteration (ms): 4217.3 | learning rate: 9.594E-06 | global batch size:   128 | lm loss: 5.343845E+00 | loss scale: 4096.0 | grad norm: 4.843 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.351 | tokens per gpu per second (tgs): 1942.465 | TFLOPs: 15.63 |
g0113: [2024-08-02 18:47:01,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=560, skipped=0, lr=[9.769233066666668e-06, 9.769233066666668e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 560 loss: 5.1837 iter time (s): 4.096 samples/sec: 31.246
g0133:  iteration      560/10000000 | consumed samples:        71680 | consumed tokens:    146800640 | elapsed time per iteration (ms): 4129.0 | learning rate: 9.769E-06 | global batch size:   128 | lm loss: 5.250024E+00 | loss scale: 4096.0 | grad norm: 5.376 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.000 | tokens per gpu per second (tgs): 1984.011 | TFLOPs: 15.97 |
g0113: [2024-08-02 18:47:41,226] [INFO] [logging.py:96:log_dist] [Rank 0] step=570, skipped=0, lr=[9.943995733333334e-06, 9.943995733333334e-06], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 570 loss: 5.1096 iter time (s): 3.962 samples/sec: 32.311
g0133:  iteration      570/10000000 | consumed samples:        72960 | consumed tokens:    149422080 | elapsed time per iteration (ms): 3994.5 | learning rate: 9.944E-06 | global batch size:   128 | lm loss: 5.158208E+00 | loss scale: 4096.0 | grad norm: 5.411 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.044 | tokens per gpu per second (tgs): 2050.817 | TFLOPs: 16.50 |
g0113: [2024-08-02 18:48:23,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=580, skipped=0, lr=[1.01187584e-05, 1.01187584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 580 loss: 5.0301 iter time (s): 4.145 samples/sec: 30.882
g0133:  iteration      580/10000000 | consumed samples:        74240 | consumed tokens:    152043520 | elapsed time per iteration (ms): 4178.1 | learning rate: 1.012E-05 | global batch size:   128 | lm loss: 5.075412E+00 | loss scale: 4096.0 | grad norm: 8.702 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.636 | tokens per gpu per second (tgs): 1960.719 | TFLOPs: 15.78 |
g0113: [2024-08-02 18:49:04,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=590, skipped=0, lr=[1.0293521066666666e-05, 1.0293521066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 590 loss: 4.9303 iter time (s): 4.074 samples/sec: 31.416
g0133:  iteration      590/10000000 | consumed samples:        75520 | consumed tokens:    154664960 | elapsed time per iteration (ms): 4107.2 | learning rate: 1.029E-05 | global batch size:   128 | lm loss: 4.979491E+00 | loss scale: 4096.0 | grad norm: 5.108 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.165 | tokens per gpu per second (tgs): 1994.566 | TFLOPs: 16.05 |
g0113: [2024-08-02 18:49:47,689] [INFO] [logging.py:96:log_dist] [Rank 0] step=600, skipped=0, lr=[1.0468283733333334e-05, 1.0468283733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 600 loss: 4.8777 iter time (s): 4.328 samples/sec: 29.576
g0133:  iteration      600/10000000 | consumed samples:        76800 | consumed tokens:    157286400 | elapsed time per iteration (ms): 4361.1 | learning rate: 1.047E-05 | global batch size:   128 | lm loss: 4.906809E+00 | loss scale: 4096.0 | grad norm: 5.639 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.350 | tokens per gpu per second (tgs): 1878.428 | TFLOPs: 15.12 |
g0113: [2024-08-02 18:50:28,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=610, skipped=0, lr=[1.06430464e-05, 1.06430464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 610 loss: 4.8097 iter time (s): 4.082 samples/sec: 31.358
g0133:  iteration      610/10000000 | consumed samples:        78080 | consumed tokens:    159907840 | elapsed time per iteration (ms): 4114.9 | learning rate: 1.064E-05 | global batch size:   128 | lm loss: 4.836580E+00 | loss scale: 4096.0 | grad norm: 6.252 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.107 | tokens per gpu per second (tgs): 1990.819 | TFLOPs: 16.02 |
g0113: [2024-08-02 18:51:10,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=620, skipped=0, lr=[1.0817809066666668e-05, 1.0817809066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 620 loss: 4.7126 iter time (s): 4.086 samples/sec: 31.328
g0133:  iteration      620/10000000 | consumed samples:        79360 | consumed tokens:    162529280 | elapsed time per iteration (ms): 4118.6 | learning rate: 1.082E-05 | global batch size:   128 | lm loss: 4.756896E+00 | loss scale: 4096.0 | grad norm: 6.014 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.079 | tokens per gpu per second (tgs): 1989.036 | TFLOPs: 16.01 |
g0113: [2024-08-02 18:51:51,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=630, skipped=0, lr=[1.0992571733333332e-05, 1.0992571733333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 630 loss: 4.6420 iter time (s): 4.159 samples/sec: 30.777
g0133:  iteration      630/10000000 | consumed samples:        80640 | consumed tokens:    165150720 | elapsed time per iteration (ms): 4191.8 | learning rate: 1.099E-05 | global batch size:   128 | lm loss: 4.683287E+00 | loss scale: 4096.0 | grad norm: 5.433 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.536 | tokens per gpu per second (tgs): 1954.305 | TFLOPs: 15.73 |
g0113: [2024-08-02 18:52:33,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=640, skipped=0, lr=[1.11673344e-05, 1.11673344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 640 loss: 4.5609 iter time (s): 4.107 samples/sec: 31.167
g0133:  iteration      640/10000000 | consumed samples:        81920 | consumed tokens:    167772160 | elapsed time per iteration (ms): 4141.1 | learning rate: 1.117E-05 | global batch size:   128 | lm loss: 4.608636E+00 | loss scale: 4096.0 | grad norm: 6.476 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.910 | tokens per gpu per second (tgs): 1978.241 | TFLOPs: 15.92 |
g0113: [2024-08-02 18:53:14,265] [INFO] [logging.py:96:log_dist] [Rank 0] step=650, skipped=0, lr=[1.1342097066666666e-05, 1.1342097066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 650 loss: 4.5130 iter time (s): 4.058 samples/sec: 31.541
g0133:  iteration      650/10000000 | consumed samples:        83200 | consumed tokens:    170393600 | elapsed time per iteration (ms): 4091.5 | learning rate: 1.134E-05 | global batch size:   128 | lm loss: 4.563065E+00 | loss scale: 4096.0 | grad norm: 5.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.284 | tokens per gpu per second (tgs): 2002.203 | TFLOPs: 16.11 |
g0113: [2024-08-02 18:53:57,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=660, skipped=0, lr=[1.1516859733333334e-05, 1.1516859733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 660 loss: 4.4410 iter time (s): 4.240 samples/sec: 30.187
g0133:  iteration      660/10000000 | consumed samples:        84480 | consumed tokens:    173015040 | elapsed time per iteration (ms): 4273.2 | learning rate: 1.152E-05 | global batch size:   128 | lm loss: 4.483741E+00 | loss scale: 4096.0 | grad norm: 4.065 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.954 | tokens per gpu per second (tgs): 1917.071 | TFLOPs: 15.43 |
g0113: [2024-08-02 18:54:40,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=670, skipped=0, lr=[1.16916224e-05, 1.16916224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 670 loss: 4.3931 iter time (s): 4.268 samples/sec: 29.992
g0133:  iteration      670/10000000 | consumed samples:        85760 | consumed tokens:    175636480 | elapsed time per iteration (ms): 4300.4 | learning rate: 1.169E-05 | global batch size:   128 | lm loss: 4.429658E+00 | loss scale: 4096.0 | grad norm: 5.414 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.765 | tokens per gpu per second (tgs): 1904.948 | TFLOPs: 15.33 |
g0113: [2024-08-02 18:55:22,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=680, skipped=0, lr=[1.1866385066666668e-05, 1.1866385066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 680 loss: 4.3605 iter time (s): 4.181 samples/sec: 30.613
g0133:  iteration      680/10000000 | consumed samples:        87040 | consumed tokens:    178257920 | elapsed time per iteration (ms): 4213.9 | learning rate: 1.187E-05 | global batch size:   128 | lm loss: 4.366942E+00 | loss scale: 4096.0 | grad norm: 4.857 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.376 | tokens per gpu per second (tgs): 1944.052 | TFLOPs: 15.64 |
g0113: [2024-08-02 18:56:07,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=690, skipped=0, lr=[1.2041147733333334e-05, 1.2041147733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 690 loss: 4.3028 iter time (s): 4.484 samples/sec: 28.544
g0133:  iteration      690/10000000 | consumed samples:        88320 | consumed tokens:    180879360 | elapsed time per iteration (ms): 4518.1 | learning rate: 1.204E-05 | global batch size:   128 | lm loss: 4.310235E+00 | loss scale: 4096.0 | grad norm: 5.901 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.330 | tokens per gpu per second (tgs): 1813.141 | TFLOPs: 14.59 |
g0113: [2024-08-02 18:56:56,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=700, skipped=0, lr=[1.22159104e-05, 1.22159104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 700 loss: 4.2342 iter time (s): 4.887 samples/sec: 26.192
g0133:  iteration      700/10000000 | consumed samples:        89600 | consumed tokens:    183500800 | elapsed time per iteration (ms): 4920.0 | learning rate: 1.222E-05 | global batch size:   128 | lm loss: 4.258909E+00 | loss scale: 4096.0 | grad norm: 5.382 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.016 | tokens per gpu per second (tgs): 1665.041 | TFLOPs: 13.40 |
g0113: [2024-08-02 18:57:47,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=710, skipped=0, lr=[1.2390673066666668e-05, 1.2390673066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 710 loss: 4.1483 iter time (s): 5.032 samples/sec: 25.438
g0133:  iteration      710/10000000 | consumed samples:        90880 | consumed tokens:    186122240 | elapsed time per iteration (ms): 5064.6 | learning rate: 1.239E-05 | global batch size:   128 | lm loss: 4.197704E+00 | loss scale: 4096.0 | grad norm: 4.275 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.274 | tokens per gpu per second (tgs): 1617.509 | TFLOPs: 13.02 |
g0113: [2024-08-02 18:58:36,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=720, skipped=0, lr=[1.2565435733333334e-05, 1.2565435733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 720 loss: 4.1449 iter time (s): 4.928 samples/sec: 25.974
g0133:  iteration      720/10000000 | consumed samples:        92160 | consumed tokens:    188743680 | elapsed time per iteration (ms): 4962.0 | learning rate: 1.257E-05 | global batch size:   128 | lm loss: 4.150350E+00 | loss scale: 4096.0 | grad norm: 4.679 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.796 | tokens per gpu per second (tgs): 1650.963 | TFLOPs: 13.29 |
g0113: [2024-08-02 18:59:26,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=730, skipped=0, lr=[1.2740198400000001e-05, 1.2740198400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 730 loss: 4.0890 iter time (s): 4.948 samples/sec: 25.869
g0133:  iteration      730/10000000 | consumed samples:        93440 | consumed tokens:    191365120 | elapsed time per iteration (ms): 4980.9 | learning rate: 1.274E-05 | global batch size:   128 | lm loss: 4.096483E+00 | loss scale: 4096.0 | grad norm: 5.137 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.698 | tokens per gpu per second (tgs): 1644.669 | TFLOPs: 13.23 |
g0113: [2024-08-02 19:00:12,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=740, skipped=0, lr=[1.2914961066666667e-05, 1.2914961066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 740 loss: 4.0296 iter time (s): 4.598 samples/sec: 27.839
g0133:  iteration      740/10000000 | consumed samples:        94720 | consumed tokens:    193986560 | elapsed time per iteration (ms): 4631.9 | learning rate: 1.291E-05 | global batch size:   128 | lm loss: 4.065030E+00 | loss scale: 4096.0 | grad norm: 3.640 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.635 | tokens per gpu per second (tgs): 1768.611 | TFLOPs: 14.23 |
g0113: [2024-08-02 19:00:58,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=750, skipped=0, lr=[1.3089723733333335e-05, 1.3089723733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 750 loss: 4.0484 iter time (s): 4.561 samples/sec: 28.062
g0133:  iteration      750/10000000 | consumed samples:        96000 | consumed tokens:    196608000 | elapsed time per iteration (ms): 4594.0 | learning rate: 1.309E-05 | global batch size:   128 | lm loss: 4.015320E+00 | loss scale: 4096.0 | grad norm: 4.421 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.862 | tokens per gpu per second (tgs): 1783.190 | TFLOPs: 14.35 |
g0113: [2024-08-02 19:01:45,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=760, skipped=0, lr=[1.3264486400000001e-05, 1.3264486400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 760 loss: 3.9621 iter time (s): 4.663 samples/sec: 27.451
g0133:  iteration      760/10000000 | consumed samples:        97280 | consumed tokens:    199229440 | elapsed time per iteration (ms): 4695.8 | learning rate: 1.326E-05 | global batch size:   128 | lm loss: 3.961357E+00 | loss scale: 4096.0 | grad norm: 6.121 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.258 | tokens per gpu per second (tgs): 1744.526 | TFLOPs: 14.04 |
g0113: [2024-08-02 19:02:32,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=770, skipped=0, lr=[1.3439249066666669e-05, 1.3439249066666669e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 770 loss: 3.9185 iter time (s): 4.640 samples/sec: 27.584
g0133:  iteration      770/10000000 | consumed samples:        98560 | consumed tokens:    201850880 | elapsed time per iteration (ms): 4673.2 | learning rate: 1.344E-05 | global batch size:   128 | lm loss: 3.929785E+00 | loss scale: 4096.0 | grad norm: 4.570 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.390 | tokens per gpu per second (tgs): 1752.982 | TFLOPs: 14.11 |
g0113: [2024-08-02 19:03:17,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=780, skipped=0, lr=[1.3614011733333333e-05, 1.3614011733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 780 loss: 3.8591 iter time (s): 4.447 samples/sec: 28.781
g0133:  iteration      780/10000000 | consumed samples:        99840 | consumed tokens:    204472320 | elapsed time per iteration (ms): 4480.5 | learning rate: 1.361E-05 | global batch size:   128 | lm loss: 3.895401E+00 | loss scale: 4096.0 | grad norm: 4.118 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.568 | tokens per gpu per second (tgs): 1828.383 | TFLOPs: 14.71 |
g0113: [2024-08-02 19:04:05,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=790, skipped=0, lr=[1.37887744e-05, 1.37887744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 790 loss: 3.8123 iter time (s): 4.736 samples/sec: 27.029
g0133:  iteration      790/10000000 | consumed samples:       101120 | consumed tokens:    207093760 | elapsed time per iteration (ms): 4768.8 | learning rate: 1.379E-05 | global batch size:   128 | lm loss: 3.858718E+00 | loss scale: 4096.0 | grad norm: 4.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.841 | tokens per gpu per second (tgs): 1717.837 | TFLOPs: 13.82 |
g0113: [2024-08-02 19:04:53,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=800, skipped=0, lr=[1.3963537066666667e-05, 1.3963537066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 800 loss: 3.8221 iter time (s): 4.792 samples/sec: 26.710
g0133:  iteration      800/10000000 | consumed samples:       102400 | consumed tokens:    209715200 | elapsed time per iteration (ms): 4825.1 | learning rate: 1.396E-05 | global batch size:   128 | lm loss: 3.827060E+00 | loss scale: 4096.0 | grad norm: 3.880 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.528 | tokens per gpu per second (tgs): 1697.804 | TFLOPs: 13.66 |
g0113: [2024-08-02 19:05:40,040] [INFO] [logging.py:96:log_dist] [Rank 0] step=810, skipped=0, lr=[1.4138299733333333e-05, 1.4138299733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 810 loss: 3.7473 iter time (s): 4.641 samples/sec: 27.579
g0133:  iteration      810/10000000 | consumed samples:       103680 | consumed tokens:    212336640 | elapsed time per iteration (ms): 4674.7 | learning rate: 1.414E-05 | global batch size:   128 | lm loss: 3.782725E+00 | loss scale: 4096.0 | grad norm: 3.507 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.381 | tokens per gpu per second (tgs): 1752.398 | TFLOPs: 14.10 |
g0113: [2024-08-02 19:06:28,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=820, skipped=0, lr=[1.43130624e-05, 1.43130624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 820 loss: 3.7128 iter time (s): 4.780 samples/sec: 26.781
g0133:  iteration      820/10000000 | consumed samples:       104960 | consumed tokens:    214958080 | elapsed time per iteration (ms): 4812.6 | learning rate: 1.431E-05 | global batch size:   128 | lm loss: 3.749540E+00 | loss scale: 4096.0 | grad norm: 4.055 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.597 | tokens per gpu per second (tgs): 1702.189 | TFLOPs: 13.70 |
g0113: [2024-08-02 19:07:17,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=830, skipped=0, lr=[1.4487825066666667e-05, 1.4487825066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 830 loss: 3.6566 iter time (s): 4.862 samples/sec: 26.324
g0133:  iteration      830/10000000 | consumed samples:       106240 | consumed tokens:    217579520 | elapsed time per iteration (ms): 4895.4 | learning rate: 1.449E-05 | global batch size:   128 | lm loss: 3.727875E+00 | loss scale: 4096.0 | grad norm: 3.820 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.147 | tokens per gpu per second (tgs): 1673.392 | TFLOPs: 13.47 |
g0113: [2024-08-02 19:08:03,175] [INFO] [logging.py:96:log_dist] [Rank 0] step=840, skipped=0, lr=[1.4662587733333333e-05, 1.4662587733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 840 loss: 3.6774 iter time (s): 4.572 samples/sec: 27.994
g0133:  iteration      840/10000000 | consumed samples:       107520 | consumed tokens:    220200960 | elapsed time per iteration (ms): 4605.4 | learning rate: 1.466E-05 | global batch size:   128 | lm loss: 3.686540E+00 | loss scale: 4096.0 | grad norm: 4.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.794 | tokens per gpu per second (tgs): 1778.800 | TFLOPs: 14.31 |
g0113: [2024-08-02 19:08:47,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=850, skipped=0, lr=[1.4837350400000001e-05, 1.4837350400000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 850 loss: 3.6794 iter time (s): 4.413 samples/sec: 29.006
g0133:  iteration      850/10000000 | consumed samples:       108800 | consumed tokens:    222822400 | elapsed time per iteration (ms): 4447.1 | learning rate: 1.484E-05 | global batch size:   128 | lm loss: 3.660298E+00 | loss scale: 4096.0 | grad norm: 3.826 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.783 | tokens per gpu per second (tgs): 1842.084 | TFLOPs: 14.82 |
g0113: [2024-08-02 19:09:30,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=860, skipped=0, lr=[1.5012113066666667e-05, 1.5012113066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 860 loss: 3.6343 iter time (s): 4.248 samples/sec: 30.133
g0133:  iteration      860/10000000 | consumed samples:       110080 | consumed tokens:    225443840 | elapsed time per iteration (ms): 4281.0 | learning rate: 1.501E-05 | global batch size:   128 | lm loss: 3.627351E+00 | loss scale: 4096.0 | grad norm: 5.597 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.900 | tokens per gpu per second (tgs): 1913.576 | TFLOPs: 15.40 |
g0113: [2024-08-02 19:10:12,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=870, skipped=0, lr=[1.5186875733333335e-05, 1.5186875733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 870 loss: 3.5694 iter time (s): 4.180 samples/sec: 30.620
g0133:  iteration      870/10000000 | consumed samples:       111360 | consumed tokens:    228065280 | elapsed time per iteration (ms): 4212.9 | learning rate: 1.519E-05 | global batch size:   128 | lm loss: 3.603831E+00 | loss scale: 4096.0 | grad norm: 4.349 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.383 | tokens per gpu per second (tgs): 1944.495 | TFLOPs: 15.65 |
g0113: [2024-08-02 19:10:54,325] [INFO] [logging.py:96:log_dist] [Rank 0] step=880, skipped=0, lr=[1.5361638400000003e-05, 1.5361638400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 880 loss: 3.6024 iter time (s): 4.141 samples/sec: 30.909
g0133:  iteration      880/10000000 | consumed samples:       112640 | consumed tokens:    230686720 | elapsed time per iteration (ms): 4173.9 | learning rate: 1.536E-05 | global batch size:   128 | lm loss: 3.574486E+00 | loss scale: 4096.0 | grad norm: 4.038 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.667 | tokens per gpu per second (tgs): 1962.659 | TFLOPs: 15.79 |
g0113: [2024-08-02 19:11:35,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=890, skipped=0, lr=[1.553640106666667e-05, 1.553640106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 890 loss: 3.5807 iter time (s): 4.050 samples/sec: 31.608
g0133:  iteration      890/10000000 | consumed samples:       113920 | consumed tokens:    233308160 | elapsed time per iteration (ms): 4082.5 | learning rate: 1.554E-05 | global batch size:   128 | lm loss: 3.554777E+00 | loss scale: 4096.0 | grad norm: 3.879 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.353 | tokens per gpu per second (tgs): 2006.604 | TFLOPs: 16.15 |
g0113: [2024-08-02 19:12:16,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=900, skipped=0, lr=[1.5711163733333335e-05, 1.5711163733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 900 loss: 3.5091 iter time (s): 4.102 samples/sec: 31.204
g0133:  iteration      900/10000000 | consumed samples:       115200 | consumed tokens:    235929600 | elapsed time per iteration (ms): 4134.9 | learning rate: 1.571E-05 | global batch size:   128 | lm loss: 3.508314E+00 | loss scale: 4096.0 | grad norm: 4.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.956 | tokens per gpu per second (tgs): 1981.188 | TFLOPs: 15.94 |
g0113: [2024-08-02 19:12:57,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=910, skipped=0, lr=[1.58859264e-05, 1.58859264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 910 loss: 3.4541 iter time (s): 4.101 samples/sec: 31.210
g0133:  iteration      910/10000000 | consumed samples:       116480 | consumed tokens:    238551040 | elapsed time per iteration (ms): 4150.5 | learning rate: 1.589E-05 | global batch size:   128 | lm loss: 3.485984E+00 | loss scale: 4096.0 | grad norm: 4.007 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.839 | tokens per gpu per second (tgs): 1973.715 | TFLOPs: 15.88 |
g0113: [2024-08-02 19:13:39,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=920, skipped=0, lr=[1.6060689066666667e-05, 1.6060689066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 920 loss: 3.4557 iter time (s): 4.136 samples/sec: 30.950
g0133:  iteration      920/10000000 | consumed samples:       117760 | consumed tokens:    241172480 | elapsed time per iteration (ms): 4173.2 | learning rate: 1.606E-05 | global batch size:   128 | lm loss: 3.469395E+00 | loss scale: 4096.0 | grad norm: 3.871 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.672 | tokens per gpu per second (tgs): 1962.979 | TFLOPs: 15.80 |
g0113: [2024-08-02 19:14:21,573] [INFO] [logging.py:96:log_dist] [Rank 0] step=930, skipped=0, lr=[1.6235451733333336e-05, 1.6235451733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 930 loss: 3.3974 iter time (s): 4.150 samples/sec: 30.840
g0133:  iteration      930/10000000 | consumed samples:       119040 | consumed tokens:    243793920 | elapsed time per iteration (ms): 4183.5 | learning rate: 1.624E-05 | global batch size:   128 | lm loss: 3.455786E+00 | loss scale: 4096.0 | grad norm: 3.828 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.596 | tokens per gpu per second (tgs): 1958.155 | TFLOPs: 15.76 |
g0113: [2024-08-02 19:15:02,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=940, skipped=0, lr=[1.6410214400000002e-05, 1.6410214400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 940 loss: 3.4202 iter time (s): 4.051 samples/sec: 31.595
g0133:  iteration      940/10000000 | consumed samples:       120320 | consumed tokens:    246415360 | elapsed time per iteration (ms): 4083.9 | learning rate: 1.641E-05 | global batch size:   128 | lm loss: 3.413988E+00 | loss scale: 4096.0 | grad norm: 3.688 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.343 | tokens per gpu per second (tgs): 2005.937 | TFLOPs: 16.14 |
g0113: [2024-08-02 19:15:43,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=950, skipped=0, lr=[1.6584977066666665e-05, 1.6584977066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 950 loss: 3.4107 iter time (s): 4.058 samples/sec: 31.546
g0133:  iteration      950/10000000 | consumed samples:       121600 | consumed tokens:    249036800 | elapsed time per iteration (ms): 4090.3 | learning rate: 1.658E-05 | global batch size:   128 | lm loss: 3.397415E+00 | loss scale: 4096.0 | grad norm: 3.751 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.293 | tokens per gpu per second (tgs): 2002.776 | TFLOPs: 16.12 |
g0113: [2024-08-02 19:16:24,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=960, skipped=0, lr=[1.6759739733333334e-05, 1.6759739733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 960 loss: 3.3593 iter time (s): 4.101 samples/sec: 31.213
g0133:  iteration      960/10000000 | consumed samples:       122880 | consumed tokens:    251658240 | elapsed time per iteration (ms): 4133.7 | learning rate: 1.676E-05 | global batch size:   128 | lm loss: 3.372793E+00 | loss scale: 4096.0 | grad norm: 3.733 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.965 | tokens per gpu per second (tgs): 1981.764 | TFLOPs: 15.95 |
g0113: [2024-08-02 19:17:05,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=970, skipped=0, lr=[1.69345024e-05, 1.69345024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 970 loss: 3.3161 iter time (s): 4.024 samples/sec: 31.807
g0133:  iteration      970/10000000 | consumed samples:       124160 | consumed tokens:    254279680 | elapsed time per iteration (ms): 4057.1 | learning rate: 1.693E-05 | global batch size:   128 | lm loss: 3.354386E+00 | loss scale: 4096.0 | grad norm: 3.527 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.549 | tokens per gpu per second (tgs): 2019.162 | TFLOPs: 16.25 |
g0113: [2024-08-02 19:17:47,147] [INFO] [logging.py:96:log_dist] [Rank 0] step=980, skipped=0, lr=[1.7109265066666667e-05, 1.7109265066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 980 loss: 3.3584 iter time (s): 4.160 samples/sec: 30.772
g0133:  iteration      980/10000000 | consumed samples:       125440 | consumed tokens:    256901120 | elapsed time per iteration (ms): 4192.3 | learning rate: 1.711E-05 | global batch size:   128 | lm loss: 3.347557E+00 | loss scale: 4096.0 | grad norm: 3.771 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.532 | tokens per gpu per second (tgs): 1954.059 | TFLOPs: 15.72 |
g0113: [2024-08-02 19:18:28,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=990, skipped=0, lr=[1.7284027733333333e-05, 1.7284027733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 990 loss: 3.3117 iter time (s): 4.122 samples/sec: 31.053
g0133:  iteration      990/10000000 | consumed samples:       126720 | consumed tokens:    259522560 | elapsed time per iteration (ms): 4154.8 | learning rate: 1.728E-05 | global batch size:   128 | lm loss: 3.315487E+00 | loss scale: 4096.0 | grad norm: 2.850 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.807 | tokens per gpu per second (tgs): 1971.675 | TFLOPs: 15.87 |
g0113: [2024-08-02 19:19:11,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=1000, skipped=0, lr=[1.7458790400000002e-05, 1.7458790400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1000 loss: 3.2970 iter time (s): 4.269 samples/sec: 29.981
g0133:  iteration     1000/10000000 | consumed samples:       128000 | consumed tokens:    262144000 | elapsed time per iteration (ms): 4302.0 | learning rate: 1.746E-05 | global batch size:   128 | lm loss: 3.293161E+00 | loss scale: 4096.0 | grad norm: 4.066 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.754 | tokens per gpu per second (tgs): 1904.225 | TFLOPs: 15.32 |
g0133: ------------------------------------------------------------------------------------------------
g0133:  validation loss at iteration 1000 | lm loss value: 3.291944E+00 | lm loss PPL: 2.689509E+01 | 
g0133: ------------------------------------------------------------------------------------------------
g0113: saving checkpoint at iteration    1000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: [2024-08-02 19:25:34,891] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step1000 is about to be saved!
g0133: [2024-08-02 19:25:34,898] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0113: [2024-08-02 19:25:34,898] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0113: [2024-08-02 19:25:34,898] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0132: [2024-08-02 19:25:34,898] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0132: [2024-08-02 19:25:34,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0133: [2024-08-02 19:25:34,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0133: [2024-08-02 19:25:34,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0113: [2024-08-02 19:25:34,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0132: [2024-08-02 19:25:34,899] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0130: [2024-08-02 19:25:34,900] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0130: [2024-08-02 19:25:34,900] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0129: [2024-08-02 19:25:34,900] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0129: [2024-08-02 19:25:34,900] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0130: [2024-08-02 19:25:34,900] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0129: [2024-08-02 19:25:34,900] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0125: [2024-08-02 19:25:34,902] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0125: [2024-08-02 19:25:34,902] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0125: [2024-08-02 19:25:34,902] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0131: [2024-08-02 19:25:34,902] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0131: [2024-08-02 19:25:34,903] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0131: [2024-08-02 19:25:34,903] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0128: [2024-08-02 19:25:34,903] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0128: [2024-08-02 19:25:34,903] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0128: [2024-08-02 19:25:34,904] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0133: [2024-08-02 19:25:34,927] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_23-model_00-model_states.pt...
g0130: [2024-08-02 19:25:34,931] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_14-model_00-model_states.pt...
g0132: [2024-08-02 19:25:34,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_20-model_00-model_states.pt...
g0129: [2024-08-02 19:25:34,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_11-model_00-model_states.pt...
g0125: [2024-08-02 19:25:34,937] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_05-model_00-model_states.pt...
g0131: [2024-08-02 19:25:34,940] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_17-model_00-model_states.pt...
g0128: [2024-08-02 19:25:34,943] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_08-model_00-model_states.pt...
g0113: [2024-08-02 19:25:34,947] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_01-model_00-model_states.pt...
g0133: [2024-08-02 19:25:35,039] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_23-model_00-model_states.pt.
g0133: [2024-08-02 19:25:35,040] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_24-model_00-model_states.pt...
g0133: [2024-08-02 19:25:35,042] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_24-model_00-model_states.pt.
g0131: [2024-08-02 19:25:35,048] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_17-model_00-model_states.pt.
g0128: [2024-08-02 19:25:35,055] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_08-model_00-model_states.pt.
g0129: [2024-08-02 19:25:35,056] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_11-model_00-model_states.pt.
g0125: [2024-08-02 19:25:35,061] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_05-model_00-model_states.pt.
g0130: [2024-08-02 19:25:35,062] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_14-model_00-model_states.pt.
g0133: [2024-08-02 19:25:35,086] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_25-model_00-model_states.pt...
g0131: [2024-08-02 19:25:35,087] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_18-model_00-model_states.pt...
g0129: [2024-08-02 19:25:35,091] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_12-model_00-model_states.pt...
g0128: [2024-08-02 19:25:35,094] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_09-model_00-model_states.pt...
g0130: [2024-08-02 19:25:35,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_15-model_00-model_states.pt...
g0125: [2024-08-02 19:25:35,098] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_06-model_00-model_states.pt...
g0132: [2024-08-02 19:25:35,110] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_20-model_00-model_states.pt.
g0132: [2024-08-02 19:25:35,144] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_21-model_00-model_states.pt...
g0113: [2024-08-02 19:25:35,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_01-model_00-model_states.pt.
g0113: [2024-08-02 19:25:35,188] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_02-model_00-model_states.pt...
g0131: [2024-08-02 19:25:35,219] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_18-model_00-model_states.pt.
g0129: [2024-08-02 19:25:35,220] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_12-model_00-model_states.pt.
g0125: [2024-08-02 19:25:35,236] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_06-model_00-model_states.pt.
g0130: [2024-08-02 19:25:35,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_15-model_00-model_states.pt.
g0132: [2024-08-02 19:25:35,247] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_21-model_00-model_states.pt.
g0131: [2024-08-02 19:25:35,254] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_19-model_00-model_states.pt...
g0129: [2024-08-02 19:25:35,253] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_13-model_00-model_states.pt...
g0125: [2024-08-02 19:25:35,271] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_07-model_00-model_states.pt...
g0130: [2024-08-02 19:25:35,273] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_16-model_00-model_states.pt...
g0132: [2024-08-02 19:25:35,280] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_22-model_00-model_states.pt...
g0128: [2024-08-02 19:25:35,301] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_09-model_00-model_states.pt.
g0133: [2024-08-02 19:25:35,328] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_25-model_00-model_states.pt.
g0133: [2024-08-02 19:25:35,329] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_07_model_states.pt...
g0128: [2024-08-02 19:25:35,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_10-model_00-model_states.pt...
g0113: [2024-08-02 19:25:35,382] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_02-model_00-model_states.pt.
g0130: [2024-08-02 19:25:35,401] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_16-model_00-model_states.pt.
g0130: [2024-08-02 19:25:35,403] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_04_model_states.pt...
g0129: [2024-08-02 19:25:35,403] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_13-model_00-model_states.pt.
g0129: [2024-08-02 19:25:35,405] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_03_model_states.pt...
g0113: [2024-08-02 19:25:35,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_03-model_00-model_states.pt...
g0125: [2024-08-02 19:25:35,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_07-model_00-model_states.pt.
g0125: [2024-08-02 19:25:35,446] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_01_model_states.pt...
g0128: [2024-08-02 19:25:35,447] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_10-model_00-model_states.pt.
g0128: [2024-08-02 19:25:35,448] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_02_model_states.pt...
g0132: [2024-08-02 19:25:35,468] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_22-model_00-model_states.pt.
g0132: [2024-08-02 19:25:35,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_06_model_states.pt...
g0131: [2024-08-02 19:25:35,531] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_19-model_00-model_states.pt.
g0131: [2024-08-02 19:25:35,533] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_05_model_states.pt...
g0113: [2024-08-02 19:25:35,548] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_03-model_00-model_states.pt.
g0113: [2024-08-02 19:25:35,571] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_04-model_00-model_states.pt...
g0113: [2024-08-02 19:25:35,692] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/layer_04-model_00-model_states.pt.
g0113: [2024-08-02 19:25:35,694] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt
g0113: [2024-08-02 19:25:35,694] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt...
g0133: [2024-08-02 19:25:37,195] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_07_model_states.pt.
g0133: [2024-08-02 19:25:37,195] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0130: [2024-08-02 19:25:37,720] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_04_model_states.pt.
g0130: [2024-08-02 19:25:37,721] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0132: [2024-08-02 19:25:37,810] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_06_model_states.pt.
g0132: [2024-08-02 19:25:37,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0125: [2024-08-02 19:25:37,817] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_01_model_states.pt.
g0125: [2024-08-02 19:25:37,818] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0128: [2024-08-02 19:25:37,844] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_02_model_states.pt.
g0128: [2024-08-02 19:25:37,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0129: [2024-08-02 19:25:38,207] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_03_model_states.pt.
g0129: [2024-08-02 19:25:38,208] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0131: [2024-08-02 19:25:38,306] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_05_model_states.pt.
g0131: [2024-08-02 19:25:38,306] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0113: [2024-08-02 19:25:39,043] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step1000/mp_rank_00_model_states.pt.
g0113: [2024-08-02 19:25:39,044] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1000 is ready now!
g0113:   successfully saved checkpoint at iteration    1000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: Checkpoint Save GB: 22.521, GB/Sec: 5.4, Latency(second): 4.171
g0133: (min, max) time across ranks (ms):
g0133:     save-checkpoint ................................: (4170.61, 4170.73)
g0128: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0128: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0128: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0128: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0113: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0113: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0132: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 19:25:42,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 19:25:42,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0129: [2024-08-02 19:25:42,980] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0130: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0130: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0113: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0132: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0129: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0113: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0133: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0132: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0132: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0131: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0129: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0125: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0125: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0133: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0129: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0133: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0131: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0125: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0131: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0125: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0125: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0131: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0132: [2024-08-02 19:25:42,981] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0133: [2024-08-02 19:25:42,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 4096 to 8192
g0113: [2024-08-02 19:26:20,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=1010, skipped=0, lr=[1.7633553066666668e-05, 1.7633553066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1010 loss: 3.2851 iter time (s): 4.086 samples/sec: 31.323
g0133:  iteration     1010/10000000 | consumed samples:       129280 | consumed tokens:    264765440 | elapsed time per iteration (ms): 42848.3 | learning rate: 1.763E-05 | global batch size:   128 | lm loss: 3.272668E+00 | loss scale: 8192.0 | grad norm: 3.747 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.987 | tokens per gpu per second (tgs): 191.186 | TFLOPs: 1.54 |
g0113: [2024-08-02 19:27:01,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=1020, skipped=0, lr=[1.7808315733333334e-05, 1.7808315733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1020 loss: 3.2302 iter time (s): 4.131 samples/sec: 30.984
g0133:  iteration     1020/10000000 | consumed samples:       130560 | consumed tokens:    267386880 | elapsed time per iteration (ms): 4164.1 | learning rate: 1.781E-05 | global batch size:   128 | lm loss: 3.241950E+00 | loss scale: 8192.0 | grad norm: 3.538 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.739 | tokens per gpu per second (tgs): 1967.289 | TFLOPs: 15.83 |
g0113: [2024-08-02 19:27:42,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=1030, skipped=0, lr=[1.79830784e-05, 1.79830784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1030 loss: 3.2581 iter time (s): 3.986 samples/sec: 32.115
g0133:  iteration     1030/10000000 | consumed samples:       131840 | consumed tokens:    270008320 | elapsed time per iteration (ms): 4019.1 | learning rate: 1.798E-05 | global batch size:   128 | lm loss: 3.247063E+00 | loss scale: 8192.0 | grad norm: 3.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.848 | tokens per gpu per second (tgs): 2038.292 | TFLOPs: 16.40 |
g0113: [2024-08-02 19:28:23,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=1040, skipped=0, lr=[1.8157841066666666e-05, 1.8157841066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1040 loss: 3.2641 iter time (s): 4.068 samples/sec: 31.464
g0133:  iteration     1040/10000000 | consumed samples:       133120 | consumed tokens:    272629760 | elapsed time per iteration (ms): 4100.8 | learning rate: 1.816E-05 | global batch size:   128 | lm loss: 3.221371E+00 | loss scale: 8192.0 | grad norm: 4.854 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.213 | tokens per gpu per second (tgs): 1997.663 | TFLOPs: 16.08 |
g0113: [2024-08-02 19:29:02,881] [INFO] [logging.py:96:log_dist] [Rank 0] step=1050, skipped=0, lr=[1.8332603733333336e-05, 1.8332603733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1050 loss: 3.2441 iter time (s): 3.951 samples/sec: 32.399
g0133:  iteration     1050/10000000 | consumed samples:       134400 | consumed tokens:    275251200 | elapsed time per iteration (ms): 3984.2 | learning rate: 1.833E-05 | global batch size:   128 | lm loss: 3.217858E+00 | loss scale: 8192.0 | grad norm: 2.923 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.127 | tokens per gpu per second (tgs): 2056.102 | TFLOPs: 16.55 |
g0113: [2024-08-02 19:29:45,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=1060, skipped=0, lr=[1.8507366400000002e-05, 1.8507366400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1060 loss: 3.1542 iter time (s): 4.201 samples/sec: 30.472
g0133:  iteration     1060/10000000 | consumed samples:       135680 | consumed tokens:    277872640 | elapsed time per iteration (ms): 4233.4 | learning rate: 1.851E-05 | global batch size:   128 | lm loss: 3.205622E+00 | loss scale: 8192.0 | grad norm: 3.053 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.235 | tokens per gpu per second (tgs): 1935.066 | TFLOPs: 15.57 |
g0113: [2024-08-02 19:30:26,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=1070, skipped=0, lr=[1.8682129066666668e-05, 1.8682129066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1070 loss: 3.1933 iter time (s): 4.050 samples/sec: 31.603
g0133:  iteration     1070/10000000 | consumed samples:       136960 | consumed tokens:    280494080 | elapsed time per iteration (ms): 4083.0 | learning rate: 1.868E-05 | global batch size:   128 | lm loss: 3.187814E+00 | loss scale: 8192.0 | grad norm: 3.104 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.350 | tokens per gpu per second (tgs): 2006.371 | TFLOPs: 16.15 |
g0113: [2024-08-02 19:31:07,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=1080, skipped=0, lr=[1.8856891733333334e-05, 1.8856891733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1080 loss: 3.1261 iter time (s): 4.102 samples/sec: 31.204
g0133:  iteration     1080/10000000 | consumed samples:       138240 | consumed tokens:    283115520 | elapsed time per iteration (ms): 4134.9 | learning rate: 1.886E-05 | global batch size:   128 | lm loss: 3.163754E+00 | loss scale: 8192.0 | grad norm: 3.134 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.956 | tokens per gpu per second (tgs): 1981.167 | TFLOPs: 15.94 |
g0113: [2024-08-02 19:31:49,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=1090, skipped=0, lr=[1.9031654400000003e-05, 1.9031654400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1090 loss: 3.0902 iter time (s): 4.160 samples/sec: 30.769
g0133:  iteration     1090/10000000 | consumed samples:       139520 | consumed tokens:    285736960 | elapsed time per iteration (ms): 4192.7 | learning rate: 1.903E-05 | global batch size:   128 | lm loss: 3.155130E+00 | loss scale: 8192.0 | grad norm: 2.628 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.529 | tokens per gpu per second (tgs): 1953.854 | TFLOPs: 15.72 |
g0113: [2024-08-02 19:32:29,891] [INFO] [logging.py:96:log_dist] [Rank 0] step=1100, skipped=0, lr=[1.920641706666667e-05, 1.920641706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1100 loss: 3.1136 iter time (s): 4.024 samples/sec: 31.809
g0133:  iteration     1100/10000000 | consumed samples:       140800 | consumed tokens:    288358400 | elapsed time per iteration (ms): 4056.8 | learning rate: 1.921E-05 | global batch size:   128 | lm loss: 3.119130E+00 | loss scale: 8192.0 | grad norm: 3.963 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.552 | tokens per gpu per second (tgs): 2019.319 | TFLOPs: 16.25 |
g0113: [2024-08-02 19:33:10,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=1110, skipped=0, lr=[1.9381179733333332e-05, 1.9381179733333332e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1110 loss: 3.0759 iter time (s): 4.009 samples/sec: 31.932
g0133:  iteration     1110/10000000 | consumed samples:       142080 | consumed tokens:    290979840 | elapsed time per iteration (ms): 4041.2 | learning rate: 1.938E-05 | global batch size:   128 | lm loss: 3.120665E+00 | loss scale: 8192.0 | grad norm: 2.829 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.674 | tokens per gpu per second (tgs): 2027.139 | TFLOPs: 16.31 |
g0113: [2024-08-02 19:33:52,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=1120, skipped=0, lr=[1.95559424e-05, 1.95559424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1120 loss: 3.1384 iter time (s): 4.184 samples/sec: 30.594
g0133:  iteration     1120/10000000 | consumed samples:       143360 | consumed tokens:    293601280 | elapsed time per iteration (ms): 4216.6 | learning rate: 1.956E-05 | global batch size:   128 | lm loss: 3.113073E+00 | loss scale: 8192.0 | grad norm: 3.694 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.356 | tokens per gpu per second (tgs): 1942.777 | TFLOPs: 15.63 |
g0113: [2024-08-02 19:34:33,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=1130, skipped=0, lr=[1.9730705066666668e-05, 1.9730705066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1130 loss: 3.0970 iter time (s): 4.032 samples/sec: 31.747
g0133:  iteration     1130/10000000 | consumed samples:       144640 | consumed tokens:    296222720 | elapsed time per iteration (ms): 4065.4 | learning rate: 1.973E-05 | global batch size:   128 | lm loss: 3.090784E+00 | loss scale: 8192.0 | grad norm: 3.386 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.486 | tokens per gpu per second (tgs): 2015.075 | TFLOPs: 16.22 |
g0113: [2024-08-02 19:35:14,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=1140, skipped=0, lr=[1.9905467733333334e-05, 1.9905467733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1140 loss: 3.0657 iter time (s): 4.109 samples/sec: 31.151
g0133:  iteration     1140/10000000 | consumed samples:       145920 | consumed tokens:    298844160 | elapsed time per iteration (ms): 4141.6 | learning rate: 1.991E-05 | global batch size:   128 | lm loss: 3.073753E+00 | loss scale: 8192.0 | grad norm: 3.148 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.906 | tokens per gpu per second (tgs): 1978.001 | TFLOPs: 15.92 |
g0113: [2024-08-02 19:35:55,304] [INFO] [logging.py:96:log_dist] [Rank 0] step=1150, skipped=0, lr=[2.00802304e-05, 2.00802304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1150 loss: 3.1170 iter time (s): 4.044 samples/sec: 31.655
g0133:  iteration     1150/10000000 | consumed samples:       147200 | consumed tokens:    301465600 | elapsed time per iteration (ms): 4076.6 | learning rate: 2.008E-05 | global batch size:   128 | lm loss: 3.068083E+00 | loss scale: 8192.0 | grad norm: 3.116 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.399 | tokens per gpu per second (tgs): 2009.532 | TFLOPs: 16.17 |
g0113: [2024-08-02 19:36:35,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=1160, skipped=0, lr=[2.0254993066666666e-05, 2.0254993066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1160 loss: 3.0286 iter time (s): 4.036 samples/sec: 31.717
g0133:  iteration     1160/10000000 | consumed samples:       148480 | consumed tokens:    304087040 | elapsed time per iteration (ms): 4068.5 | learning rate: 2.025E-05 | global batch size:   128 | lm loss: 3.049184E+00 | loss scale: 8192.0 | grad norm: 2.827 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.461 | tokens per gpu per second (tgs): 2013.529 | TFLOPs: 16.20 |
g0113: [2024-08-02 19:37:16,538] [INFO] [logging.py:96:log_dist] [Rank 0] step=1170, skipped=0, lr=[2.0429755733333335e-05, 2.0429755733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1170 loss: 3.0173 iter time (s): 4.022 samples/sec: 31.824
g0133:  iteration     1170/10000000 | consumed samples:       149760 | consumed tokens:    306708480 | elapsed time per iteration (ms): 4054.9 | learning rate: 2.043E-05 | global batch size:   128 | lm loss: 3.031455E+00 | loss scale: 8192.0 | grad norm: 3.403 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.567 | tokens per gpu per second (tgs): 2020.275 | TFLOPs: 16.26 |
g0113: [2024-08-02 19:37:58,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=1180, skipped=0, lr=[2.06045184e-05, 2.06045184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1180 loss: 3.0193 iter time (s): 4.157 samples/sec: 30.789
g0133:  iteration     1180/10000000 | consumed samples:       151040 | consumed tokens:    309329920 | elapsed time per iteration (ms): 4190.3 | learning rate: 2.060E-05 | global batch size:   128 | lm loss: 3.026027E+00 | loss scale: 8192.0 | grad norm: 4.025 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.547 | tokens per gpu per second (tgs): 1955.010 | TFLOPs: 15.73 |
g0113: [2024-08-02 19:38:39,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=1190, skipped=0, lr=[2.0779281066666667e-05, 2.0779281066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1190 loss: 3.0311 iter time (s): 4.045 samples/sec: 31.646
g0133:  iteration     1190/10000000 | consumed samples:       152320 | consumed tokens:    311951360 | elapsed time per iteration (ms): 4078.0 | learning rate: 2.078E-05 | global batch size:   128 | lm loss: 3.020082E+00 | loss scale: 8192.0 | grad norm: 2.511 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.388 | tokens per gpu per second (tgs): 2008.804 | TFLOPs: 16.17 |
g0113: [2024-08-02 19:39:20,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=1200, skipped=0, lr=[2.0954043733333333e-05, 2.0954043733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1200 loss: 2.9618 iter time (s): 4.078 samples/sec: 31.384
g0133:  iteration     1200/10000000 | consumed samples:       153600 | consumed tokens:    314572800 | elapsed time per iteration (ms): 4111.7 | learning rate: 2.095E-05 | global batch size:   128 | lm loss: 2.994534E+00 | loss scale: 8192.0 | grad norm: 3.050 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.131 | tokens per gpu per second (tgs): 1992.356 | TFLOPs: 16.03 |
g0113: [2024-08-02 19:40:02,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=1210, skipped=0, lr=[2.1128806400000003e-05, 2.1128806400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1210 loss: 2.9331 iter time (s): 4.195 samples/sec: 30.513
g0133:  iteration     1210/10000000 | consumed samples:       154880 | consumed tokens:    317194240 | elapsed time per iteration (ms): 4228.0 | learning rate: 2.113E-05 | global batch size:   128 | lm loss: 3.002469E+00 | loss scale: 8192.0 | grad norm: 2.853 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.274 | tokens per gpu per second (tgs): 1937.541 | TFLOPs: 15.59 |
g0113: [2024-08-02 19:40:44,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=1220, skipped=0, lr=[2.130356906666667e-05, 2.130356906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1220 loss: 3.0232 iter time (s): 4.165 samples/sec: 30.730
g0133:  iteration     1220/10000000 | consumed samples:       156160 | consumed tokens:    319815680 | elapsed time per iteration (ms): 4198.2 | learning rate: 2.130E-05 | global batch size:   128 | lm loss: 2.981393E+00 | loss scale: 8192.0 | grad norm: 2.769 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.490 | tokens per gpu per second (tgs): 1951.332 | TFLOPs: 15.70 |
g0113: [2024-08-02 19:41:25,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=1230, skipped=0, lr=[2.1478331733333335e-05, 2.1478331733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1230 loss: 2.9307 iter time (s): 4.086 samples/sec: 31.326
g0133:  iteration     1230/10000000 | consumed samples:       157440 | consumed tokens:    322437120 | elapsed time per iteration (ms): 4118.9 | learning rate: 2.148E-05 | global batch size:   128 | lm loss: 2.950779E+00 | loss scale: 8192.0 | grad norm: 3.124 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.076 | tokens per gpu per second (tgs): 1988.861 | TFLOPs: 16.00 |
g0113: [2024-08-02 19:42:06,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=1240, skipped=0, lr=[2.16530944e-05, 2.16530944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1240 loss: 2.9894 iter time (s): 4.009 samples/sec: 31.927
g0133:  iteration     1240/10000000 | consumed samples:       158720 | consumed tokens:    325058560 | elapsed time per iteration (ms): 4042.1 | learning rate: 2.165E-05 | global batch size:   128 | lm loss: 2.954172E+00 | loss scale: 8192.0 | grad norm: 2.730 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.667 | tokens per gpu per second (tgs): 2026.690 | TFLOPs: 16.31 |
g0113: [2024-08-02 19:42:48,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=1250, skipped=0, lr=[2.1827857066666667e-05, 2.1827857066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1250 loss: 2.9199 iter time (s): 4.201 samples/sec: 30.472
g0133:  iteration     1250/10000000 | consumed samples:       160000 | consumed tokens:    327680000 | elapsed time per iteration (ms): 4233.2 | learning rate: 2.183E-05 | global batch size:   128 | lm loss: 2.939906E+00 | loss scale: 8192.0 | grad norm: 2.837 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.237 | tokens per gpu per second (tgs): 1935.160 | TFLOPs: 15.57 |
g0113: [2024-08-02 19:43:30,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=1260, skipped=0, lr=[2.2002619733333337e-05, 2.2002619733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1260 loss: 2.9279 iter time (s): 4.179 samples/sec: 30.628
g0133:  iteration     1260/10000000 | consumed samples:       161280 | consumed tokens:    330301440 | elapsed time per iteration (ms): 4213.9 | learning rate: 2.200E-05 | global batch size:   128 | lm loss: 2.927868E+00 | loss scale: 8192.0 | grad norm: 2.615 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.375 | tokens per gpu per second (tgs): 1944.028 | TFLOPs: 15.64 |
g0113: [2024-08-02 19:44:11,866] [INFO] [logging.py:96:log_dist] [Rank 0] step=1270, skipped=0, lr=[2.2177382400000003e-05, 2.2177382400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1270 loss: 2.8858 iter time (s): 4.086 samples/sec: 31.330
g0133:  iteration     1270/10000000 | consumed samples:       162560 | consumed tokens:    332922880 | elapsed time per iteration (ms): 4118.2 | learning rate: 2.218E-05 | global batch size:   128 | lm loss: 2.921478E+00 | loss scale: 8192.0 | grad norm: 2.593 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.082 | tokens per gpu per second (tgs): 1989.232 | TFLOPs: 16.01 |
g0113: [2024-08-02 19:44:54,754] [INFO] [logging.py:96:log_dist] [Rank 0] step=1280, skipped=0, lr=[2.235214506666667e-05, 2.235214506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1280 loss: 2.9397 iter time (s): 4.213 samples/sec: 30.381
g0133:  iteration     1280/10000000 | consumed samples:       163840 | consumed tokens:    335544320 | elapsed time per iteration (ms): 4288.8 | learning rate: 2.235E-05 | global batch size:   128 | lm loss: 2.911605E+00 | loss scale: 8192.0 | grad norm: 2.973 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.845 | tokens per gpu per second (tgs): 1910.108 | TFLOPs: 15.37 |
g0113: [2024-08-02 19:45:36,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=1290, skipped=0, lr=[2.2526907733333335e-05, 2.2526907733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1290 loss: 2.8998 iter time (s): 4.108 samples/sec: 31.161
g0133:  iteration     1290/10000000 | consumed samples:       165120 | consumed tokens:    338165760 | elapsed time per iteration (ms): 4140.3 | learning rate: 2.253E-05 | global batch size:   128 | lm loss: 2.899323E+00 | loss scale: 8192.0 | grad norm: 2.988 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.916 | tokens per gpu per second (tgs): 1978.597 | TFLOPs: 15.92 |
g0113: [2024-08-02 19:46:18,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=1300, skipped=0, lr=[2.2701670400000004e-05, 2.2701670400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1300 loss: 2.8497 iter time (s): 4.178 samples/sec: 30.639
g0133:  iteration     1300/10000000 | consumed samples:       166400 | consumed tokens:    340787200 | elapsed time per iteration (ms): 4210.4 | learning rate: 2.270E-05 | global batch size:   128 | lm loss: 2.883459E+00 | loss scale: 8192.0 | grad norm: 3.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.401 | tokens per gpu per second (tgs): 1945.667 | TFLOPs: 15.66 |
g0113: [2024-08-02 19:46:58,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=1310, skipped=0, lr=[2.287643306666667e-05, 2.287643306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1310 loss: 2.8264 iter time (s): 4.012 samples/sec: 31.906
g0133:  iteration     1310/10000000 | consumed samples:       167680 | consumed tokens:    343408640 | elapsed time per iteration (ms): 4046.6 | learning rate: 2.288E-05 | global batch size:   128 | lm loss: 2.865389E+00 | loss scale: 8192.0 | grad norm: 2.530 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.631 | tokens per gpu per second (tgs): 2024.395 | TFLOPs: 16.29 |
g0113: [2024-08-02 19:47:40,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=1320, skipped=0, lr=[2.3051195733333336e-05, 2.3051195733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1320 loss: 2.8499 iter time (s): 4.121 samples/sec: 31.064
g0133:  iteration     1320/10000000 | consumed samples:       168960 | consumed tokens:    346030080 | elapsed time per iteration (ms): 4153.4 | learning rate: 2.305E-05 | global batch size:   128 | lm loss: 2.875957E+00 | loss scale: 8192.0 | grad norm: 2.700 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.818 | tokens per gpu per second (tgs): 1972.369 | TFLOPs: 15.87 |
g0113: [2024-08-02 19:48:22,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=1330, skipped=0, lr=[2.3225958400000002e-05, 2.3225958400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1330 loss: 2.8096 iter time (s): 4.224 samples/sec: 30.304
g0133:  iteration     1330/10000000 | consumed samples:       170240 | consumed tokens:    348651520 | elapsed time per iteration (ms): 4257.1 | learning rate: 2.323E-05 | global batch size:   128 | lm loss: 2.863729E+00 | loss scale: 8192.0 | grad norm: 2.670 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.067 | tokens per gpu per second (tgs): 1924.307 | TFLOPs: 15.49 |
g0113: [2024-08-02 19:49:04,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=1340, skipped=0, lr=[2.340072106666667e-05, 2.340072106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1340 loss: 2.8300 iter time (s): 4.158 samples/sec: 30.785
g0133:  iteration     1340/10000000 | consumed samples:       171520 | consumed tokens:    351272960 | elapsed time per iteration (ms): 4192.0 | learning rate: 2.340E-05 | global batch size:   128 | lm loss: 2.843992E+00 | loss scale: 8192.0 | grad norm: 3.030 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.534 | tokens per gpu per second (tgs): 1954.178 | TFLOPs: 15.73 |
g0113: [2024-08-02 19:49:46,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=1350, skipped=0, lr=[2.3575483733333338e-05, 2.3575483733333338e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1350 loss: 2.8199 iter time (s): 4.163 samples/sec: 30.750
g0133:  iteration     1350/10000000 | consumed samples:       172800 | consumed tokens:    353894400 | elapsed time per iteration (ms): 4195.7 | learning rate: 2.358E-05 | global batch size:   128 | lm loss: 2.857001E+00 | loss scale: 8192.0 | grad norm: 2.552 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.508 | tokens per gpu per second (tgs): 1952.481 | TFLOPs: 15.71 |
g0113: [2024-08-02 19:50:27,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=1360, skipped=0, lr=[2.3750246399999997e-05, 2.3750246399999997e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1360 loss: 2.8264 iter time (s): 4.021 samples/sec: 31.832
g0133:  iteration     1360/10000000 | consumed samples:       174080 | consumed tokens:    356515840 | elapsed time per iteration (ms): 4054.0 | learning rate: 2.375E-05 | global batch size:   128 | lm loss: 2.826769E+00 | loss scale: 8192.0 | grad norm: 2.638 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.574 | tokens per gpu per second (tgs): 2020.708 | TFLOPs: 16.26 |
g0113: [2024-08-02 19:51:07,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=1370, skipped=0, lr=[2.3925009066666667e-05, 2.3925009066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1370 loss: 2.8124 iter time (s): 4.013 samples/sec: 31.895
g0133:  iteration     1370/10000000 | consumed samples:       175360 | consumed tokens:    359137280 | elapsed time per iteration (ms): 4045.8 | learning rate: 2.393E-05 | global batch size:   128 | lm loss: 2.831371E+00 | loss scale: 8192.0 | grad norm: 2.620 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.638 | tokens per gpu per second (tgs): 2024.810 | TFLOPs: 16.29 |
g0113: [2024-08-02 19:51:49,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=1380, skipped=0, lr=[2.4099771733333333e-05, 2.4099771733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1380 loss: 2.8070 iter time (s): 4.116 samples/sec: 31.097
g0133:  iteration     1380/10000000 | consumed samples:       176640 | consumed tokens:    361758720 | elapsed time per iteration (ms): 4148.7 | learning rate: 2.410E-05 | global batch size:   128 | lm loss: 2.814266E+00 | loss scale: 8192.0 | grad norm: 2.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.853 | tokens per gpu per second (tgs): 1974.576 | TFLOPs: 15.89 |
g0113: [2024-08-02 19:52:31,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=1390, skipped=0, lr=[2.42745344e-05, 2.42745344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1390 loss: 2.8521 iter time (s): 4.152 samples/sec: 30.826
g0133:  iteration     1390/10000000 | consumed samples:       177920 | consumed tokens:    364380160 | elapsed time per iteration (ms): 4185.1 | learning rate: 2.427E-05 | global batch size:   128 | lm loss: 2.825078E+00 | loss scale: 8192.0 | grad norm: 2.413 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.585 | tokens per gpu per second (tgs): 1957.441 | TFLOPs: 15.75 |
g0113: [2024-08-02 19:53:12,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=1400, skipped=0, lr=[2.4449297066666665e-05, 2.4449297066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1400 loss: 2.7782 iter time (s): 4.101 samples/sec: 31.208
g0133:  iteration     1400/10000000 | consumed samples:       179200 | consumed tokens:    367001600 | elapsed time per iteration (ms): 4134.0 | learning rate: 2.445E-05 | global batch size:   128 | lm loss: 2.802770E+00 | loss scale: 8192.0 | grad norm: 3.365 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.963 | tokens per gpu per second (tgs): 1981.632 | TFLOPs: 15.95 |
g0113: [2024-08-02 19:53:52,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=1410, skipped=0, lr=[2.4624059733333334e-05, 2.4624059733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1410 loss: 2.7687 iter time (s): 3.993 samples/sec: 32.060
g0133:  iteration     1410/10000000 | consumed samples:       180480 | consumed tokens:    369623040 | elapsed time per iteration (ms): 4025.3 | learning rate: 2.462E-05 | global batch size:   128 | lm loss: 2.801736E+00 | loss scale: 8192.0 | grad norm: 2.917 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.799 | tokens per gpu per second (tgs): 2035.111 | TFLOPs: 16.38 |
g0113: [2024-08-02 19:54:34,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=1420, skipped=0, lr=[2.47988224e-05, 2.47988224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1420 loss: 2.7762 iter time (s): 4.147 samples/sec: 30.867
g0133:  iteration     1420/10000000 | consumed samples:       181760 | consumed tokens:    372244480 | elapsed time per iteration (ms): 4180.9 | learning rate: 2.480E-05 | global batch size:   128 | lm loss: 2.793497E+00 | loss scale: 8192.0 | grad norm: 2.533 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.615 | tokens per gpu per second (tgs): 1959.382 | TFLOPs: 15.77 |
g0113: [2024-08-02 19:55:15,798] [INFO] [logging.py:96:log_dist] [Rank 0] step=1430, skipped=0, lr=[2.4973585066666666e-05, 2.4973585066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1430 loss: 2.7889 iter time (s): 4.102 samples/sec: 31.206
g0133:  iteration     1430/10000000 | consumed samples:       183040 | consumed tokens:    374865920 | elapsed time per iteration (ms): 4134.8 | learning rate: 2.497E-05 | global batch size:   128 | lm loss: 2.767985E+00 | loss scale: 8192.0 | grad norm: 2.644 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.956 | tokens per gpu per second (tgs): 1981.215 | TFLOPs: 15.94 |
g0113: [2024-08-02 19:55:57,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=1440, skipped=0, lr=[2.5148347733333333e-05, 2.5148347733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1440 loss: 2.7805 iter time (s): 4.176 samples/sec: 30.654
g0133:  iteration     1440/10000000 | consumed samples:       184320 | consumed tokens:    377487360 | elapsed time per iteration (ms): 4208.5 | learning rate: 2.515E-05 | global batch size:   128 | lm loss: 2.784637E+00 | loss scale: 8192.0 | grad norm: 2.373 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.415 | tokens per gpu per second (tgs): 1946.530 | TFLOPs: 15.66 |
g0113: [2024-08-02 19:56:39,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=1450, skipped=0, lr=[2.53231104e-05, 2.53231104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1450 loss: 2.7401 iter time (s): 4.089 samples/sec: 31.306
g0133:  iteration     1450/10000000 | consumed samples:       185600 | consumed tokens:    380108800 | elapsed time per iteration (ms): 4121.9 | learning rate: 2.532E-05 | global batch size:   128 | lm loss: 2.765022E+00 | loss scale: 8192.0 | grad norm: 2.475 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.053 | tokens per gpu per second (tgs): 1987.414 | TFLOPs: 15.99 |
g0113: [2024-08-02 19:57:20,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=1460, skipped=0, lr=[2.5497873066666668e-05, 2.5497873066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1460 loss: 2.7816 iter time (s): 4.092 samples/sec: 31.280
g0133:  iteration     1460/10000000 | consumed samples:       186880 | consumed tokens:    382730240 | elapsed time per iteration (ms): 4125.1 | learning rate: 2.550E-05 | global batch size:   128 | lm loss: 2.759162E+00 | loss scale: 8192.0 | grad norm: 2.440 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.030 | tokens per gpu per second (tgs): 1985.894 | TFLOPs: 15.98 |
g0113: [2024-08-02 19:58:02,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=1470, skipped=0, lr=[2.5672635733333334e-05, 2.5672635733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1470 loss: 2.7262 iter time (s): 4.137 samples/sec: 30.941
g0133:  iteration     1470/10000000 | consumed samples:       188160 | consumed tokens:    385351680 | elapsed time per iteration (ms): 4170.2 | learning rate: 2.567E-05 | global batch size:   128 | lm loss: 2.761298E+00 | loss scale: 8192.0 | grad norm: 2.702 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.694 | tokens per gpu per second (tgs): 1964.423 | TFLOPs: 15.81 |
g0113: [2024-08-02 19:58:43,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=1480, skipped=0, lr=[2.58473984e-05, 2.58473984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1480 loss: 2.7387 iter time (s): 4.103 samples/sec: 31.196
g0133:  iteration     1480/10000000 | consumed samples:       189440 | consumed tokens:    387973120 | elapsed time per iteration (ms): 4136.1 | learning rate: 2.585E-05 | global batch size:   128 | lm loss: 2.748892E+00 | loss scale: 8192.0 | grad norm: 2.581 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.947 | tokens per gpu per second (tgs): 1980.591 | TFLOPs: 15.94 |
g0113: [2024-08-02 19:59:25,027] [INFO] [logging.py:96:log_dist] [Rank 0] step=1490, skipped=0, lr=[2.6022161066666666e-05, 2.6022161066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1490 loss: 2.7582 iter time (s): 4.128 samples/sec: 31.006
g0133:  iteration     1490/10000000 | consumed samples:       190720 | consumed tokens:    390594560 | elapsed time per iteration (ms): 4161.0 | learning rate: 2.602E-05 | global batch size:   128 | lm loss: 2.741686E+00 | loss scale: 8192.0 | grad norm: 2.542 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.762 | tokens per gpu per second (tgs): 1968.765 | TFLOPs: 15.84 |
g0113: [2024-08-02 20:00:06,084] [INFO] [logging.py:96:log_dist] [Rank 0] step=1500, skipped=0, lr=[2.6196923733333336e-05, 2.6196923733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1500 loss: 2.7408 iter time (s): 4.073 samples/sec: 31.426
g0133:  iteration     1500/10000000 | consumed samples:       192000 | consumed tokens:    393216000 | elapsed time per iteration (ms): 4105.7 | learning rate: 2.620E-05 | global batch size:   128 | lm loss: 2.739228E+00 | loss scale: 8192.0 | grad norm: 2.292 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.176 | tokens per gpu per second (tgs): 1995.278 | TFLOPs: 16.06 |
g0128: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0128: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0113: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0130: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0130: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0130: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0130: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0132: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0128: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0132: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0131: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0132: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0131: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0131: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0133: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0113: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0125: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 20:00:10,267] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0113: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0129: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 20:00:10,267] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0133: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0129: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0113: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0113: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0133: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0125: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0129: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0129: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0129: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0125: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0132: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0131: [2024-08-02 20:00:10,268] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0133: [2024-08-02 20:00:10,269] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 8192 to 16384
g0113: [2024-08-02 20:00:48,512] [INFO] [logging.py:96:log_dist] [Rank 0] step=1510, skipped=0, lr=[2.6371686400000002e-05, 2.6371686400000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1510 loss: 2.7379 iter time (s): 4.210 samples/sec: 30.401
g0133:  iteration     1510/10000000 | consumed samples:       193280 | consumed tokens:    395837440 | elapsed time per iteration (ms): 4242.8 | learning rate: 2.637E-05 | global batch size:   128 | lm loss: 2.717218E+00 | loss scale: 16384.0 | grad norm: 2.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.169 | tokens per gpu per second (tgs): 1930.818 | TFLOPs: 15.54 |
g0113: [2024-08-02 20:01:30,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=1520, skipped=0, lr=[2.6546449066666668e-05, 2.6546449066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1520 loss: 2.7043 iter time (s): 4.204 samples/sec: 30.444
g0133:  iteration     1520/10000000 | consumed samples:       194560 | consumed tokens:    398458880 | elapsed time per iteration (ms): 4236.7 | learning rate: 2.655E-05 | global batch size:   128 | lm loss: 2.730711E+00 | loss scale: 16384.0 | grad norm: 2.322 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.212 | tokens per gpu per second (tgs): 1933.564 | TFLOPs: 15.56 |
g0113: [2024-08-02 20:02:11,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=1530, skipped=0, lr=[2.6721211733333334e-05, 2.6721211733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1530 loss: 2.7433 iter time (s): 4.076 samples/sec: 31.401
g0133:  iteration     1530/10000000 | consumed samples:       195840 | consumed tokens:    401080320 | elapsed time per iteration (ms): 4109.5 | learning rate: 2.672E-05 | global batch size:   128 | lm loss: 2.737795E+00 | loss scale: 16384.0 | grad norm: 3.262 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.147 | tokens per gpu per second (tgs): 1993.424 | TFLOPs: 16.04 |
g0113: [2024-08-02 20:02:53,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=1540, skipped=0, lr=[2.68959744e-05, 2.68959744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1540 loss: 2.7135 iter time (s): 4.162 samples/sec: 30.754
g0133:  iteration     1540/10000000 | consumed samples:       197120 | consumed tokens:    403701760 | elapsed time per iteration (ms): 4194.7 | learning rate: 2.690E-05 | global batch size:   128 | lm loss: 2.715440E+00 | loss scale: 16384.0 | grad norm: 2.094 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.514 | tokens per gpu per second (tgs): 1952.920 | TFLOPs: 15.72 |
g0113: [2024-08-02 20:03:34,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=1550, skipped=0, lr=[2.707073706666667e-05, 2.707073706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1550 loss: 2.6996 iter time (s): 4.074 samples/sec: 31.419
g0133:  iteration     1550/10000000 | consumed samples:       198400 | consumed tokens:    406323200 | elapsed time per iteration (ms): 4107.2 | learning rate: 2.707E-05 | global batch size:   128 | lm loss: 2.695505E+00 | loss scale: 16384.0 | grad norm: 2.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.165 | tokens per gpu per second (tgs): 1994.561 | TFLOPs: 16.05 |
g0113: [2024-08-02 20:04:16,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=1560, skipped=0, lr=[2.7245499733333335e-05, 2.7245499733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1560 loss: 2.6661 iter time (s): 4.146 samples/sec: 30.875
g0133:  iteration     1560/10000000 | consumed samples:       199680 | consumed tokens:    408944640 | elapsed time per iteration (ms): 4178.9 | learning rate: 2.725E-05 | global batch size:   128 | lm loss: 2.692522E+00 | loss scale: 16384.0 | grad norm: 2.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.630 | tokens per gpu per second (tgs): 1960.339 | TFLOPs: 15.78 |
g0113: [2024-08-02 20:04:58,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=1570, skipped=0, lr=[2.74202624e-05, 2.74202624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1570 loss: 2.6724 iter time (s): 4.108 samples/sec: 31.161
g0133:  iteration     1570/10000000 | consumed samples:       200960 | consumed tokens:    411566080 | elapsed time per iteration (ms): 4141.2 | learning rate: 2.742E-05 | global batch size:   128 | lm loss: 2.693197E+00 | loss scale: 16384.0 | grad norm: 2.304 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.909 | tokens per gpu per second (tgs): 1978.168 | TFLOPs: 15.92 |
g0113: [2024-08-02 20:05:38,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=1580, skipped=0, lr=[2.7595025066666668e-05, 2.7595025066666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1580 loss: 2.7016 iter time (s): 4.016 samples/sec: 31.874
g0133:  iteration     1580/10000000 | consumed samples:       202240 | consumed tokens:    414187520 | elapsed time per iteration (ms): 4049.4 | learning rate: 2.760E-05 | global batch size:   128 | lm loss: 2.692015E+00 | loss scale: 16384.0 | grad norm: 2.524 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.609 | tokens per gpu per second (tgs): 2023.008 | TFLOPs: 16.28 |
g0113: [2024-08-02 20:06:20,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=1590, skipped=0, lr=[2.7769787733333337e-05, 2.7769787733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1590 loss: 2.7101 iter time (s): 4.142 samples/sec: 30.902
g0133:  iteration     1590/10000000 | consumed samples:       203520 | consumed tokens:    416808960 | elapsed time per iteration (ms): 4174.9 | learning rate: 2.777E-05 | global batch size:   128 | lm loss: 2.681550E+00 | loss scale: 16384.0 | grad norm: 2.563 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.660 | tokens per gpu per second (tgs): 1962.216 | TFLOPs: 15.79 |
g0113: [2024-08-02 20:07:01,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=1600, skipped=0, lr=[2.7944550400000003e-05, 2.7944550400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1600 loss: 2.6575 iter time (s): 4.100 samples/sec: 31.218
g0133:  iteration     1600/10000000 | consumed samples:       204800 | consumed tokens:    419430400 | elapsed time per iteration (ms): 4132.6 | learning rate: 2.794E-05 | global batch size:   128 | lm loss: 2.676865E+00 | loss scale: 16384.0 | grad norm: 2.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.973 | tokens per gpu per second (tgs): 1982.268 | TFLOPs: 15.95 |
g0113: [2024-08-02 20:07:42,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=1610, skipped=0, lr=[2.811931306666667e-05, 2.811931306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1610 loss: 2.6478 iter time (s): 4.080 samples/sec: 31.376
g0133:  iteration     1610/10000000 | consumed samples:       206080 | consumed tokens:    422051840 | elapsed time per iteration (ms): 4112.6 | learning rate: 2.812E-05 | global batch size:   128 | lm loss: 2.658860E+00 | loss scale: 16384.0 | grad norm: 2.143 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.124 | tokens per gpu per second (tgs): 1991.932 | TFLOPs: 16.03 |
g0113: [2024-08-02 20:08:24,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=1620, skipped=0, lr=[2.8294075733333335e-05, 2.8294075733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1620 loss: 2.6773 iter time (s): 4.158 samples/sec: 30.786
g0133:  iteration     1620/10000000 | consumed samples:       207360 | consumed tokens:    424673280 | elapsed time per iteration (ms): 4190.5 | learning rate: 2.829E-05 | global batch size:   128 | lm loss: 2.652123E+00 | loss scale: 16384.0 | grad norm: 1.954 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.545 | tokens per gpu per second (tgs): 1954.886 | TFLOPs: 15.73 |
g0113: [2024-08-02 20:09:06,215] [INFO] [logging.py:96:log_dist] [Rank 0] step=1630, skipped=0, lr=[2.84688384e-05, 2.84688384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1630 loss: 2.6529 iter time (s): 4.110 samples/sec: 31.147
g0133:  iteration     1630/10000000 | consumed samples:       208640 | consumed tokens:    427294720 | elapsed time per iteration (ms): 4141.9 | learning rate: 2.847E-05 | global batch size:   128 | lm loss: 2.658649E+00 | loss scale: 16384.0 | grad norm: 2.267 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.903 | tokens per gpu per second (tgs): 1977.815 | TFLOPs: 15.92 |
g0113: [2024-08-02 20:09:47,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=1640, skipped=0, lr=[2.864360106666667e-05, 2.864360106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1640 loss: 2.6403 iter time (s): 4.101 samples/sec: 31.209
g0133:  iteration     1640/10000000 | consumed samples:       209920 | consumed tokens:    429916160 | elapsed time per iteration (ms): 4133.8 | learning rate: 2.864E-05 | global batch size:   128 | lm loss: 2.658726E+00 | loss scale: 16384.0 | grad norm: 2.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.964 | tokens per gpu per second (tgs): 1981.709 | TFLOPs: 15.95 |
g0113: [2024-08-02 20:10:28,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=1650, skipped=0, lr=[2.8818363733333337e-05, 2.8818363733333337e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1650 loss: 2.6079 iter time (s): 4.077 samples/sec: 31.396
g0133:  iteration     1650/10000000 | consumed samples:       211200 | consumed tokens:    432537600 | elapsed time per iteration (ms): 4110.6 | learning rate: 2.882E-05 | global batch size:   128 | lm loss: 2.648797E+00 | loss scale: 16384.0 | grad norm: 2.057 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.139 | tokens per gpu per second (tgs): 1992.893 | TFLOPs: 16.04 |
g0113: [2024-08-02 20:11:10,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=1660, skipped=0, lr=[2.8993126400000003e-05, 2.8993126400000003e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1660 loss: 2.6004 iter time (s): 4.176 samples/sec: 30.653
g0133:  iteration     1660/10000000 | consumed samples:       212480 | consumed tokens:    435159040 | elapsed time per iteration (ms): 4209.1 | learning rate: 2.899E-05 | global batch size:   128 | lm loss: 2.627472E+00 | loss scale: 16384.0 | grad norm: 2.281 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.410 | tokens per gpu per second (tgs): 1946.242 | TFLOPs: 15.66 |
g0113: [2024-08-02 20:11:52,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=1670, skipped=0, lr=[2.916788906666667e-05, 2.916788906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1670 loss: 2.6260 iter time (s): 4.147 samples/sec: 30.864
g0133:  iteration     1670/10000000 | consumed samples:       213760 | consumed tokens:    437780480 | elapsed time per iteration (ms): 4180.1 | learning rate: 2.917E-05 | global batch size:   128 | lm loss: 2.624346E+00 | loss scale: 16384.0 | grad norm: 2.004 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.622 | tokens per gpu per second (tgs): 1959.780 | TFLOPs: 15.77 |
g0113: [2024-08-02 20:12:32,789] [INFO] [logging.py:96:log_dist] [Rank 0] step=1680, skipped=0, lr=[2.934265173333334e-05, 2.934265173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1680 loss: 2.6126 iter time (s): 3.991 samples/sec: 32.070
g0133:  iteration     1680/10000000 | consumed samples:       215040 | consumed tokens:    440401920 | elapsed time per iteration (ms): 4023.7 | learning rate: 2.934E-05 | global batch size:   128 | lm loss: 2.628204E+00 | loss scale: 16384.0 | grad norm: 2.136 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.811 | tokens per gpu per second (tgs): 2035.912 | TFLOPs: 16.38 |
g0113: [2024-08-02 20:13:13,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=1690, skipped=0, lr=[2.9517414399999998e-05, 2.9517414399999998e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1690 loss: 2.6095 iter time (s): 4.000 samples/sec: 32.003
g0133:  iteration     1690/10000000 | consumed samples:       216320 | consumed tokens:    443023360 | elapsed time per iteration (ms): 4033.7 | learning rate: 2.952E-05 | global batch size:   128 | lm loss: 2.609161E+00 | loss scale: 16384.0 | grad norm: 2.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.733 | tokens per gpu per second (tgs): 2030.908 | TFLOPs: 16.34 |
g0113: [2024-08-02 20:13:53,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=1700, skipped=0, lr=[2.9692177066666667e-05, 2.9692177066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1700 loss: 2.5714 iter time (s): 4.046 samples/sec: 31.634
g0133:  iteration     1700/10000000 | consumed samples:       217600 | consumed tokens:    445644800 | elapsed time per iteration (ms): 4078.9 | learning rate: 2.969E-05 | global batch size:   128 | lm loss: 2.603936E+00 | loss scale: 16384.0 | grad norm: 2.078 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.381 | tokens per gpu per second (tgs): 2008.371 | TFLOPs: 16.16 |
g0113: [2024-08-02 20:14:35,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=1710, skipped=0, lr=[2.9866939733333333e-05, 2.9866939733333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1710 loss: 2.6278 iter time (s): 4.091 samples/sec: 31.285
g0133:  iteration     1710/10000000 | consumed samples:       218880 | consumed tokens:    448266240 | elapsed time per iteration (ms): 4124.8 | learning rate: 2.987E-05 | global batch size:   128 | lm loss: 2.608982E+00 | loss scale: 16384.0 | grad norm: 2.331 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.032 | tokens per gpu per second (tgs): 1986.027 | TFLOPs: 15.98 |
g0113: [2024-08-02 20:15:15,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=1720, skipped=0, lr=[3.00417024e-05, 3.00417024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1720 loss: 2.6190 iter time (s): 3.986 samples/sec: 32.115
g0133:  iteration     1720/10000000 | consumed samples:       220160 | consumed tokens:    450887680 | elapsed time per iteration (ms): 4018.5 | learning rate: 3.004E-05 | global batch size:   128 | lm loss: 2.602402E+00 | loss scale: 16384.0 | grad norm: 2.043 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.853 | tokens per gpu per second (tgs): 2038.583 | TFLOPs: 16.40 |
g0113: [2024-08-02 20:15:57,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=1730, skipped=0, lr=[3.0216465066666665e-05, 3.0216465066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1730 loss: 2.6584 iter time (s): 4.183 samples/sec: 30.597
g0133:  iteration     1730/10000000 | consumed samples:       221440 | consumed tokens:    453509120 | elapsed time per iteration (ms): 4216.2 | learning rate: 3.022E-05 | global batch size:   128 | lm loss: 2.622746E+00 | loss scale: 16384.0 | grad norm: 2.067 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.359 | tokens per gpu per second (tgs): 1942.993 | TFLOPs: 15.64 |
g0113: [2024-08-02 20:16:39,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=1740, skipped=0, lr=[3.0391227733333335e-05, 3.0391227733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1740 loss: 2.5796 iter time (s): 4.134 samples/sec: 30.959
g0133:  iteration     1740/10000000 | consumed samples:       222720 | consumed tokens:    456130560 | elapsed time per iteration (ms): 4167.8 | learning rate: 3.039E-05 | global batch size:   128 | lm loss: 2.606115E+00 | loss scale: 16384.0 | grad norm: 1.973 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.712 | tokens per gpu per second (tgs): 1965.549 | TFLOPs: 15.82 |
g0113: [2024-08-02 20:17:20,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=1750, skipped=0, lr=[3.05659904e-05, 3.05659904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1750 loss: 2.5652 iter time (s): 4.063 samples/sec: 31.501
g0133:  iteration     1750/10000000 | consumed samples:       224000 | consumed tokens:    458752000 | elapsed time per iteration (ms): 4096.2 | learning rate: 3.057E-05 | global batch size:   128 | lm loss: 2.594583E+00 | loss scale: 16384.0 | grad norm: 2.098 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.248 | tokens per gpu per second (tgs): 1999.893 | TFLOPs: 16.09 |
g0113: [2024-08-02 20:18:03,112] [INFO] [logging.py:96:log_dist] [Rank 0] step=1760, skipped=0, lr=[3.0740753066666664e-05, 3.0740753066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1760 loss: 2.5562 iter time (s): 4.247 samples/sec: 30.136
g0133:  iteration     1760/10000000 | consumed samples:       225280 | consumed tokens:    461373440 | elapsed time per iteration (ms): 4298.1 | learning rate: 3.074E-05 | global batch size:   128 | lm loss: 2.571302E+00 | loss scale: 16384.0 | grad norm: 1.831 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.781 | tokens per gpu per second (tgs): 1905.970 | TFLOPs: 15.34 |
g0113: [2024-08-02 20:18:45,038] [INFO] [logging.py:96:log_dist] [Rank 0] step=1770, skipped=0, lr=[3.0915515733333336e-05, 3.0915515733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1770 loss: 2.5711 iter time (s): 4.155 samples/sec: 30.807
g0133:  iteration     1770/10000000 | consumed samples:       226560 | consumed tokens:    463994880 | elapsed time per iteration (ms): 4190.6 | learning rate: 3.092E-05 | global batch size:   128 | lm loss: 2.569350E+00 | loss scale: 16384.0 | grad norm: 2.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.545 | tokens per gpu per second (tgs): 1954.862 | TFLOPs: 15.73 |
g0113: [2024-08-02 20:19:27,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=1780, skipped=0, lr=[3.10902784e-05, 3.10902784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1780 loss: 2.6053 iter time (s): 4.202 samples/sec: 30.464
g0133:  iteration     1780/10000000 | consumed samples:       227840 | consumed tokens:    466616320 | elapsed time per iteration (ms): 4234.7 | learning rate: 3.109E-05 | global batch size:   128 | lm loss: 2.569493E+00 | loss scale: 16384.0 | grad norm: 1.893 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.226 | tokens per gpu per second (tgs): 1934.482 | TFLOPs: 15.57 |
g0113: [2024-08-02 20:20:08,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=1790, skipped=0, lr=[3.126504106666667e-05, 3.126504106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1790 loss: 2.5437 iter time (s): 4.113 samples/sec: 31.118
g0133:  iteration     1790/10000000 | consumed samples:       229120 | consumed tokens:    469237760 | elapsed time per iteration (ms): 4145.9 | learning rate: 3.127E-05 | global batch size:   128 | lm loss: 2.568465E+00 | loss scale: 16384.0 | grad norm: 2.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.874 | tokens per gpu per second (tgs): 1975.930 | TFLOPs: 15.90 |
g0113: [2024-08-02 20:20:51,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=1800, skipped=0, lr=[3.1439803733333335e-05, 3.1439803733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1800 loss: 2.5787 iter time (s): 4.240 samples/sec: 30.188
g0133:  iteration     1800/10000000 | consumed samples:       230400 | consumed tokens:    471859200 | elapsed time per iteration (ms): 4272.7 | learning rate: 3.144E-05 | global batch size:   128 | lm loss: 2.568684E+00 | loss scale: 16384.0 | grad norm: 1.764 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.958 | tokens per gpu per second (tgs): 1917.285 | TFLOPs: 15.43 |
g0113: [2024-08-02 20:21:33,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=1810, skipped=0, lr=[3.16145664e-05, 3.16145664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1810 loss: 2.5752 iter time (s): 4.127 samples/sec: 31.018
g0133:  iteration     1810/10000000 | consumed samples:       231680 | consumed tokens:    474480640 | elapsed time per iteration (ms): 4159.4 | learning rate: 3.161E-05 | global batch size:   128 | lm loss: 2.574723E+00 | loss scale: 16384.0 | grad norm: 2.034 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.773 | tokens per gpu per second (tgs): 1969.499 | TFLOPs: 15.85 |
g0113: [2024-08-02 20:22:13,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=1820, skipped=0, lr=[3.178932906666667e-05, 3.178932906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1820 loss: 2.5537 iter time (s): 4.022 samples/sec: 31.823
g0133:  iteration     1820/10000000 | consumed samples:       232960 | consumed tokens:    477102080 | elapsed time per iteration (ms): 4054.8 | learning rate: 3.179E-05 | global batch size:   128 | lm loss: 2.566606E+00 | loss scale: 16384.0 | grad norm: 2.125 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.568 | tokens per gpu per second (tgs): 2020.334 | TFLOPs: 16.26 |
g0113: [2024-08-02 20:22:54,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=1830, skipped=0, lr=[3.196409173333333e-05, 3.196409173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1830 loss: 2.5417 iter time (s): 4.020 samples/sec: 31.842
g0133:  iteration     1830/10000000 | consumed samples:       234240 | consumed tokens:    479723520 | elapsed time per iteration (ms): 4052.4 | learning rate: 3.196E-05 | global batch size:   128 | lm loss: 2.565513E+00 | loss scale: 16384.0 | grad norm: 2.409 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.586 | tokens per gpu per second (tgs): 2021.510 | TFLOPs: 16.27 |
g0113: [2024-08-02 20:23:35,755] [INFO] [logging.py:96:log_dist] [Rank 0] step=1840, skipped=0, lr=[3.21388544e-05, 3.21388544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1840 loss: 2.5507 iter time (s): 4.118 samples/sec: 31.081
g0133:  iteration     1840/10000000 | consumed samples:       235520 | consumed tokens:    482344960 | elapsed time per iteration (ms): 4151.6 | learning rate: 3.214E-05 | global batch size:   128 | lm loss: 2.565236E+00 | loss scale: 16384.0 | grad norm: 1.802 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.831 | tokens per gpu per second (tgs): 1973.213 | TFLOPs: 15.88 |
g0113: [2024-08-02 20:24:18,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=1850, skipped=0, lr=[3.2313617066666665e-05, 3.2313617066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1850 loss: 2.5550 iter time (s): 4.248 samples/sec: 30.133
g0133:  iteration     1850/10000000 | consumed samples:       236800 | consumed tokens:    484966400 | elapsed time per iteration (ms): 4285.8 | learning rate: 3.231E-05 | global batch size:   128 | lm loss: 2.539454E+00 | loss scale: 16384.0 | grad norm: 1.990 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.866 | tokens per gpu per second (tgs): 1911.425 | TFLOPs: 15.38 |
g0113: [2024-08-02 20:25:00,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=1860, skipped=0, lr=[3.248837973333334e-05, 3.248837973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1860 loss: 2.5816 iter time (s): 4.198 samples/sec: 30.488
g0133:  iteration     1860/10000000 | consumed samples:       238080 | consumed tokens:    487587840 | elapsed time per iteration (ms): 4231.1 | learning rate: 3.249E-05 | global batch size:   128 | lm loss: 2.553410E+00 | loss scale: 16384.0 | grad norm: 1.901 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.252 | tokens per gpu per second (tgs): 1936.151 | TFLOPs: 15.58 |
g0113: [2024-08-02 20:25:42,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=1870, skipped=0, lr=[3.2663142400000004e-05, 3.2663142400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1870 loss: 2.5563 iter time (s): 4.096 samples/sec: 31.249
g0133:  iteration     1870/10000000 | consumed samples:       239360 | consumed tokens:    490209280 | elapsed time per iteration (ms): 4129.5 | learning rate: 3.266E-05 | global batch size:   128 | lm loss: 2.546868E+00 | loss scale: 16384.0 | grad norm: 2.005 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.997 | tokens per gpu per second (tgs): 1983.784 | TFLOPs: 15.96 |
g0113: [2024-08-02 20:26:24,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=1880, skipped=0, lr=[3.283790506666667e-05, 3.283790506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1880 loss: 2.4819 iter time (s): 4.182 samples/sec: 30.606
g0133:  iteration     1880/10000000 | consumed samples:       240640 | consumed tokens:    492830720 | elapsed time per iteration (ms): 4215.5 | learning rate: 3.284E-05 | global batch size:   128 | lm loss: 2.530723E+00 | loss scale: 16384.0 | grad norm: 1.879 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.364 | tokens per gpu per second (tgs): 1943.311 | TFLOPs: 15.64 |
g0113: [2024-08-02 20:27:05,636] [INFO] [logging.py:96:log_dist] [Rank 0] step=1890, skipped=0, lr=[3.3012667733333336e-05, 3.3012667733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1890 loss: 2.5617 iter time (s): 4.094 samples/sec: 31.267
g0133:  iteration     1890/10000000 | consumed samples:       241920 | consumed tokens:    495452160 | elapsed time per iteration (ms): 4126.1 | learning rate: 3.301E-05 | global batch size:   128 | lm loss: 2.534935E+00 | loss scale: 16384.0 | grad norm: 2.026 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.022 | tokens per gpu per second (tgs): 1985.401 | TFLOPs: 15.98 |
g0113: [2024-08-02 20:27:48,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=1900, skipped=0, lr=[3.31874304e-05, 3.31874304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1900 loss: 2.5107 iter time (s): 4.204 samples/sec: 30.446
g0133:  iteration     1900/10000000 | consumed samples:       243200 | consumed tokens:    498073600 | elapsed time per iteration (ms): 4236.8 | learning rate: 3.319E-05 | global batch size:   128 | lm loss: 2.515054E+00 | loss scale: 16384.0 | grad norm: 1.748 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.212 | tokens per gpu per second (tgs): 1933.542 | TFLOPs: 15.56 |
g0113: [2024-08-02 20:28:30,896] [INFO] [logging.py:96:log_dist] [Rank 0] step=1910, skipped=0, lr=[3.336219306666667e-05, 3.336219306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1910 loss: 2.5725 iter time (s): 4.256 samples/sec: 30.072
g0133:  iteration     1910/10000000 | consumed samples:       244480 | consumed tokens:    500695040 | elapsed time per iteration (ms): 4289.2 | learning rate: 3.336E-05 | global batch size:   128 | lm loss: 2.518660E+00 | loss scale: 16384.0 | grad norm: 2.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.842 | tokens per gpu per second (tgs): 1909.906 | TFLOPs: 15.37 |
g0113: [2024-08-02 20:29:12,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=1920, skipped=0, lr=[3.3536955733333334e-05, 3.3536955733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1920 loss: 2.4586 iter time (s): 4.149 samples/sec: 30.853
g0133:  iteration     1920/10000000 | consumed samples:       245760 | consumed tokens:    503316480 | elapsed time per iteration (ms): 4181.7 | learning rate: 3.354E-05 | global batch size:   128 | lm loss: 2.504811E+00 | loss scale: 16384.0 | grad norm: 1.899 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.610 | tokens per gpu per second (tgs): 1959.022 | TFLOPs: 15.76 |
g0113: [2024-08-02 20:29:54,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=1930, skipped=0, lr=[3.37117184e-05, 3.37117184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1930 loss: 2.4909 iter time (s): 4.113 samples/sec: 31.124
g0133:  iteration     1930/10000000 | consumed samples:       247040 | consumed tokens:    505937920 | elapsed time per iteration (ms): 4145.9 | learning rate: 3.371E-05 | global batch size:   128 | lm loss: 2.502496E+00 | loss scale: 16384.0 | grad norm: 1.670 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.874 | tokens per gpu per second (tgs): 1975.930 | TFLOPs: 15.90 |
g0113: [2024-08-02 20:30:35,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=1940, skipped=0, lr=[3.388648106666667e-05, 3.388648106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1940 loss: 2.4753 iter time (s): 4.148 samples/sec: 30.861
g0133:  iteration     1940/10000000 | consumed samples:       248320 | consumed tokens:    508559360 | elapsed time per iteration (ms): 4180.5 | learning rate: 3.389E-05 | global batch size:   128 | lm loss: 2.510296E+00 | loss scale: 16384.0 | grad norm: 2.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.618 | tokens per gpu per second (tgs): 1959.558 | TFLOPs: 15.77 |
g0113: [2024-08-02 20:31:18,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=1950, skipped=0, lr=[3.406124373333334e-05, 3.406124373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1950 loss: 2.5061 iter time (s): 4.239 samples/sec: 30.199
g0133:  iteration     1950/10000000 | consumed samples:       249600 | consumed tokens:    511180800 | elapsed time per iteration (ms): 4271.9 | learning rate: 3.406E-05 | global batch size:   128 | lm loss: 2.524830E+00 | loss scale: 16384.0 | grad norm: 1.912 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.963 | tokens per gpu per second (tgs): 1917.648 | TFLOPs: 15.43 |
g0113: [2024-08-02 20:32:00,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=1960, skipped=0, lr=[3.4236006400000005e-05, 3.4236006400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1960 loss: 2.5063 iter time (s): 4.150 samples/sec: 30.841
g0133:  iteration     1960/10000000 | consumed samples:       250880 | consumed tokens:    513802240 | elapsed time per iteration (ms): 4182.6 | learning rate: 3.424E-05 | global batch size:   128 | lm loss: 2.519307E+00 | loss scale: 16384.0 | grad norm: 1.694 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.603 | tokens per gpu per second (tgs): 1958.600 | TFLOPs: 15.76 |
g0113: [2024-08-02 20:32:39,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=1970, skipped=0, lr=[3.441076906666667e-05, 3.441076906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1970 loss: 2.4809 iter time (s): 3.914 samples/sec: 32.701
g0133:  iteration     1970/10000000 | consumed samples:       252160 | consumed tokens:    516423680 | elapsed time per iteration (ms): 3947.0 | learning rate: 3.441E-05 | global batch size:   128 | lm loss: 2.500539E+00 | loss scale: 16384.0 | grad norm: 1.836 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.430 | tokens per gpu per second (tgs): 2075.515 | TFLOPs: 16.70 |
g0113: [2024-08-02 20:33:21,479] [INFO] [logging.py:96:log_dist] [Rank 0] step=1980, skipped=0, lr=[3.458553173333334e-05, 3.458553173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1980 loss: 2.4265 iter time (s): 4.116 samples/sec: 31.098
g0133:  iteration     1980/10000000 | consumed samples:       253440 | consumed tokens:    519045120 | elapsed time per iteration (ms): 4148.7 | learning rate: 3.459E-05 | global batch size:   128 | lm loss: 2.497802E+00 | loss scale: 16384.0 | grad norm: 1.697 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.853 | tokens per gpu per second (tgs): 1974.610 | TFLOPs: 15.89 |
g0113: [2024-08-02 20:34:02,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=1990, skipped=0, lr=[3.47602944e-05, 3.47602944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 1990 loss: 2.5466 iter time (s): 4.116 samples/sec: 31.094
g0133:  iteration     1990/10000000 | consumed samples:       254720 | consumed tokens:    521666560 | elapsed time per iteration (ms): 4149.0 | learning rate: 3.476E-05 | global batch size:   128 | lm loss: 2.492515E+00 | loss scale: 16384.0 | grad norm: 2.058 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.851 | tokens per gpu per second (tgs): 1974.450 | TFLOPs: 15.89 |
g0113: [2024-08-02 20:34:43,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=2000, skipped=0, lr=[3.493505706666667e-05, 3.493505706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2000 loss: 2.4663 iter time (s): 4.018 samples/sec: 31.855
g0133:  iteration     2000/10000000 | consumed samples:       256000 | consumed tokens:    524288000 | elapsed time per iteration (ms): 4050.7 | learning rate: 3.494E-05 | global batch size:   128 | lm loss: 2.486090E+00 | loss scale: 16384.0 | grad norm: 1.960 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.599 | tokens per gpu per second (tgs): 2022.349 | TFLOPs: 16.27 |
g0133: ------------------------------------------------------------------------------------------------
g0133:  validation loss at iteration 2000 | lm loss value: 2.498326E+00 | lm loss PPL: 1.216212E+01 | 
g0133: ------------------------------------------------------------------------------------------------
g0113: saving checkpoint at iteration    2000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: [2024-08-02 20:41:08,117] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step2000 is about to be saved!
g0113: [2024-08-02 20:41:08,123] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0113: [2024-08-02 20:41:08,123] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0133: [2024-08-02 20:41:08,123] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0133: [2024-08-02 20:41:08,123] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0133: [2024-08-02 20:41:08,123] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0113: [2024-08-02 20:41:08,123] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0131: [2024-08-02 20:41:08,126] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0131: [2024-08-02 20:41:08,126] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0131: [2024-08-02 20:41:08,126] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0125: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0125: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0130: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0125: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0132: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0132: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0129: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0129: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0129: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0130: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0130: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0132: [2024-08-02 20:41:08,127] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0128: [2024-08-02 20:41:08,130] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0128: [2024-08-02 20:41:08,130] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0128: [2024-08-02 20:41:08,130] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0133: [2024-08-02 20:41:08,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_23-model_00-model_states.pt...
g0130: [2024-08-02 20:41:08,160] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_14-model_00-model_states.pt...
g0129: [2024-08-02 20:41:08,160] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_11-model_00-model_states.pt...
g0132: [2024-08-02 20:41:08,161] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_20-model_00-model_states.pt...
g0125: [2024-08-02 20:41:08,164] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_05-model_00-model_states.pt...
g0131: [2024-08-02 20:41:08,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_17-model_00-model_states.pt...
g0113: [2024-08-02 20:41:08,168] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_01-model_00-model_states.pt...
g0128: [2024-08-02 20:41:08,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_08-model_00-model_states.pt...
g0129: [2024-08-02 20:41:08,271] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_11-model_00-model_states.pt.
g0132: [2024-08-02 20:41:08,291] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_20-model_00-model_states.pt.
g0128: [2024-08-02 20:41:08,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_08-model_00-model_states.pt.
g0131: [2024-08-02 20:41:08,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_17-model_00-model_states.pt.
g0129: [2024-08-02 20:41:08,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_12-model_00-model_states.pt...
g0125: [2024-08-02 20:41:08,309] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_05-model_00-model_states.pt.
g0132: [2024-08-02 20:41:08,326] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_21-model_00-model_states.pt...
g0128: [2024-08-02 20:41:08,337] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_09-model_00-model_states.pt...
g0131: [2024-08-02 20:41:08,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_18-model_00-model_states.pt...
g0133: [2024-08-02 20:41:08,343] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_23-model_00-model_states.pt.
g0133: [2024-08-02 20:41:08,344] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_24-model_00-model_states.pt...
g0125: [2024-08-02 20:41:08,346] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_06-model_00-model_states.pt...
g0133: [2024-08-02 20:41:08,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_24-model_00-model_states.pt.
g0113: [2024-08-02 20:41:08,348] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_01-model_00-model_states.pt.
g0113: [2024-08-02 20:41:08,372] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_02-model_00-model_states.pt...
g0133: [2024-08-02 20:41:08,399] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_25-model_00-model_states.pt...
g0129: [2024-08-02 20:41:08,421] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_12-model_00-model_states.pt.
g0129: [2024-08-02 20:41:08,447] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_13-model_00-model_states.pt...
g0131: [2024-08-02 20:41:08,483] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_18-model_00-model_states.pt.
g0113: [2024-08-02 20:41:08,504] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_02-model_00-model_states.pt.
g0131: [2024-08-02 20:41:08,518] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_19-model_00-model_states.pt...
g0113: [2024-08-02 20:41:08,536] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_03-model_00-model_states.pt...
g0133: [2024-08-02 20:41:08,553] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_25-model_00-model_states.pt.
g0133: [2024-08-02 20:41:08,555] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_07_model_states.pt...
g0130: [2024-08-02 20:41:08,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_14-model_00-model_states.pt.
g0129: [2024-08-02 20:41:08,588] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_13-model_00-model_states.pt.
g0129: [2024-08-02 20:41:08,589] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_03_model_states.pt...
g0130: [2024-08-02 20:41:08,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_15-model_00-model_states.pt...
g0113: [2024-08-02 20:41:08,628] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_03-model_00-model_states.pt.
g0131: [2024-08-02 20:41:08,633] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_19-model_00-model_states.pt.
g0131: [2024-08-02 20:41:08,635] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_05_model_states.pt...
g0113: [2024-08-02 20:41:08,653] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_04-model_00-model_states.pt...
g0113: [2024-08-02 20:41:08,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_04-model_00-model_states.pt.
g0113: [2024-08-02 20:41:08,779] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt
g0113: [2024-08-02 20:41:08,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt...
g0128: [2024-08-02 20:41:08,818] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_09-model_00-model_states.pt.
g0128: [2024-08-02 20:41:08,854] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_10-model_00-model_states.pt...
g0128: [2024-08-02 20:41:08,970] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_10-model_00-model_states.pt.
g0128: [2024-08-02 20:41:08,972] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_02_model_states.pt...
g0132: [2024-08-02 20:41:09,045] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_21-model_00-model_states.pt.
g0130: [2024-08-02 20:41:09,067] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_15-model_00-model_states.pt.
g0132: [2024-08-02 20:41:09,076] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_22-model_00-model_states.pt...
g0130: [2024-08-02 20:41:09,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_16-model_00-model_states.pt...
g0125: [2024-08-02 20:41:09,134] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_06-model_00-model_states.pt.
g0125: [2024-08-02 20:41:09,167] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_07-model_00-model_states.pt...
g0132: [2024-08-02 20:41:09,188] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_22-model_00-model_states.pt.
g0132: [2024-08-02 20:41:09,189] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_06_model_states.pt...
g0130: [2024-08-02 20:41:09,234] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_16-model_00-model_states.pt.
g0130: [2024-08-02 20:41:09,236] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_04_model_states.pt...
g0125: [2024-08-02 20:41:09,340] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/layer_07-model_00-model_states.pt.
g0125: [2024-08-02 20:41:09,342] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_01_model_states.pt...
g0133: [2024-08-02 20:41:10,527] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_07_model_states.pt.
g0133: [2024-08-02 20:41:10,527] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0131: [2024-08-02 20:41:10,997] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_05_model_states.pt.
g0131: [2024-08-02 20:41:10,997] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0128: [2024-08-02 20:41:11,368] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_02_model_states.pt.
g0128: [2024-08-02 20:41:11,369] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0130: [2024-08-02 20:41:11,579] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_04_model_states.pt.
g0130: [2024-08-02 20:41:11,579] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0125: [2024-08-02 20:41:11,691] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_01_model_states.pt.
g0125: [2024-08-02 20:41:11,691] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0113: [2024-08-02 20:41:12,162] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_00_model_states.pt.
g0113: [2024-08-02 20:41:12,163] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0129: [2024-08-02 20:41:12,236] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_03_model_states.pt.
g0129: [2024-08-02 20:41:12,236] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0132: [2024-08-02 20:41:12,479] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step2000/mp_rank_06_model_states.pt.
g0132: [2024-08-02 20:41:12,479] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2000 is ready now!
g0113:   successfully saved checkpoint at iteration    2000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: Checkpoint Save GB: 22.521, GB/Sec: 5.14, Latency(second): 4.379
g0133: (min, max) time across ranks (ms):
g0133:     save-checkpoint ................................: (4379.08, 4379.23)
g0132: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0113: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0130: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0132: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0128: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0133: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0125: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0125: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0129: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0113: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0129: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0129: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0129: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0128: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0128: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0113: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0128: [2024-08-02 20:41:16,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0133: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0131: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0131: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0125: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0113: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0131: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0133: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 20:41:16,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0131: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 20:41:16,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0113: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0133: [2024-08-02 20:41:16,622] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 20:41:16,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0130: [2024-08-02 20:41:16,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0113: [2024-08-02 20:41:16,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0132: [2024-08-02 20:41:16,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0131: [2024-08-02 20:41:16,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0133: [2024-08-02 20:41:16,623] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384 to 32768
g0113: [2024-08-02 20:41:53,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=2010, skipped=0, lr=[3.5109819733333335e-05, 3.5109819733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2010 loss: 2.4662 iter time (s): 4.080 samples/sec: 31.371
g0133:  iteration     2010/10000000 | consumed samples:       257280 | consumed tokens:    526909440 | elapsed time per iteration (ms): 43009.9 | learning rate: 3.511E-05 | global batch size:   128 | lm loss: 2.483330E+00 | loss scale: 32768.0 | grad norm: 1.841 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.976 | tokens per gpu per second (tgs): 190.468 | TFLOPs: 1.53 |
g0113: [2024-08-02 20:42:34,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=2020, skipped=0, lr=[3.52845824e-05, 3.52845824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2020 loss: 2.5381 iter time (s): 4.066 samples/sec: 31.482
g0133:  iteration     2020/10000000 | consumed samples:       258560 | consumed tokens:    529530880 | elapsed time per iteration (ms): 4098.7 | learning rate: 3.528E-05 | global batch size:   128 | lm loss: 2.483778E+00 | loss scale: 32768.0 | grad norm: 1.849 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.230 | tokens per gpu per second (tgs): 1998.705 | TFLOPs: 16.08 |
g0113: [2024-08-02 20:43:15,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=2030, skipped=0, lr=[3.545934506666667e-05, 3.545934506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2030 loss: 2.4648 iter time (s): 4.043 samples/sec: 31.656
g0133:  iteration     2030/10000000 | consumed samples:       259840 | consumed tokens:    532152320 | elapsed time per iteration (ms): 4076.6 | learning rate: 3.546E-05 | global batch size:   128 | lm loss: 2.470778E+00 | loss scale: 32768.0 | grad norm: 1.797 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.399 | tokens per gpu per second (tgs): 2009.505 | TFLOPs: 16.17 |
g0113: [2024-08-02 20:43:56,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=2040, skipped=0, lr=[3.5634107733333334e-05, 3.5634107733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2040 loss: 2.4640 iter time (s): 4.094 samples/sec: 31.263
g0133:  iteration     2040/10000000 | consumed samples:       261120 | consumed tokens:    534773760 | elapsed time per iteration (ms): 4127.1 | learning rate: 3.563E-05 | global batch size:   128 | lm loss: 2.489821E+00 | loss scale: 32768.0 | grad norm: 1.964 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.014 | tokens per gpu per second (tgs): 1984.915 | TFLOPs: 15.97 |
g0113: [2024-08-02 20:44:38,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=2050, skipped=0, lr=[3.58088704e-05, 3.58088704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2050 loss: 2.4782 iter time (s): 4.108 samples/sec: 31.155
g0133:  iteration     2050/10000000 | consumed samples:       262400 | consumed tokens:    537395200 | elapsed time per iteration (ms): 4141.0 | learning rate: 3.581E-05 | global batch size:   128 | lm loss: 2.465508E+00 | loss scale: 32768.0 | grad norm: 1.606 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.911 | tokens per gpu per second (tgs): 1978.281 | TFLOPs: 15.92 |
g0113: [2024-08-02 20:45:20,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=2060, skipped=0, lr=[3.5983633066666666e-05, 3.5983633066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2060 loss: 2.5101 iter time (s): 4.199 samples/sec: 30.487
g0133:  iteration     2060/10000000 | consumed samples:       263680 | consumed tokens:    540016640 | elapsed time per iteration (ms): 4231.0 | learning rate: 3.598E-05 | global batch size:   128 | lm loss: 2.469295E+00 | loss scale: 32768.0 | grad norm: 2.132 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.253 | tokens per gpu per second (tgs): 1936.193 | TFLOPs: 15.58 |
g0113: [2024-08-02 20:46:02,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=2070, skipped=0, lr=[3.615839573333333e-05, 3.615839573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2070 loss: 2.5142 iter time (s): 4.169 samples/sec: 30.702
g0133:  iteration     2070/10000000 | consumed samples:       264960 | consumed tokens:    542638080 | elapsed time per iteration (ms): 4201.8 | learning rate: 3.616E-05 | global batch size:   128 | lm loss: 2.470722E+00 | loss scale: 32768.0 | grad norm: 1.676 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.463 | tokens per gpu per second (tgs): 1949.627 | TFLOPs: 15.69 |
g0113: [2024-08-02 20:46:44,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=2080, skipped=0, lr=[3.63331584e-05, 3.63331584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2080 loss: 2.4820 iter time (s): 4.158 samples/sec: 30.783
g0133:  iteration     2080/10000000 | consumed samples:       266240 | consumed tokens:    545259520 | elapsed time per iteration (ms): 4190.6 | learning rate: 3.633E-05 | global batch size:   128 | lm loss: 2.455588E+00 | loss scale: 32768.0 | grad norm: 1.631 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.545 | tokens per gpu per second (tgs): 1954.849 | TFLOPs: 15.73 |
g0113: [2024-08-02 20:47:26,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=2090, skipped=0, lr=[3.6507921066666664e-05, 3.6507921066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2090 loss: 2.4284 iter time (s): 4.224 samples/sec: 30.304
g0133:  iteration     2090/10000000 | consumed samples:       267520 | consumed tokens:    547880960 | elapsed time per iteration (ms): 4256.6 | learning rate: 3.651E-05 | global batch size:   128 | lm loss: 2.443198E+00 | loss scale: 32768.0 | grad norm: 1.625 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.071 | tokens per gpu per second (tgs): 1924.523 | TFLOPs: 15.49 |
g0113: [2024-08-02 20:48:08,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=2100, skipped=0, lr=[3.668268373333334e-05, 3.668268373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2100 loss: 2.4263 iter time (s): 4.131 samples/sec: 30.984
g0133:  iteration     2100/10000000 | consumed samples:       268800 | consumed tokens:    550502400 | elapsed time per iteration (ms): 4163.6 | learning rate: 3.668E-05 | global batch size:   128 | lm loss: 2.464294E+00 | loss scale: 32768.0 | grad norm: 1.701 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.742 | tokens per gpu per second (tgs): 1967.508 | TFLOPs: 15.83 |
g0113: [2024-08-02 20:48:50,054] [INFO] [logging.py:96:log_dist] [Rank 0] step=2110, skipped=0, lr=[3.68574464e-05, 3.68574464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2110 loss: 2.4344 iter time (s): 4.121 samples/sec: 31.060
g0133:  iteration     2110/10000000 | consumed samples:       270080 | consumed tokens:    553123840 | elapsed time per iteration (ms): 4162.2 | learning rate: 3.686E-05 | global batch size:   128 | lm loss: 2.441236E+00 | loss scale: 32768.0 | grad norm: 1.893 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.753 | tokens per gpu per second (tgs): 1968.190 | TFLOPs: 15.84 |
g0113: [2024-08-02 20:49:32,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=2120, skipped=0, lr=[3.703220906666667e-05, 3.703220906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2120 loss: 2.4421 iter time (s): 4.175 samples/sec: 30.662
g0133:  iteration     2120/10000000 | consumed samples:       271360 | consumed tokens:    555745280 | elapsed time per iteration (ms): 4207.1 | learning rate: 3.703E-05 | global batch size:   128 | lm loss: 2.448651E+00 | loss scale: 32768.0 | grad norm: 1.675 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.425 | tokens per gpu per second (tgs): 1947.185 | TFLOPs: 15.67 |
g0113: [2024-08-02 20:50:14,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=2130, skipped=0, lr=[3.7206971733333335e-05, 3.7206971733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2130 loss: 2.4569 iter time (s): 4.186 samples/sec: 30.581
g0133:  iteration     2130/10000000 | consumed samples:       272640 | consumed tokens:    558366720 | elapsed time per iteration (ms): 4218.1 | learning rate: 3.721E-05 | global batch size:   128 | lm loss: 2.456389E+00 | loss scale: 32768.0 | grad norm: 1.788 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.346 | tokens per gpu per second (tgs): 1942.119 | TFLOPs: 15.63 |
g0113: [2024-08-02 20:50:56,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=2140, skipped=0, lr=[3.73817344e-05, 3.73817344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2140 loss: 2.4422 iter time (s): 4.153 samples/sec: 30.821
g0133:  iteration     2140/10000000 | consumed samples:       273920 | consumed tokens:    560988160 | elapsed time per iteration (ms): 4186.0 | learning rate: 3.738E-05 | global batch size:   128 | lm loss: 2.427710E+00 | loss scale: 32768.0 | grad norm: 1.568 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.578 | tokens per gpu per second (tgs): 1957.008 | TFLOPs: 15.75 |
g0113: [2024-08-02 20:51:38,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=2150, skipped=0, lr=[3.755649706666667e-05, 3.755649706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2150 loss: 2.4532 iter time (s): 4.183 samples/sec: 30.602
g0133:  iteration     2150/10000000 | consumed samples:       275200 | consumed tokens:    563609600 | elapsed time per iteration (ms): 4215.2 | learning rate: 3.756E-05 | global batch size:   128 | lm loss: 2.441890E+00 | loss scale: 32768.0 | grad norm: 1.651 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.366 | tokens per gpu per second (tgs): 1943.420 | TFLOPs: 15.64 |
g0113: [2024-08-02 20:52:20,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=2160, skipped=0, lr=[3.773125973333333e-05, 3.773125973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2160 loss: 2.4559 iter time (s): 4.197 samples/sec: 30.498
g0133:  iteration     2160/10000000 | consumed samples:       276480 | consumed tokens:    566231040 | elapsed time per iteration (ms): 4231.3 | learning rate: 3.773E-05 | global batch size:   128 | lm loss: 2.443150E+00 | loss scale: 32768.0 | grad norm: 1.635 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.251 | tokens per gpu per second (tgs): 1936.054 | TFLOPs: 15.58 |
g0113: [2024-08-02 20:53:01,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=2170, skipped=0, lr=[3.79060224e-05, 3.79060224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2170 loss: 2.4132 iter time (s): 4.082 samples/sec: 31.361
g0133:  iteration     2170/10000000 | consumed samples:       277760 | consumed tokens:    568852480 | elapsed time per iteration (ms): 4114.2 | learning rate: 3.791E-05 | global batch size:   128 | lm loss: 2.435763E+00 | loss scale: 32768.0 | grad norm: 1.585 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.112 | tokens per gpu per second (tgs): 1991.156 | TFLOPs: 16.02 |
g0113: [2024-08-02 20:53:43,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=2180, skipped=0, lr=[3.8080785066666665e-05, 3.8080785066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2180 loss: 2.4184 iter time (s): 4.093 samples/sec: 31.274
g0133:  iteration     2180/10000000 | consumed samples:       279040 | consumed tokens:    571473920 | elapsed time per iteration (ms): 4125.5 | learning rate: 3.808E-05 | global batch size:   128 | lm loss: 2.433469E+00 | loss scale: 32768.0 | grad norm: 1.561 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.027 | tokens per gpu per second (tgs): 1985.713 | TFLOPs: 15.98 |
g0113: [2024-08-02 20:54:23,945] [INFO] [logging.py:96:log_dist] [Rank 0] step=2190, skipped=0, lr=[3.825554773333334e-05, 3.825554773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2190 loss: 2.3985 iter time (s): 4.057 samples/sec: 31.549
g0133:  iteration     2190/10000000 | consumed samples:       280320 | consumed tokens:    574095360 | elapsed time per iteration (ms): 4090.1 | learning rate: 3.826E-05 | global batch size:   128 | lm loss: 2.429418E+00 | loss scale: 32768.0 | grad norm: 1.473 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.295 | tokens per gpu per second (tgs): 2002.901 | TFLOPs: 16.12 |
g0113: [2024-08-02 20:55:05,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=2200, skipped=0, lr=[3.8430310400000004e-05, 3.8430310400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2200 loss: 2.3929 iter time (s): 4.116 samples/sec: 31.095
g0133:  iteration     2200/10000000 | consumed samples:       281600 | consumed tokens:    576716800 | elapsed time per iteration (ms): 4149.1 | learning rate: 3.843E-05 | global batch size:   128 | lm loss: 2.428540E+00 | loss scale: 32768.0 | grad norm: 1.680 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.850 | tokens per gpu per second (tgs): 1974.396 | TFLOPs: 15.89 |
g0113: [2024-08-02 20:55:46,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=2210, skipped=0, lr=[3.860507306666667e-05, 3.860507306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2210 loss: 2.4619 iter time (s): 4.111 samples/sec: 31.139
g0133:  iteration     2210/10000000 | consumed samples:       282880 | consumed tokens:    579338240 | elapsed time per iteration (ms): 4143.3 | learning rate: 3.861E-05 | global batch size:   128 | lm loss: 2.423745E+00 | loss scale: 32768.0 | grad norm: 1.674 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.893 | tokens per gpu per second (tgs): 1977.174 | TFLOPs: 15.91 |
g0113: [2024-08-02 20:56:26,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=2220, skipped=0, lr=[3.8779835733333336e-05, 3.8779835733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2220 loss: 2.4571 iter time (s): 3.962 samples/sec: 32.305
g0133:  iteration     2220/10000000 | consumed samples:       284160 | consumed tokens:    581959680 | elapsed time per iteration (ms): 3994.7 | learning rate: 3.878E-05 | global batch size:   128 | lm loss: 2.413499E+00 | loss scale: 32768.0 | grad norm: 1.497 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.042 | tokens per gpu per second (tgs): 2050.704 | TFLOPs: 16.50 |
g0113: [2024-08-02 20:57:08,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=2230, skipped=0, lr=[3.89545984e-05, 3.89545984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2230 loss: 2.4122 iter time (s): 4.111 samples/sec: 31.133
g0133:  iteration     2230/10000000 | consumed samples:       285440 | consumed tokens:    584581120 | elapsed time per iteration (ms): 4144.1 | learning rate: 3.895E-05 | global batch size:   128 | lm loss: 2.421822E+00 | loss scale: 32768.0 | grad norm: 1.965 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.888 | tokens per gpu per second (tgs): 1976.805 | TFLOPs: 15.91 |
g0113: [2024-08-02 20:57:50,213] [INFO] [logging.py:96:log_dist] [Rank 0] step=2240, skipped=0, lr=[3.912936106666667e-05, 3.912936106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2240 loss: 2.4169 iter time (s): 4.163 samples/sec: 30.748
g0133:  iteration     2240/10000000 | consumed samples:       286720 | consumed tokens:    587202560 | elapsed time per iteration (ms): 4195.5 | learning rate: 3.913E-05 | global batch size:   128 | lm loss: 2.408885E+00 | loss scale: 32768.0 | grad norm: 1.793 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.509 | tokens per gpu per second (tgs): 1952.583 | TFLOPs: 15.71 |
g0113: [2024-08-02 20:58:31,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=2250, skipped=0, lr=[3.9304123733333334e-05, 3.9304123733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2250 loss: 2.4347 iter time (s): 4.111 samples/sec: 31.135
g0133:  iteration     2250/10000000 | consumed samples:       288000 | consumed tokens:    589824000 | elapsed time per iteration (ms): 4143.9 | learning rate: 3.930E-05 | global batch size:   128 | lm loss: 2.414381E+00 | loss scale: 32768.0 | grad norm: 1.639 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.889 | tokens per gpu per second (tgs): 1976.898 | TFLOPs: 15.91 |
g0113: [2024-08-02 20:59:12,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=2260, skipped=0, lr=[3.94788864e-05, 3.94788864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2260 loss: 2.4085 iter time (s): 4.075 samples/sec: 31.410
g0133:  iteration     2260/10000000 | consumed samples:       289280 | consumed tokens:    592445440 | elapsed time per iteration (ms): 4107.8 | learning rate: 3.948E-05 | global batch size:   128 | lm loss: 2.404849E+00 | loss scale: 32768.0 | grad norm: 1.539 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.161 | tokens per gpu per second (tgs): 1994.278 | TFLOPs: 16.05 |
g0113: [2024-08-02 20:59:53,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=2270, skipped=0, lr=[3.965364906666667e-05, 3.965364906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2270 loss: 2.4107 iter time (s): 4.094 samples/sec: 31.268
g0133:  iteration     2270/10000000 | consumed samples:       290560 | consumed tokens:    595066880 | elapsed time per iteration (ms): 4126.5 | learning rate: 3.965E-05 | global batch size:   128 | lm loss: 2.409846E+00 | loss scale: 32768.0 | grad norm: 1.735 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.019 | tokens per gpu per second (tgs): 1985.212 | TFLOPs: 15.98 |
g0113: [2024-08-02 21:00:35,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=2280, skipped=0, lr=[3.982841173333334e-05, 3.982841173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2280 loss: 2.3849 iter time (s): 4.124 samples/sec: 31.041
g0133:  iteration     2280/10000000 | consumed samples:       291840 | consumed tokens:    597688320 | elapsed time per iteration (ms): 4157.7 | learning rate: 3.983E-05 | global batch size:   128 | lm loss: 2.406113E+00 | loss scale: 32768.0 | grad norm: 1.683 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.787 | tokens per gpu per second (tgs): 1970.339 | TFLOPs: 15.86 |
g0113: [2024-08-02 21:01:17,121] [INFO] [logging.py:96:log_dist] [Rank 0] step=2290, skipped=0, lr=[4.0003174400000006e-05, 4.0003174400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2290 loss: 2.4021 iter time (s): 4.123 samples/sec: 31.047
g0133:  iteration     2290/10000000 | consumed samples:       293120 | consumed tokens:    600309760 | elapsed time per iteration (ms): 4155.0 | learning rate: 4.000E-05 | global batch size:   128 | lm loss: 2.391095E+00 | loss scale: 32768.0 | grad norm: 1.465 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.806 | tokens per gpu per second (tgs): 1971.604 | TFLOPs: 15.87 |
g0113: [2024-08-02 21:01:59,126] [INFO] [logging.py:96:log_dist] [Rank 0] step=2300, skipped=0, lr=[4.017793706666667e-05, 4.017793706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2300 loss: 2.4349 iter time (s): 4.168 samples/sec: 30.709
g0133:  iteration     2300/10000000 | consumed samples:       294400 | consumed tokens:    602931200 | elapsed time per iteration (ms): 4200.7 | learning rate: 4.018E-05 | global batch size:   128 | lm loss: 2.395932E+00 | loss scale: 32768.0 | grad norm: 1.842 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.471 | tokens per gpu per second (tgs): 1950.150 | TFLOPs: 15.69 |
g0113: [2024-08-02 21:02:41,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=2310, skipped=0, lr=[4.035269973333334e-05, 4.035269973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2310 loss: 2.3607 iter time (s): 4.230 samples/sec: 30.262
g0133:  iteration     2310/10000000 | consumed samples:       295680 | consumed tokens:    605552640 | elapsed time per iteration (ms): 4263.1 | learning rate: 4.035E-05 | global batch size:   128 | lm loss: 2.384912E+00 | loss scale: 32768.0 | grad norm: 1.553 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.025 | tokens per gpu per second (tgs): 1921.619 | TFLOPs: 15.46 |
g0113: [2024-08-02 21:03:24,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=2320, skipped=0, lr=[4.0527462400000004e-05, 4.0527462400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2320 loss: 2.3881 iter time (s): 4.194 samples/sec: 30.522
g0133:  iteration     2320/10000000 | consumed samples:       296960 | consumed tokens:    608174080 | elapsed time per iteration (ms): 4226.4 | learning rate: 4.053E-05 | global batch size:   128 | lm loss: 2.383764E+00 | loss scale: 32768.0 | grad norm: 1.573 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.286 | tokens per gpu per second (tgs): 1938.272 | TFLOPs: 15.60 |
g0113: [2024-08-02 21:04:05,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=2330, skipped=0, lr=[4.070222506666667e-05, 4.070222506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2330 loss: 2.4076 iter time (s): 4.137 samples/sec: 30.939
g0133:  iteration     2330/10000000 | consumed samples:       298240 | consumed tokens:    610795520 | elapsed time per iteration (ms): 4169.9 | learning rate: 4.070E-05 | global batch size:   128 | lm loss: 2.388484E+00 | loss scale: 32768.0 | grad norm: 1.576 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.696 | tokens per gpu per second (tgs): 1964.566 | TFLOPs: 15.81 |
g0113: [2024-08-02 21:04:47,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=2340, skipped=0, lr=[4.0876987733333336e-05, 4.0876987733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2340 loss: 2.3301 iter time (s): 4.153 samples/sec: 30.820
g0133:  iteration     2340/10000000 | consumed samples:       299520 | consumed tokens:    613416960 | elapsed time per iteration (ms): 4185.5 | learning rate: 4.088E-05 | global batch size:   128 | lm loss: 2.390876E+00 | loss scale: 32768.0 | grad norm: 1.436 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.582 | tokens per gpu per second (tgs): 1957.246 | TFLOPs: 15.75 |
g0113: [2024-08-02 21:05:29,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=2350, skipped=0, lr=[4.10517504e-05, 4.10517504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2350 loss: 2.4149 iter time (s): 4.160 samples/sec: 30.771
g0133:  iteration     2350/10000000 | consumed samples:       300800 | consumed tokens:    616038400 | elapsed time per iteration (ms): 4193.2 | learning rate: 4.105E-05 | global batch size:   128 | lm loss: 2.383464E+00 | loss scale: 32768.0 | grad norm: 1.451 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.525 | tokens per gpu per second (tgs): 1953.631 | TFLOPs: 15.72 |
g0113: [2024-08-02 21:06:11,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=2360, skipped=0, lr=[4.122651306666667e-05, 4.122651306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2360 loss: 2.3780 iter time (s): 4.199 samples/sec: 30.481
g0133:  iteration     2360/10000000 | consumed samples:       302080 | consumed tokens:    618659840 | elapsed time per iteration (ms): 4232.6 | learning rate: 4.123E-05 | global batch size:   128 | lm loss: 2.387414E+00 | loss scale: 32768.0 | grad norm: 1.428 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.242 | tokens per gpu per second (tgs): 1935.474 | TFLOPs: 15.58 |
g0113: [2024-08-02 21:06:53,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=2370, skipped=0, lr=[4.1401275733333334e-05, 4.1401275733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2370 loss: 2.4094 iter time (s): 4.169 samples/sec: 30.704
g0133:  iteration     2370/10000000 | consumed samples:       303360 | consumed tokens:    621281280 | elapsed time per iteration (ms): 4201.1 | learning rate: 4.140E-05 | global batch size:   128 | lm loss: 2.376290E+00 | loss scale: 32768.0 | grad norm: 1.419 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.468 | tokens per gpu per second (tgs): 1949.958 | TFLOPs: 15.69 |
g0113: [2024-08-02 21:07:36,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=2380, skipped=0, lr=[4.15760384e-05, 4.15760384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2380 loss: 2.3679 iter time (s): 4.198 samples/sec: 30.488
g0133:  iteration     2380/10000000 | consumed samples:       304640 | consumed tokens:    623902720 | elapsed time per iteration (ms): 4230.7 | learning rate: 4.158E-05 | global batch size:   128 | lm loss: 2.358912E+00 | loss scale: 32768.0 | grad norm: 1.679 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.255 | tokens per gpu per second (tgs): 1936.343 | TFLOPs: 15.58 |
g0113: [2024-08-02 21:08:17,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=2390, skipped=0, lr=[4.1750801066666666e-05, 4.1750801066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2390 loss: 2.4214 iter time (s): 4.109 samples/sec: 31.150
g0133:  iteration     2390/10000000 | consumed samples:       305920 | consumed tokens:    626524160 | elapsed time per iteration (ms): 4142.1 | learning rate: 4.175E-05 | global batch size:   128 | lm loss: 2.383275E+00 | loss scale: 32768.0 | grad norm: 1.446 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.902 | tokens per gpu per second (tgs): 1977.733 | TFLOPs: 15.92 |
g0113: [2024-08-02 21:08:57,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=2400, skipped=0, lr=[4.192556373333333e-05, 4.192556373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2400 loss: 2.3670 iter time (s): 3.949 samples/sec: 32.411
g0133:  iteration     2400/10000000 | consumed samples:       307200 | consumed tokens:    629145600 | elapsed time per iteration (ms): 3982.1 | learning rate: 4.193E-05 | global batch size:   128 | lm loss: 2.365547E+00 | loss scale: 32768.0 | grad norm: 1.543 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.144 | tokens per gpu per second (tgs): 2057.197 | TFLOPs: 16.55 |
g0113: [2024-08-02 21:09:38,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=2410, skipped=0, lr=[4.21003264e-05, 4.21003264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2410 loss: 2.3423 iter time (s): 4.052 samples/sec: 31.586
g0133:  iteration     2410/10000000 | consumed samples:       308480 | consumed tokens:    631767040 | elapsed time per iteration (ms): 4085.0 | learning rate: 4.210E-05 | global batch size:   128 | lm loss: 2.366627E+00 | loss scale: 32768.0 | grad norm: 1.509 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.334 | tokens per gpu per second (tgs): 2005.400 | TFLOPs: 16.14 |
g0113: [2024-08-02 21:10:19,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=2420, skipped=0, lr=[4.2275089066666664e-05, 4.2275089066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2420 loss: 2.3302 iter time (s): 4.084 samples/sec: 31.341
g0133:  iteration     2420/10000000 | consumed samples:       309760 | consumed tokens:    634388480 | elapsed time per iteration (ms): 4116.3 | learning rate: 4.228E-05 | global batch size:   128 | lm loss: 2.359810E+00 | loss scale: 32768.0 | grad norm: 1.410 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.096 | tokens per gpu per second (tgs): 1990.129 | TFLOPs: 16.01 |
g0113: [2024-08-02 21:11:01,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=2430, skipped=0, lr=[4.244985173333334e-05, 4.244985173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2430 loss: 2.3753 iter time (s): 4.135 samples/sec: 30.957
g0133:  iteration     2430/10000000 | consumed samples:       311040 | consumed tokens:    637009920 | elapsed time per iteration (ms): 4167.4 | learning rate: 4.245E-05 | global batch size:   128 | lm loss: 2.368794E+00 | loss scale: 32768.0 | grad norm: 1.383 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.715 | tokens per gpu per second (tgs): 1965.747 | TFLOPs: 15.82 |
g0113: [2024-08-02 21:11:42,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=2440, skipped=0, lr=[4.26246144e-05, 4.26246144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2440 loss: 2.3547 iter time (s): 4.152 samples/sec: 30.831
g0133:  iteration     2440/10000000 | consumed samples:       312320 | consumed tokens:    639631360 | elapsed time per iteration (ms): 4184.4 | learning rate: 4.262E-05 | global batch size:   128 | lm loss: 2.370715E+00 | loss scale: 32768.0 | grad norm: 1.360 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.590 | tokens per gpu per second (tgs): 1957.742 | TFLOPs: 15.75 |
g0113: [2024-08-02 21:12:24,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=2450, skipped=0, lr=[4.279937706666667e-05, 4.279937706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2450 loss: 2.4208 iter time (s): 4.120 samples/sec: 31.065
g0133:  iteration     2450/10000000 | consumed samples:       313600 | consumed tokens:    642252800 | elapsed time per iteration (ms): 4152.7 | learning rate: 4.280E-05 | global batch size:   128 | lm loss: 2.362086E+00 | loss scale: 32768.0 | grad norm: 1.601 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.824 | tokens per gpu per second (tgs): 1972.704 | TFLOPs: 15.87 |
g0113: [2024-08-02 21:13:06,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=2460, skipped=0, lr=[4.2974139733333335e-05, 4.2974139733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2460 loss: 2.3523 iter time (s): 4.186 samples/sec: 30.580
g0133:  iteration     2460/10000000 | consumed samples:       314880 | consumed tokens:    644874240 | elapsed time per iteration (ms): 4218.0 | learning rate: 4.297E-05 | global batch size:   128 | lm loss: 2.353069E+00 | loss scale: 32768.0 | grad norm: 1.359 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.346 | tokens per gpu per second (tgs): 1942.155 | TFLOPs: 15.63 |
g0113: [2024-08-02 21:13:48,665] [INFO] [logging.py:96:log_dist] [Rank 0] step=2470, skipped=0, lr=[4.31489024e-05, 4.31489024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2470 loss: 2.3501 iter time (s): 4.171 samples/sec: 30.689
g0133:  iteration     2470/10000000 | consumed samples:       316160 | consumed tokens:    647495680 | elapsed time per iteration (ms): 4203.1 | learning rate: 4.315E-05 | global batch size:   128 | lm loss: 2.347482E+00 | loss scale: 32768.0 | grad norm: 1.451 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.454 | tokens per gpu per second (tgs): 1949.036 | TFLOPs: 15.68 |
g0113: [2024-08-02 21:14:30,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=2480, skipped=0, lr=[4.332366506666667e-05, 4.332366506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2480 loss: 2.3362 iter time (s): 4.185 samples/sec: 30.584
g0133:  iteration     2480/10000000 | consumed samples:       317440 | consumed tokens:    650117120 | elapsed time per iteration (ms): 4217.6 | learning rate: 4.332E-05 | global batch size:   128 | lm loss: 2.348459E+00 | loss scale: 32768.0 | grad norm: 1.444 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.349 | tokens per gpu per second (tgs): 1942.331 | TFLOPs: 15.63 |
g0113: [2024-08-02 21:15:13,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=2490, skipped=0, lr=[4.3498427733333334e-05, 4.3498427733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2490 loss: 2.3976 iter time (s): 4.195 samples/sec: 30.516
g0133:  iteration     2490/10000000 | consumed samples:       318720 | consumed tokens:    652738560 | elapsed time per iteration (ms): 4227.7 | learning rate: 4.350E-05 | global batch size:   128 | lm loss: 2.365808E+00 | loss scale: 32768.0 | grad norm: 1.503 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.276 | tokens per gpu per second (tgs): 1937.693 | TFLOPs: 15.59 |
g0113: [2024-08-02 21:15:55,957] [INFO] [logging.py:96:log_dist] [Rank 0] step=2500, skipped=0, lr=[4.36731904e-05, 4.36731904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2500 loss: 2.3622 iter time (s): 4.251 samples/sec: 30.108
g0133:  iteration     2500/10000000 | consumed samples:       320000 | consumed tokens:    655360000 | elapsed time per iteration (ms): 4283.9 | learning rate: 4.367E-05 | global batch size:   128 | lm loss: 2.350447E+00 | loss scale: 32768.0 | grad norm: 1.336 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.880 | tokens per gpu per second (tgs): 1912.298 | TFLOPs: 15.39 |
g0113: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0130: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0130: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0130: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0130: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0113: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0129: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0125: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0131: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0129: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0129: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0131: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0131: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0133: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0128: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0131: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0132: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0125: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0128: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0128: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0133: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0128: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0132: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0132: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0132: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0133: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0113: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0125: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0130: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0129: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0133: [2024-08-02 21:15:59,302] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0113: [2024-08-02 21:15:59,301] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 21:15:59,303] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768 to 65536
g0113: [2024-08-02 21:16:36,243] [INFO] [logging.py:96:log_dist] [Rank 0] step=2510, skipped=0, lr=[4.3847953066666666e-05, 4.3847953066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2510 loss: 2.3724 iter time (s): 3.995 samples/sec: 32.036
g0133:  iteration     2510/10000000 | consumed samples:       321280 | consumed tokens:    657981440 | elapsed time per iteration (ms): 4028.7 | learning rate: 4.385E-05 | global batch size:   128 | lm loss: 2.363075E+00 | loss scale: 65536.0 | grad norm: 1.542 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.772 | tokens per gpu per second (tgs): 2033.422 | TFLOPs: 16.36 |
g0113: [2024-08-02 21:17:18,498] [INFO] [logging.py:96:log_dist] [Rank 0] step=2520, skipped=0, lr=[4.402271573333334e-05, 4.402271573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2520 loss: 2.3145 iter time (s): 4.193 samples/sec: 30.527
g0133:  iteration     2520/10000000 | consumed samples:       322560 | consumed tokens:    660602880 | elapsed time per iteration (ms): 4225.4 | learning rate: 4.402E-05 | global batch size:   128 | lm loss: 2.337624E+00 | loss scale: 65536.0 | grad norm: 1.320 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.293 | tokens per gpu per second (tgs): 1938.751 | TFLOPs: 15.60 |
g0113: [2024-08-02 21:17:59,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=2530, skipped=0, lr=[4.4197478400000005e-05, 4.4197478400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2530 loss: 2.3396 iter time (s): 4.095 samples/sec: 31.261
g0133:  iteration     2530/10000000 | consumed samples:       323840 | consumed tokens:    663224320 | elapsed time per iteration (ms): 4126.9 | learning rate: 4.420E-05 | global batch size:   128 | lm loss: 2.347806E+00 | loss scale: 65536.0 | grad norm: 1.350 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.016 | tokens per gpu per second (tgs): 1985.006 | TFLOPs: 15.97 |
g0113: [2024-08-02 21:18:40,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=2540, skipped=0, lr=[4.437224106666667e-05, 4.437224106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2540 loss: 2.3399 iter time (s): 4.029 samples/sec: 31.767
g0133:  iteration     2540/10000000 | consumed samples:       325120 | consumed tokens:    665845760 | elapsed time per iteration (ms): 4061.8 | learning rate: 4.437E-05 | global batch size:   128 | lm loss: 2.341751E+00 | loss scale: 65536.0 | grad norm: 1.726 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.513 | tokens per gpu per second (tgs): 2016.855 | TFLOPs: 16.23 |
g0113: [2024-08-02 21:19:21,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=2550, skipped=0, lr=[4.454700373333334e-05, 4.454700373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2550 loss: 2.2960 iter time (s): 4.090 samples/sec: 31.294
g0133:  iteration     2550/10000000 | consumed samples:       326400 | consumed tokens:    668467200 | elapsed time per iteration (ms): 4122.8 | learning rate: 4.455E-05 | global batch size:   128 | lm loss: 2.327731E+00 | loss scale: 65536.0 | grad norm: 1.342 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.047 | tokens per gpu per second (tgs): 1986.976 | TFLOPs: 15.99 |
g0113: [2024-08-02 21:20:03,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=2560, skipped=0, lr=[4.47217664e-05, 4.47217664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2560 loss: 2.3331 iter time (s): 4.115 samples/sec: 31.102
g0133:  iteration     2560/10000000 | consumed samples:       327680 | consumed tokens:    671088640 | elapsed time per iteration (ms): 4148.4 | learning rate: 4.472E-05 | global batch size:   128 | lm loss: 2.323279E+00 | loss scale: 65536.0 | grad norm: 1.482 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.856 | tokens per gpu per second (tgs): 1974.754 | TFLOPs: 15.89 |
g0113: [2024-08-02 21:20:45,230] [INFO] [logging.py:96:log_dist] [Rank 0] step=2570, skipped=0, lr=[4.489652906666667e-05, 4.489652906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2570 loss: 2.3099 iter time (s): 4.181 samples/sec: 30.618
g0133:  iteration     2570/10000000 | consumed samples:       328960 | consumed tokens:    673710080 | elapsed time per iteration (ms): 4213.2 | learning rate: 4.490E-05 | global batch size:   128 | lm loss: 2.324216E+00 | loss scale: 65536.0 | grad norm: 1.360 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.381 | tokens per gpu per second (tgs): 1944.358 | TFLOPs: 15.65 |
g0113: [2024-08-02 21:21:27,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=2580, skipped=0, lr=[4.5071291733333335e-05, 4.5071291733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2580 loss: 2.3041 iter time (s): 4.162 samples/sec: 30.754
g0133:  iteration     2580/10000000 | consumed samples:       330240 | consumed tokens:    676331520 | elapsed time per iteration (ms): 4195.0 | learning rate: 4.507E-05 | global batch size:   128 | lm loss: 2.335412E+00 | loss scale: 65536.0 | grad norm: 1.538 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.512 | tokens per gpu per second (tgs): 1952.793 | TFLOPs: 15.71 |
g0113: [2024-08-02 21:22:09,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=2590, skipped=0, lr=[4.52460544e-05, 4.52460544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2590 loss: 2.3399 iter time (s): 4.181 samples/sec: 30.615
g0133:  iteration     2590/10000000 | consumed samples:       331520 | consumed tokens:    678952960 | elapsed time per iteration (ms): 4213.8 | learning rate: 4.525E-05 | global batch size:   128 | lm loss: 2.336400E+00 | loss scale: 65536.0 | grad norm: 1.349 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.377 | tokens per gpu per second (tgs): 1944.100 | TFLOPs: 15.64 |
g0113: [2024-08-02 21:22:51,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=2600, skipped=0, lr=[4.542081706666667e-05, 4.542081706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2600 loss: 2.3391 iter time (s): 4.141 samples/sec: 30.911
g0133:  iteration     2600/10000000 | consumed samples:       332800 | consumed tokens:    681574400 | elapsed time per iteration (ms): 4175.3 | learning rate: 4.542E-05 | global batch size:   128 | lm loss: 2.307947E+00 | loss scale: 65536.0 | grad norm: 1.348 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.657 | tokens per gpu per second (tgs): 1962.027 | TFLOPs: 15.79 |
g0113: [2024-08-02 21:23:34,149] [INFO] [logging.py:96:log_dist] [Rank 0] step=2610, skipped=0, lr=[4.559557973333334e-05, 4.559557973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2610 loss: 2.3561 iter time (s): 4.255 samples/sec: 30.085
g0133:  iteration     2610/10000000 | consumed samples:       334080 | consumed tokens:    684195840 | elapsed time per iteration (ms): 4307.7 | learning rate: 4.560E-05 | global batch size:   128 | lm loss: 2.327293E+00 | loss scale: 65536.0 | grad norm: 1.316 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.714 | tokens per gpu per second (tgs): 1901.716 | TFLOPs: 15.30 |
g0113: [2024-08-02 21:24:15,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=2620, skipped=0, lr=[4.5770342400000006e-05, 4.5770342400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2620 loss: 2.2930 iter time (s): 4.061 samples/sec: 31.521
g0133:  iteration     2620/10000000 | consumed samples:       335360 | consumed tokens:    686817280 | elapsed time per iteration (ms): 4093.4 | learning rate: 4.577E-05 | global batch size:   128 | lm loss: 2.304225E+00 | loss scale: 65536.0 | grad norm: 1.504 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.270 | tokens per gpu per second (tgs): 2001.250 | TFLOPs: 16.10 |
g0113: [2024-08-02 21:24:57,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=2630, skipped=0, lr=[4.594510506666667e-05, 4.594510506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2630 loss: 2.3502 iter time (s): 4.233 samples/sec: 30.242
g0133:  iteration     2630/10000000 | consumed samples:       336640 | consumed tokens:    689438720 | elapsed time per iteration (ms): 4265.8 | learning rate: 4.595E-05 | global batch size:   128 | lm loss: 2.330910E+00 | loss scale: 65536.0 | grad norm: 1.682 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.006 | tokens per gpu per second (tgs): 1920.388 | TFLOPs: 15.45 |
g0113: [2024-08-02 21:25:38,947] [INFO] [logging.py:96:log_dist] [Rank 0] step=2640, skipped=0, lr=[4.611986773333334e-05, 4.611986773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2640 loss: 2.3305 iter time (s): 4.088 samples/sec: 31.311
g0133:  iteration     2640/10000000 | consumed samples:       337920 | consumed tokens:    692060160 | elapsed time per iteration (ms): 4120.4 | learning rate: 4.612E-05 | global batch size:   128 | lm loss: 2.317196E+00 | loss scale: 65536.0 | grad norm: 1.231 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.065 | tokens per gpu per second (tgs): 1988.170 | TFLOPs: 16.00 |
g0113: [2024-08-02 21:26:20,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=2650, skipped=0, lr=[4.6294630400000004e-05, 4.6294630400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2650 loss: 2.3276 iter time (s): 4.161 samples/sec: 30.764
g0133:  iteration     2650/10000000 | consumed samples:       339200 | consumed tokens:    694681600 | elapsed time per iteration (ms): 4193.0 | learning rate: 4.629E-05 | global batch size:   128 | lm loss: 2.318420E+00 | loss scale: 65536.0 | grad norm: 1.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.527 | tokens per gpu per second (tgs): 1953.748 | TFLOPs: 15.72 |
g0113: [2024-08-02 21:27:01,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=2660, skipped=0, lr=[4.646939306666667e-05, 4.646939306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2660 loss: 2.3032 iter time (s): 4.074 samples/sec: 31.418
g0133:  iteration     2660/10000000 | consumed samples:       340480 | consumed tokens:    697303040 | elapsed time per iteration (ms): 4108.8 | learning rate: 4.647E-05 | global batch size:   128 | lm loss: 2.319766E+00 | loss scale: 65536.0 | grad norm: 1.265 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.152 | tokens per gpu per second (tgs): 1993.757 | TFLOPs: 16.04 |
g0113: [2024-08-02 21:27:42,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=2670, skipped=0, lr=[4.6644155733333336e-05, 4.6644155733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2670 loss: 2.2906 iter time (s): 4.026 samples/sec: 31.793
g0133:  iteration     2670/10000000 | consumed samples:       341760 | consumed tokens:    699924480 | elapsed time per iteration (ms): 4059.1 | learning rate: 4.664E-05 | global batch size:   128 | lm loss: 2.302838E+00 | loss scale: 65536.0 | grad norm: 1.415 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.534 | tokens per gpu per second (tgs): 2018.190 | TFLOPs: 16.24 |
g0113: [2024-08-02 21:28:23,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=2680, skipped=0, lr=[4.68189184e-05, 4.68189184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2680 loss: 2.3176 iter time (s): 4.018 samples/sec: 31.853
g0133:  iteration     2680/10000000 | consumed samples:       343040 | consumed tokens:    702545920 | elapsed time per iteration (ms): 4051.4 | learning rate: 4.682E-05 | global batch size:   128 | lm loss: 2.303777E+00 | loss scale: 65536.0 | grad norm: 1.259 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.594 | tokens per gpu per second (tgs): 2021.996 | TFLOPs: 16.27 |
g0113: [2024-08-02 21:29:05,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=2690, skipped=0, lr=[4.699368106666667e-05, 4.699368106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2690 loss: 2.3365 iter time (s): 4.159 samples/sec: 30.778
g0133:  iteration     2690/10000000 | consumed samples:       344320 | consumed tokens:    705167360 | elapsed time per iteration (ms): 4193.1 | learning rate: 4.699E-05 | global batch size:   128 | lm loss: 2.300756E+00 | loss scale: 65536.0 | grad norm: 1.270 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.526 | tokens per gpu per second (tgs): 1953.682 | TFLOPs: 15.72 |
g0113: [2024-08-02 21:29:46,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=2700, skipped=0, lr=[4.716844373333334e-05, 4.716844373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2700 loss: 2.2873 iter time (s): 4.085 samples/sec: 31.334
g0133:  iteration     2700/10000000 | consumed samples:       345600 | consumed tokens:    707788800 | elapsed time per iteration (ms): 4120.0 | learning rate: 4.717E-05 | global batch size:   128 | lm loss: 2.302748E+00 | loss scale: 65536.0 | grad norm: 1.415 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.068 | tokens per gpu per second (tgs): 1988.373 | TFLOPs: 16.00 |
g0113: [2024-08-02 21:30:26,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=2710, skipped=0, lr=[4.734320640000001e-05, 4.734320640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2710 loss: 2.2763 iter time (s): 4.014 samples/sec: 31.886
g0133:  iteration     2710/10000000 | consumed samples:       346880 | consumed tokens:    710410240 | elapsed time per iteration (ms): 4046.9 | learning rate: 4.734E-05 | global batch size:   128 | lm loss: 2.304347E+00 | loss scale: 65536.0 | grad norm: 1.281 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.629 | tokens per gpu per second (tgs): 2024.242 | TFLOPs: 16.29 |
g0113: [2024-08-02 21:31:07,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=2720, skipped=0, lr=[4.751796906666667e-05, 4.751796906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2720 loss: 2.3190 iter time (s): 4.055 samples/sec: 31.569
g0133:  iteration     2720/10000000 | consumed samples:       348160 | consumed tokens:    713031680 | elapsed time per iteration (ms): 4087.1 | learning rate: 4.752E-05 | global batch size:   128 | lm loss: 2.306773E+00 | loss scale: 65536.0 | grad norm: 1.285 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.318 | tokens per gpu per second (tgs): 2004.363 | TFLOPs: 16.13 |
g0113: [2024-08-02 21:31:49,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=2730, skipped=0, lr=[4.769273173333334e-05, 4.769273173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2730 loss: 2.3164 iter time (s): 4.127 samples/sec: 31.017
g0133:  iteration     2730/10000000 | consumed samples:       349440 | consumed tokens:    715653120 | elapsed time per iteration (ms): 4159.1 | learning rate: 4.769E-05 | global batch size:   128 | lm loss: 2.294385E+00 | loss scale: 65536.0 | grad norm: 1.353 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.776 | tokens per gpu per second (tgs): 1969.652 | TFLOPs: 15.85 |
g0113: [2024-08-02 21:32:30,945] [INFO] [logging.py:96:log_dist] [Rank 0] step=2740, skipped=0, lr=[4.7867494400000005e-05, 4.7867494400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2740 loss: 2.2264 iter time (s): 4.148 samples/sec: 30.858
g0133:  iteration     2740/10000000 | consumed samples:       350720 | consumed tokens:    718274560 | elapsed time per iteration (ms): 4181.2 | learning rate: 4.787E-05 | global batch size:   128 | lm loss: 2.295513E+00 | loss scale: 65536.0 | grad norm: 1.279 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.613 | tokens per gpu per second (tgs): 1959.249 | TFLOPs: 15.77 |
g0113: [2024-08-02 21:33:12,312] [INFO] [logging.py:96:log_dist] [Rank 0] step=2750, skipped=0, lr=[4.804225706666667e-05, 4.804225706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2750 loss: 2.2776 iter time (s): 4.104 samples/sec: 31.187
g0133:  iteration     2750/10000000 | consumed samples:       352000 | consumed tokens:    720896000 | elapsed time per iteration (ms): 4136.6 | learning rate: 4.804E-05 | global batch size:   128 | lm loss: 2.293578E+00 | loss scale: 65536.0 | grad norm: 1.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.943 | tokens per gpu per second (tgs): 1980.350 | TFLOPs: 15.94 |
g0113: [2024-08-02 21:33:54,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=2760, skipped=0, lr=[4.821701973333334e-05, 4.821701973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2760 loss: 2.3357 iter time (s): 4.209 samples/sec: 30.411
g0133:  iteration     2760/10000000 | consumed samples:       353280 | consumed tokens:    723517440 | elapsed time per iteration (ms): 4247.1 | learning rate: 4.822E-05 | global batch size:   128 | lm loss: 2.288189E+00 | loss scale: 65536.0 | grad norm: 1.329 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.138 | tokens per gpu per second (tgs): 1928.834 | TFLOPs: 15.52 |
g0113: [2024-08-02 21:34:36,177] [INFO] [logging.py:96:log_dist] [Rank 0] step=2770, skipped=0, lr=[4.8391782400000004e-05, 4.8391782400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2770 loss: 2.3052 iter time (s): 4.107 samples/sec: 31.166
g0133:  iteration     2770/10000000 | consumed samples:       354560 | consumed tokens:    726138880 | elapsed time per iteration (ms): 4139.3 | learning rate: 4.839E-05 | global batch size:   128 | lm loss: 2.291820E+00 | loss scale: 65536.0 | grad norm: 1.336 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.923 | tokens per gpu per second (tgs): 1979.079 | TFLOPs: 15.93 |
g0113: [2024-08-02 21:35:18,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=2780, skipped=0, lr=[4.856654506666667e-05, 4.856654506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2780 loss: 2.3107 iter time (s): 4.151 samples/sec: 30.840
g0133:  iteration     2780/10000000 | consumed samples:       355840 | consumed tokens:    728760320 | elapsed time per iteration (ms): 4182.7 | learning rate: 4.857E-05 | global batch size:   128 | lm loss: 2.299302E+00 | loss scale: 65536.0 | grad norm: 1.287 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.603 | tokens per gpu per second (tgs): 1958.561 | TFLOPs: 15.76 |
g0113: [2024-08-02 21:36:00,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=2790, skipped=0, lr=[4.874130773333334e-05, 4.874130773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2790 loss: 2.2970 iter time (s): 4.198 samples/sec: 30.492
g0133:  iteration     2790/10000000 | consumed samples:       357120 | consumed tokens:    731381760 | elapsed time per iteration (ms): 4230.3 | learning rate: 4.874E-05 | global batch size:   128 | lm loss: 2.294353E+00 | loss scale: 65536.0 | grad norm: 1.253 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.258 | tokens per gpu per second (tgs): 1936.498 | TFLOPs: 15.58 |
g0113: [2024-08-02 21:36:42,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=2800, skipped=0, lr=[4.891607040000001e-05, 4.891607040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2800 loss: 2.3236 iter time (s): 4.175 samples/sec: 30.661
g0133:  iteration     2800/10000000 | consumed samples:       358400 | consumed tokens:    734003200 | elapsed time per iteration (ms): 4207.8 | learning rate: 4.892E-05 | global batch size:   128 | lm loss: 2.285996E+00 | loss scale: 65536.0 | grad norm: 1.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.419 | tokens per gpu per second (tgs): 1946.838 | TFLOPs: 15.67 |
g0113: [2024-08-02 21:37:23,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=2810, skipped=0, lr=[4.9090833066666675e-05, 4.9090833066666675e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2810 loss: 2.2838 iter time (s): 4.087 samples/sec: 31.322
g0133:  iteration     2810/10000000 | consumed samples:       359680 | consumed tokens:    736624640 | elapsed time per iteration (ms): 4119.2 | learning rate: 4.909E-05 | global batch size:   128 | lm loss: 2.255313E+00 | loss scale: 65536.0 | grad norm: 1.257 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.074 | tokens per gpu per second (tgs): 1988.751 | TFLOPs: 16.00 |
g0113: [2024-08-02 21:38:03,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=2820, skipped=0, lr=[4.926559573333334e-05, 4.926559573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2820 loss: 2.2835 iter time (s): 3.986 samples/sec: 32.114
g0133:  iteration     2820/10000000 | consumed samples:       360960 | consumed tokens:    739246080 | elapsed time per iteration (ms): 4018.3 | learning rate: 4.927E-05 | global batch size:   128 | lm loss: 2.277405E+00 | loss scale: 65536.0 | grad norm: 1.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.854 | tokens per gpu per second (tgs): 2038.661 | TFLOPs: 16.41 |
g0113: [2024-08-02 21:38:45,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=2830, skipped=0, lr=[4.944035840000001e-05, 4.944035840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2830 loss: 2.2820 iter time (s): 4.105 samples/sec: 31.181
g0133:  iteration     2830/10000000 | consumed samples:       362240 | consumed tokens:    741867520 | elapsed time per iteration (ms): 4137.6 | learning rate: 4.944E-05 | global batch size:   128 | lm loss: 2.275651E+00 | loss scale: 65536.0 | grad norm: 1.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.936 | tokens per gpu per second (tgs): 1979.892 | TFLOPs: 15.93 |
g0113: [2024-08-02 21:39:26,966] [INFO] [logging.py:96:log_dist] [Rank 0] step=2840, skipped=0, lr=[4.961512106666667e-05, 4.961512106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2840 loss: 2.2584 iter time (s): 4.150 samples/sec: 30.840
g0133:  iteration     2840/10000000 | consumed samples:       363520 | consumed tokens:    744488960 | elapsed time per iteration (ms): 4183.2 | learning rate: 4.962E-05 | global batch size:   128 | lm loss: 2.269119E+00 | loss scale: 65536.0 | grad norm: 1.150 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.599 | tokens per gpu per second (tgs): 1958.314 | TFLOPs: 15.76 |
g0113: [2024-08-02 21:40:09,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=2850, skipped=0, lr=[4.978988373333333e-05, 4.978988373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2850 loss: 2.2701 iter time (s): 4.201 samples/sec: 30.467
g0133:  iteration     2850/10000000 | consumed samples:       364800 | consumed tokens:    747110400 | elapsed time per iteration (ms): 4233.8 | learning rate: 4.979E-05 | global batch size:   128 | lm loss: 2.283042E+00 | loss scale: 65536.0 | grad norm: 1.368 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.233 | tokens per gpu per second (tgs): 1934.916 | TFLOPs: 15.57 |
g0113: [2024-08-02 21:40:50,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=2860, skipped=0, lr=[4.99646464e-05, 4.99646464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2860 loss: 2.2487 iter time (s): 4.134 samples/sec: 30.963
g0133:  iteration     2860/10000000 | consumed samples:       366080 | consumed tokens:    749731840 | elapsed time per iteration (ms): 4166.3 | learning rate: 4.996E-05 | global batch size:   128 | lm loss: 2.275596E+00 | loss scale: 65536.0 | grad norm: 1.270 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.722 | tokens per gpu per second (tgs): 1966.236 | TFLOPs: 15.82 |
g0113: [2024-08-02 21:41:31,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=2870, skipped=0, lr=[5.0139409066666664e-05, 5.0139409066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2870 loss: 2.2935 iter time (s): 4.024 samples/sec: 31.808
g0133:  iteration     2870/10000000 | consumed samples:       367360 | consumed tokens:    752353280 | elapsed time per iteration (ms): 4056.6 | learning rate: 5.014E-05 | global batch size:   128 | lm loss: 2.264971E+00 | loss scale: 65536.0 | grad norm: 1.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.554 | tokens per gpu per second (tgs): 2019.427 | TFLOPs: 16.25 |
g0113: [2024-08-02 21:42:12,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=2880, skipped=0, lr=[5.031417173333333e-05, 5.031417173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2880 loss: 2.2606 iter time (s): 4.085 samples/sec: 31.337
g0133:  iteration     2880/10000000 | consumed samples:       368640 | consumed tokens:    754974720 | elapsed time per iteration (ms): 4117.1 | learning rate: 5.031E-05 | global batch size:   128 | lm loss: 2.275146E+00 | loss scale: 65536.0 | grad norm: 1.162 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.090 | tokens per gpu per second (tgs): 1989.737 | TFLOPs: 16.01 |
g0113: [2024-08-02 21:42:55,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=2890, skipped=0, lr=[5.0488934399999996e-05, 5.0488934399999996e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2890 loss: 2.2957 iter time (s): 4.254 samples/sec: 30.093
g0133:  iteration     2890/10000000 | consumed samples:       369920 | consumed tokens:    757596160 | elapsed time per iteration (ms): 4286.0 | learning rate: 5.049E-05 | global batch size:   128 | lm loss: 2.255964E+00 | loss scale: 65536.0 | grad norm: 1.164 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.865 | tokens per gpu per second (tgs): 1911.338 | TFLOPs: 15.38 |
g0113: [2024-08-02 21:43:37,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=2900, skipped=0, lr=[5.066369706666666e-05, 5.066369706666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2900 loss: 2.2137 iter time (s): 4.161 samples/sec: 30.760
g0133:  iteration     2900/10000000 | consumed samples:       371200 | consumed tokens:    760217600 | elapsed time per iteration (ms): 4193.6 | learning rate: 5.066E-05 | global batch size:   128 | lm loss: 2.254866E+00 | loss scale: 65536.0 | grad norm: 1.150 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.522 | tokens per gpu per second (tgs): 1953.439 | TFLOPs: 15.72 |
g0113: [2024-08-02 21:44:18,789] [INFO] [logging.py:96:log_dist] [Rank 0] step=2910, skipped=0, lr=[5.083845973333333e-05, 5.083845973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2910 loss: 2.2679 iter time (s): 4.096 samples/sec: 31.251
g0133:  iteration     2910/10000000 | consumed samples:       372480 | consumed tokens:    762839040 | elapsed time per iteration (ms): 4128.5 | learning rate: 5.084E-05 | global batch size:   128 | lm loss: 2.258877E+00 | loss scale: 65536.0 | grad norm: 1.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.004 | tokens per gpu per second (tgs): 1984.252 | TFLOPs: 15.97 |
g0113: [2024-08-02 21:44:58,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=2920, skipped=0, lr=[5.10132224e-05, 5.10132224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2920 loss: 2.2667 iter time (s): 3.978 samples/sec: 32.174
g0133:  iteration     2920/10000000 | consumed samples:       373760 | consumed tokens:    765460480 | elapsed time per iteration (ms): 4010.9 | learning rate: 5.101E-05 | global batch size:   128 | lm loss: 2.260095E+00 | loss scale: 65536.0 | grad norm: 1.091 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.913 | tokens per gpu per second (tgs): 2042.410 | TFLOPs: 16.44 |
g0113: [2024-08-02 21:45:39,097] [INFO] [logging.py:96:log_dist] [Rank 0] step=2930, skipped=0, lr=[5.118798506666667e-05, 5.118798506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2930 loss: 2.2783 iter time (s): 3.987 samples/sec: 32.107
g0133:  iteration     2930/10000000 | consumed samples:       375040 | consumed tokens:    768081920 | elapsed time per iteration (ms): 4019.8 | learning rate: 5.119E-05 | global batch size:   128 | lm loss: 2.266710E+00 | loss scale: 65536.0 | grad norm: 1.137 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.843 | tokens per gpu per second (tgs): 2037.937 | TFLOPs: 16.40 |
g0113: [2024-08-02 21:46:20,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=2940, skipped=0, lr=[5.1362747733333334e-05, 5.1362747733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2940 loss: 2.2518 iter time (s): 4.157 samples/sec: 30.795
g0133:  iteration     2940/10000000 | consumed samples:       376320 | consumed tokens:    770703360 | elapsed time per iteration (ms): 4189.4 | learning rate: 5.136E-05 | global batch size:   128 | lm loss: 2.252709E+00 | loss scale: 65536.0 | grad norm: 1.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.553 | tokens per gpu per second (tgs): 1955.397 | TFLOPs: 15.74 |
g0113: [2024-08-02 21:47:03,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=2950, skipped=0, lr=[5.15375104e-05, 5.15375104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2950 loss: 2.2606 iter time (s): 4.183 samples/sec: 30.597
g0133:  iteration     2950/10000000 | consumed samples:       377600 | consumed tokens:    773324800 | elapsed time per iteration (ms): 4215.9 | learning rate: 5.154E-05 | global batch size:   128 | lm loss: 2.247329E+00 | loss scale: 65536.0 | grad norm: 1.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.361 | tokens per gpu per second (tgs): 1943.100 | TFLOPs: 15.64 |
g0113: [2024-08-02 21:47:44,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=2960, skipped=0, lr=[5.1712273066666666e-05, 5.1712273066666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2960 loss: 2.2543 iter time (s): 4.097 samples/sec: 31.244
g0133:  iteration     2960/10000000 | consumed samples:       378880 | consumed tokens:    775946240 | elapsed time per iteration (ms): 4130.2 | learning rate: 5.171E-05 | global batch size:   128 | lm loss: 2.262353E+00 | loss scale: 65536.0 | grad norm: 1.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.991 | tokens per gpu per second (tgs): 1983.448 | TFLOPs: 15.96 |
g0113: [2024-08-02 21:48:26,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=2970, skipped=0, lr=[5.188703573333333e-05, 5.188703573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2970 loss: 2.2644 iter time (s): 4.174 samples/sec: 30.663
g0133:  iteration     2970/10000000 | consumed samples:       380160 | consumed tokens:    778567680 | elapsed time per iteration (ms): 4207.5 | learning rate: 5.189E-05 | global batch size:   128 | lm loss: 2.242264E+00 | loss scale: 65536.0 | grad norm: 1.120 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.422 | tokens per gpu per second (tgs): 1947.011 | TFLOPs: 15.67 |
g0113: [2024-08-02 21:49:09,172] [INFO] [logging.py:96:log_dist] [Rank 0] step=2980, skipped=0, lr=[5.20617984e-05, 5.20617984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2980 loss: 2.2117 iter time (s): 4.232 samples/sec: 30.247
g0133:  iteration     2980/10000000 | consumed samples:       381440 | consumed tokens:    781189120 | elapsed time per iteration (ms): 4264.6 | learning rate: 5.206E-05 | global batch size:   128 | lm loss: 2.237185E+00 | loss scale: 65536.0 | grad norm: 1.089 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.015 | tokens per gpu per second (tgs): 1920.947 | TFLOPs: 15.46 |
g0113: [2024-08-02 21:49:50,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=2990, skipped=0, lr=[5.2236561066666664e-05, 5.2236561066666664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 2990 loss: 2.2548 iter time (s): 4.114 samples/sec: 31.117
g0133:  iteration     2990/10000000 | consumed samples:       382720 | consumed tokens:    783810560 | elapsed time per iteration (ms): 4146.0 | learning rate: 5.224E-05 | global batch size:   128 | lm loss: 2.257295E+00 | loss scale: 65536.0 | grad norm: 1.115 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.873 | tokens per gpu per second (tgs): 1975.858 | TFLOPs: 15.90 |
g0113: [2024-08-02 21:50:32,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=3000, skipped=0, lr=[5.241132373333334e-05, 5.241132373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3000 loss: 2.2501 iter time (s): 4.149 samples/sec: 30.851
g0133:  iteration     3000/10000000 | consumed samples:       384000 | consumed tokens:    786432000 | elapsed time per iteration (ms): 4181.9 | learning rate: 5.241E-05 | global batch size:   128 | lm loss: 2.233031E+00 | loss scale: 65536.0 | grad norm: 1.141 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.608 | tokens per gpu per second (tgs): 1958.915 | TFLOPs: 15.76 |
g0133: ------------------------------------------------------------------------------------------------
g0133:  validation loss at iteration 3000 | lm loss value: 2.243478E+00 | lm loss PPL: 9.426056E+00 | 
g0133: ------------------------------------------------------------------------------------------------
g0113: saving checkpoint at iteration    3000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: [2024-08-02 21:56:53,266] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step3000 is about to be saved!
g0113: [2024-08-02 21:56:53,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0113: [2024-08-02 21:56:53,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0133: [2024-08-02 21:56:53,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0133: [2024-08-02 21:56:53,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0133: [2024-08-02 21:56:53,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0130: [2024-08-02 21:56:53,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0130: [2024-08-02 21:56:53,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0130: [2024-08-02 21:56:53,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0113: [2024-08-02 21:56:53,274] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0128: [2024-08-02 21:56:53,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0128: [2024-08-02 21:56:53,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0128: [2024-08-02 21:56:53,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0125: [2024-08-02 21:56:53,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0125: [2024-08-02 21:56:53,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0125: [2024-08-02 21:56:53,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0129: [2024-08-02 21:56:53,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0129: [2024-08-02 21:56:53,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0129: [2024-08-02 21:56:53,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0132: [2024-08-02 21:56:53,276] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0132: [2024-08-02 21:56:53,276] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0132: [2024-08-02 21:56:53,277] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0131: [2024-08-02 21:56:53,281] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0131: [2024-08-02 21:56:53,281] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0131: [2024-08-02 21:56:53,281] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0133: [2024-08-02 21:56:53,299] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_23-model_00-model_states.pt...
g0132: [2024-08-02 21:56:53,305] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_20-model_00-model_states.pt...
g0130: [2024-08-02 21:56:53,307] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_14-model_00-model_states.pt...
g0129: [2024-08-02 21:56:53,307] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_11-model_00-model_states.pt...
g0128: [2024-08-02 21:56:53,310] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_08-model_00-model_states.pt...
g0125: [2024-08-02 21:56:53,311] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_05-model_00-model_states.pt...
g0131: [2024-08-02 21:56:53,316] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_17-model_00-model_states.pt...
g0113: [2024-08-02 21:56:53,319] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_01-model_00-model_states.pt...
g0129: [2024-08-02 21:56:53,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_11-model_00-model_states.pt.
g0128: [2024-08-02 21:56:53,419] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_08-model_00-model_states.pt.
g0129: [2024-08-02 21:56:53,448] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_12-model_00-model_states.pt...
g0131: [2024-08-02 21:56:53,451] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_17-model_00-model_states.pt.
g0128: [2024-08-02 21:56:53,459] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_09-model_00-model_states.pt...
g0131: [2024-08-02 21:56:53,490] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_18-model_00-model_states.pt...
g0113: [2024-08-02 21:56:53,501] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_01-model_00-model_states.pt.
g0130: [2024-08-02 21:56:53,520] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_14-model_00-model_states.pt.
g0113: [2024-08-02 21:56:53,528] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_02-model_00-model_states.pt...
g0130: [2024-08-02 21:56:53,554] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_15-model_00-model_states.pt...
g0133: [2024-08-02 21:56:53,586] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_23-model_00-model_states.pt.
g0133: [2024-08-02 21:56:53,587] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_24-model_00-model_states.pt...
g0133: [2024-08-02 21:56:53,589] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_24-model_00-model_states.pt.
g0128: [2024-08-02 21:56:53,611] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_09-model_00-model_states.pt.
g0133: [2024-08-02 21:56:53,635] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_25-model_00-model_states.pt...
g0131: [2024-08-02 21:56:53,640] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_18-model_00-model_states.pt.
g0128: [2024-08-02 21:56:53,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_10-model_00-model_states.pt...
g0113: [2024-08-02 21:56:53,668] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_02-model_00-model_states.pt.
g0130: [2024-08-02 21:56:53,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_15-model_00-model_states.pt.
g0131: [2024-08-02 21:56:53,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_19-model_00-model_states.pt...
g0113: [2024-08-02 21:56:53,695] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_03-model_00-model_states.pt...
g0130: [2024-08-02 21:56:53,702] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_16-model_00-model_states.pt...
g0129: [2024-08-02 21:56:53,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_12-model_00-model_states.pt.
g0129: [2024-08-02 21:56:53,735] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_13-model_00-model_states.pt...
g0131: [2024-08-02 21:56:53,773] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_19-model_00-model_states.pt.
g0131: [2024-08-02 21:56:53,775] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_05_model_states.pt...
g0128: [2024-08-02 21:56:53,786] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_10-model_00-model_states.pt.
g0128: [2024-08-02 21:56:53,788] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_02_model_states.pt...
g0113: [2024-08-02 21:56:53,809] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_03-model_00-model_states.pt.
g0113: [2024-08-02 21:56:53,833] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_04-model_00-model_states.pt...
g0130: [2024-08-02 21:56:53,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_16-model_00-model_states.pt.
g0130: [2024-08-02 21:56:53,851] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_04_model_states.pt...
g0133: [2024-08-02 21:56:53,869] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_25-model_00-model_states.pt.
g0129: [2024-08-02 21:56:53,869] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_13-model_00-model_states.pt.
g0133: [2024-08-02 21:56:53,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_07_model_states.pt...
g0129: [2024-08-02 21:56:53,871] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_03_model_states.pt...
g0125: [2024-08-02 21:56:53,919] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_05-model_00-model_states.pt.
g0125: [2024-08-02 21:56:53,957] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_06-model_00-model_states.pt...
g0125: [2024-08-02 21:56:54,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_06-model_00-model_states.pt.
g0125: [2024-08-02 21:56:54,109] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_07-model_00-model_states.pt...
g0125: [2024-08-02 21:56:54,212] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_07-model_00-model_states.pt.
g0125: [2024-08-02 21:56:54,214] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_01_model_states.pt...
g0132: [2024-08-02 21:56:54,331] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_20-model_00-model_states.pt.
g0132: [2024-08-02 21:56:54,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_21-model_00-model_states.pt...
g0132: [2024-08-02 21:56:54,487] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_21-model_00-model_states.pt.
g0132: [2024-08-02 21:56:54,517] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_22-model_00-model_states.pt...
g0132: [2024-08-02 21:56:54,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_22-model_00-model_states.pt.
g0132: [2024-08-02 21:56:54,641] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_06_model_states.pt...
g0131: [2024-08-02 21:56:56,136] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_05_model_states.pt.
g0131: [2024-08-02 21:56:56,136] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0128: [2024-08-02 21:56:56,246] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_02_model_states.pt.
g0128: [2024-08-02 21:56:56,247] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0129: [2024-08-02 21:56:56,248] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_03_model_states.pt.
g0129: [2024-08-02 21:56:56,249] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0113: [2024-08-02 21:56:56,266] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/layer_04-model_00-model_states.pt.
g0113: [2024-08-02 21:56:56,267] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt
g0113: [2024-08-02 21:56:56,267] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt...
g0130: [2024-08-02 21:56:56,389] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_04_model_states.pt.
g0130: [2024-08-02 21:56:56,389] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0133: [2024-08-02 21:56:56,578] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_07_model_states.pt.
g0133: [2024-08-02 21:56:56,579] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0125: [2024-08-02 21:56:56,581] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_01_model_states.pt.
g0125: [2024-08-02 21:56:56,581] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0132: [2024-08-02 21:56:57,030] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_06_model_states.pt.
g0132: [2024-08-02 21:56:57,030] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0113: [2024-08-02 21:56:59,640] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step3000/mp_rank_00_model_states.pt.
g0113: [2024-08-02 21:56:59,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3000 is ready now!
g0113:   successfully saved checkpoint at iteration    3000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: Checkpoint Save GB: 22.521, GB/Sec: 3.52, Latency(second): 6.392
g0133: (min, max) time across ranks (ms):
g0133:     save-checkpoint ................................: (6391.09, 6392.04)
g0132: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0132: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0113: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0128: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0131: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0128: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0133: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0113: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0113: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0131: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0113: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0113: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0129: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0133: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0132: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0131: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0128: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0130: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0125: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0131: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0125: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0131: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0132: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0130: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0128: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0132: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0133: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0133: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0131: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0133: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0113: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0125: [2024-08-02 21:57:03,590] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536 to 131072
g0113: [2024-08-02 21:57:40,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=3010, skipped=0, lr=[5.25860864e-05, 5.25860864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3010 loss: 2.2319 iter time (s): 4.059 samples/sec: 31.537
g0133:  iteration     3010/10000000 | consumed samples:       385280 | consumed tokens:    789053440 | elapsed time per iteration (ms): 42808.4 | learning rate: 5.259E-05 | global batch size:   128 | lm loss: 2.238369E+00 | loss scale: 131072.0 | grad norm: 1.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.990 | tokens per gpu per second (tgs): 191.364 | TFLOPs: 1.54 |
g0113: [2024-08-02 21:58:20,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=3020, skipped=0, lr=[5.276084906666667e-05, 5.276084906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3020 loss: 2.2656 iter time (s): 3.990 samples/sec: 32.079
g0133:  iteration     3020/10000000 | consumed samples:       386560 | consumed tokens:    791674880 | elapsed time per iteration (ms): 4022.8 | learning rate: 5.276E-05 | global batch size:   128 | lm loss: 2.237751E+00 | loss scale: 131072.0 | grad norm: 1.085 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.819 | tokens per gpu per second (tgs): 2036.390 | TFLOPs: 16.39 |
g0113: [2024-08-02 21:59:01,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=3030, skipped=0, lr=[5.2935611733333335e-05, 5.2935611733333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3030 loss: 2.2633 iter time (s): 4.078 samples/sec: 31.391
g0133:  iteration     3030/10000000 | consumed samples:       387840 | consumed tokens:    794296320 | elapsed time per iteration (ms): 4110.5 | learning rate: 5.294E-05 | global batch size:   128 | lm loss: 2.240048E+00 | loss scale: 131072.0 | grad norm: 1.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.140 | tokens per gpu per second (tgs): 1992.968 | TFLOPs: 16.04 |
g0113: [2024-08-02 21:59:43,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=3040, skipped=0, lr=[5.31103744e-05, 5.31103744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3040 loss: 2.2546 iter time (s): 4.130 samples/sec: 30.989
g0133:  iteration     3040/10000000 | consumed samples:       389120 | consumed tokens:    796917760 | elapsed time per iteration (ms): 4163.3 | learning rate: 5.311E-05 | global batch size:   128 | lm loss: 2.233352E+00 | loss scale: 131072.0 | grad norm: 1.140 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.745 | tokens per gpu per second (tgs): 1967.661 | TFLOPs: 15.83 |
g0113: [2024-08-02 22:00:23,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=3050, skipped=0, lr=[5.328513706666667e-05, 5.328513706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3050 loss: 2.2486 iter time (s): 3.983 samples/sec: 32.135
g0133:  iteration     3050/10000000 | consumed samples:       390400 | consumed tokens:    799539200 | elapsed time per iteration (ms): 4015.7 | learning rate: 5.329E-05 | global batch size:   128 | lm loss: 2.234489E+00 | loss scale: 131072.0 | grad norm: 1.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.875 | tokens per gpu per second (tgs): 2039.999 | TFLOPs: 16.42 |
g0113: [2024-08-02 22:01:05,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=3060, skipped=0, lr=[5.345989973333333e-05, 5.345989973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3060 loss: 2.2186 iter time (s): 4.167 samples/sec: 30.720
g0133:  iteration     3060/10000000 | consumed samples:       391680 | consumed tokens:    802160640 | elapsed time per iteration (ms): 4199.2 | learning rate: 5.346E-05 | global batch size:   128 | lm loss: 2.219299E+00 | loss scale: 131072.0 | grad norm: 1.119 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.482 | tokens per gpu per second (tgs): 1950.827 | TFLOPs: 15.70 |
g0113: [2024-08-02 22:01:48,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=3070, skipped=0, lr=[5.36346624e-05, 5.36346624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3070 loss: 2.2598 iter time (s): 4.291 samples/sec: 29.831
g0133:  iteration     3070/10000000 | consumed samples:       392960 | consumed tokens:    804782080 | elapsed time per iteration (ms): 4325.2 | learning rate: 5.363E-05 | global batch size:   128 | lm loss: 2.235811E+00 | loss scale: 131072.0 | grad norm: 1.145 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.594 | tokens per gpu per second (tgs): 1894.015 | TFLOPs: 15.24 |
g0113: [2024-08-02 22:02:29,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=3080, skipped=0, lr=[5.3809425066666665e-05, 5.3809425066666665e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3080 loss: 2.2037 iter time (s): 4.067 samples/sec: 31.473
g0133:  iteration     3080/10000000 | consumed samples:       394240 | consumed tokens:    807403520 | elapsed time per iteration (ms): 4100.1 | learning rate: 5.381E-05 | global batch size:   128 | lm loss: 2.215515E+00 | loss scale: 131072.0 | grad norm: 1.145 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.219 | tokens per gpu per second (tgs): 1998.022 | TFLOPs: 16.08 |
g0113: [2024-08-02 22:03:11,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=3090, skipped=0, lr=[5.398418773333334e-05, 5.398418773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3090 loss: 2.2373 iter time (s): 4.129 samples/sec: 31.002
g0133:  iteration     3090/10000000 | consumed samples:       395520 | consumed tokens:    810024960 | elapsed time per iteration (ms): 4161.8 | learning rate: 5.398E-05 | global batch size:   128 | lm loss: 2.238094E+00 | loss scale: 131072.0 | grad norm: 1.050 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.756 | tokens per gpu per second (tgs): 1968.360 | TFLOPs: 15.84 |
g0113: [2024-08-02 22:03:53,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=3100, skipped=0, lr=[5.4158950400000004e-05, 5.4158950400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3100 loss: 2.1837 iter time (s): 4.157 samples/sec: 30.789
g0133:  iteration     3100/10000000 | consumed samples:       396800 | consumed tokens:    812646400 | elapsed time per iteration (ms): 4190.1 | learning rate: 5.416E-05 | global batch size:   128 | lm loss: 2.215920E+00 | loss scale: 131072.0 | grad norm: 1.108 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.548 | tokens per gpu per second (tgs): 1955.102 | TFLOPs: 15.73 |
g0113: [2024-08-02 22:04:35,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=3110, skipped=0, lr=[5.433371306666667e-05, 5.433371306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3110 loss: 2.2087 iter time (s): 4.223 samples/sec: 30.311
g0133:  iteration     3110/10000000 | consumed samples:       398080 | consumed tokens:    815267840 | elapsed time per iteration (ms): 4255.2 | learning rate: 5.433E-05 | global batch size:   128 | lm loss: 2.234345E+00 | loss scale: 131072.0 | grad norm: 1.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.081 | tokens per gpu per second (tgs): 1925.160 | TFLOPs: 15.49 |
g0113: [2024-08-02 22:05:17,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=3120, skipped=0, lr=[5.4508475733333336e-05, 5.4508475733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3120 loss: 2.2165 iter time (s): 4.147 samples/sec: 30.867
g0133:  iteration     3120/10000000 | consumed samples:       399360 | consumed tokens:    817889280 | elapsed time per iteration (ms): 4179.7 | learning rate: 5.451E-05 | global batch size:   128 | lm loss: 2.211483E+00 | loss scale: 131072.0 | grad norm: 1.077 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.624 | tokens per gpu per second (tgs): 1959.948 | TFLOPs: 15.77 |
g0113: [2024-08-02 22:05:59,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=3130, skipped=0, lr=[5.46832384e-05, 5.46832384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3130 loss: 2.2322 iter time (s): 4.095 samples/sec: 31.261
g0133:  iteration     3130/10000000 | consumed samples:       400640 | consumed tokens:    820510720 | elapsed time per iteration (ms): 4127.2 | learning rate: 5.468E-05 | global batch size:   128 | lm loss: 2.221229E+00 | loss scale: 131072.0 | grad norm: 1.161 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.014 | tokens per gpu per second (tgs): 1984.875 | TFLOPs: 15.97 |
g0113: [2024-08-02 22:06:40,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=3140, skipped=0, lr=[5.485800106666667e-05, 5.485800106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3140 loss: 2.2148 iter time (s): 4.147 samples/sec: 30.865
g0133:  iteration     3140/10000000 | consumed samples:       401920 | consumed tokens:    823132160 | elapsed time per iteration (ms): 4179.6 | learning rate: 5.486E-05 | global batch size:   128 | lm loss: 2.219427E+00 | loss scale: 131072.0 | grad norm: 1.053 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.625 | tokens per gpu per second (tgs): 1959.985 | TFLOPs: 15.77 |
g0113: [2024-08-02 22:07:23,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=3150, skipped=0, lr=[5.5032763733333334e-05, 5.5032763733333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3150 loss: 2.2361 iter time (s): 4.266 samples/sec: 30.007
g0133:  iteration     3150/10000000 | consumed samples:       403200 | consumed tokens:    825753600 | elapsed time per iteration (ms): 4298.2 | learning rate: 5.503E-05 | global batch size:   128 | lm loss: 2.223720E+00 | loss scale: 131072.0 | grad norm: 1.078 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.780 | tokens per gpu per second (tgs): 1905.930 | TFLOPs: 15.34 |
g0113: [2024-08-02 22:08:05,133] [INFO] [logging.py:96:log_dist] [Rank 0] step=3160, skipped=0, lr=[5.52075264e-05, 5.52075264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3160 loss: 2.2318 iter time (s): 4.098 samples/sec: 31.233
g0133:  iteration     3160/10000000 | consumed samples:       404480 | consumed tokens:    828375040 | elapsed time per iteration (ms): 4131.1 | learning rate: 5.521E-05 | global batch size:   128 | lm loss: 2.208332E+00 | loss scale: 131072.0 | grad norm: 1.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.985 | tokens per gpu per second (tgs): 1983.021 | TFLOPs: 15.96 |
g0113: [2024-08-02 22:08:45,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=3170, skipped=0, lr=[5.5382289066666667e-05, 5.5382289066666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3170 loss: 2.2309 iter time (s): 3.981 samples/sec: 32.156
g0133:  iteration     3170/10000000 | consumed samples:       405760 | consumed tokens:    830996480 | elapsed time per iteration (ms): 4013.0 | learning rate: 5.538E-05 | global batch size:   128 | lm loss: 2.215779E+00 | loss scale: 131072.0 | grad norm: 1.059 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.896 | tokens per gpu per second (tgs): 2041.356 | TFLOPs: 16.43 |
g0113: [2024-08-02 22:09:26,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=3180, skipped=0, lr=[5.555705173333334e-05, 5.555705173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3180 loss: 2.1783 iter time (s): 4.113 samples/sec: 31.122
g0133:  iteration     3180/10000000 | consumed samples:       407040 | consumed tokens:    833617920 | elapsed time per iteration (ms): 4145.9 | learning rate: 5.556E-05 | global batch size:   128 | lm loss: 2.191076E+00 | loss scale: 131072.0 | grad norm: 1.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.874 | tokens per gpu per second (tgs): 1975.938 | TFLOPs: 15.90 |
g0113: [2024-08-02 22:10:07,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=3190, skipped=0, lr=[5.5731814400000005e-05, 5.5731814400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3190 loss: 2.1950 iter time (s): 4.075 samples/sec: 31.410
g0133:  iteration     3190/10000000 | consumed samples:       408320 | consumed tokens:    836239360 | elapsed time per iteration (ms): 4107.6 | learning rate: 5.573E-05 | global batch size:   128 | lm loss: 2.197819E+00 | loss scale: 131072.0 | grad norm: 1.105 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.162 | tokens per gpu per second (tgs): 1994.339 | TFLOPs: 16.05 |
g0113: [2024-08-02 22:10:49,220] [INFO] [logging.py:96:log_dist] [Rank 0] step=3200, skipped=0, lr=[5.590657706666667e-05, 5.590657706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3200 loss: 2.1964 iter time (s): 4.109 samples/sec: 31.147
g0133:  iteration     3200/10000000 | consumed samples:       409600 | consumed tokens:    838860800 | elapsed time per iteration (ms): 4142.0 | learning rate: 5.591E-05 | global batch size:   128 | lm loss: 2.206564E+00 | loss scale: 131072.0 | grad norm: 1.040 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.903 | tokens per gpu per second (tgs): 1977.788 | TFLOPs: 15.92 |
g0113: [2024-08-02 22:11:30,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=3210, skipped=0, lr=[5.608133973333334e-05, 5.608133973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3210 loss: 2.2307 iter time (s): 4.112 samples/sec: 31.126
g0133:  iteration     3210/10000000 | consumed samples:       410880 | consumed tokens:    841482240 | elapsed time per iteration (ms): 4144.9 | learning rate: 5.608E-05 | global batch size:   128 | lm loss: 2.221119E+00 | loss scale: 131072.0 | grad norm: 1.109 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.881 | tokens per gpu per second (tgs): 1976.406 | TFLOPs: 15.90 |
g0113: [2024-08-02 22:12:13,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=3220, skipped=0, lr=[5.6256102400000004e-05, 5.6256102400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3220 loss: 2.1867 iter time (s): 4.210 samples/sec: 30.401
g0133:  iteration     3220/10000000 | consumed samples:       412160 | consumed tokens:    844103680 | elapsed time per iteration (ms): 4242.9 | learning rate: 5.626E-05 | global batch size:   128 | lm loss: 2.208182E+00 | loss scale: 131072.0 | grad norm: 1.107 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.168 | tokens per gpu per second (tgs): 1930.742 | TFLOPs: 15.54 |
g0113: [2024-08-02 22:12:54,657] [INFO] [logging.py:96:log_dist] [Rank 0] step=3230, skipped=0, lr=[5.643086506666667e-05, 5.643086506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3230 loss: 2.2103 iter time (s): 4.122 samples/sec: 31.052
g0133:  iteration     3230/10000000 | consumed samples:       413440 | consumed tokens:    846725120 | elapsed time per iteration (ms): 4155.8 | learning rate: 5.643E-05 | global batch size:   128 | lm loss: 2.203744E+00 | loss scale: 131072.0 | grad norm: 1.037 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.800 | tokens per gpu per second (tgs): 1971.222 | TFLOPs: 15.86 |
g0113: [2024-08-02 22:13:36,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=3240, skipped=0, lr=[5.6605627733333336e-05, 5.6605627733333336e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3240 loss: 2.1794 iter time (s): 4.189 samples/sec: 30.558
g0133:  iteration     3240/10000000 | consumed samples:       414720 | consumed tokens:    849346560 | elapsed time per iteration (ms): 4221.1 | learning rate: 5.661E-05 | global batch size:   128 | lm loss: 2.208019E+00 | loss scale: 131072.0 | grad norm: 1.080 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.324 | tokens per gpu per second (tgs): 1940.741 | TFLOPs: 15.62 |
g0113: [2024-08-02 22:14:18,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=3250, skipped=0, lr=[5.67803904e-05, 5.67803904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3250 loss: 2.1662 iter time (s): 4.093 samples/sec: 31.270
g0133:  iteration     3250/10000000 | consumed samples:       416000 | consumed tokens:    851968000 | elapsed time per iteration (ms): 4126.2 | learning rate: 5.678E-05 | global batch size:   128 | lm loss: 2.201799E+00 | loss scale: 131072.0 | grad norm: 1.090 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.021 | tokens per gpu per second (tgs): 1985.348 | TFLOPs: 15.98 |
g0113: [2024-08-02 22:14:59,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=3260, skipped=0, lr=[5.695515306666667e-05, 5.695515306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3260 loss: 2.2257 iter time (s): 4.111 samples/sec: 31.137
g0133:  iteration     3260/10000000 | consumed samples:       417280 | consumed tokens:    854589440 | elapsed time per iteration (ms): 4143.3 | learning rate: 5.696E-05 | global batch size:   128 | lm loss: 2.204562E+00 | loss scale: 131072.0 | grad norm: 0.996 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.893 | tokens per gpu per second (tgs): 1977.146 | TFLOPs: 15.91 |
g0113: [2024-08-02 22:15:40,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=3270, skipped=0, lr=[5.712991573333334e-05, 5.712991573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3270 loss: 2.1696 iter time (s): 4.093 samples/sec: 31.276
g0133:  iteration     3270/10000000 | consumed samples:       418560 | consumed tokens:    857210880 | elapsed time per iteration (ms): 4125.0 | learning rate: 5.713E-05 | global batch size:   128 | lm loss: 2.211341E+00 | loss scale: 131072.0 | grad norm: 0.987 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.030 | tokens per gpu per second (tgs): 1985.929 | TFLOPs: 15.98 |
g0113: [2024-08-02 22:16:20,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=3280, skipped=0, lr=[5.730467840000001e-05, 5.730467840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3280 loss: 2.2164 iter time (s): 3.982 samples/sec: 32.148
g0133:  iteration     3280/10000000 | consumed samples:       419840 | consumed tokens:    859832320 | elapsed time per iteration (ms): 4014.1 | learning rate: 5.730E-05 | global batch size:   128 | lm loss: 2.205674E+00 | loss scale: 131072.0 | grad norm: 1.105 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.888 | tokens per gpu per second (tgs): 2040.830 | TFLOPs: 16.42 |
g0113: [2024-08-02 22:17:01,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=3290, skipped=0, lr=[5.747944106666667e-05, 5.747944106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3290 loss: 2.2147 iter time (s): 4.064 samples/sec: 31.498
g0133:  iteration     3290/10000000 | consumed samples:       421120 | consumed tokens:    862453760 | elapsed time per iteration (ms): 4096.2 | learning rate: 5.748E-05 | global batch size:   128 | lm loss: 2.193583E+00 | loss scale: 131072.0 | grad norm: 1.017 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.248 | tokens per gpu per second (tgs): 1999.889 | TFLOPs: 16.09 |
g0113: [2024-08-02 22:17:41,984] [INFO] [logging.py:96:log_dist] [Rank 0] step=3300, skipped=0, lr=[5.765420373333334e-05, 5.765420373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3300 loss: 2.1650 iter time (s): 3.974 samples/sec: 32.210
g0133:  iteration     3300/10000000 | consumed samples:       422400 | consumed tokens:    865075200 | elapsed time per iteration (ms): 4006.6 | learning rate: 5.765E-05 | global batch size:   128 | lm loss: 2.192936E+00 | loss scale: 131072.0 | grad norm: 0.989 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.947 | tokens per gpu per second (tgs): 2044.614 | TFLOPs: 16.45 |
g0113: [2024-08-02 22:18:24,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=3310, skipped=0, lr=[5.7828966400000005e-05, 5.7828966400000005e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3310 loss: 2.2318 iter time (s): 4.171 samples/sec: 30.685
g0133:  iteration     3310/10000000 | consumed samples:       423680 | consumed tokens:    867696640 | elapsed time per iteration (ms): 4204.6 | learning rate: 5.783E-05 | global batch size:   128 | lm loss: 2.207775E+00 | loss scale: 131072.0 | grad norm: 1.099 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.443 | tokens per gpu per second (tgs): 1948.350 | TFLOPs: 15.68 |
g0113: [2024-08-02 22:19:05,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=3320, skipped=0, lr=[5.800372906666667e-05, 5.800372906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3320 loss: 2.2560 iter time (s): 4.138 samples/sec: 30.936
g0133:  iteration     3320/10000000 | consumed samples:       424960 | consumed tokens:    870318080 | elapsed time per iteration (ms): 4170.4 | learning rate: 5.800E-05 | global batch size:   128 | lm loss: 2.205711E+00 | loss scale: 131072.0 | grad norm: 1.037 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.693 | tokens per gpu per second (tgs): 1964.331 | TFLOPs: 15.81 |
g0113: [2024-08-02 22:19:47,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=3330, skipped=0, lr=[5.817849173333334e-05, 5.817849173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3330 loss: 2.1373 iter time (s): 4.170 samples/sec: 30.693
g0133:  iteration     3330/10000000 | consumed samples:       426240 | consumed tokens:    872939520 | elapsed time per iteration (ms): 4203.2 | learning rate: 5.818E-05 | global batch size:   128 | lm loss: 2.190762E+00 | loss scale: 131072.0 | grad norm: 0.954 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.453 | tokens per gpu per second (tgs): 1948.986 | TFLOPs: 15.68 |
g0113: [2024-08-02 22:20:29,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=3340, skipped=0, lr=[5.83532544e-05, 5.83532544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3340 loss: 2.1714 iter time (s): 4.125 samples/sec: 31.034
g0133:  iteration     3340/10000000 | consumed samples:       427520 | consumed tokens:    875560960 | elapsed time per iteration (ms): 4157.2 | learning rate: 5.835E-05 | global batch size:   128 | lm loss: 2.188191E+00 | loss scale: 131072.0 | grad norm: 1.103 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.790 | tokens per gpu per second (tgs): 1970.562 | TFLOPs: 15.86 |
g0113: [2024-08-02 22:21:10,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=3350, skipped=0, lr=[5.852801706666667e-05, 5.852801706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3350 loss: 2.1809 iter time (s): 4.090 samples/sec: 31.294
g0133:  iteration     3350/10000000 | consumed samples:       428800 | consumed tokens:    878182400 | elapsed time per iteration (ms): 4123.4 | learning rate: 5.853E-05 | global batch size:   128 | lm loss: 2.184406E+00 | loss scale: 131072.0 | grad norm: 1.001 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.043 | tokens per gpu per second (tgs): 1986.721 | TFLOPs: 15.99 |
g0113: [2024-08-02 22:21:52,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=3360, skipped=0, lr=[5.870277973333334e-05, 5.870277973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3360 loss: 2.1380 iter time (s): 4.155 samples/sec: 30.810
g0133:  iteration     3360/10000000 | consumed samples:       430080 | consumed tokens:    880803840 | elapsed time per iteration (ms): 4187.3 | learning rate: 5.870E-05 | global batch size:   128 | lm loss: 2.178336E+00 | loss scale: 131072.0 | grad norm: 0.963 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.569 | tokens per gpu per second (tgs): 1956.408 | TFLOPs: 15.74 |
g0113: [2024-08-02 22:22:34,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=3370, skipped=0, lr=[5.887754240000001e-05, 5.887754240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3370 loss: 2.1683 iter time (s): 4.129 samples/sec: 30.997
g0133:  iteration     3370/10000000 | consumed samples:       431360 | consumed tokens:    883425280 | elapsed time per iteration (ms): 4161.9 | learning rate: 5.888E-05 | global batch size:   128 | lm loss: 2.172080E+00 | loss scale: 131072.0 | grad norm: 0.984 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.755 | tokens per gpu per second (tgs): 1968.330 | TFLOPs: 15.84 |
g0113: [2024-08-02 22:23:15,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=3380, skipped=0, lr=[5.9052305066666674e-05, 5.9052305066666674e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3380 loss: 2.1693 iter time (s): 4.147 samples/sec: 30.869
g0133:  iteration     3380/10000000 | consumed samples:       432640 | consumed tokens:    886046720 | elapsed time per iteration (ms): 4179.8 | learning rate: 5.905E-05 | global batch size:   128 | lm loss: 2.180568E+00 | loss scale: 131072.0 | grad norm: 0.946 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.624 | tokens per gpu per second (tgs): 1959.922 | TFLOPs: 15.77 |
g0113: [2024-08-02 22:23:56,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=3390, skipped=0, lr=[5.922706773333334e-05, 5.922706773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3390 loss: 2.1823 iter time (s): 4.058 samples/sec: 31.546
g0133:  iteration     3390/10000000 | consumed samples:       433920 | consumed tokens:    888668160 | elapsed time per iteration (ms): 4090.1 | learning rate: 5.923E-05 | global batch size:   128 | lm loss: 2.180310E+00 | loss scale: 131072.0 | grad norm: 1.013 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.295 | tokens per gpu per second (tgs): 2002.881 | TFLOPs: 16.12 |
g0113: [2024-08-02 22:24:35,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=3400, skipped=0, lr=[5.9401830400000006e-05, 5.9401830400000006e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3400 loss: 2.1510 iter time (s): 3.880 samples/sec: 32.992
g0133:  iteration     3400/10000000 | consumed samples:       435200 | consumed tokens:    891289600 | elapsed time per iteration (ms): 3913.6 | learning rate: 5.940E-05 | global batch size:   128 | lm loss: 2.169184E+00 | loss scale: 131072.0 | grad norm: 0.954 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.706 | tokens per gpu per second (tgs): 2093.201 | TFLOPs: 16.84 |
g0113: [2024-08-02 22:25:16,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=3410, skipped=0, lr=[5.957659306666667e-05, 5.957659306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3410 loss: 2.1524 iter time (s): 4.027 samples/sec: 31.784
g0133:  iteration     3410/10000000 | consumed samples:       436480 | consumed tokens:    893911040 | elapsed time per iteration (ms): 4059.8 | learning rate: 5.958E-05 | global batch size:   128 | lm loss: 2.160899E+00 | loss scale: 131072.0 | grad norm: 0.915 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.528 | tokens per gpu per second (tgs): 2017.813 | TFLOPs: 16.24 |
g0113: [2024-08-02 22:25:57,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=3420, skipped=0, lr=[5.975135573333334e-05, 5.975135573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3420 loss: 2.2052 iter time (s): 4.055 samples/sec: 31.564
g0133:  iteration     3420/10000000 | consumed samples:       437760 | consumed tokens:    896532480 | elapsed time per iteration (ms): 4087.7 | learning rate: 5.975E-05 | global batch size:   128 | lm loss: 2.167493E+00 | loss scale: 131072.0 | grad norm: 1.031 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.313 | tokens per gpu per second (tgs): 2004.056 | TFLOPs: 16.13 |
g0113: [2024-08-02 22:26:38,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=3430, skipped=0, lr=[5.9926118400000004e-05, 5.9926118400000004e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3430 loss: 2.2028 iter time (s): 4.038 samples/sec: 31.695
g0133:  iteration     3430/10000000 | consumed samples:       439040 | consumed tokens:    899153920 | elapsed time per iteration (ms): 4071.0 | learning rate: 5.993E-05 | global batch size:   128 | lm loss: 2.175638E+00 | loss scale: 131072.0 | grad norm: 0.923 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.442 | tokens per gpu per second (tgs): 2012.263 | TFLOPs: 16.19 |
g0113: [2024-08-02 22:27:19,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=3440, skipped=0, lr=[6.010088106666667e-05, 6.010088106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3440 loss: 2.1738 iter time (s): 4.153 samples/sec: 30.822
g0133:  iteration     3440/10000000 | consumed samples:       440320 | consumed tokens:    901775360 | elapsed time per iteration (ms): 4186.3 | learning rate: 6.010E-05 | global batch size:   128 | lm loss: 2.173019E+00 | loss scale: 131072.0 | grad norm: 1.006 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.576 | tokens per gpu per second (tgs): 1956.857 | TFLOPs: 15.75 |
g0113: [2024-08-02 22:28:02,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=3450, skipped=0, lr=[6.027564373333334e-05, 6.027564373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3450 loss: 2.1106 iter time (s): 4.188 samples/sec: 30.564
g0133:  iteration     3450/10000000 | consumed samples:       441600 | consumed tokens:    904396800 | elapsed time per iteration (ms): 4236.0 | learning rate: 6.028E-05 | global batch size:   128 | lm loss: 2.153667E+00 | loss scale: 131072.0 | grad norm: 0.947 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.217 | tokens per gpu per second (tgs): 1933.877 | TFLOPs: 15.56 |
g0113: [2024-08-02 22:28:43,945] [INFO] [logging.py:96:log_dist] [Rank 0] step=3460, skipped=0, lr=[6.045040640000001e-05, 6.045040640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3460 loss: 2.1913 iter time (s): 4.127 samples/sec: 31.016
g0133:  iteration     3460/10000000 | consumed samples:       442880 | consumed tokens:    907018240 | elapsed time per iteration (ms): 4163.6 | learning rate: 6.045E-05 | global batch size:   128 | lm loss: 2.169822E+00 | loss scale: 131072.0 | grad norm: 0.930 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.743 | tokens per gpu per second (tgs): 1967.528 | TFLOPs: 15.83 |
g0113: [2024-08-02 22:29:26,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=3470, skipped=0, lr=[6.0625169066666676e-05, 6.0625169066666676e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3470 loss: 2.1685 iter time (s): 4.260 samples/sec: 30.050
g0133:  iteration     3470/10000000 | consumed samples:       444160 | consumed tokens:    909639680 | elapsed time per iteration (ms): 4293.8 | learning rate: 6.063E-05 | global batch size:   128 | lm loss: 2.170297E+00 | loss scale: 131072.0 | grad norm: 0.966 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.810 | tokens per gpu per second (tgs): 1907.857 | TFLOPs: 15.35 |
g0113: [2024-08-02 22:30:08,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=3480, skipped=0, lr=[6.079993173333334e-05, 6.079993173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3480 loss: 2.1403 iter time (s): 4.096 samples/sec: 31.252
g0133:  iteration     3480/10000000 | consumed samples:       445440 | consumed tokens:    912261120 | elapsed time per iteration (ms): 4128.1 | learning rate: 6.080E-05 | global batch size:   128 | lm loss: 2.154910E+00 | loss scale: 131072.0 | grad norm: 0.957 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.007 | tokens per gpu per second (tgs): 1984.441 | TFLOPs: 15.97 |
g0113: [2024-08-02 22:30:50,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=3490, skipped=0, lr=[6.097469440000001e-05, 6.097469440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3490 loss: 2.1348 iter time (s): 4.220 samples/sec: 30.333
g0133:  iteration     3490/10000000 | consumed samples:       446720 | consumed tokens:    914882560 | elapsed time per iteration (ms): 4252.4 | learning rate: 6.097E-05 | global batch size:   128 | lm loss: 2.162057E+00 | loss scale: 131072.0 | grad norm: 0.974 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.100 | tokens per gpu per second (tgs): 1926.426 | TFLOPs: 15.50 |
g0113: [2024-08-02 22:31:32,212] [INFO] [logging.py:96:log_dist] [Rank 0] step=3500, skipped=0, lr=[6.114945706666668e-05, 6.114945706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3500 loss: 2.1722 iter time (s): 4.120 samples/sec: 31.071
g0133:  iteration     3500/10000000 | consumed samples:       448000 | consumed tokens:    917504000 | elapsed time per iteration (ms): 4152.2 | learning rate: 6.115E-05 | global batch size:   128 | lm loss: 2.157523E+00 | loss scale: 131072.0 | grad norm: 0.928 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.827 | tokens per gpu per second (tgs): 1972.916 | TFLOPs: 15.88 |
g0113: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0125: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0131: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0131: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0131: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0131: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0130: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0129: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0133: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0133: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0125: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0113: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0130: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0130: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0132: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0130: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0132: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0132: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0113: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0113: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0130: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0128: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0133: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0129: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0128: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0129: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0128: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0128: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0129: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0128: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0129: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0133: [2024-08-02 22:31:36,519] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072 to 262144
g0113: [2024-08-02 22:32:14,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=3510, skipped=0, lr=[6.132421973333333e-05, 6.132421973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3510 loss: 2.1398 iter time (s): 4.204 samples/sec: 30.444
g0133:  iteration     3510/10000000 | consumed samples:       449280 | consumed tokens:    920125440 | elapsed time per iteration (ms): 4237.1 | learning rate: 6.132E-05 | global batch size:   128 | lm loss: 2.159949E+00 | loss scale: 262144.0 | grad norm: 0.957 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.209 | tokens per gpu per second (tgs): 1933.401 | TFLOPs: 15.56 |
g0113: [2024-08-02 22:32:56,998] [INFO] [logging.py:96:log_dist] [Rank 0] step=3520, skipped=0, lr=[6.14989824e-05, 6.14989824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3520 loss: 2.1357 iter time (s): 4.209 samples/sec: 30.413
g0133:  iteration     3520/10000000 | consumed samples:       450560 | consumed tokens:    922746880 | elapsed time per iteration (ms): 4241.4 | learning rate: 6.150E-05 | global batch size:   128 | lm loss: 2.151627E+00 | loss scale: 262144.0 | grad norm: 1.024 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.179 | tokens per gpu per second (tgs): 1931.435 | TFLOPs: 15.54 |
g0113: [2024-08-02 22:33:38,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=3530, skipped=0, lr=[6.167374506666667e-05, 6.167374506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3530 loss: 2.0781 iter time (s): 4.070 samples/sec: 31.449
g0133:  iteration     3530/10000000 | consumed samples:       451840 | consumed tokens:    925368320 | elapsed time per iteration (ms): 4102.5 | learning rate: 6.167E-05 | global batch size:   128 | lm loss: 2.147970E+00 | loss scale: 262144.0 | grad norm: 0.943 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.200 | tokens per gpu per second (tgs): 1996.823 | TFLOPs: 16.07 |
g0113: [2024-08-02 22:34:20,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=3540, skipped=0, lr=[6.184850773333333e-05, 6.184850773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3540 loss: 2.1806 iter time (s): 4.217 samples/sec: 30.356
g0133:  iteration     3540/10000000 | consumed samples:       453120 | consumed tokens:    927989760 | elapsed time per iteration (ms): 4251.9 | learning rate: 6.185E-05 | global batch size:   128 | lm loss: 2.161726E+00 | loss scale: 262144.0 | grad norm: 0.951 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.104 | tokens per gpu per second (tgs): 1926.672 | TFLOPs: 15.50 |
g0113: [2024-08-02 22:35:00,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=3550, skipped=0, lr=[6.20232704e-05, 6.20232704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3550 loss: 2.1412 iter time (s): 3.983 samples/sec: 32.137
g0133:  iteration     3550/10000000 | consumed samples:       454400 | consumed tokens:    930611200 | elapsed time per iteration (ms): 4018.3 | learning rate: 6.202E-05 | global batch size:   128 | lm loss: 2.149439E+00 | loss scale: 262144.0 | grad norm: 0.909 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.854 | tokens per gpu per second (tgs): 2038.654 | TFLOPs: 16.41 |
g0113: [2024-08-02 22:35:40,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=3560, skipped=0, lr=[6.219803306666666e-05, 6.219803306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3560 loss: 2.1533 iter time (s): 3.935 samples/sec: 32.530
g0133:  iteration     3560/10000000 | consumed samples:       455680 | consumed tokens:    933232640 | elapsed time per iteration (ms): 3967.6 | learning rate: 6.220E-05 | global batch size:   128 | lm loss: 2.141605E+00 | loss scale: 262144.0 | grad norm: 0.922 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.261 | tokens per gpu per second (tgs): 2064.728 | TFLOPs: 16.62 |
g0113: [2024-08-02 22:36:21,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=3570, skipped=0, lr=[6.237279573333333e-05, 6.237279573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3570 loss: 2.1414 iter time (s): 4.087 samples/sec: 31.316
g0133:  iteration     3570/10000000 | consumed samples:       456960 | consumed tokens:    935854080 | elapsed time per iteration (ms): 4120.5 | learning rate: 6.237E-05 | global batch size:   128 | lm loss: 2.145268E+00 | loss scale: 262144.0 | grad norm: 0.901 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.064 | tokens per gpu per second (tgs): 1988.116 | TFLOPs: 16.00 |
g0113: [2024-08-02 22:37:02,857] [INFO] [logging.py:96:log_dist] [Rank 0] step=3580, skipped=0, lr=[6.25475584e-05, 6.25475584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3580 loss: 2.1780 iter time (s): 4.092 samples/sec: 31.280
g0133:  iteration     3580/10000000 | consumed samples:       458240 | consumed tokens:    938475520 | elapsed time per iteration (ms): 4124.8 | learning rate: 6.255E-05 | global batch size:   128 | lm loss: 2.151112E+00 | loss scale: 262144.0 | grad norm: 0.942 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.031 | tokens per gpu per second (tgs): 1986.014 | TFLOPs: 15.98 |
g0113: [2024-08-02 22:37:45,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=3590, skipped=0, lr=[6.272232106666666e-05, 6.272232106666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3590 loss: 2.1154 iter time (s): 4.195 samples/sec: 30.511
g0133:  iteration     3590/10000000 | consumed samples:       459520 | consumed tokens:    941096960 | elapsed time per iteration (ms): 4228.3 | learning rate: 6.272E-05 | global batch size:   128 | lm loss: 2.139310E+00 | loss scale: 262144.0 | grad norm: 0.904 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.272 | tokens per gpu per second (tgs): 1937.405 | TFLOPs: 15.59 |
g0113: [2024-08-02 22:38:25,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=3600, skipped=0, lr=[6.289708373333333e-05, 6.289708373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3600 loss: 2.1691 iter time (s): 4.040 samples/sec: 31.686
g0133:  iteration     3600/10000000 | consumed samples:       460800 | consumed tokens:    943718400 | elapsed time per iteration (ms): 4072.9 | learning rate: 6.290E-05 | global batch size:   128 | lm loss: 2.142267E+00 | loss scale: 262144.0 | grad norm: 0.892 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.428 | tokens per gpu per second (tgs): 2011.364 | TFLOPs: 16.19 |
g0113: [2024-08-02 22:39:08,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=3610, skipped=0, lr=[6.30718464e-05, 6.30718464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3610 loss: 2.0701 iter time (s): 4.215 samples/sec: 30.371
g0133:  iteration     3610/10000000 | consumed samples:       462080 | consumed tokens:    946339840 | elapsed time per iteration (ms): 4247.0 | learning rate: 6.307E-05 | global batch size:   128 | lm loss: 2.137064E+00 | loss scale: 262144.0 | grad norm: 0.954 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.139 | tokens per gpu per second (tgs): 1928.888 | TFLOPs: 15.52 |
g0113: [2024-08-02 22:39:50,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=3620, skipped=0, lr=[6.324660906666667e-05, 6.324660906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3620 loss: 2.1289 iter time (s): 4.230 samples/sec: 30.259
g0133:  iteration     3620/10000000 | consumed samples:       463360 | consumed tokens:    948961280 | elapsed time per iteration (ms): 4264.0 | learning rate: 6.325E-05 | global batch size:   128 | lm loss: 2.136496E+00 | loss scale: 262144.0 | grad norm: 0.889 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.019 | tokens per gpu per second (tgs): 1921.187 | TFLOPs: 15.46 |
g0113: [2024-08-02 22:40:33,120] [INFO] [logging.py:96:log_dist] [Rank 0] step=3630, skipped=0, lr=[6.342137173333334e-05, 6.342137173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3630 loss: 2.1429 iter time (s): 4.181 samples/sec: 30.612
g0133:  iteration     3630/10000000 | consumed samples:       464640 | consumed tokens:    951582720 | elapsed time per iteration (ms): 4214.0 | learning rate: 6.342E-05 | global batch size:   128 | lm loss: 2.143449E+00 | loss scale: 262144.0 | grad norm: 0.879 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.375 | tokens per gpu per second (tgs): 1943.987 | TFLOPs: 15.64 |
g0113: [2024-08-02 22:41:15,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=3640, skipped=0, lr=[6.35961344e-05, 6.35961344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3640 loss: 2.1937 iter time (s): 4.160 samples/sec: 30.768
g0133:  iteration     3640/10000000 | consumed samples:       465920 | consumed tokens:    954204160 | elapsed time per iteration (ms): 4194.2 | learning rate: 6.360E-05 | global batch size:   128 | lm loss: 2.140174E+00 | loss scale: 262144.0 | grad norm: 0.929 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.518 | tokens per gpu per second (tgs): 1953.175 | TFLOPs: 15.72 |
g0113: [2024-08-02 22:41:57,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=3650, skipped=0, lr=[6.377089706666667e-05, 6.377089706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3650 loss: 2.0889 iter time (s): 4.255 samples/sec: 30.083
g0133:  iteration     3650/10000000 | consumed samples:       467200 | consumed tokens:    956825600 | elapsed time per iteration (ms): 4287.7 | learning rate: 6.377E-05 | global batch size:   128 | lm loss: 2.126978E+00 | loss scale: 262144.0 | grad norm: 0.915 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.853 | tokens per gpu per second (tgs): 1910.593 | TFLOPs: 15.37 |
g0113: [2024-08-02 22:42:38,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=3660, skipped=0, lr=[6.394565973333334e-05, 6.394565973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3660 loss: 2.1184 iter time (s): 4.026 samples/sec: 31.790
g0133:  iteration     3660/10000000 | consumed samples:       468480 | consumed tokens:    959447040 | elapsed time per iteration (ms): 4060.0 | learning rate: 6.395E-05 | global batch size:   128 | lm loss: 2.130759E+00 | loss scale: 262144.0 | grad norm: 0.890 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.527 | tokens per gpu per second (tgs): 2017.724 | TFLOPs: 16.24 |
g0113: [2024-08-02 22:43:20,061] [INFO] [logging.py:96:log_dist] [Rank 0] step=3670, skipped=0, lr=[6.41204224e-05, 6.41204224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3670 loss: 2.2027 iter time (s): 4.119 samples/sec: 31.074
g0133:  iteration     3670/10000000 | consumed samples:       469760 | consumed tokens:    962068480 | elapsed time per iteration (ms): 4152.2 | learning rate: 6.412E-05 | global batch size:   128 | lm loss: 2.139851E+00 | loss scale: 262144.0 | grad norm: 0.885 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.827 | tokens per gpu per second (tgs): 1972.938 | TFLOPs: 15.88 |
g0113: [2024-08-02 22:44:00,609] [INFO] [logging.py:96:log_dist] [Rank 0] step=3680, skipped=0, lr=[6.429518506666667e-05, 6.429518506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3680 loss: 2.0874 iter time (s): 4.022 samples/sec: 31.829
g0133:  iteration     3680/10000000 | consumed samples:       471040 | consumed tokens:    964689920 | elapsed time per iteration (ms): 4054.7 | learning rate: 6.430E-05 | global batch size:   128 | lm loss: 2.119330E+00 | loss scale: 262144.0 | grad norm: 0.897 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.568 | tokens per gpu per second (tgs): 2020.362 | TFLOPs: 16.26 |
g0113: [2024-08-02 22:44:41,581] [INFO] [logging.py:96:log_dist] [Rank 0] step=3690, skipped=0, lr=[6.446994773333334e-05, 6.446994773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3690 loss: 2.1191 iter time (s): 4.065 samples/sec: 31.492
g0133:  iteration     3690/10000000 | consumed samples:       472320 | consumed tokens:    967311360 | elapsed time per iteration (ms): 4097.2 | learning rate: 6.447E-05 | global batch size:   128 | lm loss: 2.117731E+00 | loss scale: 262144.0 | grad norm: 0.877 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.241 | tokens per gpu per second (tgs): 1999.427 | TFLOPs: 16.09 |
g0113: [2024-08-02 22:45:22,492] [INFO] [logging.py:96:log_dist] [Rank 0] step=3700, skipped=0, lr=[6.46447104e-05, 6.46447104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3700 loss: 2.1171 iter time (s): 4.057 samples/sec: 31.548
g0133:  iteration     3700/10000000 | consumed samples:       473600 | consumed tokens:    969932800 | elapsed time per iteration (ms): 4091.1 | learning rate: 6.464E-05 | global batch size:   128 | lm loss: 2.130826E+00 | loss scale: 262144.0 | grad norm: 0.864 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.287 | tokens per gpu per second (tgs): 2002.394 | TFLOPs: 16.11 |
g0113: [2024-08-02 22:46:04,534] [INFO] [logging.py:96:log_dist] [Rank 0] step=3710, skipped=0, lr=[6.481947306666667e-05, 6.481947306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3710 loss: 2.1157 iter time (s): 4.171 samples/sec: 30.687
g0133:  iteration     3710/10000000 | consumed samples:       474880 | consumed tokens:    972554240 | elapsed time per iteration (ms): 4204.1 | learning rate: 6.482E-05 | global batch size:   128 | lm loss: 2.124662E+00 | loss scale: 262144.0 | grad norm: 0.851 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.446 | tokens per gpu per second (tgs): 1948.551 | TFLOPs: 15.68 |
g0113: [2024-08-02 22:46:46,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=3720, skipped=0, lr=[6.499423573333333e-05, 6.499423573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3720 loss: 2.1172 iter time (s): 4.194 samples/sec: 30.521
g0133:  iteration     3720/10000000 | consumed samples:       476160 | consumed tokens:    975175680 | elapsed time per iteration (ms): 4228.1 | learning rate: 6.499E-05 | global batch size:   128 | lm loss: 2.118181E+00 | loss scale: 262144.0 | grad norm: 0.904 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.273 | tokens per gpu per second (tgs): 1937.502 | TFLOPs: 15.59 |
g0113: [2024-08-02 22:47:28,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=3730, skipped=0, lr=[6.51689984e-05, 6.51689984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3730 loss: 2.0769 iter time (s): 4.175 samples/sec: 30.662
g0133:  iteration     3730/10000000 | consumed samples:       477440 | consumed tokens:    977797120 | elapsed time per iteration (ms): 4207.5 | learning rate: 6.517E-05 | global batch size:   128 | lm loss: 2.104218E+00 | loss scale: 262144.0 | grad norm: 0.949 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.422 | tokens per gpu per second (tgs): 1947.013 | TFLOPs: 15.67 |
g0113: [2024-08-02 22:48:09,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=3740, skipped=0, lr=[6.534376106666667e-05, 6.534376106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3740 loss: 2.1274 iter time (s): 3.984 samples/sec: 32.125
g0133:  iteration     3740/10000000 | consumed samples:       478720 | consumed tokens:    980418560 | elapsed time per iteration (ms): 4016.9 | learning rate: 6.534E-05 | global batch size:   128 | lm loss: 2.121120E+00 | loss scale: 262144.0 | grad norm: 0.851 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.866 | tokens per gpu per second (tgs): 2039.406 | TFLOPs: 16.41 |
g0113: [2024-08-02 22:48:50,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=3750, skipped=0, lr=[6.551852373333333e-05, 6.551852373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3750 loss: 2.1089 iter time (s): 4.149 samples/sec: 30.851
g0133:  iteration     3750/10000000 | consumed samples:       480000 | consumed tokens:    983040000 | elapsed time per iteration (ms): 4182.2 | learning rate: 6.552E-05 | global batch size:   128 | lm loss: 2.118494E+00 | loss scale: 262144.0 | grad norm: 0.878 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.606 | tokens per gpu per second (tgs): 1958.798 | TFLOPs: 15.76 |
g0113: [2024-08-02 22:49:32,470] [INFO] [logging.py:96:log_dist] [Rank 0] step=3760, skipped=0, lr=[6.56932864e-05, 6.56932864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3760 loss: 2.1512 iter time (s): 4.126 samples/sec: 31.022
g0133:  iteration     3760/10000000 | consumed samples:       481280 | consumed tokens:    985661440 | elapsed time per iteration (ms): 4158.9 | learning rate: 6.569E-05 | global batch size:   128 | lm loss: 2.109719E+00 | loss scale: 262144.0 | grad norm: 0.930 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.777 | tokens per gpu per second (tgs): 1969.734 | TFLOPs: 15.85 |
g0113: [2024-08-02 22:50:13,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=3770, skipped=0, lr=[6.586804906666666e-05, 6.586804906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3770 loss: 2.1235 iter time (s): 4.066 samples/sec: 31.480
g0133:  iteration     3770/10000000 | consumed samples:       482560 | consumed tokens:    988282880 | elapsed time per iteration (ms): 4098.8 | learning rate: 6.587E-05 | global batch size:   128 | lm loss: 2.106053E+00 | loss scale: 262144.0 | grad norm: 0.879 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.229 | tokens per gpu per second (tgs): 1998.639 | TFLOPs: 16.08 |
g0113: [2024-08-02 22:50:54,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=3780, skipped=0, lr=[6.604281173333333e-05, 6.604281173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3780 loss: 2.1264 iter time (s): 4.115 samples/sec: 31.103
g0133:  iteration     3780/10000000 | consumed samples:       483840 | consumed tokens:    990904320 | elapsed time per iteration (ms): 4147.9 | learning rate: 6.604E-05 | global batch size:   128 | lm loss: 2.100824E+00 | loss scale: 262144.0 | grad norm: 0.899 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.859 | tokens per gpu per second (tgs): 1974.996 | TFLOPs: 15.89 |
g0113: [2024-08-02 22:51:37,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=3790, skipped=0, lr=[6.62175744e-05, 6.62175744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3790 loss: 2.0903 iter time (s): 4.236 samples/sec: 30.219
g0133:  iteration     3790/10000000 | consumed samples:       485120 | consumed tokens:    993525760 | elapsed time per iteration (ms): 4269.4 | learning rate: 6.622E-05 | global batch size:   128 | lm loss: 2.102439E+00 | loss scale: 262144.0 | grad norm: 0.811 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.981 | tokens per gpu per second (tgs): 1918.792 | TFLOPs: 15.44 |
g0113: [2024-08-02 22:52:19,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=3800, skipped=0, lr=[6.639233706666668e-05, 6.639233706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3800 loss: 2.1284 iter time (s): 4.185 samples/sec: 30.587
g0133:  iteration     3800/10000000 | consumed samples:       486400 | consumed tokens:    996147200 | elapsed time per iteration (ms): 4218.0 | learning rate: 6.639E-05 | global batch size:   128 | lm loss: 2.093121E+00 | loss scale: 262144.0 | grad norm: 0.870 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.346 | tokens per gpu per second (tgs): 1942.152 | TFLOPs: 15.63 |
g0113: [2024-08-02 22:52:59,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=3810, skipped=0, lr=[6.656709973333334e-05, 6.656709973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3810 loss: 2.1184 iter time (s): 3.979 samples/sec: 32.172
g0133:  iteration     3810/10000000 | consumed samples:       487680 | consumed tokens:    998768640 | elapsed time per iteration (ms): 4011.2 | learning rate: 6.657E-05 | global batch size:   128 | lm loss: 2.106476E+00 | loss scale: 262144.0 | grad norm: 1.051 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.911 | tokens per gpu per second (tgs): 2042.295 | TFLOPs: 16.43 |
g0113: [2024-08-02 22:53:40,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=3820, skipped=0, lr=[6.674186240000001e-05, 6.674186240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3820 loss: 2.0744 iter time (s): 4.060 samples/sec: 31.527
g0133:  iteration     3820/10000000 | consumed samples:       488960 | consumed tokens:   1001390080 | elapsed time per iteration (ms): 4094.7 | learning rate: 6.674E-05 | global batch size:   128 | lm loss: 2.100959E+00 | loss scale: 262144.0 | grad norm: 0.928 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.260 | tokens per gpu per second (tgs): 2000.652 | TFLOPs: 16.10 |
g0113: [2024-08-02 22:54:21,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=3830, skipped=0, lr=[6.691662506666667e-05, 6.691662506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3830 loss: 2.0946 iter time (s): 4.005 samples/sec: 31.963
g0133:  iteration     3830/10000000 | consumed samples:       490240 | consumed tokens:   1004011520 | elapsed time per iteration (ms): 4038.0 | learning rate: 6.692E-05 | global batch size:   128 | lm loss: 2.112562E+00 | loss scale: 262144.0 | grad norm: 0.834 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.699 | tokens per gpu per second (tgs): 2028.721 | TFLOPs: 16.33 |
g0113: [2024-08-02 22:55:03,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=3840, skipped=0, lr=[6.709138773333334e-05, 6.709138773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3840 loss: 2.0830 iter time (s): 4.168 samples/sec: 30.710
g0133:  iteration     3840/10000000 | consumed samples:       491520 | consumed tokens:   1006632960 | elapsed time per iteration (ms): 4201.2 | learning rate: 6.709E-05 | global batch size:   128 | lm loss: 2.105187E+00 | loss scale: 262144.0 | grad norm: 0.872 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.468 | tokens per gpu per second (tgs): 1949.934 | TFLOPs: 15.69 |
g0113: [2024-08-02 22:55:45,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=3850, skipped=0, lr=[6.72661504e-05, 6.72661504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3850 loss: 2.0531 iter time (s): 4.158 samples/sec: 30.786
g0133:  iteration     3850/10000000 | consumed samples:       492800 | consumed tokens:   1009254400 | elapsed time per iteration (ms): 4190.9 | learning rate: 6.727E-05 | global batch size:   128 | lm loss: 2.109943E+00 | loss scale: 262144.0 | grad norm: 0.823 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.543 | tokens per gpu per second (tgs): 1954.734 | TFLOPs: 15.73 |
g0113: [2024-08-02 22:56:25,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=3860, skipped=0, lr=[6.744091306666667e-05, 6.744091306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3860 loss: 2.1133 iter time (s): 3.970 samples/sec: 32.246
g0133:  iteration     3860/10000000 | consumed samples:       494080 | consumed tokens:   1011875840 | elapsed time per iteration (ms): 4002.1 | learning rate: 6.744E-05 | global batch size:   128 | lm loss: 2.105971E+00 | loss scale: 262144.0 | grad norm: 0.904 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.983 | tokens per gpu per second (tgs): 2046.930 | TFLOPs: 16.47 |
g0113: [2024-08-02 22:57:08,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=3870, skipped=0, lr=[6.761567573333334e-05, 6.761567573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3870 loss: 2.0897 iter time (s): 4.254 samples/sec: 30.089
g0133:  iteration     3870/10000000 | consumed samples:       495360 | consumed tokens:   1014497280 | elapsed time per iteration (ms): 4286.9 | learning rate: 6.762E-05 | global batch size:   128 | lm loss: 2.096565E+00 | loss scale: 262144.0 | grad norm: 0.866 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.858 | tokens per gpu per second (tgs): 1910.935 | TFLOPs: 15.38 |
g0113: [2024-08-02 22:57:50,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=3880, skipped=0, lr=[6.77904384e-05, 6.77904384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3880 loss: 2.1013 iter time (s): 4.224 samples/sec: 30.305
g0133:  iteration     3880/10000000 | consumed samples:       496640 | consumed tokens:   1017118720 | elapsed time per iteration (ms): 4256.5 | learning rate: 6.779E-05 | global batch size:   128 | lm loss: 2.094612E+00 | loss scale: 262144.0 | grad norm: 0.852 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.072 | tokens per gpu per second (tgs): 1924.596 | TFLOPs: 15.49 |
g0113: [2024-08-02 22:58:33,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=3890, skipped=0, lr=[6.796520106666667e-05, 6.796520106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3890 loss: 2.0780 iter time (s): 4.281 samples/sec: 29.899
g0133:  iteration     3890/10000000 | consumed samples:       497920 | consumed tokens:   1019740160 | elapsed time per iteration (ms): 4316.0 | learning rate: 6.797E-05 | global batch size:   128 | lm loss: 2.090844E+00 | loss scale: 262144.0 | grad norm: 0.856 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.657 | tokens per gpu per second (tgs): 1898.032 | TFLOPs: 15.27 |
g0113: [2024-08-02 22:59:15,384] [INFO] [logging.py:96:log_dist] [Rank 0] step=3900, skipped=0, lr=[6.813996373333334e-05, 6.813996373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3900 loss: 2.1089 iter time (s): 4.127 samples/sec: 31.017
g0133:  iteration     3900/10000000 | consumed samples:       499200 | consumed tokens:   1022361600 | elapsed time per iteration (ms): 4159.9 | learning rate: 6.814E-05 | global batch size:   128 | lm loss: 2.090625E+00 | loss scale: 262144.0 | grad norm: 0.845 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.770 | tokens per gpu per second (tgs): 1969.259 | TFLOPs: 15.85 |
g0113: [2024-08-02 22:59:56,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=3910, skipped=0, lr=[6.83147264e-05, 6.83147264e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3910 loss: 2.0626 iter time (s): 4.128 samples/sec: 31.007
g0133:  iteration     3910/10000000 | consumed samples:       500480 | consumed tokens:   1024983040 | elapsed time per iteration (ms): 4160.7 | learning rate: 6.831E-05 | global batch size:   128 | lm loss: 2.096702E+00 | loss scale: 262144.0 | grad norm: 0.830 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.764 | tokens per gpu per second (tgs): 1968.915 | TFLOPs: 15.84 |
g0113: [2024-08-02 23:00:39,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=3920, skipped=0, lr=[6.848948906666667e-05, 6.848948906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3920 loss: 2.0494 iter time (s): 4.214 samples/sec: 30.372
g0133:  iteration     3920/10000000 | consumed samples:       501760 | consumed tokens:   1027604480 | elapsed time per iteration (ms): 4246.9 | learning rate: 6.849E-05 | global batch size:   128 | lm loss: 2.082806E+00 | loss scale: 262144.0 | grad norm: 0.845 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.139 | tokens per gpu per second (tgs): 1928.925 | TFLOPs: 15.52 |
g0113: [2024-08-02 23:01:21,055] [INFO] [logging.py:96:log_dist] [Rank 0] step=3930, skipped=0, lr=[6.866425173333333e-05, 6.866425173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3930 loss: 2.0618 iter time (s): 4.127 samples/sec: 31.016
g0133:  iteration     3930/10000000 | consumed samples:       503040 | consumed tokens:   1030225920 | elapsed time per iteration (ms): 4159.3 | learning rate: 6.866E-05 | global batch size:   128 | lm loss: 2.073292E+00 | loss scale: 262144.0 | grad norm: 0.843 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.774 | tokens per gpu per second (tgs): 1969.549 | TFLOPs: 15.85 |
g0113: [2024-08-02 23:02:02,244] [INFO] [logging.py:96:log_dist] [Rank 0] step=3940, skipped=0, lr=[6.88390144e-05, 6.88390144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3940 loss: 2.0774 iter time (s): 4.086 samples/sec: 31.323
g0133:  iteration     3940/10000000 | consumed samples:       504320 | consumed tokens:   1032847360 | elapsed time per iteration (ms): 4121.5 | learning rate: 6.884E-05 | global batch size:   128 | lm loss: 2.073182E+00 | loss scale: 262144.0 | grad norm: 0.832 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.057 | tokens per gpu per second (tgs): 1987.630 | TFLOPs: 15.99 |
g0113: [2024-08-02 23:02:43,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=3950, skipped=0, lr=[6.901377706666667e-05, 6.901377706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3950 loss: 2.0816 iter time (s): 4.094 samples/sec: 31.262
g0133:  iteration     3950/10000000 | consumed samples:       505600 | consumed tokens:   1035468800 | elapsed time per iteration (ms): 4127.0 | learning rate: 6.901E-05 | global batch size:   128 | lm loss: 2.095567E+00 | loss scale: 262144.0 | grad norm: 0.820 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.015 | tokens per gpu per second (tgs): 1984.978 | TFLOPs: 15.97 |
g0113: [2024-08-02 23:03:25,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=3960, skipped=0, lr=[6.918853973333333e-05, 6.918853973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3960 loss: 2.0960 iter time (s): 4.125 samples/sec: 31.033
g0133:  iteration     3960/10000000 | consumed samples:       506880 | consumed tokens:   1038090240 | elapsed time per iteration (ms): 4157.3 | learning rate: 6.919E-05 | global batch size:   128 | lm loss: 2.076931E+00 | loss scale: 262144.0 | grad norm: 0.810 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.790 | tokens per gpu per second (tgs): 1970.529 | TFLOPs: 15.86 |
g0113: [2024-08-02 23:04:06,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=3970, skipped=0, lr=[6.93633024e-05, 6.93633024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3970 loss: 2.0836 iter time (s): 4.064 samples/sec: 31.498
g0133:  iteration     3970/10000000 | consumed samples:       508160 | consumed tokens:   1040711680 | elapsed time per iteration (ms): 4096.6 | learning rate: 6.936E-05 | global batch size:   128 | lm loss: 2.080266E+00 | loss scale: 262144.0 | grad norm: 0.784 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.245 | tokens per gpu per second (tgs): 1999.691 | TFLOPs: 16.09 |
g0113: [2024-08-02 23:04:47,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=3980, skipped=0, lr=[6.953806506666668e-05, 6.953806506666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3980 loss: 2.0414 iter time (s): 4.074 samples/sec: 31.422
g0133:  iteration     3980/10000000 | consumed samples:       509440 | consumed tokens:   1043333120 | elapsed time per iteration (ms): 4105.9 | learning rate: 6.954E-05 | global batch size:   128 | lm loss: 2.079296E+00 | loss scale: 262144.0 | grad norm: 0.784 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.175 | tokens per gpu per second (tgs): 1995.176 | TFLOPs: 16.06 |
g0113: [2024-08-02 23:05:28,193] [INFO] [logging.py:96:log_dist] [Rank 0] step=3990, skipped=0, lr=[6.971282773333334e-05, 6.971282773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 3990 loss: 2.0909 iter time (s): 4.073 samples/sec: 31.426
g0133:  iteration     3990/10000000 | consumed samples:       510720 | consumed tokens:   1045954560 | elapsed time per iteration (ms): 4105.5 | learning rate: 6.971E-05 | global batch size:   128 | lm loss: 2.089116E+00 | loss scale: 262144.0 | grad norm: 0.821 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.177 | tokens per gpu per second (tgs): 1995.359 | TFLOPs: 16.06 |
g0113: [2024-08-02 23:06:08,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=4000, skipped=0, lr=[6.988759040000001e-05, 6.988759040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4000 loss: 2.1175 iter time (s): 4.046 samples/sec: 31.633
g0133:  iteration     4000/10000000 | consumed samples:       512000 | consumed tokens:   1048576000 | elapsed time per iteration (ms): 4079.5 | learning rate: 6.989E-05 | global batch size:   128 | lm loss: 2.086935E+00 | loss scale: 262144.0 | grad norm: 0.798 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.377 | tokens per gpu per second (tgs): 2008.108 | TFLOPs: 16.16 |
g0133: ------------------------------------------------------------------------------------------------
g0133:  validation loss at iteration 4000 | lm loss value: 2.073534E+00 | lm loss PPL: 7.952883E+00 | 
g0133: ------------------------------------------------------------------------------------------------
g0113: saving checkpoint at iteration    4000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: [2024-08-02 23:12:28,222] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step4000 is about to be saved!
g0133: [2024-08-02 23:12:28,228] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0113: [2024-08-02 23:12:28,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0133: [2024-08-02 23:12:28,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0133: [2024-08-02 23:12:28,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0113: [2024-08-02 23:12:28,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0113: [2024-08-02 23:12:28,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0131: [2024-08-02 23:12:28,230] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0131: [2024-08-02 23:12:28,230] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0125: [2024-08-02 23:12:28,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0131: [2024-08-02 23:12:28,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0125: [2024-08-02 23:12:28,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0125: [2024-08-02 23:12:28,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0130: [2024-08-02 23:12:28,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0130: [2024-08-02 23:12:28,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0130: [2024-08-02 23:12:28,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0132: [2024-08-02 23:12:28,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0132: [2024-08-02 23:12:28,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0132: [2024-08-02 23:12:28,232] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0128: [2024-08-02 23:12:28,233] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0128: [2024-08-02 23:12:28,233] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0128: [2024-08-02 23:12:28,233] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0129: [2024-08-02 23:12:28,234] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0129: [2024-08-02 23:12:28,234] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0129: [2024-08-02 23:12:28,234] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0133: [2024-08-02 23:12:28,253] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_23-model_00-model_states.pt...
g0132: [2024-08-02 23:12:28,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_20-model_00-model_states.pt...
g0125: [2024-08-02 23:12:28,264] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_05-model_00-model_states.pt...
g0130: [2024-08-02 23:12:28,264] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_14-model_00-model_states.pt...
g0131: [2024-08-02 23:12:28,266] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_17-model_00-model_states.pt...
g0129: [2024-08-02 23:12:28,267] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_11-model_00-model_states.pt...
g0128: [2024-08-02 23:12:28,269] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_08-model_00-model_states.pt...
g0113: [2024-08-02 23:12:28,278] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_01-model_00-model_states.pt...
g0133: [2024-08-02 23:12:28,358] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_23-model_00-model_states.pt.
g0133: [2024-08-02 23:12:28,358] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_24-model_00-model_states.pt...
g0133: [2024-08-02 23:12:28,360] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_24-model_00-model_states.pt.
g0128: [2024-08-02 23:12:28,381] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_08-model_00-model_states.pt.
g0132: [2024-08-02 23:12:28,390] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_20-model_00-model_states.pt.
g0129: [2024-08-02 23:12:28,392] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_11-model_00-model_states.pt.
g0131: [2024-08-02 23:12:28,395] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_17-model_00-model_states.pt.
g0133: [2024-08-02 23:12:28,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_25-model_00-model_states.pt...
g0128: [2024-08-02 23:12:28,420] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_09-model_00-model_states.pt...
g0132: [2024-08-02 23:12:28,425] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_21-model_00-model_states.pt...
g0129: [2024-08-02 23:12:28,427] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_12-model_00-model_states.pt...
g0131: [2024-08-02 23:12:28,434] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_18-model_00-model_states.pt...
g0130: [2024-08-02 23:12:28,443] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_14-model_00-model_states.pt.
g0130: [2024-08-02 23:12:28,479] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_15-model_00-model_states.pt...
g0125: [2024-08-02 23:12:28,484] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_05-model_00-model_states.pt.
g0125: [2024-08-02 23:12:28,521] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_06-model_00-model_states.pt...
g0132: [2024-08-02 23:12:28,527] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_21-model_00-model_states.pt.
g0132: [2024-08-02 23:12:28,557] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_22-model_00-model_states.pt...
g0131: [2024-08-02 23:12:28,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_18-model_00-model_states.pt.
g0129: [2024-08-02 23:12:28,560] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_12-model_00-model_states.pt.
g0129: [2024-08-02 23:12:28,586] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_13-model_00-model_states.pt...
g0113: [2024-08-02 23:12:28,590] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_01-model_00-model_states.pt.
g0131: [2024-08-02 23:12:28,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_19-model_00-model_states.pt...
g0113: [2024-08-02 23:12:28,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_02-model_00-model_states.pt...
g0130: [2024-08-02 23:12:28,661] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_15-model_00-model_states.pt.
g0125: [2024-08-02 23:12:28,683] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_06-model_00-model_states.pt.
g0129: [2024-08-02 23:12:28,690] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_13-model_00-model_states.pt.
g0130: [2024-08-02 23:12:28,692] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_16-model_00-model_states.pt...
g0129: [2024-08-02 23:12:28,692] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_03_model_states.pt...
g0131: [2024-08-02 23:12:28,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_19-model_00-model_states.pt.
g0131: [2024-08-02 23:12:28,709] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_05_model_states.pt...
g0125: [2024-08-02 23:12:28,715] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_07-model_00-model_states.pt...
g0113: [2024-08-02 23:12:28,733] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_02-model_00-model_states.pt.
g0132: [2024-08-02 23:12:28,743] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_22-model_00-model_states.pt.
g0132: [2024-08-02 23:12:28,745] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_06_model_states.pt...
g0113: [2024-08-02 23:12:28,757] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_03-model_00-model_states.pt...
g0125: [2024-08-02 23:12:28,842] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_07-model_00-model_states.pt.
g0125: [2024-08-02 23:12:28,844] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_01_model_states.pt...
g0113: [2024-08-02 23:12:28,864] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_03-model_00-model_states.pt.
g0113: [2024-08-02 23:12:28,889] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_04-model_00-model_states.pt...
g0113: [2024-08-02 23:12:29,008] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_04-model_00-model_states.pt.
g0113: [2024-08-02 23:12:29,009] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt
g0113: [2024-08-02 23:12:29,010] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt...
g0128: [2024-08-02 23:12:29,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_09-model_00-model_states.pt.
g0128: [2024-08-02 23:12:29,088] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_10-model_00-model_states.pt...
g0128: [2024-08-02 23:12:29,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_10-model_00-model_states.pt.
g0128: [2024-08-02 23:12:29,200] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_02_model_states.pt...
g0130: [2024-08-02 23:12:29,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_16-model_00-model_states.pt.
g0130: [2024-08-02 23:12:29,769] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_04_model_states.pt...
g0133: [2024-08-02 23:12:30,218] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/layer_25-model_00-model_states.pt.
g0133: [2024-08-02 23:12:30,220] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_07_model_states.pt...
g0132: [2024-08-02 23:12:31,052] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_06_model_states.pt.
g0132: [2024-08-02 23:12:31,052] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0131: [2024-08-02 23:12:31,089] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_05_model_states.pt.
g0131: [2024-08-02 23:12:31,090] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0125: [2024-08-02 23:12:31,176] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_01_model_states.pt.
g0125: [2024-08-02 23:12:31,177] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0128: [2024-08-02 23:12:31,547] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_02_model_states.pt.
g0128: [2024-08-02 23:12:31,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0130: [2024-08-02 23:12:32,077] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_04_model_states.pt.
g0130: [2024-08-02 23:12:32,078] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0133: [2024-08-02 23:12:32,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_07_model_states.pt.
g0133: [2024-08-02 23:12:32,106] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0113: [2024-08-02 23:12:32,695] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_00_model_states.pt.
g0113: [2024-08-02 23:12:32,698] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0129: [2024-08-02 23:12:35,315] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step4000/mp_rank_03_model_states.pt.
g0129: [2024-08-02 23:12:35,315] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4000 is ready now!
g0113:   successfully saved checkpoint at iteration    4000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: Checkpoint Save GB: 22.521, GB/Sec: 3.17, Latency(second): 7.107
g0133: (min, max) time across ranks (ms):
g0133:     save-checkpoint ................................: (7106.60, 7106.76)
g0129: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0130: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0133: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0128: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0125: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0125: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0133: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0131: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0130: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0129: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0125: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0130: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0133: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0132: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0133: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0128: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0130: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0113: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0125: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0125: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0131: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0130: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0131: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0125: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0130: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0131: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0113: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0132: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0113: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 23:12:39,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0132: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0131: [2024-08-02 23:12:39,366] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0129: [2024-08-02 23:12:39,367] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144 to 524288
g0113: [2024-08-02 23:13:16,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=4010, skipped=0, lr=[7.006235306666668e-05, 7.006235306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4010 loss: 2.0851 iter time (s): 4.053 samples/sec: 31.584
g0133:  iteration     4010/10000000 | consumed samples:       513280 | consumed tokens:   1051197440 | elapsed time per iteration (ms): 42715.4 | learning rate: 7.006E-05 | global batch size:   128 | lm loss: 2.079760E+00 | loss scale: 524288.0 | grad norm: 0.794 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.997 | tokens per gpu per second (tgs): 191.781 | TFLOPs: 1.54 |
g0113: [2024-08-02 23:13:57,928] [INFO] [logging.py:96:log_dist] [Rank 0] step=4020, skipped=0, lr=[7.023711573333334e-05, 7.023711573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4020 loss: 2.0608 iter time (s): 4.145 samples/sec: 30.879
g0133:  iteration     4020/10000000 | consumed samples:       514560 | consumed tokens:   1053818880 | elapsed time per iteration (ms): 4178.5 | learning rate: 7.024E-05 | global batch size:   128 | lm loss: 2.047379E+00 | loss scale: 524288.0 | grad norm: 0.760 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.633 | tokens per gpu per second (tgs): 1960.494 | TFLOPs: 15.78 |
g0113: [2024-08-02 23:14:38,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=4030, skipped=0, lr=[7.041187840000001e-05, 7.041187840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4030 loss: 2.0709 iter time (s): 4.055 samples/sec: 31.563
g0133:  iteration     4030/10000000 | consumed samples:       515840 | consumed tokens:   1056440320 | elapsed time per iteration (ms): 4088.1 | learning rate: 7.041E-05 | global batch size:   128 | lm loss: 2.071933E+00 | loss scale: 524288.0 | grad norm: 0.873 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.310 | tokens per gpu per second (tgs): 2003.871 | TFLOPs: 16.13 |
g0113: [2024-08-02 23:15:20,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=4040, skipped=0, lr=[7.058664106666667e-05, 7.058664106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4040 loss: 2.1248 iter time (s): 4.118 samples/sec: 31.085
g0133:  iteration     4040/10000000 | consumed samples:       517120 | consumed tokens:   1059061760 | elapsed time per iteration (ms): 4150.8 | learning rate: 7.059E-05 | global batch size:   128 | lm loss: 2.071617E+00 | loss scale: 524288.0 | grad norm: 0.776 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.837 | tokens per gpu per second (tgs): 1973.590 | TFLOPs: 15.88 |
g0113: [2024-08-02 23:16:01,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=4050, skipped=0, lr=[7.076140373333334e-05, 7.076140373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4050 loss: 2.0378 iter time (s): 4.056 samples/sec: 31.556
g0133:  iteration     4050/10000000 | consumed samples:       518400 | consumed tokens:   1061683200 | elapsed time per iteration (ms): 4088.6 | learning rate: 7.076E-05 | global batch size:   128 | lm loss: 2.053867E+00 | loss scale: 524288.0 | grad norm: 0.801 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.307 | tokens per gpu per second (tgs): 2003.637 | TFLOPs: 16.12 |
g0113: [2024-08-02 23:16:42,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=4060, skipped=0, lr=[7.093616640000001e-05, 7.093616640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4060 loss: 2.1121 iter time (s): 4.129 samples/sec: 31.001
g0133:  iteration     4060/10000000 | consumed samples:       519680 | consumed tokens:   1064304640 | elapsed time per iteration (ms): 4161.5 | learning rate: 7.094E-05 | global batch size:   128 | lm loss: 2.072372E+00 | loss scale: 524288.0 | grad norm: 0.908 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.758 | tokens per gpu per second (tgs): 1968.535 | TFLOPs: 15.84 |
g0113: [2024-08-02 23:17:24,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=4070, skipped=0, lr=[7.111092906666667e-05, 7.111092906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4070 loss: 2.0708 iter time (s): 4.179 samples/sec: 30.627
g0133:  iteration     4070/10000000 | consumed samples:       520960 | consumed tokens:   1066926080 | elapsed time per iteration (ms): 4211.7 | learning rate: 7.111E-05 | global batch size:   128 | lm loss: 2.054800E+00 | loss scale: 524288.0 | grad norm: 0.828 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.391 | tokens per gpu per second (tgs): 1945.035 | TFLOPs: 15.65 |
g0113: [2024-08-02 23:18:06,230] [INFO] [logging.py:96:log_dist] [Rank 0] step=4080, skipped=0, lr=[7.128569173333334e-05, 7.128569173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4080 loss: 2.0513 iter time (s): 4.097 samples/sec: 31.245
g0133:  iteration     4080/10000000 | consumed samples:       522240 | consumed tokens:   1069547520 | elapsed time per iteration (ms): 4129.4 | learning rate: 7.129E-05 | global batch size:   128 | lm loss: 2.058197E+00 | loss scale: 524288.0 | grad norm: 0.796 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.997 | tokens per gpu per second (tgs): 1983.837 | TFLOPs: 15.96 |
g0113: [2024-08-02 23:18:46,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=4090, skipped=0, lr=[7.14604544e-05, 7.14604544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4090 loss: 2.0044 iter time (s): 4.018 samples/sec: 31.854
g0133:  iteration     4090/10000000 | consumed samples:       523520 | consumed tokens:   1072168960 | elapsed time per iteration (ms): 4051.0 | learning rate: 7.146E-05 | global batch size:   128 | lm loss: 2.059290E+00 | loss scale: 524288.0 | grad norm: 0.774 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.597 | tokens per gpu per second (tgs): 2022.219 | TFLOPs: 16.27 |
g0113: [2024-08-02 23:19:28,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=4100, skipped=0, lr=[7.163521706666667e-05, 7.163521706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4100 loss: 2.0602 iter time (s): 4.135 samples/sec: 30.953
g0133:  iteration     4100/10000000 | consumed samples:       524800 | consumed tokens:   1074790400 | elapsed time per iteration (ms): 4168.0 | learning rate: 7.164E-05 | global batch size:   128 | lm loss: 2.058799E+00 | loss scale: 524288.0 | grad norm: 0.767 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.710 | tokens per gpu per second (tgs): 1965.448 | TFLOPs: 15.82 |
g0113: [2024-08-02 23:20:09,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=4110, skipped=0, lr=[7.180997973333334e-05, 7.180997973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4110 loss: 2.0308 iter time (s): 4.031 samples/sec: 31.754
g0133:  iteration     4110/10000000 | consumed samples:       526080 | consumed tokens:   1077411840 | elapsed time per iteration (ms): 4064.3 | learning rate: 7.181E-05 | global batch size:   128 | lm loss: 2.051196E+00 | loss scale: 524288.0 | grad norm: 0.773 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.494 | tokens per gpu per second (tgs): 2015.612 | TFLOPs: 16.22 |
g0113: [2024-08-02 23:20:50,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=4120, skipped=0, lr=[7.19847424e-05, 7.19847424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4120 loss: 2.0590 iter time (s): 4.102 samples/sec: 31.202
g0133:  iteration     4120/10000000 | consumed samples:       527360 | consumed tokens:   1080033280 | elapsed time per iteration (ms): 4135.3 | learning rate: 7.198E-05 | global batch size:   128 | lm loss: 2.053577E+00 | loss scale: 524288.0 | grad norm: 0.931 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.953 | tokens per gpu per second (tgs): 1981.013 | TFLOPs: 15.94 |
g0113: [2024-08-02 23:21:32,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=4130, skipped=0, lr=[7.215950506666667e-05, 7.215950506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4130 loss: 2.1048 iter time (s): 4.157 samples/sec: 30.788
g0133:  iteration     4130/10000000 | consumed samples:       528640 | consumed tokens:   1082654720 | elapsed time per iteration (ms): 4190.0 | learning rate: 7.216E-05 | global batch size:   128 | lm loss: 2.057062E+00 | loss scale: 524288.0 | grad norm: 0.783 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.549 | tokens per gpu per second (tgs): 1955.152 | TFLOPs: 15.73 |
g0113: [2024-08-02 23:22:13,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=4140, skipped=0, lr=[7.233426773333334e-05, 7.233426773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4140 loss: 2.0711 iter time (s): 4.090 samples/sec: 31.298
g0133:  iteration     4140/10000000 | consumed samples:       529920 | consumed tokens:   1085276160 | elapsed time per iteration (ms): 4122.5 | learning rate: 7.233E-05 | global batch size:   128 | lm loss: 2.032048E+00 | loss scale: 524288.0 | grad norm: 0.784 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.049 | tokens per gpu per second (tgs): 1987.129 | TFLOPs: 15.99 |
g0113: [2024-08-02 23:22:54,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=4150, skipped=0, lr=[7.25090304e-05, 7.25090304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4150 loss: 2.0147 iter time (s): 4.036 samples/sec: 31.716
g0133:  iteration     4150/10000000 | consumed samples:       531200 | consumed tokens:   1087897600 | elapsed time per iteration (ms): 4068.4 | learning rate: 7.251E-05 | global batch size:   128 | lm loss: 2.048951E+00 | loss scale: 524288.0 | grad norm: 0.843 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.462 | tokens per gpu per second (tgs): 2013.550 | TFLOPs: 16.20 |
g0113: [2024-08-02 23:23:34,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=4160, skipped=0, lr=[7.268379306666668e-05, 7.268379306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4160 loss: 2.0318 iter time (s): 4.011 samples/sec: 31.909
g0133:  iteration     4160/10000000 | consumed samples:       532480 | consumed tokens:   1090519040 | elapsed time per iteration (ms): 4044.7 | learning rate: 7.268E-05 | global batch size:   128 | lm loss: 2.035306E+00 | loss scale: 524288.0 | grad norm: 0.774 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.646 | tokens per gpu per second (tgs): 2025.371 | TFLOPs: 16.30 |
g0113: [2024-08-02 23:24:17,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=4170, skipped=0, lr=[7.285855573333333e-05, 7.285855573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4170 loss: 2.0997 iter time (s): 4.225 samples/sec: 30.294
g0133:  iteration     4170/10000000 | consumed samples:       533760 | consumed tokens:   1093140480 | elapsed time per iteration (ms): 4258.4 | learning rate: 7.286E-05 | global batch size:   128 | lm loss: 2.056797E+00 | loss scale: 524288.0 | grad norm: 0.755 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.058 | tokens per gpu per second (tgs): 1923.715 | TFLOPs: 15.48 |
g0113: [2024-08-02 23:24:57,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=4180, skipped=0, lr=[7.30333184e-05, 7.30333184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4180 loss: 2.0541 iter time (s): 3.988 samples/sec: 32.092
g0133:  iteration     4180/10000000 | consumed samples:       535040 | consumed tokens:   1095761920 | elapsed time per iteration (ms): 4021.0 | learning rate: 7.303E-05 | global batch size:   128 | lm loss: 2.056025E+00 | loss scale: 524288.0 | grad norm: 0.753 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.833 | tokens per gpu per second (tgs): 2037.304 | TFLOPs: 16.39 |
g0113: [2024-08-02 23:25:39,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=4190, skipped=0, lr=[7.320808106666667e-05, 7.320808106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4190 loss: 2.0149 iter time (s): 4.157 samples/sec: 30.788
g0133:  iteration     4190/10000000 | consumed samples:       536320 | consumed tokens:   1098383360 | elapsed time per iteration (ms): 4190.0 | learning rate: 7.321E-05 | global batch size:   128 | lm loss: 2.050821E+00 | loss scale: 524288.0 | grad norm: 0.746 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.549 | tokens per gpu per second (tgs): 1955.137 | TFLOPs: 15.73 |
g0113: [2024-08-02 23:26:21,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=4200, skipped=0, lr=[7.338284373333333e-05, 7.338284373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4200 loss: 1.9816 iter time (s): 4.159 samples/sec: 30.779
g0133:  iteration     4200/10000000 | consumed samples:       537600 | consumed tokens:   1101004800 | elapsed time per iteration (ms): 4191.1 | learning rate: 7.338E-05 | global batch size:   128 | lm loss: 2.018744E+00 | loss scale: 524288.0 | grad norm: 0.772 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.541 | tokens per gpu per second (tgs): 1954.599 | TFLOPs: 15.73 |
g0113: [2024-08-02 23:27:02,588] [INFO] [logging.py:96:log_dist] [Rank 0] step=4210, skipped=0, lr=[7.35576064e-05, 7.35576064e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4210 loss: 2.0444 iter time (s): 4.098 samples/sec: 31.232
g0133:  iteration     4210/10000000 | consumed samples:       538880 | consumed tokens:   1103626240 | elapsed time per iteration (ms): 4131.0 | learning rate: 7.356E-05 | global batch size:   128 | lm loss: 2.022083E+00 | loss scale: 524288.0 | grad norm: 0.810 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.985 | tokens per gpu per second (tgs): 1983.045 | TFLOPs: 15.96 |
g0113: [2024-08-02 23:27:43,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=4220, skipped=0, lr=[7.373236906666666e-05, 7.373236906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4220 loss: 2.0478 iter time (s): 4.099 samples/sec: 31.231
g0133:  iteration     4220/10000000 | consumed samples:       540160 | consumed tokens:   1106247680 | elapsed time per iteration (ms): 4131.9 | learning rate: 7.373E-05 | global batch size:   128 | lm loss: 2.050496E+00 | loss scale: 524288.0 | grad norm: 0.891 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.979 | tokens per gpu per second (tgs): 1982.638 | TFLOPs: 15.95 |
g0113: [2024-08-02 23:28:26,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=4230, skipped=0, lr=[7.390713173333333e-05, 7.390713173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4230 loss: 2.0301 iter time (s): 4.188 samples/sec: 30.563
g0133:  iteration     4230/10000000 | consumed samples:       541440 | consumed tokens:   1108869120 | elapsed time per iteration (ms): 4220.8 | learning rate: 7.391E-05 | global batch size:   128 | lm loss: 2.034116E+00 | loss scale: 524288.0 | grad norm: 0.809 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.326 | tokens per gpu per second (tgs): 1940.849 | TFLOPs: 15.62 |
g0113: [2024-08-02 23:29:07,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=4240, skipped=0, lr=[7.40818944e-05, 7.40818944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4240 loss: 2.0287 iter time (s): 4.105 samples/sec: 31.183
g0133:  iteration     4240/10000000 | consumed samples:       542720 | consumed tokens:   1111490560 | elapsed time per iteration (ms): 4137.3 | learning rate: 7.408E-05 | global batch size:   128 | lm loss: 2.026551E+00 | loss scale: 524288.0 | grad norm: 0.747 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.938 | tokens per gpu per second (tgs): 1980.021 | TFLOPs: 15.93 |
g0113: [2024-08-02 23:29:49,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=4250, skipped=0, lr=[7.425665706666666e-05, 7.425665706666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4250 loss: 1.9996 iter time (s): 4.180 samples/sec: 30.620
g0133:  iteration     4250/10000000 | consumed samples:       544000 | consumed tokens:   1114112000 | elapsed time per iteration (ms): 4213.0 | learning rate: 7.426E-05 | global batch size:   128 | lm loss: 2.023297E+00 | loss scale: 524288.0 | grad norm: 0.762 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.382 | tokens per gpu per second (tgs): 1944.449 | TFLOPs: 15.65 |
g0113: [2024-08-02 23:30:31,002] [INFO] [logging.py:96:log_dist] [Rank 0] step=4260, skipped=0, lr=[7.443141973333333e-05, 7.443141973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4260 loss: 2.0605 iter time (s): 4.105 samples/sec: 31.178
g0133:  iteration     4260/10000000 | consumed samples:       545280 | consumed tokens:   1116733440 | elapsed time per iteration (ms): 4138.2 | learning rate: 7.443E-05 | global batch size:   128 | lm loss: 2.034373E+00 | loss scale: 524288.0 | grad norm: 0.764 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.932 | tokens per gpu per second (tgs): 1979.619 | TFLOPs: 15.93 |
g0113: [2024-08-02 23:31:13,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=4270, skipped=0, lr=[7.46061824e-05, 7.46061824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4270 loss: 2.0226 iter time (s): 4.250 samples/sec: 30.114
g0133:  iteration     4270/10000000 | consumed samples:       546560 | consumed tokens:   1119354880 | elapsed time per iteration (ms): 4282.9 | learning rate: 7.461E-05 | global batch size:   128 | lm loss: 2.030123E+00 | loss scale: 524288.0 | grad norm: 0.760 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.887 | tokens per gpu per second (tgs): 1912.740 | TFLOPs: 15.39 |
g0113: [2024-08-02 23:31:56,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=4280, skipped=0, lr=[7.478094506666666e-05, 7.478094506666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4280 loss: 2.0182 iter time (s): 4.240 samples/sec: 30.191
g0133:  iteration     4280/10000000 | consumed samples:       547840 | consumed tokens:   1121976320 | elapsed time per iteration (ms): 4272.1 | learning rate: 7.478E-05 | global batch size:   128 | lm loss: 2.013404E+00 | loss scale: 524288.0 | grad norm: 0.748 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.962 | tokens per gpu per second (tgs): 1917.570 | TFLOPs: 15.43 |
g0113: [2024-08-02 23:32:38,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=4290, skipped=0, lr=[7.495570773333334e-05, 7.495570773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4290 loss: 2.0385 iter time (s): 4.115 samples/sec: 31.106
g0133:  iteration     4290/10000000 | consumed samples:       549120 | consumed tokens:   1124597760 | elapsed time per iteration (ms): 4147.4 | learning rate: 7.496E-05 | global batch size:   128 | lm loss: 2.027687E+00 | loss scale: 524288.0 | grad norm: 0.763 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.863 | tokens per gpu per second (tgs): 1975.203 | TFLOPs: 15.89 |
g0113: [2024-08-02 23:33:19,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=4300, skipped=0, lr=[7.51304704e-05, 7.51304704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4300 loss: 2.0424 iter time (s): 4.106 samples/sec: 31.176
g0133:  iteration     4300/10000000 | consumed samples:       550400 | consumed tokens:   1127219200 | elapsed time per iteration (ms): 4159.5 | learning rate: 7.513E-05 | global batch size:   128 | lm loss: 2.019915E+00 | loss scale: 524288.0 | grad norm: 0.748 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.773 | tokens per gpu per second (tgs): 1969.489 | TFLOPs: 15.85 |
g0113: [2024-08-02 23:34:01,547] [INFO] [logging.py:96:log_dist] [Rank 0] step=4310, skipped=0, lr=[7.530523306666667e-05, 7.530523306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4310 loss: 2.0318 iter time (s): 4.157 samples/sec: 30.788
g0133:  iteration     4310/10000000 | consumed samples:       551680 | consumed tokens:   1129840640 | elapsed time per iteration (ms): 4196.4 | learning rate: 7.531E-05 | global batch size:   128 | lm loss: 2.019138E+00 | loss scale: 524288.0 | grad norm: 0.780 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.503 | tokens per gpu per second (tgs): 1952.168 | TFLOPs: 15.71 |
g0113: [2024-08-02 23:34:43,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=4320, skipped=0, lr=[7.547999573333334e-05, 7.547999573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4320 loss: 2.0215 iter time (s): 4.152 samples/sec: 30.831
g0133:  iteration     4320/10000000 | consumed samples:       552960 | consumed tokens:   1132462080 | elapsed time per iteration (ms): 4184.5 | learning rate: 7.548E-05 | global batch size:   128 | lm loss: 1.999219E+00 | loss scale: 524288.0 | grad norm: 0.734 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.589 | tokens per gpu per second (tgs): 1957.724 | TFLOPs: 15.75 |
g0113: [2024-08-02 23:35:25,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=4330, skipped=0, lr=[7.56547584e-05, 7.56547584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4330 loss: 2.0358 iter time (s): 4.145 samples/sec: 30.878
g0133:  iteration     4330/10000000 | consumed samples:       554240 | consumed tokens:   1135083520 | elapsed time per iteration (ms): 4178.2 | learning rate: 7.565E-05 | global batch size:   128 | lm loss: 2.007687E+00 | loss scale: 524288.0 | grad norm: 0.746 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.635 | tokens per gpu per second (tgs): 1960.647 | TFLOPs: 15.78 |
g0113: [2024-08-02 23:36:06,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=4340, skipped=0, lr=[7.582952106666667e-05, 7.582952106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4340 loss: 1.9807 iter time (s): 4.060 samples/sec: 31.524
g0133:  iteration     4340/10000000 | consumed samples:       555520 | consumed tokens:   1137704960 | elapsed time per iteration (ms): 4093.0 | learning rate: 7.583E-05 | global batch size:   128 | lm loss: 1.991424E+00 | loss scale: 524288.0 | grad norm: 0.831 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.273 | tokens per gpu per second (tgs): 2001.450 | TFLOPs: 16.11 |
g0113: [2024-08-02 23:36:48,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=4350, skipped=0, lr=[7.600428373333334e-05, 7.600428373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4350 loss: 2.0332 iter time (s): 4.164 samples/sec: 30.737
g0133:  iteration     4350/10000000 | consumed samples:       556800 | consumed tokens:   1140326400 | elapsed time per iteration (ms): 4197.1 | learning rate: 7.600E-05 | global batch size:   128 | lm loss: 2.004689E+00 | loss scale: 524288.0 | grad norm: 0.802 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.497 | tokens per gpu per second (tgs): 1951.807 | TFLOPs: 15.71 |
g0113: [2024-08-02 23:37:29,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=4360, skipped=0, lr=[7.61790464e-05, 7.61790464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4360 loss: 2.0133 iter time (s): 4.143 samples/sec: 30.892
g0133:  iteration     4360/10000000 | consumed samples:       558080 | consumed tokens:   1142947840 | elapsed time per iteration (ms): 4176.1 | learning rate: 7.618E-05 | global batch size:   128 | lm loss: 1.999084E+00 | loss scale: 524288.0 | grad norm: 0.783 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.650 | tokens per gpu per second (tgs): 1961.626 | TFLOPs: 15.79 |
g0113: [2024-08-02 23:38:11,798] [INFO] [logging.py:96:log_dist] [Rank 0] step=4370, skipped=0, lr=[7.635380906666667e-05, 7.635380906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4370 loss: 1.9939 iter time (s): 4.160 samples/sec: 30.771
g0133:  iteration     4370/10000000 | consumed samples:       559360 | consumed tokens:   1145569280 | elapsed time per iteration (ms): 4192.3 | learning rate: 7.635E-05 | global batch size:   128 | lm loss: 1.995231E+00 | loss scale: 524288.0 | grad norm: 0.783 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.532 | tokens per gpu per second (tgs): 1954.066 | TFLOPs: 15.72 |
g0113: [2024-08-02 23:38:52,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=4380, skipped=0, lr=[7.652857173333333e-05, 7.652857173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4380 loss: 2.0052 iter time (s): 4.061 samples/sec: 31.522
g0133:  iteration     4380/10000000 | consumed samples:       560640 | consumed tokens:   1148190720 | elapsed time per iteration (ms): 4092.9 | learning rate: 7.653E-05 | global batch size:   128 | lm loss: 1.999193E+00 | loss scale: 524288.0 | grad norm: 0.716 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.274 | tokens per gpu per second (tgs): 2001.504 | TFLOPs: 16.11 |
g0113: [2024-08-02 23:39:35,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=4390, skipped=0, lr=[7.67033344e-05, 7.67033344e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4390 loss: 1.9513 iter time (s): 4.260 samples/sec: 30.049
g0133:  iteration     4390/10000000 | consumed samples:       561920 | consumed tokens:   1150812160 | elapsed time per iteration (ms): 4294.1 | learning rate: 7.670E-05 | global batch size:   128 | lm loss: 2.001406E+00 | loss scale: 524288.0 | grad norm: 0.791 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.808 | tokens per gpu per second (tgs): 1907.722 | TFLOPs: 15.35 |
g0113: [2024-08-02 23:40:16,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=4400, skipped=0, lr=[7.687809706666667e-05, 7.687809706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4400 loss: 1.9668 iter time (s): 4.002 samples/sec: 31.984
g0133:  iteration     4400/10000000 | consumed samples:       563200 | consumed tokens:   1153433600 | elapsed time per iteration (ms): 4034.3 | learning rate: 7.688E-05 | global batch size:   128 | lm loss: 1.998452E+00 | loss scale: 524288.0 | grad norm: 0.818 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.728 | tokens per gpu per second (tgs): 2030.573 | TFLOPs: 16.34 |
g0113: [2024-08-02 23:40:57,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=4410, skipped=0, lr=[7.705285973333333e-05, 7.705285973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4410 loss: 1.9533 iter time (s): 4.128 samples/sec: 31.011
g0133:  iteration     4410/10000000 | consumed samples:       564480 | consumed tokens:   1156055040 | elapsed time per iteration (ms): 4160.0 | learning rate: 7.705E-05 | global batch size:   128 | lm loss: 1.991096E+00 | loss scale: 524288.0 | grad norm: 0.758 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.769 | tokens per gpu per second (tgs): 1969.222 | TFLOPs: 15.85 |
g0113: [2024-08-02 23:41:38,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=4420, skipped=0, lr=[7.72276224e-05, 7.72276224e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4420 loss: 2.0234 iter time (s): 4.040 samples/sec: 31.682
g0133:  iteration     4420/10000000 | consumed samples:       565760 | consumed tokens:   1158676480 | elapsed time per iteration (ms): 4072.9 | learning rate: 7.723E-05 | global batch size:   128 | lm loss: 1.992421E+00 | loss scale: 524288.0 | grad norm: 0.722 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.427 | tokens per gpu per second (tgs): 2011.337 | TFLOPs: 16.19 |
g0113: [2024-08-02 23:42:18,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=4430, skipped=0, lr=[7.740238506666667e-05, 7.740238506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4430 loss: 1.9632 iter time (s): 4.014 samples/sec: 31.885
g0133:  iteration     4430/10000000 | consumed samples:       567040 | consumed tokens:   1161297920 | elapsed time per iteration (ms): 4047.6 | learning rate: 7.740E-05 | global batch size:   128 | lm loss: 1.978606E+00 | loss scale: 524288.0 | grad norm: 0.720 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.623 | tokens per gpu per second (tgs): 2023.891 | TFLOPs: 16.29 |
g0113: [2024-08-02 23:42:59,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=4440, skipped=0, lr=[7.757714773333333e-05, 7.757714773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4440 loss: 1.9898 iter time (s): 4.081 samples/sec: 31.363
g0133:  iteration     4440/10000000 | consumed samples:       568320 | consumed tokens:   1163919360 | elapsed time per iteration (ms): 4114.5 | learning rate: 7.758E-05 | global batch size:   128 | lm loss: 1.988629E+00 | loss scale: 524288.0 | grad norm: 0.734 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.109 | tokens per gpu per second (tgs): 1991.001 | TFLOPs: 16.02 |
g0113: [2024-08-02 23:43:42,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=4450, skipped=0, lr=[7.77519104e-05, 7.77519104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4450 loss: 1.9066 iter time (s): 4.198 samples/sec: 30.489
g0133:  iteration     4450/10000000 | consumed samples:       569600 | consumed tokens:   1166540800 | elapsed time per iteration (ms): 4230.7 | learning rate: 7.775E-05 | global batch size:   128 | lm loss: 1.957399E+00 | loss scale: 524288.0 | grad norm: 0.734 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.255 | tokens per gpu per second (tgs): 1936.325 | TFLOPs: 15.58 |
g0113: [2024-08-02 23:44:23,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=4460, skipped=0, lr=[7.792667306666666e-05, 7.792667306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4460 loss: 1.8714 iter time (s): 4.123 samples/sec: 31.048
g0133:  iteration     4460/10000000 | consumed samples:       570880 | consumed tokens:   1169162240 | elapsed time per iteration (ms): 4155.3 | learning rate: 7.793E-05 | global batch size:   128 | lm loss: 1.955363E+00 | loss scale: 524288.0 | grad norm: 0.743 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.804 | tokens per gpu per second (tgs): 1971.449 | TFLOPs: 15.86 |
g0113: [2024-08-02 23:45:05,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=4470, skipped=0, lr=[7.810143573333334e-05, 7.810143573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4470 loss: 1.9624 iter time (s): 4.184 samples/sec: 30.591
g0133:  iteration     4470/10000000 | consumed samples:       572160 | consumed tokens:   1171783680 | elapsed time per iteration (ms): 4216.5 | learning rate: 7.810E-05 | global batch size:   128 | lm loss: 1.975142E+00 | loss scale: 524288.0 | grad norm: 0.733 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.357 | tokens per gpu per second (tgs): 1942.862 | TFLOPs: 15.63 |
g0113: [2024-08-02 23:45:47,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=4480, skipped=0, lr=[7.827619840000001e-05, 7.827619840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4480 loss: 1.9827 iter time (s): 4.069 samples/sec: 31.458
g0133:  iteration     4480/10000000 | consumed samples:       573440 | consumed tokens:   1174405120 | elapsed time per iteration (ms): 4101.5 | learning rate: 7.828E-05 | global batch size:   128 | lm loss: 1.982784E+00 | loss scale: 524288.0 | grad norm: 0.727 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.208 | tokens per gpu per second (tgs): 1997.301 | TFLOPs: 16.07 |
g0113: [2024-08-02 23:46:29,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=4490, skipped=0, lr=[7.845096106666667e-05, 7.845096106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4490 loss: 1.9544 iter time (s): 4.183 samples/sec: 30.600
g0133:  iteration     4490/10000000 | consumed samples:       574720 | consumed tokens:   1177026560 | elapsed time per iteration (ms): 4215.6 | learning rate: 7.845E-05 | global batch size:   128 | lm loss: 1.971485E+00 | loss scale: 524288.0 | grad norm: 0.705 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.363 | tokens per gpu per second (tgs): 1943.239 | TFLOPs: 15.64 |
g0113: [2024-08-02 23:47:11,274] [INFO] [logging.py:96:log_dist] [Rank 0] step=4500, skipped=0, lr=[7.862572373333334e-05, 7.862572373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4500 loss: 1.9664 iter time (s): 4.179 samples/sec: 30.629
g0133:  iteration     4500/10000000 | consumed samples:       576000 | consumed tokens:   1179648000 | elapsed time per iteration (ms): 4211.3 | learning rate: 7.863E-05 | global batch size:   128 | lm loss: 1.967587E+00 | loss scale: 524288.0 | grad norm: 0.720 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.395 | tokens per gpu per second (tgs): 1945.260 | TFLOPs: 15.65 |
g0113: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0113: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0113: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0129: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0130: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0129: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0129: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0128: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0128: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0128: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0132: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0125: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0133: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0131: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0125: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0133: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0125: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0133: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0125: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0133: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0133: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0133: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0132: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0132: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0132: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0113: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0132: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0131: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0131: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0132: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0132: [2024-08-02 23:47:15,607] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0113: [2024-08-02 23:47:15,606] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-02 23:47:15,607] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288 to 1048576
g0113: [2024-08-02 23:47:52,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=4510, skipped=0, lr=[7.880048640000001e-05, 7.880048640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4510 loss: 1.9334 iter time (s): 4.117 samples/sec: 31.093
g0133:  iteration     4510/10000000 | consumed samples:       577280 | consumed tokens:   1182269440 | elapsed time per iteration (ms): 4148.9 | learning rate: 7.880E-05 | global batch size:   128 | lm loss: 1.955530E+00 | loss scale: 1048576.0 | grad norm: 0.708 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.852 | tokens per gpu per second (tgs): 1974.509 | TFLOPs: 15.89 |
g0113: [2024-08-02 23:48:35,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=4520, skipped=0, lr=[7.897524906666667e-05, 7.897524906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4520 loss: 1.9422 iter time (s): 4.202 samples/sec: 30.459
g0133:  iteration     4520/10000000 | consumed samples:       578560 | consumed tokens:   1184890880 | elapsed time per iteration (ms): 4235.3 | learning rate: 7.898E-05 | global batch size:   128 | lm loss: 1.965062E+00 | loss scale: 1048576.0 | grad norm: 0.729 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.222 | tokens per gpu per second (tgs): 1934.229 | TFLOPs: 15.57 |
g0113: [2024-08-02 23:49:16,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=4530, skipped=0, lr=[7.915001173333334e-05, 7.915001173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4530 loss: 2.0141 iter time (s): 4.117 samples/sec: 31.092
g0133:  iteration     4530/10000000 | consumed samples:       579840 | consumed tokens:   1187512320 | elapsed time per iteration (ms): 4149.4 | learning rate: 7.915E-05 | global batch size:   128 | lm loss: 1.967737E+00 | loss scale: 1048576.0 | grad norm: 0.761 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.848 | tokens per gpu per second (tgs): 1974.255 | TFLOPs: 15.89 |
g0113: [2024-08-02 23:49:58,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=4540, skipped=0, lr=[7.93247744e-05, 7.93247744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4540 loss: 1.9499 iter time (s): 4.142 samples/sec: 30.901
g0133:  iteration     4540/10000000 | consumed samples:       581120 | consumed tokens:   1190133760 | elapsed time per iteration (ms): 4174.8 | learning rate: 7.932E-05 | global batch size:   128 | lm loss: 1.953076E+00 | loss scale: 1048576.0 | grad norm: 0.728 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.660 | tokens per gpu per second (tgs): 1962.257 | TFLOPs: 15.79 |
g0113: [2024-08-02 23:50:40,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=4550, skipped=0, lr=[7.949953706666667e-05, 7.949953706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4550 loss: 1.9456 iter time (s): 4.211 samples/sec: 30.396
g0133:  iteration     4550/10000000 | consumed samples:       582400 | consumed tokens:   1192755200 | elapsed time per iteration (ms): 4243.6 | learning rate: 7.950E-05 | global batch size:   128 | lm loss: 1.950325E+00 | loss scale: 1048576.0 | grad norm: 0.726 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.163 | tokens per gpu per second (tgs): 1930.447 | TFLOPs: 15.53 |
g0113: [2024-08-02 23:51:23,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=4560, skipped=0, lr=[7.967429973333334e-05, 7.967429973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4560 loss: 1.9260 iter time (s): 4.232 samples/sec: 30.243
g0133:  iteration     4560/10000000 | consumed samples:       583680 | consumed tokens:   1195376640 | elapsed time per iteration (ms): 4264.8 | learning rate: 7.967E-05 | global batch size:   128 | lm loss: 1.942406E+00 | loss scale: 1048576.0 | grad norm: 0.719 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.013 | tokens per gpu per second (tgs): 1920.838 | TFLOPs: 15.46 |
g0113: [2024-08-02 23:52:05,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=4570, skipped=0, lr=[7.98490624e-05, 7.98490624e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4570 loss: 1.9979 iter time (s): 4.144 samples/sec: 30.888
g0133:  iteration     4570/10000000 | consumed samples:       584960 | consumed tokens:   1197998080 | elapsed time per iteration (ms): 4176.7 | learning rate: 7.985E-05 | global batch size:   128 | lm loss: 1.940394E+00 | loss scale: 1048576.0 | grad norm: 0.740 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.646 | tokens per gpu per second (tgs): 1961.337 | TFLOPs: 15.78 |
g0128: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: Grad overflow on iteration 4576
g0128: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: Grad overflow on iteration 4576
g0113: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 4576
g0113: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 4576
g0128: Grad overflow on iteration 4576
g0130: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0128: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: Grad overflow on iteration 4576
g0128: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0128: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0130: Grad overflow on iteration 4576
g0130: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 4576
g0113: Grad overflow on iteration 4576
g0113: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0130: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 4576
g0130: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0130: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0113: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0130: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0113: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 4576
g0130: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0131: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 4576
g0129: Grad overflow on iteration 4576
g0132: Grad overflow on iteration 4576
g0129: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: Grad overflow on iteration 4576
g0131: Grad overflow on iteration 4576
g0132: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0131: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: Grad overflow on iteration 4576
g0132: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 4576
g0131: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 4576
g0131: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0125: Grad overflow on iteration 4576
g0125: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0129: Grad overflow on iteration 4576
g0129: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: Grad overflow on iteration 4576
g0129: Grad overflow on iteration 4576
g0133: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0132: Grad overflow on iteration 4576
g0133: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0133: [2024-08-02 23:52:34,331] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 4576
g0133: Grad overflow on iteration 4576
g0132: Grad overflow on iteration 4576
g0133: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0128: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0132: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0129: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: Grad overflow on iteration 4576
g0132: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0133: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0125: Grad overflow on iteration 4576
g0132: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0133: Grad overflow on iteration 4576
g0131: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0129: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0131: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0131: Grad overflow on iteration 4576
g0125: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0125: Grad overflow on iteration 4576
g0125: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0125: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0125: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0125: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0113: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0131: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0113: [2024-08-02 23:52:34,332] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1048576, reducing to 524288.0
g0133: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0130: [2024-08-02 23:52:34,332] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576 to 524288.0
g0113: [2024-08-02 23:52:46,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=4580, skipped=1, lr=[8.002382506666667e-05, 8.002382506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4580 loss: 1.9339 iter time (s): 4.128 samples/sec: 31.007
g0133:  iteration     4580/10000000 | consumed samples:       586240 | consumed tokens:   1200619520 | elapsed time per iteration (ms): 4160.3 | learning rate: 8.002E-05 | global batch size:   128 | lm loss: 1.924718E+00 | loss scale: 524288.0 | grad norm: 0.736 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.767 | tokens per gpu per second (tgs): 1969.101 | TFLOPs: 15.85 |
g0113: [2024-08-02 23:53:28,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=4590, skipped=1, lr=[8.019858773333334e-05, 8.019858773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4590 loss: 1.9042 iter time (s): 4.123 samples/sec: 31.047
g0133:  iteration     4590/10000000 | consumed samples:       587520 | consumed tokens:   1203240960 | elapsed time per iteration (ms): 4154.9 | learning rate: 8.020E-05 | global batch size:   128 | lm loss: 1.934712E+00 | loss scale: 524288.0 | grad norm: 0.718 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.807 | tokens per gpu per second (tgs): 1971.669 | TFLOPs: 15.87 |
g0113: [2024-08-02 23:54:08,960] [INFO] [logging.py:96:log_dist] [Rank 0] step=4600, skipped=1, lr=[8.03733504e-05, 8.03733504e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4600 loss: 1.9287 iter time (s): 4.027 samples/sec: 31.783
g0133:  iteration     4600/10000000 | consumed samples:       588800 | consumed tokens:   1205862400 | elapsed time per iteration (ms): 4059.9 | learning rate: 8.037E-05 | global batch size:   128 | lm loss: 1.923804E+00 | loss scale: 524288.0 | grad norm: 0.745 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.528 | tokens per gpu per second (tgs): 2017.781 | TFLOPs: 16.24 |
g0113: [2024-08-02 23:54:49,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=4610, skipped=1, lr=[8.054811306666667e-05, 8.054811306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4610 loss: 1.9356 iter time (s): 4.063 samples/sec: 31.500
g0133:  iteration     4610/10000000 | consumed samples:       590080 | consumed tokens:   1208483840 | elapsed time per iteration (ms): 4096.1 | learning rate: 8.055E-05 | global batch size:   128 | lm loss: 1.911386E+00 | loss scale: 524288.0 | grad norm: 0.730 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.249 | tokens per gpu per second (tgs): 1999.961 | TFLOPs: 16.09 |
g0113: [2024-08-02 23:55:31,957] [INFO] [logging.py:96:log_dist] [Rank 0] step=4620, skipped=1, lr=[8.072287573333333e-05, 8.072287573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4620 loss: 1.9328 iter time (s): 4.171 samples/sec: 30.686
g0133:  iteration     4620/10000000 | consumed samples:       591360 | consumed tokens:   1211105280 | elapsed time per iteration (ms): 4203.5 | learning rate: 8.072E-05 | global batch size:   128 | lm loss: 1.916904E+00 | loss scale: 524288.0 | grad norm: 0.805 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.450 | tokens per gpu per second (tgs): 1948.829 | TFLOPs: 15.68 |
g0113: [2024-08-02 23:56:13,053] [INFO] [logging.py:96:log_dist] [Rank 0] step=4630, skipped=1, lr=[8.08976384e-05, 8.08976384e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4630 loss: 1.8865 iter time (s): 4.077 samples/sec: 31.395
g0133:  iteration     4630/10000000 | consumed samples:       592640 | consumed tokens:   1213726720 | elapsed time per iteration (ms): 4109.7 | learning rate: 8.090E-05 | global batch size:   128 | lm loss: 1.902755E+00 | loss scale: 524288.0 | grad norm: 0.756 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.146 | tokens per gpu per second (tgs): 1993.335 | TFLOPs: 16.04 |
g0113: [2024-08-02 23:56:55,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=4640, skipped=1, lr=[8.107240106666668e-05, 8.107240106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4640 loss: 1.9027 iter time (s): 4.174 samples/sec: 30.669
g0133:  iteration     4640/10000000 | consumed samples:       593920 | consumed tokens:   1216348160 | elapsed time per iteration (ms): 4206.1 | learning rate: 8.107E-05 | global batch size:   128 | lm loss: 1.912154E+00 | loss scale: 524288.0 | grad norm: 0.719 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.432 | tokens per gpu per second (tgs): 1947.641 | TFLOPs: 15.67 |
g0113: [2024-08-02 23:57:35,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=4650, skipped=1, lr=[8.124716373333335e-05, 8.124716373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4650 loss: 1.9236 iter time (s): 4.048 samples/sec: 31.622
g0133:  iteration     4650/10000000 | consumed samples:       595200 | consumed tokens:   1218969600 | elapsed time per iteration (ms): 4080.6 | learning rate: 8.125E-05 | global batch size:   128 | lm loss: 1.897844E+00 | loss scale: 524288.0 | grad norm: 0.719 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.368 | tokens per gpu per second (tgs): 2007.546 | TFLOPs: 16.16 |
g0113: [2024-08-02 23:58:18,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=4660, skipped=1, lr=[8.142192640000001e-05, 8.142192640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4660 loss: 1.9283 iter time (s): 4.199 samples/sec: 30.481
g0133:  iteration     4660/10000000 | consumed samples:       596480 | consumed tokens:   1221591040 | elapsed time per iteration (ms): 4231.8 | learning rate: 8.142E-05 | global batch size:   128 | lm loss: 1.904023E+00 | loss scale: 524288.0 | grad norm: 0.792 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.247 | tokens per gpu per second (tgs): 1935.815 | TFLOPs: 15.58 |
g0113: [2024-08-02 23:59:00,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=4670, skipped=1, lr=[8.159668906666668e-05, 8.159668906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4670 loss: 1.8550 iter time (s): 4.223 samples/sec: 30.308
g0133:  iteration     4670/10000000 | consumed samples:       597760 | consumed tokens:   1224212480 | elapsed time per iteration (ms): 4255.7 | learning rate: 8.160E-05 | global batch size:   128 | lm loss: 1.888239E+00 | loss scale: 524288.0 | grad norm: 0.761 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.077 | tokens per gpu per second (tgs): 1924.933 | TFLOPs: 15.49 |
g0113: [2024-08-02 23:59:43,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=4680, skipped=1, lr=[8.177145173333334e-05, 8.177145173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4680 loss: 1.8461 iter time (s): 4.244 samples/sec: 30.159
g0133:  iteration     4680/10000000 | consumed samples:       599040 | consumed tokens:   1226833920 | elapsed time per iteration (ms): 4276.9 | learning rate: 8.177E-05 | global batch size:   128 | lm loss: 1.882871E+00 | loss scale: 524288.0 | grad norm: 0.766 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.928 | tokens per gpu per second (tgs): 1915.384 | TFLOPs: 15.41 |
g0113: [2024-08-03 00:00:26,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=4690, skipped=1, lr=[8.194621440000001e-05, 8.194621440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4690 loss: 1.8820 iter time (s): 4.308 samples/sec: 29.712
g0133:  iteration     4690/10000000 | consumed samples:       600320 | consumed tokens:   1229455360 | elapsed time per iteration (ms): 4340.7 | learning rate: 8.195E-05 | global batch size:   128 | lm loss: 1.865022E+00 | loss scale: 524288.0 | grad norm: 0.781 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.488 | tokens per gpu per second (tgs): 1887.235 | TFLOPs: 15.19 |
g0113: [2024-08-03 00:01:08,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=4700, skipped=1, lr=[8.212097706666668e-05, 8.212097706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4700 loss: 1.8981 iter time (s): 4.162 samples/sec: 30.752
g0133:  iteration     4700/10000000 | consumed samples:       601600 | consumed tokens:   1232076800 | elapsed time per iteration (ms): 4195.9 | learning rate: 8.212E-05 | global batch size:   128 | lm loss: 1.874203E+00 | loss scale: 524288.0 | grad norm: 0.780 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.506 | tokens per gpu per second (tgs): 1952.373 | TFLOPs: 15.71 |
g0113: [2024-08-03 00:01:52,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=4710, skipped=1, lr=[8.229573973333334e-05, 8.229573973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4710 loss: 1.8212 iter time (s): 4.286 samples/sec: 29.865
g0133:  iteration     4710/10000000 | consumed samples:       602880 | consumed tokens:   1234698240 | elapsed time per iteration (ms): 4318.5 | learning rate: 8.230E-05 | global batch size:   128 | lm loss: 1.844095E+00 | loss scale: 524288.0 | grad norm: 0.755 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.640 | tokens per gpu per second (tgs): 1896.961 | TFLOPs: 15.27 |
g0113: [2024-08-03 00:02:33,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=4720, skipped=1, lr=[8.247050240000001e-05, 8.247050240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4720 loss: 1.8859 iter time (s): 4.108 samples/sec: 31.158
g0133:  iteration     4720/10000000 | consumed samples:       604160 | consumed tokens:   1237319680 | elapsed time per iteration (ms): 4140.6 | learning rate: 8.247E-05 | global batch size:   128 | lm loss: 1.856472E+00 | loss scale: 524288.0 | grad norm: 0.740 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.913 | tokens per gpu per second (tgs): 1978.444 | TFLOPs: 15.92 |
g0113: [2024-08-03 00:03:15,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=4730, skipped=1, lr=[8.264526506666667e-05, 8.264526506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4730 loss: 1.8418 iter time (s): 4.186 samples/sec: 30.580
g0133:  iteration     4730/10000000 | consumed samples:       605440 | consumed tokens:   1239941120 | elapsed time per iteration (ms): 4218.4 | learning rate: 8.265E-05 | global batch size:   128 | lm loss: 1.846614E+00 | loss scale: 524288.0 | grad norm: 0.762 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.343 | tokens per gpu per second (tgs): 1941.983 | TFLOPs: 15.63 |
g0113: [2024-08-03 00:03:56,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=4740, skipped=1, lr=[8.282002773333334e-05, 8.282002773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4740 loss: 1.8220 iter time (s): 4.087 samples/sec: 31.320
g0133:  iteration     4740/10000000 | consumed samples:       606720 | consumed tokens:   1242562560 | elapsed time per iteration (ms): 4120.9 | learning rate: 8.282E-05 | global batch size:   128 | lm loss: 1.833422E+00 | loss scale: 524288.0 | grad norm: 0.749 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.061 | tokens per gpu per second (tgs): 1987.915 | TFLOPs: 16.00 |
g0113: [2024-08-03 00:04:38,109] [INFO] [logging.py:96:log_dist] [Rank 0] step=4750, skipped=1, lr=[8.29947904e-05, 8.29947904e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4750 loss: 1.8187 iter time (s): 4.086 samples/sec: 31.324
g0133:  iteration     4750/10000000 | consumed samples:       608000 | consumed tokens:   1245184000 | elapsed time per iteration (ms): 4119.0 | learning rate: 8.299E-05 | global batch size:   128 | lm loss: 1.817281E+00 | loss scale: 524288.0 | grad norm: 0.859 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.075 | tokens per gpu per second (tgs): 1988.815 | TFLOPs: 16.00 |
g0113: [2024-08-03 00:05:18,844] [INFO] [logging.py:96:log_dist] [Rank 0] step=4760, skipped=1, lr=[8.316955306666667e-05, 8.316955306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4760 loss: 1.8409 iter time (s): 4.041 samples/sec: 31.676
g0133:  iteration     4760/10000000 | consumed samples:       609280 | consumed tokens:   1247805440 | elapsed time per iteration (ms): 4073.5 | learning rate: 8.317E-05 | global batch size:   128 | lm loss: 1.842494E+00 | loss scale: 524288.0 | grad norm: 0.737 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.423 | tokens per gpu per second (tgs): 2011.050 | TFLOPs: 16.18 |
g0113: [2024-08-03 00:06:00,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=4770, skipped=1, lr=[8.334431573333334e-05, 8.334431573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4770 loss: 1.7488 iter time (s): 4.144 samples/sec: 30.885
g0133:  iteration     4770/10000000 | consumed samples:       610560 | consumed tokens:   1250426880 | elapsed time per iteration (ms): 4176.8 | learning rate: 8.334E-05 | global batch size:   128 | lm loss: 1.801289E+00 | loss scale: 524288.0 | grad norm: 0.748 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.646 | tokens per gpu per second (tgs): 1961.317 | TFLOPs: 15.78 |
g0113: [2024-08-03 00:06:42,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=4780, skipped=1, lr=[8.35190784e-05, 8.35190784e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4780 loss: 1.7794 iter time (s): 4.136 samples/sec: 30.946
g0133:  iteration     4780/10000000 | consumed samples:       611840 | consumed tokens:   1253048320 | elapsed time per iteration (ms): 4168.7 | learning rate: 8.352E-05 | global batch size:   128 | lm loss: 1.788692E+00 | loss scale: 524288.0 | grad norm: 0.726 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.705 | tokens per gpu per second (tgs): 1965.125 | TFLOPs: 15.81 |
g0113: [2024-08-03 00:07:24,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=4790, skipped=1, lr=[8.369384106666667e-05, 8.369384106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4790 loss: 1.7700 iter time (s): 4.172 samples/sec: 30.681
g0133:  iteration     4790/10000000 | consumed samples:       613120 | consumed tokens:   1255669760 | elapsed time per iteration (ms): 4206.2 | learning rate: 8.369E-05 | global batch size:   128 | lm loss: 1.803658E+00 | loss scale: 524288.0 | grad norm: 0.773 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.431 | tokens per gpu per second (tgs): 1947.599 | TFLOPs: 15.67 |
g0113: [2024-08-03 00:08:06,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=4800, skipped=1, lr=[8.386860373333334e-05, 8.386860373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4800 loss: 1.7705 iter time (s): 4.156 samples/sec: 30.796
g0133:  iteration     4800/10000000 | consumed samples:       614400 | consumed tokens:   1258291200 | elapsed time per iteration (ms): 4188.7 | learning rate: 8.387E-05 | global batch size:   128 | lm loss: 1.794359E+00 | loss scale: 524288.0 | grad norm: 0.748 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.559 | tokens per gpu per second (tgs): 1955.760 | TFLOPs: 15.74 |
g0113: [2024-08-03 00:08:47,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=4810, skipped=1, lr=[8.40433664e-05, 8.40433664e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4810 loss: 1.7662 iter time (s): 4.106 samples/sec: 31.176
g0133:  iteration     4810/10000000 | consumed samples:       615680 | consumed tokens:   1260912640 | elapsed time per iteration (ms): 4138.1 | learning rate: 8.404E-05 | global batch size:   128 | lm loss: 1.799969E+00 | loss scale: 524288.0 | grad norm: 0.712 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.932 | tokens per gpu per second (tgs): 1979.651 | TFLOPs: 15.93 |
g0113: [2024-08-03 00:09:29,747] [INFO] [logging.py:96:log_dist] [Rank 0] step=4820, skipped=1, lr=[8.421812906666668e-05, 8.421812906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4820 loss: 1.7337 iter time (s): 4.179 samples/sec: 30.627
g0133:  iteration     4820/10000000 | consumed samples:       616960 | consumed tokens:   1263534080 | elapsed time per iteration (ms): 4211.8 | learning rate: 8.422E-05 | global batch size:   128 | lm loss: 1.765450E+00 | loss scale: 524288.0 | grad norm: 0.734 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.391 | tokens per gpu per second (tgs): 1945.010 | TFLOPs: 15.65 |
g0113: [2024-08-03 00:10:11,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=4830, skipped=1, lr=[8.439289173333333e-05, 8.439289173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4830 loss: 1.7975 iter time (s): 4.094 samples/sec: 31.267
g0133:  iteration     4830/10000000 | consumed samples:       618240 | consumed tokens:   1266155520 | elapsed time per iteration (ms): 4126.1 | learning rate: 8.439E-05 | global batch size:   128 | lm loss: 1.780349E+00 | loss scale: 524288.0 | grad norm: 0.733 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.022 | tokens per gpu per second (tgs): 1985.405 | TFLOPs: 15.98 |
g0113: [2024-08-03 00:10:53,660] [INFO] [logging.py:96:log_dist] [Rank 0] step=4840, skipped=1, lr=[8.45676544e-05, 8.45676544e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4840 loss: 1.7844 iter time (s): 4.232 samples/sec: 30.244
g0133:  iteration     4840/10000000 | consumed samples:       619520 | consumed tokens:   1268776960 | elapsed time per iteration (ms): 4265.1 | learning rate: 8.457E-05 | global batch size:   128 | lm loss: 1.785757E+00 | loss scale: 524288.0 | grad norm: 0.779 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.011 | tokens per gpu per second (tgs): 1920.692 | TFLOPs: 15.46 |
g0113: [2024-08-03 00:11:36,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=4850, skipped=1, lr=[8.474241706666667e-05, 8.474241706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4850 loss: 1.7709 iter time (s): 4.209 samples/sec: 30.413
g0133:  iteration     4850/10000000 | consumed samples:       620800 | consumed tokens:   1271398400 | elapsed time per iteration (ms): 4241.8 | learning rate: 8.474E-05 | global batch size:   128 | lm loss: 1.760373E+00 | loss scale: 524288.0 | grad norm: 0.722 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.176 | tokens per gpu per second (tgs): 1931.277 | TFLOPs: 15.54 |
g0113: [2024-08-03 00:12:17,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=4860, skipped=1, lr=[8.491717973333333e-05, 8.491717973333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4860 loss: 1.7583 iter time (s): 4.147 samples/sec: 30.868
g0133:  iteration     4860/10000000 | consumed samples:       622080 | consumed tokens:   1274019840 | elapsed time per iteration (ms): 4179.2 | learning rate: 8.492E-05 | global batch size:   128 | lm loss: 1.758298E+00 | loss scale: 524288.0 | grad norm: 0.745 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.628 | tokens per gpu per second (tgs): 1960.189 | TFLOPs: 15.77 |
g0113: [2024-08-03 00:12:58,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=4870, skipped=1, lr=[8.50919424e-05, 8.50919424e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4870 loss: 1.8190 iter time (s): 4.051 samples/sec: 31.597
g0133:  iteration     4870/10000000 | consumed samples:       623360 | consumed tokens:   1276641280 | elapsed time per iteration (ms): 4083.9 | learning rate: 8.509E-05 | global batch size:   128 | lm loss: 1.774545E+00 | loss scale: 524288.0 | grad norm: 0.716 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.343 | tokens per gpu per second (tgs): 2005.948 | TFLOPs: 16.14 |
g0113: [2024-08-03 00:13:39,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=4880, skipped=1, lr=[8.526670506666666e-05, 8.526670506666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4880 loss: 1.7255 iter time (s): 4.078 samples/sec: 31.386
g0133:  iteration     4880/10000000 | consumed samples:       624640 | consumed tokens:   1279262720 | elapsed time per iteration (ms): 4110.8 | learning rate: 8.527E-05 | global batch size:   128 | lm loss: 1.755230E+00 | loss scale: 524288.0 | grad norm: 0.704 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.137 | tokens per gpu per second (tgs): 1992.786 | TFLOPs: 16.04 |
g0113: [2024-08-03 00:14:21,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=4890, skipped=1, lr=[8.544146773333333e-05, 8.544146773333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4890 loss: 1.7566 iter time (s): 4.149 samples/sec: 30.851
g0133:  iteration     4890/10000000 | consumed samples:       625920 | consumed tokens:   1281884160 | elapsed time per iteration (ms): 4181.8 | learning rate: 8.544E-05 | global batch size:   128 | lm loss: 1.745371E+00 | loss scale: 524288.0 | grad norm: 0.698 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.609 | tokens per gpu per second (tgs): 1958.971 | TFLOPs: 15.76 |
g0113: [2024-08-03 00:15:03,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=4900, skipped=1, lr=[8.56162304e-05, 8.56162304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4900 loss: 1.7502 iter time (s): 4.157 samples/sec: 30.792
g0133:  iteration     4900/10000000 | consumed samples:       627200 | consumed tokens:   1284505600 | elapsed time per iteration (ms): 4189.7 | learning rate: 8.562E-05 | global batch size:   128 | lm loss: 1.735526E+00 | loss scale: 524288.0 | grad norm: 0.750 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.551 | tokens per gpu per second (tgs): 1955.285 | TFLOPs: 15.73 |
g0113: [2024-08-03 00:15:46,044] [INFO] [logging.py:96:log_dist] [Rank 0] step=4910, skipped=1, lr=[8.579099306666666e-05, 8.579099306666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4910 loss: 1.7685 iter time (s): 4.219 samples/sec: 30.342
g0133:  iteration     4910/10000000 | consumed samples:       628480 | consumed tokens:   1287127040 | elapsed time per iteration (ms): 4251.2 | learning rate: 8.579E-05 | global batch size:   128 | lm loss: 1.753406E+00 | loss scale: 524288.0 | grad norm: 0.742 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.109 | tokens per gpu per second (tgs): 1927.001 | TFLOPs: 15.51 |
g0113: [2024-08-03 00:16:27,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=4920, skipped=1, lr=[8.596575573333333e-05, 8.596575573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4920 loss: 1.7772 iter time (s): 4.121 samples/sec: 31.058
g0133:  iteration     4920/10000000 | consumed samples:       629760 | consumed tokens:   1289748480 | elapsed time per iteration (ms): 4154.1 | learning rate: 8.597E-05 | global batch size:   128 | lm loss: 1.743705E+00 | loss scale: 524288.0 | grad norm: 0.702 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.813 | tokens per gpu per second (tgs): 1972.005 | TFLOPs: 15.87 |
g0113: [2024-08-03 00:17:08,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=4930, skipped=1, lr=[8.61405184e-05, 8.61405184e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4930 loss: 1.7923 iter time (s): 4.092 samples/sec: 31.281
g0133:  iteration     4930/10000000 | consumed samples:       631040 | consumed tokens:   1292369920 | elapsed time per iteration (ms): 4124.4 | learning rate: 8.614E-05 | global batch size:   128 | lm loss: 1.744968E+00 | loss scale: 524288.0 | grad norm: 0.687 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.035 | tokens per gpu per second (tgs): 1986.246 | TFLOPs: 15.98 |
g0130: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 4932
g0113: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 4932
g0129: Grad overflow on iteration 4932
g0113: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: Grad overflow on iteration 4932
g0129: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0113: Grad overflow on iteration 4932
g0132: Grad overflow on iteration 4932
g0130: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0129: Grad overflow on iteration 4932
g0130: Grad overflow on iteration 4932
g0132: Grad overflow on iteration 4932
g0131: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0131: Grad overflow on iteration 4932
g0125: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0132: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 4932
g0132: Grad overflow on iteration 4932
g0132: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: Grad overflow on iteration 4932
g0128: Grad overflow on iteration 4932
g0125: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 4932
g0130: Grad overflow on iteration 4932
g0133: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0133: Grad overflow on iteration 4932
g0128: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 4932
g0133: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0133: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: Grad overflow on iteration 4932
g0128: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: Grad overflow on iteration 4932
g0113: Grad overflow on iteration 4932
g0125: Grad overflow on iteration 4932
g0113: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0128: Grad overflow on iteration 4932
g0125: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0113: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: Grad overflow on iteration 4932
g0113: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0128: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0129: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0128: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: Grad overflow on iteration 4932
g0131: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0129: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0131: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0133: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0132: Grad overflow on iteration 4932
g0129: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: Grad overflow on iteration 4932
g0125: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0132: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0130: Grad overflow on iteration 4932
g0130: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0132: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0131: Grad overflow on iteration 4932
g0130: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0131: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0131: Grad overflow on iteration 4932
g0133: Grad overflow on iteration 4932
g0113: [2024-08-03 00:17:21,123] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0133: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0131: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0131: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0131: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0133: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: Grad overflow on iteration 4932
g0129: [2024-08-03 00:17:21,123] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0133: [2024-08-03 00:17:21,124] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0113: [2024-08-03 00:17:51,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=4940, skipped=2, lr=[8.631528106666666e-05, 8.631528106666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4940 loss: 1.7398 iter time (s): 4.228 samples/sec: 30.272
g0133:  iteration     4940/10000000 | consumed samples:       632320 | consumed tokens:   1294991360 | elapsed time per iteration (ms): 4261.8 | learning rate: 8.632E-05 | global batch size:   128 | lm loss: 1.754395E+00 | loss scale: 262144.0 | grad norm: 0.712 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.034 | tokens per gpu per second (tgs): 1922.207 | TFLOPs: 15.47 |
g0113: [2024-08-03 00:18:33,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=4950, skipped=2, lr=[8.649004373333334e-05, 8.649004373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4950 loss: 1.7228 iter time (s): 4.164 samples/sec: 30.736
g0133:  iteration     4950/10000000 | consumed samples:       633600 | consumed tokens:   1297612800 | elapsed time per iteration (ms): 4198.0 | learning rate: 8.649E-05 | global batch size:   128 | lm loss: 1.746506E+00 | loss scale: 262144.0 | grad norm: 0.753 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.491 | tokens per gpu per second (tgs): 1951.407 | TFLOPs: 15.70 |
g0113: [2024-08-03 00:19:15,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=4960, skipped=2, lr=[8.666480640000001e-05, 8.666480640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4960 loss: 1.7165 iter time (s): 4.169 samples/sec: 30.706
g0133:  iteration     4960/10000000 | consumed samples:       634880 | consumed tokens:   1300234240 | elapsed time per iteration (ms): 4201.6 | learning rate: 8.666E-05 | global batch size:   128 | lm loss: 1.710666E+00 | loss scale: 262144.0 | grad norm: 0.666 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.465 | tokens per gpu per second (tgs): 1949.742 | TFLOPs: 15.69 |
g0113: [2024-08-03 00:19:57,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=4970, skipped=2, lr=[8.683956906666667e-05, 8.683956906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4970 loss: 1.7371 iter time (s): 4.181 samples/sec: 30.613
g0133:  iteration     4970/10000000 | consumed samples:       636160 | consumed tokens:   1302855680 | elapsed time per iteration (ms): 4213.7 | learning rate: 8.684E-05 | global batch size:   128 | lm loss: 1.712860E+00 | loss scale: 262144.0 | grad norm: 0.695 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.377 | tokens per gpu per second (tgs): 1944.127 | TFLOPs: 15.64 |
g0113: [2024-08-03 00:20:39,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=4980, skipped=2, lr=[8.701433173333334e-05, 8.701433173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4980 loss: 1.6862 iter time (s): 4.196 samples/sec: 30.508
g0133:  iteration     4980/10000000 | consumed samples:       637440 | consumed tokens:   1305477120 | elapsed time per iteration (ms): 4229.4 | learning rate: 8.701E-05 | global batch size:   128 | lm loss: 1.710757E+00 | loss scale: 262144.0 | grad norm: 1.034 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.264 | tokens per gpu per second (tgs): 1936.926 | TFLOPs: 15.59 |
g0113: [2024-08-03 00:21:21,543] [INFO] [logging.py:96:log_dist] [Rank 0] step=4990, skipped=2, lr=[8.71890944e-05, 8.71890944e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 4990 loss: 1.7270 iter time (s): 4.134 samples/sec: 30.966
g0133:  iteration     4990/10000000 | consumed samples:       638720 | consumed tokens:   1308098560 | elapsed time per iteration (ms): 4166.8 | learning rate: 8.719E-05 | global batch size:   128 | lm loss: 1.726492E+00 | loss scale: 262144.0 | grad norm: 0.728 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.719 | tokens per gpu per second (tgs): 1965.996 | TFLOPs: 15.82 |
g0113: [2024-08-03 00:22:04,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=5000, skipped=2, lr=[8.736385706666667e-05, 8.736385706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5000 loss: 1.6896 iter time (s): 4.241 samples/sec: 30.179
g0133:  iteration     5000/10000000 | consumed samples:       640000 | consumed tokens:   1310720000 | elapsed time per iteration (ms): 4273.9 | learning rate: 8.736E-05 | global batch size:   128 | lm loss: 1.720922E+00 | loss scale: 262144.0 | grad norm: 0.657 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.950 | tokens per gpu per second (tgs): 1916.772 | TFLOPs: 15.42 |
g0133: ------------------------------------------------------------------------------------------------
g0133:  validation loss at iteration 5000 | lm loss value: 1.708728E+00 | lm loss PPL: 5.521933E+00 | 
g0133: ------------------------------------------------------------------------------------------------
g0113: saving checkpoint at iteration    5000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: [2024-08-03 00:28:23,368] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step5000 is about to be saved!
g0113: [2024-08-03 00:28:23,375] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0113: [2024-08-03 00:28:23,375] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0113: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0125: [2024-08-03 00:28:23,375] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0125: [2024-08-03 00:28:23,375] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0129: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0129: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0129: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0133: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0133: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0133: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0130: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0130: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0130: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0125: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0128: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0128: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0128: [2024-08-03 00:28:23,376] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0131: [2024-08-03 00:28:23,377] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0131: [2024-08-03 00:28:23,377] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0131: [2024-08-03 00:28:23,377] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0132: [2024-08-03 00:28:23,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0132: [2024-08-03 00:28:23,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0132: [2024-08-03 00:28:23,378] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0133: [2024-08-03 00:28:23,405] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_23-model_00-model_states.pt...
g0132: [2024-08-03 00:28:23,409] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_20-model_00-model_states.pt...
g0125: [2024-08-03 00:28:23,409] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_05-model_00-model_states.pt...
g0130: [2024-08-03 00:28:23,409] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_14-model_00-model_states.pt...
g0129: [2024-08-03 00:28:23,409] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_11-model_00-model_states.pt...
g0128: [2024-08-03 00:28:23,411] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_08-model_00-model_states.pt...
g0131: [2024-08-03 00:28:23,413] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_17-model_00-model_states.pt...
g0113: [2024-08-03 00:28:23,423] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_01-model_00-model_states.pt...
g0133: [2024-08-03 00:28:23,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_23-model_00-model_states.pt.
g0133: [2024-08-03 00:28:23,510] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_24-model_00-model_states.pt...
g0133: [2024-08-03 00:28:23,515] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_24-model_00-model_states.pt.
g0125: [2024-08-03 00:28:23,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_05-model_00-model_states.pt.
g0128: [2024-08-03 00:28:23,544] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_08-model_00-model_states.pt.
g0130: [2024-08-03 00:28:23,552] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_14-model_00-model_states.pt.
g0133: [2024-08-03 00:28:23,567] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_25-model_00-model_states.pt...
g0125: [2024-08-03 00:28:23,576] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_06-model_00-model_states.pt...
g0132: [2024-08-03 00:28:23,579] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_20-model_00-model_states.pt.
g0128: [2024-08-03 00:28:23,583] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_09-model_00-model_states.pt...
g0130: [2024-08-03 00:28:23,587] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_15-model_00-model_states.pt...
g0131: [2024-08-03 00:28:23,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_17-model_00-model_states.pt.
g0129: [2024-08-03 00:28:23,607] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_11-model_00-model_states.pt.
g0132: [2024-08-03 00:28:23,613] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_21-model_00-model_states.pt...
g0131: [2024-08-03 00:28:23,632] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_18-model_00-model_states.pt...
g0129: [2024-08-03 00:28:23,643] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_12-model_00-model_states.pt...
g0113: [2024-08-03 00:28:23,696] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_01-model_00-model_states.pt.
g0130: [2024-08-03 00:28:23,715] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_15-model_00-model_states.pt.
g0113: [2024-08-03 00:28:23,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_02-model_00-model_states.pt...
g0133: [2024-08-03 00:28:23,730] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_25-model_00-model_states.pt.
g0133: [2024-08-03 00:28:23,731] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_07_model_states.pt...
g0130: [2024-08-03 00:28:23,746] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_16-model_00-model_states.pt...
g0131: [2024-08-03 00:28:23,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_18-model_00-model_states.pt.
g0125: [2024-08-03 00:28:23,782] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_06-model_00-model_states.pt.
g0128: [2024-08-03 00:28:23,791] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_09-model_00-model_states.pt.
g0131: [2024-08-03 00:28:23,800] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_19-model_00-model_states.pt...
g0129: [2024-08-03 00:28:23,801] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_12-model_00-model_states.pt.
g0125: [2024-08-03 00:28:23,815] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_07-model_00-model_states.pt...
g0128: [2024-08-03 00:28:23,826] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_10-model_00-model_states.pt...
g0129: [2024-08-03 00:28:23,827] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_13-model_00-model_states.pt...
g0113: [2024-08-03 00:28:23,836] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_02-model_00-model_states.pt.
g0113: [2024-08-03 00:28:23,862] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_03-model_00-model_states.pt...
g0125: [2024-08-03 00:28:23,940] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_07-model_00-model_states.pt.
g0125: [2024-08-03 00:28:23,942] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_01_model_states.pt...
g0130: [2024-08-03 00:28:23,956] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_16-model_00-model_states.pt.
g0130: [2024-08-03 00:28:23,959] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_04_model_states.pt...
g0128: [2024-08-03 00:28:24,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_10-model_00-model_states.pt.
g0128: [2024-08-03 00:28:24,005] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_02_model_states.pt...
g0129: [2024-08-03 00:28:24,029] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_13-model_00-model_states.pt.
g0129: [2024-08-03 00:28:24,031] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_03_model_states.pt...
g0113: [2024-08-03 00:28:24,094] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_03-model_00-model_states.pt.
g0113: [2024-08-03 00:28:24,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_04-model_00-model_states.pt...
g0132: [2024-08-03 00:28:24,180] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_21-model_00-model_states.pt.
g0132: [2024-08-03 00:28:24,210] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_22-model_00-model_states.pt...
g0131: [2024-08-03 00:28:24,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_19-model_00-model_states.pt.
g0131: [2024-08-03 00:28:24,215] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_05_model_states.pt...
g0113: [2024-08-03 00:28:24,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_04-model_00-model_states.pt.
g0113: [2024-08-03 00:28:24,227] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt
g0113: [2024-08-03 00:28:24,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt...
g0132: [2024-08-03 00:28:24,485] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/layer_22-model_00-model_states.pt.
g0132: [2024-08-03 00:28:24,487] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_06_model_states.pt...
g0133: [2024-08-03 00:28:25,626] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_07_model_states.pt.
g0133: [2024-08-03 00:28:25,627] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0125: [2024-08-03 00:28:26,289] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_01_model_states.pt.
g0125: [2024-08-03 00:28:26,290] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0129: [2024-08-03 00:28:26,434] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_03_model_states.pt.
g0129: [2024-08-03 00:28:26,435] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0131: [2024-08-03 00:28:26,567] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_05_model_states.pt.
g0131: [2024-08-03 00:28:26,568] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0130: [2024-08-03 00:28:26,584] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_04_model_states.pt.
g0130: [2024-08-03 00:28:26,585] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0128: [2024-08-03 00:28:26,610] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_02_model_states.pt.
g0128: [2024-08-03 00:28:26,610] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0132: [2024-08-03 00:28:27,056] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_06_model_states.pt.
g0132: [2024-08-03 00:28:27,056] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0113: [2024-08-03 00:28:27,611] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step5000/mp_rank_00_model_states.pt.
g0113: [2024-08-03 00:28:27,612] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5000 is ready now!
g0113:   successfully saved checkpoint at iteration    5000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: Checkpoint Save GB: 22.521, GB/Sec: 5.29, Latency(second): 4.26
g0133: (min, max) time across ranks (ms):
g0133:     save-checkpoint ................................: (4260.16, 4260.43)
g0113: [2024-08-03 00:29:08,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=5010, skipped=2, lr=[8.753861973333334e-05, 8.753861973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5010 loss: 1.6986 iter time (s): 4.073 samples/sec: 31.424
g0133:  iteration     5010/10000000 | consumed samples:       641280 | consumed tokens:   1313341440 | elapsed time per iteration (ms): 42435.4 | learning rate: 8.754E-05 | global batch size:   128 | lm loss: 1.702684E+00 | loss scale: 262144.0 | grad norm: 0.743 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.016 | tokens per gpu per second (tgs): 193.046 | TFLOPs: 1.55 |
g0113: [2024-08-03 00:29:50,463] [INFO] [logging.py:96:log_dist] [Rank 0] step=5020, skipped=2, lr=[8.77133824e-05, 8.77133824e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5020 loss: 1.7039 iter time (s): 4.150 samples/sec: 30.843
g0133:  iteration     5020/10000000 | consumed samples:       642560 | consumed tokens:   1315962880 | elapsed time per iteration (ms): 4182.8 | learning rate: 8.771E-05 | global batch size:   128 | lm loss: 1.707153E+00 | loss scale: 262144.0 | grad norm: 0.662 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.601 | tokens per gpu per second (tgs): 1958.491 | TFLOPs: 15.76 |
g0113: [2024-08-03 00:30:31,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=5030, skipped=2, lr=[8.788814506666667e-05, 8.788814506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5030 loss: 1.7132 iter time (s): 4.079 samples/sec: 31.380
g0133:  iteration     5030/10000000 | consumed samples:       643840 | consumed tokens:   1318584320 | elapsed time per iteration (ms): 4111.6 | learning rate: 8.789E-05 | global batch size:   128 | lm loss: 1.693711E+00 | loss scale: 262144.0 | grad norm: 0.674 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.132 | tokens per gpu per second (tgs): 1992.435 | TFLOPs: 16.03 |
g0113: [2024-08-03 00:31:12,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=5040, skipped=2, lr=[8.806290773333334e-05, 8.806290773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5040 loss: 1.6634 iter time (s): 4.090 samples/sec: 31.294
g0133:  iteration     5040/10000000 | consumed samples:       645120 | consumed tokens:   1321205760 | elapsed time per iteration (ms): 4122.6 | learning rate: 8.806E-05 | global batch size:   128 | lm loss: 1.696022E+00 | loss scale: 262144.0 | grad norm: 0.656 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.048 | tokens per gpu per second (tgs): 1987.073 | TFLOPs: 15.99 |
g0113: [2024-08-03 00:31:54,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=5050, skipped=2, lr=[8.82376704e-05, 8.82376704e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5050 loss: 1.6797 iter time (s): 4.096 samples/sec: 31.252
g0133:  iteration     5050/10000000 | consumed samples:       646400 | consumed tokens:   1323827200 | elapsed time per iteration (ms): 4127.9 | learning rate: 8.824E-05 | global batch size:   128 | lm loss: 1.694622E+00 | loss scale: 262144.0 | grad norm: 0.657 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.008 | tokens per gpu per second (tgs): 1984.521 | TFLOPs: 15.97 |
g0113: [2024-08-03 00:32:35,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=5060, skipped=2, lr=[8.841243306666667e-05, 8.841243306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5060 loss: 1.7159 iter time (s): 4.154 samples/sec: 30.812
g0133:  iteration     5060/10000000 | consumed samples:       647680 | consumed tokens:   1326448640 | elapsed time per iteration (ms): 4186.4 | learning rate: 8.841E-05 | global batch size:   128 | lm loss: 1.692260E+00 | loss scale: 262144.0 | grad norm: 0.694 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.575 | tokens per gpu per second (tgs): 1956.800 | TFLOPs: 15.75 |
g0113: [2024-08-03 00:33:16,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=5070, skipped=2, lr=[8.858719573333333e-05, 8.858719573333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5070 loss: 1.6820 iter time (s): 4.058 samples/sec: 31.541
g0133:  iteration     5070/10000000 | consumed samples:       648960 | consumed tokens:   1329070080 | elapsed time per iteration (ms): 4090.4 | learning rate: 8.859E-05 | global batch size:   128 | lm loss: 1.690190E+00 | loss scale: 262144.0 | grad norm: 0.635 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.293 | tokens per gpu per second (tgs): 2002.738 | TFLOPs: 16.12 |
g0113: [2024-08-03 00:33:59,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=5080, skipped=2, lr=[8.87619584e-05, 8.87619584e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5080 loss: 1.6845 iter time (s): 4.192 samples/sec: 30.532
g0133:  iteration     5080/10000000 | consumed samples:       650240 | consumed tokens:   1331691520 | elapsed time per iteration (ms): 4224.6 | learning rate: 8.876E-05 | global batch size:   128 | lm loss: 1.680714E+00 | loss scale: 262144.0 | grad norm: 0.631 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.299 | tokens per gpu per second (tgs): 1939.118 | TFLOPs: 15.60 |
g0113: [2024-08-03 00:34:40,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=5090, skipped=2, lr=[8.893672106666667e-05, 8.893672106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5090 loss: 1.6861 iter time (s): 4.139 samples/sec: 30.924
g0133:  iteration     5090/10000000 | consumed samples:       651520 | consumed tokens:   1334312960 | elapsed time per iteration (ms): 4171.5 | learning rate: 8.894E-05 | global batch size:   128 | lm loss: 1.699573E+00 | loss scale: 262144.0 | grad norm: 0.627 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.685 | tokens per gpu per second (tgs): 1963.822 | TFLOPs: 15.80 |
g0113: [2024-08-03 00:35:22,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=5100, skipped=2, lr=[8.911148373333333e-05, 8.911148373333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5100 loss: 1.6815 iter time (s): 4.177 samples/sec: 30.646
g0133:  iteration     5100/10000000 | consumed samples:       652800 | consumed tokens:   1336934400 | elapsed time per iteration (ms): 4208.9 | learning rate: 8.911E-05 | global batch size:   128 | lm loss: 1.691111E+00 | loss scale: 262144.0 | grad norm: 0.640 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.412 | tokens per gpu per second (tgs): 1946.365 | TFLOPs: 15.66 |
g0113: [2024-08-03 00:36:05,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=5110, skipped=2, lr=[8.92862464e-05, 8.92862464e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5110 loss: 1.6749 iter time (s): 4.179 samples/sec: 30.631
g0133:  iteration     5110/10000000 | consumed samples:       654080 | consumed tokens:   1339555840 | elapsed time per iteration (ms): 4211.5 | learning rate: 8.929E-05 | global batch size:   128 | lm loss: 1.687639E+00 | loss scale: 262144.0 | grad norm: 0.612 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.393 | tokens per gpu per second (tgs): 1945.152 | TFLOPs: 15.65 |
g0113: [2024-08-03 00:36:46,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=5120, skipped=2, lr=[8.946100906666666e-05, 8.946100906666666e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5120 loss: 1.6890 iter time (s): 4.108 samples/sec: 31.157
g0133:  iteration     5120/10000000 | consumed samples:       655360 | consumed tokens:   1342177280 | elapsed time per iteration (ms): 4140.5 | learning rate: 8.946E-05 | global batch size:   128 | lm loss: 1.663643E+00 | loss scale: 262144.0 | grad norm: 0.675 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.914 | tokens per gpu per second (tgs): 1978.481 | TFLOPs: 15.92 |
g0113: [2024-08-03 00:37:27,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=5130, skipped=2, lr=[8.963577173333334e-05, 8.963577173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5130 loss: 1.7092 iter time (s): 4.083 samples/sec: 31.348
g0133:  iteration     5130/10000000 | consumed samples:       656640 | consumed tokens:   1344798720 | elapsed time per iteration (ms): 4115.4 | learning rate: 8.964E-05 | global batch size:   128 | lm loss: 1.686761E+00 | loss scale: 262144.0 | grad norm: 0.670 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.103 | tokens per gpu per second (tgs): 1990.561 | TFLOPs: 16.02 |
g0113: [2024-08-03 00:38:09,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=5140, skipped=2, lr=[8.981053440000001e-05, 8.981053440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5140 loss: 1.6265 iter time (s): 4.167 samples/sec: 30.716
g0133:  iteration     5140/10000000 | consumed samples:       657920 | consumed tokens:   1347420160 | elapsed time per iteration (ms): 4220.8 | learning rate: 8.981E-05 | global batch size:   128 | lm loss: 1.666890E+00 | loss scale: 262144.0 | grad norm: 0.661 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.326 | tokens per gpu per second (tgs): 1940.844 | TFLOPs: 15.62 |
g0113: [2024-08-03 00:38:52,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=5150, skipped=2, lr=[8.998529706666668e-05, 8.998529706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5150 loss: 1.6475 iter time (s): 4.229 samples/sec: 30.265
g0133:  iteration     5150/10000000 | consumed samples:       659200 | consumed tokens:   1350041600 | elapsed time per iteration (ms): 4264.0 | learning rate: 8.999E-05 | global batch size:   128 | lm loss: 1.674999E+00 | loss scale: 262144.0 | grad norm: 0.612 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.019 | tokens per gpu per second (tgs): 1921.216 | TFLOPs: 15.46 |
g0113: [2024-08-03 00:39:35,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=5160, skipped=2, lr=[9.016005973333334e-05, 9.016005973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5160 loss: 1.6421 iter time (s): 4.236 samples/sec: 30.215
g0133:  iteration     5160/10000000 | consumed samples:       660480 | consumed tokens:   1352663040 | elapsed time per iteration (ms): 4273.1 | learning rate: 9.016E-05 | global batch size:   128 | lm loss: 1.657495E+00 | loss scale: 262144.0 | grad norm: 0.621 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.955 | tokens per gpu per second (tgs): 1917.107 | TFLOPs: 15.43 |
g0113: [2024-08-03 00:40:17,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=5170, skipped=2, lr=[9.033482240000001e-05, 9.033482240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5170 loss: 1.6755 iter time (s): 4.189 samples/sec: 30.559
g0133:  iteration     5170/10000000 | consumed samples:       661760 | consumed tokens:   1355284480 | elapsed time per iteration (ms): 4220.8 | learning rate: 9.033E-05 | global batch size:   128 | lm loss: 1.658398E+00 | loss scale: 262144.0 | grad norm: 0.630 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.326 | tokens per gpu per second (tgs): 1940.850 | TFLOPs: 15.62 |
g0113: [2024-08-03 00:41:00,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=5180, skipped=2, lr=[9.050958506666667e-05, 9.050958506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5180 loss: 1.6292 iter time (s): 4.258 samples/sec: 30.064
g0133:  iteration     5180/10000000 | consumed samples:       663040 | consumed tokens:   1357905920 | elapsed time per iteration (ms): 4290.2 | learning rate: 9.051E-05 | global batch size:   128 | lm loss: 1.661441E+00 | loss scale: 262144.0 | grad norm: 0.648 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.835 | tokens per gpu per second (tgs): 1909.450 | TFLOPs: 15.37 |
g0113: [2024-08-03 00:41:42,616] [INFO] [logging.py:96:log_dist] [Rank 0] step=5190, skipped=2, lr=[9.068434773333334e-05, 9.068434773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5190 loss: 1.6030 iter time (s): 4.202 samples/sec: 30.459
g0133:  iteration     5190/10000000 | consumed samples:       664320 | consumed tokens:   1360527360 | elapsed time per iteration (ms): 4234.6 | learning rate: 9.068E-05 | global batch size:   128 | lm loss: 1.646831E+00 | loss scale: 262144.0 | grad norm: 0.630 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.227 | tokens per gpu per second (tgs): 1934.539 | TFLOPs: 15.57 |
g0113: [2024-08-03 00:42:24,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=5200, skipped=2, lr=[9.08591104e-05, 9.08591104e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5200 loss: 1.7015 iter time (s): 4.136 samples/sec: 30.950
g0133:  iteration     5200/10000000 | consumed samples:       665600 | consumed tokens:   1363148800 | elapsed time per iteration (ms): 4168.2 | learning rate: 9.086E-05 | global batch size:   128 | lm loss: 1.650392E+00 | loss scale: 262144.0 | grad norm: 0.644 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.708 | tokens per gpu per second (tgs): 1965.341 | TFLOPs: 15.82 |
g0113: [2024-08-03 00:43:06,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=5210, skipped=2, lr=[9.103387306666667e-05, 9.103387306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5210 loss: 1.6081 iter time (s): 4.182 samples/sec: 30.607
g0133:  iteration     5210/10000000 | consumed samples:       666880 | consumed tokens:   1365770240 | elapsed time per iteration (ms): 4214.4 | learning rate: 9.103E-05 | global batch size:   128 | lm loss: 1.649968E+00 | loss scale: 262144.0 | grad norm: 0.602 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.372 | tokens per gpu per second (tgs): 1943.833 | TFLOPs: 15.64 |
g0113: [2024-08-03 00:43:48,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=5220, skipped=2, lr=[9.120863573333334e-05, 9.120863573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5220 loss: 1.6446 iter time (s): 4.127 samples/sec: 31.016
g0133:  iteration     5220/10000000 | consumed samples:       668160 | consumed tokens:   1368391680 | elapsed time per iteration (ms): 4159.4 | learning rate: 9.121E-05 | global batch size:   128 | lm loss: 1.646728E+00 | loss scale: 262144.0 | grad norm: 0.640 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.774 | tokens per gpu per second (tgs): 1969.531 | TFLOPs: 15.85 |
g0113: [2024-08-03 00:44:29,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=5230, skipped=2, lr=[9.13833984e-05, 9.13833984e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5230 loss: 1.6379 iter time (s): 4.163 samples/sec: 30.744
g0133:  iteration     5230/10000000 | consumed samples:       669440 | consumed tokens:   1371013120 | elapsed time per iteration (ms): 4195.7 | learning rate: 9.138E-05 | global batch size:   128 | lm loss: 1.646116E+00 | loss scale: 262144.0 | grad norm: 0.618 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.507 | tokens per gpu per second (tgs): 1952.472 | TFLOPs: 15.71 |
g0113: [2024-08-03 00:45:12,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=5240, skipped=2, lr=[9.155816106666667e-05, 9.155816106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5240 loss: 1.6771 iter time (s): 4.191 samples/sec: 30.539
g0133:  iteration     5240/10000000 | consumed samples:       670720 | consumed tokens:   1373634560 | elapsed time per iteration (ms): 4224.9 | learning rate: 9.156E-05 | global batch size:   128 | lm loss: 1.639642E+00 | loss scale: 262144.0 | grad norm: 0.628 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.296 | tokens per gpu per second (tgs): 1938.966 | TFLOPs: 15.60 |
g0113: [2024-08-03 00:45:53,952] [INFO] [logging.py:96:log_dist] [Rank 0] step=5250, skipped=2, lr=[9.173292373333334e-05, 9.173292373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5250 loss: 1.6159 iter time (s): 4.138 samples/sec: 30.930
g0133:  iteration     5250/10000000 | consumed samples:       672000 | consumed tokens:   1376256000 | elapsed time per iteration (ms): 4170.8 | learning rate: 9.173E-05 | global batch size:   128 | lm loss: 1.646985E+00 | loss scale: 262144.0 | grad norm: 0.603 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.689 | tokens per gpu per second (tgs): 1964.119 | TFLOPs: 15.81 |
g0113: [2024-08-03 00:46:35,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=5260, skipped=2, lr=[9.19076864e-05, 9.19076864e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5260 loss: 1.5954 iter time (s): 4.140 samples/sec: 30.920
g0133:  iteration     5260/10000000 | consumed samples:       673280 | consumed tokens:   1378877440 | elapsed time per iteration (ms): 4172.3 | learning rate: 9.191E-05 | global batch size:   128 | lm loss: 1.626068E+00 | loss scale: 262144.0 | grad norm: 0.591 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.679 | tokens per gpu per second (tgs): 1963.431 | TFLOPs: 15.80 |
g0113: [2024-08-03 00:47:17,888] [INFO] [logging.py:96:log_dist] [Rank 0] step=5270, skipped=2, lr=[9.208244906666667e-05, 9.208244906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5270 loss: 1.6592 iter time (s): 4.189 samples/sec: 30.556
g0133:  iteration     5270/10000000 | consumed samples:       674560 | consumed tokens:   1381498880 | elapsed time per iteration (ms): 4221.3 | learning rate: 9.208E-05 | global batch size:   128 | lm loss: 1.632249E+00 | loss scale: 262144.0 | grad norm: 0.623 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.323 | tokens per gpu per second (tgs): 1940.650 | TFLOPs: 15.62 |
g0113: [2024-08-03 00:47:59,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=5280, skipped=2, lr=[9.225721173333333e-05, 9.225721173333333e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5280 loss: 1.6873 iter time (s): 4.178 samples/sec: 30.635
g0133:  iteration     5280/10000000 | consumed samples:       675840 | consumed tokens:   1384120320 | elapsed time per iteration (ms): 4210.3 | learning rate: 9.226E-05 | global batch size:   128 | lm loss: 1.659088E+00 | loss scale: 262144.0 | grad norm: 0.598 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.401 | tokens per gpu per second (tgs): 1945.691 | TFLOPs: 15.66 |
g0113: [2024-08-03 00:48:42,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=5290, skipped=2, lr=[9.24319744e-05, 9.24319744e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5290 loss: 1.6479 iter time (s): 4.212 samples/sec: 30.393
g0133:  iteration     5290/10000000 | consumed samples:       677120 | consumed tokens:   1386741760 | elapsed time per iteration (ms): 4244.8 | learning rate: 9.243E-05 | global batch size:   128 | lm loss: 1.648989E+00 | loss scale: 262144.0 | grad norm: 0.641 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.155 | tokens per gpu per second (tgs): 1929.906 | TFLOPs: 15.53 |
g0113: [2024-08-03 00:49:22,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=5300, skipped=2, lr=[9.260673706666667e-05, 9.260673706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5300 loss: 1.6352 iter time (s): 4.020 samples/sec: 31.842
g0133:  iteration     5300/10000000 | consumed samples:       678400 | consumed tokens:   1389363200 | elapsed time per iteration (ms): 4054.0 | learning rate: 9.261E-05 | global batch size:   128 | lm loss: 1.618849E+00 | loss scale: 262144.0 | grad norm: 0.587 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.574 | tokens per gpu per second (tgs): 2020.730 | TFLOPs: 16.26 |
g0113: [2024-08-03 00:50:03,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=5310, skipped=2, lr=[9.278149973333335e-05, 9.278149973333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5310 loss: 1.6206 iter time (s): 3.987 samples/sec: 32.104
g0133:  iteration     5310/10000000 | consumed samples:       679680 | consumed tokens:   1391984640 | elapsed time per iteration (ms): 4022.2 | learning rate: 9.278E-05 | global batch size:   128 | lm loss: 1.648498E+00 | loss scale: 262144.0 | grad norm: 0.562 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.824 | tokens per gpu per second (tgs): 2036.708 | TFLOPs: 16.39 |
g0113: [2024-08-03 00:50:45,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=5320, skipped=2, lr=[9.295626240000001e-05, 9.295626240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5320 loss: 1.6681 iter time (s): 4.152 samples/sec: 30.829
g0133:  iteration     5320/10000000 | consumed samples:       680960 | consumed tokens:   1394606080 | elapsed time per iteration (ms): 4184.8 | learning rate: 9.296E-05 | global batch size:   128 | lm loss: 1.627344E+00 | loss scale: 262144.0 | grad norm: 0.574 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.587 | tokens per gpu per second (tgs): 1957.541 | TFLOPs: 15.75 |
g0113: [2024-08-03 00:51:26,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=5330, skipped=2, lr=[9.313102506666668e-05, 9.313102506666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5330 loss: 1.6262 iter time (s): 4.097 samples/sec: 31.239
g0133:  iteration     5330/10000000 | consumed samples:       682240 | consumed tokens:   1397227520 | elapsed time per iteration (ms): 4130.1 | learning rate: 9.313E-05 | global batch size:   128 | lm loss: 1.629579E+00 | loss scale: 262144.0 | grad norm: 0.641 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.992 | tokens per gpu per second (tgs): 1983.503 | TFLOPs: 15.96 |
g0113: [2024-08-03 00:52:08,922] [INFO] [logging.py:96:log_dist] [Rank 0] step=5340, skipped=2, lr=[9.330578773333334e-05, 9.330578773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5340 loss: 1.5822 iter time (s): 4.225 samples/sec: 30.297
g0133:  iteration     5340/10000000 | consumed samples:       683520 | consumed tokens:   1399848960 | elapsed time per iteration (ms): 4257.2 | learning rate: 9.331E-05 | global batch size:   128 | lm loss: 1.624721E+00 | loss scale: 262144.0 | grad norm: 0.588 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.067 | tokens per gpu per second (tgs): 1924.287 | TFLOPs: 15.49 |
g0113: [2024-08-03 00:52:50,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=5350, skipped=2, lr=[9.348055040000001e-05, 9.348055040000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5350 loss: 1.6910 iter time (s): 4.162 samples/sec: 30.753
g0133:  iteration     5350/10000000 | consumed samples:       684800 | consumed tokens:   1402470400 | elapsed time per iteration (ms): 4194.6 | learning rate: 9.348E-05 | global batch size:   128 | lm loss: 1.641835E+00 | loss scale: 262144.0 | grad norm: 0.590 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.516 | tokens per gpu per second (tgs): 1953.010 | TFLOPs: 15.72 |
g0113: [2024-08-03 00:53:33,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=5360, skipped=2, lr=[9.365531306666668e-05, 9.365531306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5360 loss: 1.6308 iter time (s): 4.261 samples/sec: 30.040
g0133:  iteration     5360/10000000 | consumed samples:       686080 | consumed tokens:   1405091840 | elapsed time per iteration (ms): 4293.4 | learning rate: 9.366E-05 | global batch size:   128 | lm loss: 1.626413E+00 | loss scale: 262144.0 | grad norm: 0.606 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.813 | tokens per gpu per second (tgs): 1908.047 | TFLOPs: 15.35 |
g0113: [2024-08-03 00:54:15,325] [INFO] [logging.py:96:log_dist] [Rank 0] step=5370, skipped=2, lr=[9.383007573333334e-05, 9.383007573333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5370 loss: 1.6418 iter time (s): 4.120 samples/sec: 31.068
g0133:  iteration     5370/10000000 | consumed samples:       687360 | consumed tokens:   1407713280 | elapsed time per iteration (ms): 4152.3 | learning rate: 9.383E-05 | global batch size:   128 | lm loss: 1.631700E+00 | loss scale: 262144.0 | grad norm: 0.580 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.826 | tokens per gpu per second (tgs): 1972.867 | TFLOPs: 15.88 |
g0113: [2024-08-03 00:54:57,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=5380, skipped=2, lr=[9.400483840000001e-05, 9.400483840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5380 loss: 1.6115 iter time (s): 4.178 samples/sec: 30.637
g0133:  iteration     5380/10000000 | consumed samples:       688640 | consumed tokens:   1410334720 | elapsed time per iteration (ms): 4210.5 | learning rate: 9.400E-05 | global batch size:   128 | lm loss: 1.622694E+00 | loss scale: 262144.0 | grad norm: 0.636 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.400 | tokens per gpu per second (tgs): 1945.619 | TFLOPs: 15.66 |
g0113: [2024-08-03 00:55:39,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=5390, skipped=2, lr=[9.417960106666667e-05, 9.417960106666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5390 loss: 1.6486 iter time (s): 4.163 samples/sec: 30.748
g0133:  iteration     5390/10000000 | consumed samples:       689920 | consumed tokens:   1412956160 | elapsed time per iteration (ms): 4195.1 | learning rate: 9.418E-05 | global batch size:   128 | lm loss: 1.606241E+00 | loss scale: 262144.0 | grad norm: 0.607 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.512 | tokens per gpu per second (tgs): 1952.760 | TFLOPs: 15.71 |
g0113: [2024-08-03 00:56:22,148] [INFO] [logging.py:96:log_dist] [Rank 0] step=5400, skipped=2, lr=[9.435436373333334e-05, 9.435436373333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5400 loss: 1.6304 iter time (s): 4.244 samples/sec: 30.159
g0133:  iteration     5400/10000000 | consumed samples:       691200 | consumed tokens:   1415577600 | elapsed time per iteration (ms): 4276.6 | learning rate: 9.435E-05 | global batch size:   128 | lm loss: 1.603847E+00 | loss scale: 262144.0 | grad norm: 0.570 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.930 | tokens per gpu per second (tgs): 1915.522 | TFLOPs: 15.41 |
g0113: [2024-08-03 00:57:04,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=5410, skipped=2, lr=[9.452912640000001e-05, 9.452912640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5410 loss: 1.6113 iter time (s): 4.171 samples/sec: 30.688
g0133:  iteration     5410/10000000 | consumed samples:       692480 | consumed tokens:   1418199040 | elapsed time per iteration (ms): 4203.4 | learning rate: 9.453E-05 | global batch size:   128 | lm loss: 1.616756E+00 | loss scale: 262144.0 | grad norm: 0.575 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.452 | tokens per gpu per second (tgs): 1948.901 | TFLOPs: 15.68 |
g0113: [2024-08-03 00:57:45,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=5420, skipped=2, lr=[9.470388906666667e-05, 9.470388906666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5420 loss: 1.5825 iter time (s): 4.147 samples/sec: 30.862
g0133:  iteration     5420/10000000 | consumed samples:       693760 | consumed tokens:   1420820480 | elapsed time per iteration (ms): 4179.8 | learning rate: 9.470E-05 | global batch size:   128 | lm loss: 1.608921E+00 | loss scale: 262144.0 | grad norm: 0.566 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.624 | tokens per gpu per second (tgs): 1959.906 | TFLOPs: 15.77 |
g0113: [2024-08-03 00:58:27,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=5430, skipped=2, lr=[9.487865173333334e-05, 9.487865173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5430 loss: 1.5654 iter time (s): 4.165 samples/sec: 30.734
g0133:  iteration     5430/10000000 | consumed samples:       695040 | consumed tokens:   1423441920 | elapsed time per iteration (ms): 4197.1 | learning rate: 9.488E-05 | global batch size:   128 | lm loss: 1.619372E+00 | loss scale: 262144.0 | grad norm: 0.575 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.498 | tokens per gpu per second (tgs): 1951.847 | TFLOPs: 15.71 |
g0113: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0113: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0131: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0128: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0132: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0132: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0132: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0131: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0132: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0130: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0132: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0129: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0128: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0129: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0125: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0130: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0130: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0130: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0113: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0133: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0125: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0125: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0128: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0129: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0128: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0129: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0129: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0133: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0131: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0130: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0132: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0133: [2024-08-03 00:58:44,162] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 00:58:44,163] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0113: [2024-08-03 00:59:09,531] [INFO] [logging.py:96:log_dist] [Rank 0] step=5440, skipped=2, lr=[9.50534144e-05, 9.50534144e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5440 loss: 1.5899 iter time (s): 4.126 samples/sec: 31.025
g0133:  iteration     5440/10000000 | consumed samples:       696320 | consumed tokens:   1426063360 | elapsed time per iteration (ms): 4158.1 | learning rate: 9.505E-05 | global batch size:   128 | lm loss: 1.611918E+00 | loss scale: 524288.0 | grad norm: 0.557 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.784 | tokens per gpu per second (tgs): 1970.151 | TFLOPs: 15.85 |
g0113: [2024-08-03 00:59:50,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=5450, skipped=2, lr=[9.522817706666667e-05, 9.522817706666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5450 loss: 1.6359 iter time (s): 4.058 samples/sec: 31.545
g0133:  iteration     5450/10000000 | consumed samples:       697600 | consumed tokens:   1428684800 | elapsed time per iteration (ms): 4090.0 | learning rate: 9.523E-05 | global batch size:   128 | lm loss: 1.627165E+00 | loss scale: 524288.0 | grad norm: 0.578 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.295 | tokens per gpu per second (tgs): 2002.911 | TFLOPs: 16.12 |
g0113: [2024-08-03 01:00:32,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=5460, skipped=2, lr=[9.540293973333334e-05, 9.540293973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5460 loss: 1.5920 iter time (s): 4.162 samples/sec: 30.754
g0133:  iteration     5460/10000000 | consumed samples:       698880 | consumed tokens:   1431306240 | elapsed time per iteration (ms): 4194.6 | learning rate: 9.540E-05 | global batch size:   128 | lm loss: 1.616829E+00 | loss scale: 524288.0 | grad norm: 0.558 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.516 | tokens per gpu per second (tgs): 1953.008 | TFLOPs: 15.72 |
g0113: [2024-08-03 01:01:14,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=5470, skipped=2, lr=[9.55777024e-05, 9.55777024e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5470 loss: 1.6236 iter time (s): 4.199 samples/sec: 30.481
g0133:  iteration     5470/10000000 | consumed samples:       700160 | consumed tokens:   1433927680 | elapsed time per iteration (ms): 4231.8 | learning rate: 9.558E-05 | global batch size:   128 | lm loss: 1.615688E+00 | loss scale: 524288.0 | grad norm: 0.550 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.247 | tokens per gpu per second (tgs): 1935.809 | TFLOPs: 15.58 |
g0113: [2024-08-03 01:01:57,111] [INFO] [logging.py:96:log_dist] [Rank 0] step=5480, skipped=2, lr=[9.575246506666667e-05, 9.575246506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5480 loss: 1.5945 iter time (s): 4.209 samples/sec: 30.411
g0133:  iteration     5480/10000000 | consumed samples:       701440 | consumed tokens:   1436549120 | elapsed time per iteration (ms): 4241.5 | learning rate: 9.575E-05 | global batch size:   128 | lm loss: 1.598182E+00 | loss scale: 524288.0 | grad norm: 0.557 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.178 | tokens per gpu per second (tgs): 1931.395 | TFLOPs: 15.54 |
g0113: [2024-08-03 01:02:39,052] [INFO] [logging.py:96:log_dist] [Rank 0] step=5490, skipped=2, lr=[9.592722773333335e-05, 9.592722773333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5490 loss: 1.5747 iter time (s): 4.161 samples/sec: 30.761
g0133:  iteration     5490/10000000 | consumed samples:       702720 | consumed tokens:   1439170560 | elapsed time per iteration (ms): 4194.1 | learning rate: 9.593E-05 | global batch size:   128 | lm loss: 1.600174E+00 | loss scale: 524288.0 | grad norm: 0.552 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.519 | tokens per gpu per second (tgs): 1953.240 | TFLOPs: 15.72 |
g0113: [2024-08-03 01:03:21,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=5500, skipped=2, lr=[9.610199040000002e-05, 9.610199040000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5500 loss: 1.5529 iter time (s): 4.193 samples/sec: 30.529
g0133:  iteration     5500/10000000 | consumed samples:       704000 | consumed tokens:   1441792000 | elapsed time per iteration (ms): 4225.0 | learning rate: 9.610E-05 | global batch size:   128 | lm loss: 1.606988E+00 | loss scale: 524288.0 | grad norm: 0.556 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.296 | tokens per gpu per second (tgs): 1938.919 | TFLOPs: 15.60 |
g0113: [2024-08-03 01:04:04,048] [INFO] [logging.py:96:log_dist] [Rank 0] step=5510, skipped=2, lr=[9.627675306666668e-05, 9.627675306666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5510 loss: 1.5836 iter time (s): 4.242 samples/sec: 30.173
g0133:  iteration     5510/10000000 | consumed samples:       705280 | consumed tokens:   1444413440 | elapsed time per iteration (ms): 4274.5 | learning rate: 9.628E-05 | global batch size:   128 | lm loss: 1.601510E+00 | loss scale: 524288.0 | grad norm: 0.555 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.945 | tokens per gpu per second (tgs): 1916.472 | TFLOPs: 15.42 |
g0113: [2024-08-03 01:04:46,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=5520, skipped=2, lr=[9.645151573333335e-05, 9.645151573333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5520 loss: 1.5955 iter time (s): 4.171 samples/sec: 30.690
g0133:  iteration     5520/10000000 | consumed samples:       706560 | consumed tokens:   1447034880 | elapsed time per iteration (ms): 4203.1 | learning rate: 9.645E-05 | global batch size:   128 | lm loss: 1.593476E+00 | loss scale: 524288.0 | grad norm: 0.618 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.454 | tokens per gpu per second (tgs): 1949.031 | TFLOPs: 15.68 |
g0113: [2024-08-03 01:05:28,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=5530, skipped=2, lr=[9.662627840000001e-05, 9.662627840000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5530 loss: 1.5652 iter time (s): 4.182 samples/sec: 30.605
g0133:  iteration     5530/10000000 | consumed samples:       707840 | consumed tokens:   1449656320 | elapsed time per iteration (ms): 4214.8 | learning rate: 9.663E-05 | global batch size:   128 | lm loss: 1.588297E+00 | loss scale: 524288.0 | grad norm: 0.550 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.369 | tokens per gpu per second (tgs): 1943.620 | TFLOPs: 15.64 |
g0113: [2024-08-03 01:06:09,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=5540, skipped=2, lr=[9.680104106666668e-05, 9.680104106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5540 loss: 1.6135 iter time (s): 4.133 samples/sec: 30.967
g0133:  iteration     5540/10000000 | consumed samples:       709120 | consumed tokens:   1452277760 | elapsed time per iteration (ms): 4165.7 | learning rate: 9.680E-05 | global batch size:   128 | lm loss: 1.596507E+00 | loss scale: 524288.0 | grad norm: 0.630 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.727 | tokens per gpu per second (tgs): 1966.555 | TFLOPs: 15.83 |
g0113: [2024-08-03 01:06:51,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=5550, skipped=2, lr=[9.697580373333335e-05, 9.697580373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5550 loss: 1.5967 iter time (s): 4.134 samples/sec: 30.966
g0133:  iteration     5550/10000000 | consumed samples:       710400 | consumed tokens:   1454899200 | elapsed time per iteration (ms): 4166.0 | learning rate: 9.698E-05 | global batch size:   128 | lm loss: 1.593799E+00 | loss scale: 524288.0 | grad norm: 0.533 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.725 | tokens per gpu per second (tgs): 1966.411 | TFLOPs: 15.82 |
g0113: [2024-08-03 01:07:33,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=5560, skipped=2, lr=[9.715056640000001e-05, 9.715056640000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5560 loss: 1.5349 iter time (s): 4.136 samples/sec: 30.945
g0133:  iteration     5560/10000000 | consumed samples:       711680 | consumed tokens:   1457520640 | elapsed time per iteration (ms): 4169.4 | learning rate: 9.715E-05 | global batch size:   128 | lm loss: 1.593845E+00 | loss scale: 524288.0 | grad norm: 0.547 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.700 | tokens per gpu per second (tgs): 1964.788 | TFLOPs: 15.81 |
g0113: [2024-08-03 01:08:14,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=5570, skipped=2, lr=[9.732532906666668e-05, 9.732532906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5570 loss: 1.6184 iter time (s): 4.053 samples/sec: 31.581
g0133:  iteration     5570/10000000 | consumed samples:       712960 | consumed tokens:   1460142080 | elapsed time per iteration (ms): 4085.6 | learning rate: 9.733E-05 | global batch size:   128 | lm loss: 1.594185E+00 | loss scale: 524288.0 | grad norm: 0.527 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.329 | tokens per gpu per second (tgs): 2005.078 | TFLOPs: 16.14 |
g0113: [2024-08-03 01:08:55,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=5580, skipped=2, lr=[9.750009173333334e-05, 9.750009173333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5580 loss: 1.5491 iter time (s): 4.077 samples/sec: 31.392
g0133:  iteration     5580/10000000 | consumed samples:       714240 | consumed tokens:   1462763520 | elapsed time per iteration (ms): 4113.4 | learning rate: 9.750E-05 | global batch size:   128 | lm loss: 1.589595E+00 | loss scale: 524288.0 | grad norm: 0.549 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.118 | tokens per gpu per second (tgs): 1991.532 | TFLOPs: 16.03 |
g0113: [2024-08-03 01:09:35,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=5590, skipped=2, lr=[9.767485440000001e-05, 9.767485440000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5590 loss: 1.5094 iter time (s): 4.012 samples/sec: 31.903
g0133:  iteration     5590/10000000 | consumed samples:       715520 | consumed tokens:   1465384960 | elapsed time per iteration (ms): 4045.0 | learning rate: 9.767E-05 | global batch size:   128 | lm loss: 1.566144E+00 | loss scale: 524288.0 | grad norm: 0.537 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.644 | tokens per gpu per second (tgs): 2025.221 | TFLOPs: 16.30 |
g0113: [2024-08-03 01:10:16,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=5600, skipped=2, lr=[9.784961706666668e-05, 9.784961706666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5600 loss: 1.5975 iter time (s): 4.046 samples/sec: 31.640
g0133:  iteration     5600/10000000 | consumed samples:       716800 | consumed tokens:   1468006400 | elapsed time per iteration (ms): 4078.2 | learning rate: 9.785E-05 | global batch size:   128 | lm loss: 1.580298E+00 | loss scale: 524288.0 | grad norm: 0.541 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.387 | tokens per gpu per second (tgs): 2008.739 | TFLOPs: 16.16 |
g0113: [2024-08-03 01:10:56,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=5610, skipped=2, lr=[9.802437973333334e-05, 9.802437973333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5610 loss: 1.6454 iter time (s): 4.013 samples/sec: 31.899
g0133:  iteration     5610/10000000 | consumed samples:       718080 | consumed tokens:   1470627840 | elapsed time per iteration (ms): 4044.9 | learning rate: 9.802E-05 | global batch size:   128 | lm loss: 1.572399E+00 | loss scale: 524288.0 | grad norm: 0.530 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.645 | tokens per gpu per second (tgs): 2025.256 | TFLOPs: 16.30 |
g0113: [2024-08-03 01:11:37,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=5620, skipped=2, lr=[9.819914240000001e-05, 9.819914240000001e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5620 loss: 1.5507 iter time (s): 4.050 samples/sec: 31.606
g0133:  iteration     5620/10000000 | consumed samples:       719360 | consumed tokens:   1473249280 | elapsed time per iteration (ms): 4082.5 | learning rate: 9.820E-05 | global batch size:   128 | lm loss: 1.581824E+00 | loss scale: 524288.0 | grad norm: 0.520 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.354 | tokens per gpu per second (tgs): 2006.628 | TFLOPs: 16.15 |
g0113: [2024-08-03 01:12:18,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=5630, skipped=2, lr=[9.837390506666667e-05, 9.837390506666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5630 loss: 1.6181 iter time (s): 4.084 samples/sec: 31.343
g0133:  iteration     5630/10000000 | consumed samples:       720640 | consumed tokens:   1475870720 | elapsed time per iteration (ms): 4118.1 | learning rate: 9.837E-05 | global batch size:   128 | lm loss: 1.576904E+00 | loss scale: 524288.0 | grad norm: 0.523 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.082 | tokens per gpu per second (tgs): 1989.268 | TFLOPs: 16.01 |
g0113: [2024-08-03 01:13:01,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=5640, skipped=2, lr=[9.854866773333334e-05, 9.854866773333334e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5640 loss: 1.5500 iter time (s): 4.234 samples/sec: 30.228
g0133:  iteration     5640/10000000 | consumed samples:       721920 | consumed tokens:   1478492160 | elapsed time per iteration (ms): 4267.1 | learning rate: 9.855E-05 | global batch size:   128 | lm loss: 1.555731E+00 | loss scale: 524288.0 | grad norm: 0.532 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.997 | tokens per gpu per second (tgs): 1919.799 | TFLOPs: 15.45 |
g0113: [2024-08-03 01:13:44,553] [INFO] [logging.py:96:log_dist] [Rank 0] step=5650, skipped=2, lr=[9.87234304e-05, 9.87234304e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5650 loss: 1.5545 iter time (s): 4.264 samples/sec: 30.018
g0133:  iteration     5650/10000000 | consumed samples:       723200 | consumed tokens:   1481113600 | elapsed time per iteration (ms): 4296.5 | learning rate: 9.872E-05 | global batch size:   128 | lm loss: 1.581118E+00 | loss scale: 524288.0 | grad norm: 0.524 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.791 | tokens per gpu per second (tgs): 1906.649 | TFLOPs: 15.34 |
g0113: [2024-08-03 01:14:27,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=5660, skipped=2, lr=[9.889819306666667e-05, 9.889819306666667e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5660 loss: 1.5240 iter time (s): 4.247 samples/sec: 30.138
g0133:  iteration     5660/10000000 | consumed samples:       724480 | consumed tokens:   1483735040 | elapsed time per iteration (ms): 4279.9 | learning rate: 9.890E-05 | global batch size:   128 | lm loss: 1.561198E+00 | loss scale: 524288.0 | grad norm: 0.527 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.907 | tokens per gpu per second (tgs): 1914.048 | TFLOPs: 15.40 |
g0113: [2024-08-03 01:15:10,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=5670, skipped=2, lr=[9.907295573333335e-05, 9.907295573333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5670 loss: 1.5225 iter time (s): 4.248 samples/sec: 30.133
g0133:  iteration     5670/10000000 | consumed samples:       725760 | consumed tokens:   1486356480 | elapsed time per iteration (ms): 4280.2 | learning rate: 9.907E-05 | global batch size:   128 | lm loss: 1.569072E+00 | loss scale: 524288.0 | grad norm: 0.516 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.905 | tokens per gpu per second (tgs): 1913.909 | TFLOPs: 15.40 |
g0113: [2024-08-03 01:15:52,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=5680, skipped=2, lr=[9.924771840000002e-05, 9.924771840000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5680 loss: 1.5176 iter time (s): 4.231 samples/sec: 30.251
g0133:  iteration     5680/10000000 | consumed samples:       727040 | consumed tokens:   1488977920 | elapsed time per iteration (ms): 4263.9 | learning rate: 9.925E-05 | global batch size:   128 | lm loss: 1.564504E+00 | loss scale: 524288.0 | grad norm: 0.507 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.020 | tokens per gpu per second (tgs): 1921.255 | TFLOPs: 15.46 |
g0113: [2024-08-03 01:16:34,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=5690, skipped=2, lr=[9.942248106666668e-05, 9.942248106666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5690 loss: 1.5540 iter time (s): 4.130 samples/sec: 30.995
g0133:  iteration     5690/10000000 | consumed samples:       728320 | consumed tokens:   1491599360 | elapsed time per iteration (ms): 4161.9 | learning rate: 9.942E-05 | global batch size:   128 | lm loss: 1.557487E+00 | loss scale: 524288.0 | grad norm: 0.519 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.755 | tokens per gpu per second (tgs): 1968.335 | TFLOPs: 15.84 |
g0113: [2024-08-03 01:17:17,230] [INFO] [logging.py:96:log_dist] [Rank 0] step=5700, skipped=2, lr=[9.959724373333335e-05, 9.959724373333335e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5700 loss: 1.6076 iter time (s): 4.250 samples/sec: 30.121
g0133:  iteration     5700/10000000 | consumed samples:       729600 | consumed tokens:   1494220800 | elapsed time per iteration (ms): 4281.9 | learning rate: 9.960E-05 | global batch size:   128 | lm loss: 1.573332E+00 | loss scale: 524288.0 | grad norm: 0.522 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.893 | tokens per gpu per second (tgs): 1913.157 | TFLOPs: 15.40 |
g0113: [2024-08-03 01:17:58,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=5710, skipped=2, lr=[9.977200640000002e-05, 9.977200640000002e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5710 loss: 1.5207 iter time (s): 4.088 samples/sec: 31.314
g0133:  iteration     5710/10000000 | consumed samples:       730880 | consumed tokens:   1496842240 | elapsed time per iteration (ms): 4120.1 | learning rate: 9.977E-05 | global batch size:   128 | lm loss: 1.558258E+00 | loss scale: 524288.0 | grad norm: 0.568 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.067 | tokens per gpu per second (tgs): 1988.311 | TFLOPs: 16.00 |
g0113: [2024-08-03 01:18:40,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=5720, skipped=2, lr=[9.994676906666668e-05, 9.994676906666668e-05], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5720 loss: 1.5193 iter time (s): 4.134 samples/sec: 30.962
g0133:  iteration     5720/10000000 | consumed samples:       732160 | consumed tokens:   1499463680 | elapsed time per iteration (ms): 4166.7 | learning rate: 9.995E-05 | global batch size:   128 | lm loss: 1.560853E+00 | loss scale: 524288.0 | grad norm: 0.522 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.719 | tokens per gpu per second (tgs): 1966.042 | TFLOPs: 15.82 |
g0113: [2024-08-03 01:19:21,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=5730, skipped=2, lr=[0.00010012153173333335, 0.00010012153173333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5730 loss: 1.5700 iter time (s): 4.125 samples/sec: 31.033
g0133:  iteration     5730/10000000 | consumed samples:       733440 | consumed tokens:   1502085120 | elapsed time per iteration (ms): 4157.2 | learning rate: 1.001E-04 | global batch size:   128 | lm loss: 1.567532E+00 | loss scale: 524288.0 | grad norm: 0.523 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.790 | tokens per gpu per second (tgs): 1970.535 | TFLOPs: 15.86 |
g0113: [2024-08-03 01:20:03,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=5740, skipped=2, lr=[0.00010029629440000001, 0.00010029629440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5740 loss: 1.5733 iter time (s): 4.102 samples/sec: 31.207
g0133:  iteration     5740/10000000 | consumed samples:       734720 | consumed tokens:   1504706560 | elapsed time per iteration (ms): 4133.8 | learning rate: 1.003E-04 | global batch size:   128 | lm loss: 1.547395E+00 | loss scale: 524288.0 | grad norm: 0.533 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.964 | tokens per gpu per second (tgs): 1981.705 | TFLOPs: 15.95 |
g0113: [2024-08-03 01:20:44,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=5750, skipped=2, lr=[0.00010047105706666668, 0.00010047105706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5750 loss: 1.5811 iter time (s): 4.132 samples/sec: 30.976
g0133:  iteration     5750/10000000 | consumed samples:       736000 | consumed tokens:   1507328000 | elapsed time per iteration (ms): 4164.6 | learning rate: 1.005E-04 | global batch size:   128 | lm loss: 1.566090E+00 | loss scale: 524288.0 | grad norm: 0.540 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.735 | tokens per gpu per second (tgs): 1967.049 | TFLOPs: 15.83 |
g0113: [2024-08-03 01:21:27,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=5760, skipped=2, lr=[0.00010064581973333335, 0.00010064581973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5760 loss: 1.5405 iter time (s): 4.203 samples/sec: 30.453
g0133:  iteration     5760/10000000 | consumed samples:       737280 | consumed tokens:   1509949440 | elapsed time per iteration (ms): 4235.4 | learning rate: 1.006E-04 | global batch size:   128 | lm loss: 1.562140E+00 | loss scale: 524288.0 | grad norm: 0.561 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.222 | tokens per gpu per second (tgs): 1934.195 | TFLOPs: 15.56 |
g0113: [2024-08-03 01:22:09,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=5770, skipped=2, lr=[0.00010082058240000001, 0.00010082058240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5770 loss: 1.5521 iter time (s): 4.194 samples/sec: 30.517
g0133:  iteration     5770/10000000 | consumed samples:       738560 | consumed tokens:   1512570880 | elapsed time per iteration (ms): 4227.6 | learning rate: 1.008E-04 | global batch size:   128 | lm loss: 1.544641E+00 | loss scale: 524288.0 | grad norm: 0.523 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.277 | tokens per gpu per second (tgs): 1937.732 | TFLOPs: 15.59 |
g0113: [2024-08-03 01:22:51,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=5780, skipped=2, lr=[0.00010099534506666668, 0.00010099534506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5780 loss: 1.5610 iter time (s): 4.209 samples/sec: 30.410
g0133:  iteration     5780/10000000 | consumed samples:       739840 | consumed tokens:   1515192320 | elapsed time per iteration (ms): 4242.2 | learning rate: 1.010E-04 | global batch size:   128 | lm loss: 1.570836E+00 | loss scale: 524288.0 | grad norm: 0.547 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.173 | tokens per gpu per second (tgs): 1931.072 | TFLOPs: 15.54 |
g0113: [2024-08-03 01:23:33,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=5790, skipped=2, lr=[0.00010117010773333334, 0.00010117010773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5790 loss: 1.5776 iter time (s): 4.125 samples/sec: 31.030
g0133:  iteration     5790/10000000 | consumed samples:       741120 | consumed tokens:   1517813760 | elapsed time per iteration (ms): 4157.6 | learning rate: 1.012E-04 | global batch size:   128 | lm loss: 1.543190E+00 | loss scale: 524288.0 | grad norm: 0.497 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.787 | tokens per gpu per second (tgs): 1970.347 | TFLOPs: 15.86 |
g0113: [2024-08-03 01:24:15,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=5800, skipped=2, lr=[0.00010134487040000001, 0.00010134487040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5800 loss: 1.5348 iter time (s): 4.226 samples/sec: 30.287
g0133:  iteration     5800/10000000 | consumed samples:       742400 | consumed tokens:   1520435200 | elapsed time per iteration (ms): 4258.8 | learning rate: 1.013E-04 | global batch size:   128 | lm loss: 1.557618E+00 | loss scale: 524288.0 | grad norm: 0.505 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.055 | tokens per gpu per second (tgs): 1923.550 | TFLOPs: 15.48 |
g0113: [2024-08-03 01:24:57,858] [INFO] [logging.py:96:log_dist] [Rank 0] step=5810, skipped=2, lr=[0.00010151963306666668, 0.00010151963306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5810 loss: 1.5515 iter time (s): 4.166 samples/sec: 30.727
g0133:  iteration     5810/10000000 | consumed samples:       743680 | consumed tokens:   1523056640 | elapsed time per iteration (ms): 4198.0 | learning rate: 1.015E-04 | global batch size:   128 | lm loss: 1.541487E+00 | loss scale: 524288.0 | grad norm: 0.514 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.491 | tokens per gpu per second (tgs): 1951.402 | TFLOPs: 15.70 |
g0113: [2024-08-03 01:25:40,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=5820, skipped=2, lr=[0.00010169439573333333, 0.00010169439573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5820 loss: 1.5875 iter time (s): 4.243 samples/sec: 30.168
g0133:  iteration     5820/10000000 | consumed samples:       744960 | consumed tokens:   1525678080 | elapsed time per iteration (ms): 4275.4 | learning rate: 1.017E-04 | global batch size:   128 | lm loss: 1.542892E+00 | loss scale: 524288.0 | grad norm: 0.490 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.939 | tokens per gpu per second (tgs): 1916.092 | TFLOPs: 15.42 |
g0113: [2024-08-03 01:26:22,514] [INFO] [logging.py:96:log_dist] [Rank 0] step=5830, skipped=2, lr=[0.0001018691584, 0.0001018691584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5830 loss: 1.5282 iter time (s): 4.158 samples/sec: 30.787
g0133:  iteration     5830/10000000 | consumed samples:       746240 | consumed tokens:   1528299520 | elapsed time per iteration (ms): 4190.2 | learning rate: 1.019E-04 | global batch size:   128 | lm loss: 1.540546E+00 | loss scale: 524288.0 | grad norm: 0.537 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.547 | tokens per gpu per second (tgs): 1955.029 | TFLOPs: 15.73 |
g0113: [2024-08-03 01:27:02,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=5840, skipped=2, lr=[0.00010204392106666666, 0.00010204392106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5840 loss: 1.5644 iter time (s): 4.016 samples/sec: 31.875
g0133:  iteration     5840/10000000 | consumed samples:       747520 | consumed tokens:   1530920960 | elapsed time per iteration (ms): 4048.0 | learning rate: 1.020E-04 | global batch size:   128 | lm loss: 1.550604E+00 | loss scale: 524288.0 | grad norm: 0.503 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.620 | tokens per gpu per second (tgs): 2023.706 | TFLOPs: 16.29 |
g0113: [2024-08-03 01:27:45,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=5850, skipped=2, lr=[0.00010221868373333333, 0.00010221868373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5850 loss: 1.5399 iter time (s): 4.175 samples/sec: 30.655
g0133:  iteration     5850/10000000 | consumed samples:       748800 | consumed tokens:   1533542400 | elapsed time per iteration (ms): 4208.0 | learning rate: 1.022E-04 | global batch size:   128 | lm loss: 1.526286E+00 | loss scale: 524288.0 | grad norm: 0.594 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.418 | tokens per gpu per second (tgs): 1946.768 | TFLOPs: 15.67 |
g0113: [2024-08-03 01:28:27,159] [INFO] [logging.py:96:log_dist] [Rank 0] step=5860, skipped=2, lr=[0.0001023934464, 0.0001023934464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5860 loss: 1.5521 iter time (s): 4.175 samples/sec: 30.656
g0133:  iteration     5860/10000000 | consumed samples:       750080 | consumed tokens:   1536163840 | elapsed time per iteration (ms): 4208.5 | learning rate: 1.024E-04 | global batch size:   128 | lm loss: 1.557428E+00 | loss scale: 524288.0 | grad norm: 0.537 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.414 | tokens per gpu per second (tgs): 1946.527 | TFLOPs: 15.66 |
g0113: [2024-08-03 01:29:07,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=5870, skipped=2, lr=[0.00010256820906666666, 0.00010256820906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5870 loss: 1.5915 iter time (s): 4.022 samples/sec: 31.821
g0133:  iteration     5870/10000000 | consumed samples:       751360 | consumed tokens:   1538785280 | elapsed time per iteration (ms): 4054.9 | learning rate: 1.026E-04 | global batch size:   128 | lm loss: 1.540763E+00 | loss scale: 524288.0 | grad norm: 0.534 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.567 | tokens per gpu per second (tgs): 2020.281 | TFLOPs: 16.26 |
g0113: [2024-08-03 01:29:49,405] [INFO] [logging.py:96:log_dist] [Rank 0] step=5880, skipped=2, lr=[0.00010274297173333333, 0.00010274297173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5880 loss: 1.5515 iter time (s): 4.137 samples/sec: 30.938
g0133:  iteration     5880/10000000 | consumed samples:       752640 | consumed tokens:   1541406720 | elapsed time per iteration (ms): 4169.7 | learning rate: 1.027E-04 | global batch size:   128 | lm loss: 1.542748E+00 | loss scale: 524288.0 | grad norm: 0.491 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.698 | tokens per gpu per second (tgs): 1964.672 | TFLOPs: 15.81 |
g0113: [2024-08-03 01:30:31,213] [INFO] [logging.py:96:log_dist] [Rank 0] step=5890, skipped=2, lr=[0.00010291773439999999, 0.00010291773439999999], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5890 loss: 1.5380 iter time (s): 4.148 samples/sec: 30.861
g0133:  iteration     5890/10000000 | consumed samples:       753920 | consumed tokens:   1544028160 | elapsed time per iteration (ms): 4180.7 | learning rate: 1.029E-04 | global batch size:   128 | lm loss: 1.533358E+00 | loss scale: 524288.0 | grad norm: 0.490 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.617 | tokens per gpu per second (tgs): 1959.467 | TFLOPs: 15.77 |
g0113: [2024-08-03 01:31:11,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=5900, skipped=2, lr=[0.00010309249706666666, 0.00010309249706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5900 loss: 1.5386 iter time (s): 4.039 samples/sec: 31.691
g0133:  iteration     5900/10000000 | consumed samples:       755200 | consumed tokens:   1546649600 | elapsed time per iteration (ms): 4071.4 | learning rate: 1.031E-04 | global batch size:   128 | lm loss: 1.539571E+00 | loss scale: 524288.0 | grad norm: 0.476 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.439 | tokens per gpu per second (tgs): 2012.065 | TFLOPs: 16.19 |
g0113: [2024-08-03 01:31:54,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=5910, skipped=2, lr=[0.00010326725973333332, 0.00010326725973333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5910 loss: 1.5470 iter time (s): 4.187 samples/sec: 30.569
g0133:  iteration     5910/10000000 | consumed samples:       756480 | consumed tokens:   1549271040 | elapsed time per iteration (ms): 4219.6 | learning rate: 1.033E-04 | global batch size:   128 | lm loss: 1.536689E+00 | loss scale: 524288.0 | grad norm: 0.511 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.334 | tokens per gpu per second (tgs): 1941.405 | TFLOPs: 15.62 |
g0113: [2024-08-03 01:32:35,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=5920, skipped=2, lr=[0.0001034420224, 0.0001034420224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5920 loss: 1.5733 iter time (s): 4.134 samples/sec: 30.960
g0133:  iteration     5920/10000000 | consumed samples:       757760 | consumed tokens:   1551892480 | elapsed time per iteration (ms): 4167.9 | learning rate: 1.034E-04 | global batch size:   128 | lm loss: 1.544332E+00 | loss scale: 524288.0 | grad norm: 0.506 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.711 | tokens per gpu per second (tgs): 1965.516 | TFLOPs: 15.82 |
g0113: [2024-08-03 01:33:17,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=5930, skipped=2, lr=[0.00010361678506666667, 0.00010361678506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5930 loss: 1.5048 iter time (s): 4.104 samples/sec: 31.186
g0133:  iteration     5930/10000000 | consumed samples:       759040 | consumed tokens:   1554513920 | elapsed time per iteration (ms): 4137.0 | learning rate: 1.036E-04 | global batch size:   128 | lm loss: 1.520354E+00 | loss scale: 524288.0 | grad norm: 0.487 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.940 | tokens per gpu per second (tgs): 1980.158 | TFLOPs: 15.93 |
g0113: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0113: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0125: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0130: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0131: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0129: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0132: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0129: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0133: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0128: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0130: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0132: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0132: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0125: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0125: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0129: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0133: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0130: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0129: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0131: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0130: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0131: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0131: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0129: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0132: [2024-08-03 01:33:33,073] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0131: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0125: [2024-08-03 01:33:33,073] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0128: [2024-08-03 01:33:33,073] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0130: [2024-08-03 01:33:33,073] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0113: [2024-08-03 01:33:33,073] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0133: [2024-08-03 01:33:33,073] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0113: [2024-08-03 01:33:33,072] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 01:33:33,073] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0113: [2024-08-03 01:33:58,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=5940, skipped=2, lr=[0.00010379154773333334, 0.00010379154773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5940 loss: 1.4850 iter time (s): 4.068 samples/sec: 31.468
g0133:  iteration     5940/10000000 | consumed samples:       760320 | consumed tokens:   1557135360 | elapsed time per iteration (ms): 4100.1 | learning rate: 1.038E-04 | global batch size:   128 | lm loss: 1.507759E+00 | loss scale: 1048576.0 | grad norm: 0.476 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.219 | tokens per gpu per second (tgs): 1997.991 | TFLOPs: 16.08 |
g0113: [2024-08-03 01:34:39,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=5950, skipped=2, lr=[0.0001039663104, 0.0001039663104], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5950 loss: 1.5847 iter time (s): 4.135 samples/sec: 30.956
g0133:  iteration     5950/10000000 | consumed samples:       761600 | consumed tokens:   1559756800 | elapsed time per iteration (ms): 4167.4 | learning rate: 1.040E-04 | global batch size:   128 | lm loss: 1.546212E+00 | loss scale: 1048576.0 | grad norm: 0.482 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.715 | tokens per gpu per second (tgs): 1965.747 | TFLOPs: 15.82 |
g0113: [2024-08-03 01:35:20,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=5960, skipped=2, lr=[0.00010414107306666667, 0.00010414107306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5960 loss: 1.5963 iter time (s): 4.067 samples/sec: 31.473
g0133:  iteration     5960/10000000 | consumed samples:       762880 | consumed tokens:   1562378240 | elapsed time per iteration (ms): 4099.5 | learning rate: 1.041E-04 | global batch size:   128 | lm loss: 1.539538E+00 | loss scale: 1048576.0 | grad norm: 0.470 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.224 | tokens per gpu per second (tgs): 1998.304 | TFLOPs: 16.08 |
g0113: [2024-08-03 01:36:02,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=5970, skipped=2, lr=[0.00010431583573333333, 0.00010431583573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5970 loss: 1.5251 iter time (s): 4.160 samples/sec: 30.773
g0133:  iteration     5970/10000000 | consumed samples:       764160 | consumed tokens:   1564999680 | elapsed time per iteration (ms): 4191.9 | learning rate: 1.043E-04 | global batch size:   128 | lm loss: 1.515063E+00 | loss scale: 1048576.0 | grad norm: 0.503 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.535 | tokens per gpu per second (tgs): 1954.256 | TFLOPs: 15.73 |
g0113: [2024-08-03 01:36:43,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=5980, skipped=2, lr=[0.0001044905984, 0.0001044905984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5980 loss: 1.5426 iter time (s): 4.085 samples/sec: 31.331
g0133:  iteration     5980/10000000 | consumed samples:       765440 | consumed tokens:   1567621120 | elapsed time per iteration (ms): 4117.7 | learning rate: 1.045E-04 | global batch size:   128 | lm loss: 1.534460E+00 | loss scale: 1048576.0 | grad norm: 0.469 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.086 | tokens per gpu per second (tgs): 1989.479 | TFLOPs: 16.01 |
g0113: [2024-08-03 01:37:25,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=5990, skipped=2, lr=[0.00010466536106666667, 0.00010466536106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 5990 loss: 1.5611 iter time (s): 4.138 samples/sec: 30.932
g0133:  iteration     5990/10000000 | consumed samples:       766720 | consumed tokens:   1570242560 | elapsed time per iteration (ms): 4171.1 | learning rate: 1.047E-04 | global batch size:   128 | lm loss: 1.524094E+00 | loss scale: 1048576.0 | grad norm: 0.488 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.687 | tokens per gpu per second (tgs): 1963.992 | TFLOPs: 15.80 |
g0113: [2024-08-03 01:38:05,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=6000, skipped=2, lr=[0.00010484012373333333, 0.00010484012373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6000 loss: 1.5241 iter time (s): 3.951 samples/sec: 32.401
g0133:  iteration     6000/10000000 | consumed samples:       768000 | consumed tokens:   1572864000 | elapsed time per iteration (ms): 3983.3 | learning rate: 1.048E-04 | global batch size:   128 | lm loss: 1.526916E+00 | loss scale: 1048576.0 | grad norm: 0.489 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.134 | tokens per gpu per second (tgs): 2056.575 | TFLOPs: 16.55 |
g0133: ------------------------------------------------------------------------------------------------
g0133:  validation loss at iteration 6000 | lm loss value: 1.521865E+00 | lm loss PPL: 4.580758E+00 | 
g0133: ------------------------------------------------------------------------------------------------
g0113: saving checkpoint at iteration    6000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: [2024-08-03 01:44:25,469] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step6000 is about to be saved!
g0133: [2024-08-03 01:44:25,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0133: [2024-08-03 01:44:25,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0133: [2024-08-03 01:44:25,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0113: [2024-08-03 01:44:25,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0125: [2024-08-03 01:44:25,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0125: [2024-08-03 01:44:25,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0125: [2024-08-03 01:44:25,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0113: [2024-08-03 01:44:25,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0113: [2024-08-03 01:44:25,481] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0131: [2024-08-03 01:44:25,483] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0131: [2024-08-03 01:44:25,483] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0131: [2024-08-03 01:44:25,483] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0129: [2024-08-03 01:44:25,485] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0129: [2024-08-03 01:44:25,485] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0129: [2024-08-03 01:44:25,485] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0130: [2024-08-03 01:44:25,486] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0130: [2024-08-03 01:44:25,487] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0130: [2024-08-03 01:44:25,487] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0128: [2024-08-03 01:44:25,487] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0128: [2024-08-03 01:44:25,487] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0128: [2024-08-03 01:44:25,487] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0132: [2024-08-03 01:44:25,489] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0132: [2024-08-03 01:44:25,489] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0132: [2024-08-03 01:44:25,490] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0133: [2024-08-03 01:44:25,512] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_23-model_00-model_states.pt...
g0125: [2024-08-03 01:44:25,515] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_05-model_00-model_states.pt...
g0129: [2024-08-03 01:44:25,518] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_11-model_00-model_states.pt...
g0130: [2024-08-03 01:44:25,518] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_14-model_00-model_states.pt...
g0132: [2024-08-03 01:44:25,518] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_20-model_00-model_states.pt...
g0131: [2024-08-03 01:44:25,521] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_17-model_00-model_states.pt...
g0128: [2024-08-03 01:44:25,523] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_08-model_00-model_states.pt...
g0113: [2024-08-03 01:44:25,526] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_01-model_00-model_states.pt...
g0129: [2024-08-03 01:44:25,636] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_11-model_00-model_states.pt.
g0129: [2024-08-03 01:44:25,672] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_12-model_00-model_states.pt...
g0132: [2024-08-03 01:44:25,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_20-model_00-model_states.pt.
g0131: [2024-08-03 01:44:25,708] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_17-model_00-model_states.pt.
g0125: [2024-08-03 01:44:25,718] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_05-model_00-model_states.pt.
g0132: [2024-08-03 01:44:25,741] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_21-model_00-model_states.pt...
g0133: [2024-08-03 01:44:25,746] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_23-model_00-model_states.pt.
g0133: [2024-08-03 01:44:25,747] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_24-model_00-model_states.pt...
g0131: [2024-08-03 01:44:25,748] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_18-model_00-model_states.pt...
g0133: [2024-08-03 01:44:25,749] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_24-model_00-model_states.pt.
g0113: [2024-08-03 01:44:25,750] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_01-model_00-model_states.pt.
g0125: [2024-08-03 01:44:25,755] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_06-model_00-model_states.pt...
g0130: [2024-08-03 01:44:25,759] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_14-model_00-model_states.pt.
g0128: [2024-08-03 01:44:25,770] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_08-model_00-model_states.pt.
g0113: [2024-08-03 01:44:25,774] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_02-model_00-model_states.pt...
g0130: [2024-08-03 01:44:25,793] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_15-model_00-model_states.pt...
g0133: [2024-08-03 01:44:25,802] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_25-model_00-model_states.pt...
g0128: [2024-08-03 01:44:25,810] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_09-model_00-model_states.pt...
g0132: [2024-08-03 01:44:25,854] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_21-model_00-model_states.pt.
g0131: [2024-08-03 01:44:25,854] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_18-model_00-model_states.pt.
g0132: [2024-08-03 01:44:25,884] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_22-model_00-model_states.pt...
g0131: [2024-08-03 01:44:25,885] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_19-model_00-model_states.pt...
g0130: [2024-08-03 01:44:25,887] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_15-model_00-model_states.pt.
g0130: [2024-08-03 01:44:25,916] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_16-model_00-model_states.pt...
g0125: [2024-08-03 01:44:25,952] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_06-model_00-model_states.pt.
g0128: [2024-08-03 01:44:25,962] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_09-model_00-model_states.pt.
g0125: [2024-08-03 01:44:25,985] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_07-model_00-model_states.pt...
g0131: [2024-08-03 01:44:25,988] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_19-model_00-model_states.pt.
g0131: [2024-08-03 01:44:25,990] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_05_model_states.pt...
g0128: [2024-08-03 01:44:25,997] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_10-model_00-model_states.pt...
g0129: [2024-08-03 01:44:26,010] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_12-model_00-model_states.pt.
g0129: [2024-08-03 01:44:26,036] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_13-model_00-model_states.pt...
g0130: [2024-08-03 01:44:26,039] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_16-model_00-model_states.pt.
g0130: [2024-08-03 01:44:26,041] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_04_model_states.pt...
g0132: [2024-08-03 01:44:26,063] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_22-model_00-model_states.pt.
g0132: [2024-08-03 01:44:26,065] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_06_model_states.pt...
g0125: [2024-08-03 01:44:26,099] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_07-model_00-model_states.pt.
g0125: [2024-08-03 01:44:26,101] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_01_model_states.pt...
g0128: [2024-08-03 01:44:26,262] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_10-model_00-model_states.pt.
g0128: [2024-08-03 01:44:26,264] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_02_model_states.pt...
g0133: [2024-08-03 01:44:26,387] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_25-model_00-model_states.pt.
g0133: [2024-08-03 01:44:26,388] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_07_model_states.pt...
g0129: [2024-08-03 01:44:27,203] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_13-model_00-model_states.pt.
g0129: [2024-08-03 01:44:27,205] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_03_model_states.pt...
g0113: [2024-08-03 01:44:28,159] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_02-model_00-model_states.pt.
g0113: [2024-08-03 01:44:28,184] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_03-model_00-model_states.pt...
g0130: [2024-08-03 01:44:28,311] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_04_model_states.pt.
g0130: [2024-08-03 01:44:28,312] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0133: [2024-08-03 01:44:28,333] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_07_model_states.pt.
g0133: [2024-08-03 01:44:28,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0131: [2024-08-03 01:44:28,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_05_model_states.pt.
g0131: [2024-08-03 01:44:28,398] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0125: [2024-08-03 01:44:28,411] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_01_model_states.pt.
g0125: [2024-08-03 01:44:28,412] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0113: [2024-08-03 01:44:28,475] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_03-model_00-model_states.pt.
g0113: [2024-08-03 01:44:28,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_04-model_00-model_states.pt...
g0113: [2024-08-03 01:44:28,590] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/layer_04-model_00-model_states.pt.
g0113: [2024-08-03 01:44:28,591] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt
g0113: [2024-08-03 01:44:28,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt...
g0132: [2024-08-03 01:44:28,743] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_06_model_states.pt.
g0132: [2024-08-03 01:44:28,743] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0128: [2024-08-03 01:44:29,071] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_02_model_states.pt.
g0128: [2024-08-03 01:44:29,071] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0129: [2024-08-03 01:44:29,512] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_03_model_states.pt.
g0129: [2024-08-03 01:44:29,513] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0113: [2024-08-03 01:44:32,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step6000/mp_rank_00_model_states.pt.
g0113: [2024-08-03 01:44:32,004] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6000 is ready now!
g0113:   successfully saved checkpoint at iteration    6000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: Checkpoint Save GB: 22.521, GB/Sec: 3.44, Latency(second): 6.551
g0133: (min, max) time across ranks (ms):
g0133:     save-checkpoint ................................: (6549.65, 6551.16)
g0113: [2024-08-03 01:45:11,495] [INFO] [logging.py:96:log_dist] [Rank 0] step=6010, skipped=2, lr=[0.0001050148864, 0.0001050148864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6010 loss: 1.5178 iter time (s): 3.916 samples/sec: 32.690
g0133:  iteration     6010/10000000 | consumed samples:       769280 | consumed tokens:   1575485440 | elapsed time per iteration (ms): 42601.2 | learning rate: 1.050E-04 | global batch size:   128 | lm loss: 1.515413E+00 | loss scale: 1048576.0 | grad norm: 0.459 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.005 | tokens per gpu per second (tgs): 192.295 | TFLOPs: 1.55 |
g0113: [2024-08-03 01:45:51,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=6020, skipped=2, lr=[0.00010518964906666666, 0.00010518964906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6020 loss: 1.5595 iter time (s): 4.012 samples/sec: 31.903
g0133:  iteration     6020/10000000 | consumed samples:       770560 | consumed tokens:   1578106880 | elapsed time per iteration (ms): 4045.7 | learning rate: 1.052E-04 | global batch size:   128 | lm loss: 1.520525E+00 | loss scale: 1048576.0 | grad norm: 0.475 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.638 | tokens per gpu per second (tgs): 2024.844 | TFLOPs: 16.29 |
g0113: [2024-08-03 01:46:34,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=6030, skipped=2, lr=[0.00010536441173333333, 0.00010536441173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6030 loss: 1.5596 iter time (s): 4.198 samples/sec: 30.493
g0133:  iteration     6030/10000000 | consumed samples:       771840 | consumed tokens:   1580728320 | elapsed time per iteration (ms): 4230.3 | learning rate: 1.054E-04 | global batch size:   128 | lm loss: 1.529569E+00 | loss scale: 1048576.0 | grad norm: 0.466 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.258 | tokens per gpu per second (tgs): 1936.505 | TFLOPs: 15.58 |
g0113: [2024-08-03 01:47:15,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=6040, skipped=2, lr=[0.0001055391744, 0.0001055391744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6040 loss: 1.5131 iter time (s): 4.135 samples/sec: 30.955
g0133:  iteration     6040/10000000 | consumed samples:       773120 | consumed tokens:   1583349760 | elapsed time per iteration (ms): 4167.6 | learning rate: 1.055E-04 | global batch size:   128 | lm loss: 1.499953E+00 | loss scale: 1048576.0 | grad norm: 0.458 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.713 | tokens per gpu per second (tgs): 1965.630 | TFLOPs: 15.82 |
g0113: [2024-08-03 01:47:57,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=6050, skipped=2, lr=[0.00010571393706666666, 0.00010571393706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6050 loss: 1.5062 iter time (s): 4.093 samples/sec: 31.273
g0133:  iteration     6050/10000000 | consumed samples:       774400 | consumed tokens:   1585971200 | elapsed time per iteration (ms): 4127.1 | learning rate: 1.057E-04 | global batch size:   128 | lm loss: 1.524418E+00 | loss scale: 1048576.0 | grad norm: 0.493 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.015 | tokens per gpu per second (tgs): 1984.942 | TFLOPs: 15.97 |
g0113: [2024-08-03 01:48:38,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=6060, skipped=2, lr=[0.00010588869973333333, 0.00010588869973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6060 loss: 1.5553 iter time (s): 4.052 samples/sec: 31.590
g0133:  iteration     6060/10000000 | consumed samples:       775680 | consumed tokens:   1588592640 | elapsed time per iteration (ms): 4084.3 | learning rate: 1.059E-04 | global batch size:   128 | lm loss: 1.537054E+00 | loss scale: 1048576.0 | grad norm: 0.457 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.339 | tokens per gpu per second (tgs): 2005.716 | TFLOPs: 16.14 |
g0113: [2024-08-03 01:49:18,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=6070, skipped=2, lr=[0.0001060634624, 0.0001060634624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6070 loss: 1.4880 iter time (s): 4.036 samples/sec: 31.713
g0133:  iteration     6070/10000000 | consumed samples:       776960 | consumed tokens:   1591214080 | elapsed time per iteration (ms): 4069.5 | learning rate: 1.061E-04 | global batch size:   128 | lm loss: 1.518842E+00 | loss scale: 1048576.0 | grad norm: 0.456 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.454 | tokens per gpu per second (tgs): 2013.034 | TFLOPs: 16.20 |
g0113: [2024-08-03 01:50:00,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=6080, skipped=2, lr=[0.00010623822506666666, 0.00010623822506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6080 loss: 1.5043 iter time (s): 4.160 samples/sec: 30.772
g0133:  iteration     6080/10000000 | consumed samples:       778240 | consumed tokens:   1593835520 | elapsed time per iteration (ms): 4192.5 | learning rate: 1.062E-04 | global batch size:   128 | lm loss: 1.513483E+00 | loss scale: 1048576.0 | grad norm: 0.505 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.531 | tokens per gpu per second (tgs): 1953.963 | TFLOPs: 15.72 |
g0113: [2024-08-03 01:50:42,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=6090, skipped=2, lr=[0.00010641298773333333, 0.00010641298773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6090 loss: 1.5308 iter time (s): 4.193 samples/sec: 30.528
g0133:  iteration     6090/10000000 | consumed samples:       779520 | consumed tokens:   1596456960 | elapsed time per iteration (ms): 4225.4 | learning rate: 1.064E-04 | global batch size:   128 | lm loss: 1.513006E+00 | loss scale: 1048576.0 | grad norm: 0.484 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.293 | tokens per gpu per second (tgs): 1938.732 | TFLOPs: 15.60 |
g0113: [2024-08-03 01:51:25,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=6100, skipped=2, lr=[0.0001065877504, 0.0001065877504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6100 loss: 1.5131 iter time (s): 4.214 samples/sec: 30.375
g0133:  iteration     6100/10000000 | consumed samples:       780800 | consumed tokens:   1599078400 | elapsed time per iteration (ms): 4246.4 | learning rate: 1.066E-04 | global batch size:   128 | lm loss: 1.517618E+00 | loss scale: 1048576.0 | grad norm: 0.477 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.143 | tokens per gpu per second (tgs): 1929.164 | TFLOPs: 15.52 |
g0113: [2024-08-03 01:52:12,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=6110, skipped=2, lr=[0.00010676251306666667, 0.00010676251306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6110 loss: 1.5678 iter time (s): 4.659 samples/sec: 27.474
g0133:  iteration     6110/10000000 | consumed samples:       782080 | consumed tokens:   1601699840 | elapsed time per iteration (ms): 4691.4 | learning rate: 1.068E-04 | global batch size:   128 | lm loss: 1.536887E+00 | loss scale: 1048576.0 | grad norm: 0.476 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.284 | tokens per gpu per second (tgs): 1746.159 | TFLOPs: 14.05 |
g0113: [2024-08-03 01:52:53,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=6120, skipped=2, lr=[0.00010693727573333334, 0.00010693727573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6120 loss: 1.4798 iter time (s): 4.115 samples/sec: 31.107
g0133:  iteration     6120/10000000 | consumed samples:       783360 | consumed tokens:   1604321280 | elapsed time per iteration (ms): 4147.2 | learning rate: 1.069E-04 | global batch size:   128 | lm loss: 1.500783E+00 | loss scale: 1048576.0 | grad norm: 0.477 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.864 | tokens per gpu per second (tgs): 1975.307 | TFLOPs: 15.90 |
g0113: [2024-08-03 01:53:35,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=6130, skipped=2, lr=[0.0001071120384, 0.0001071120384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6130 loss: 1.4967 iter time (s): 4.160 samples/sec: 30.771
g0133:  iteration     6130/10000000 | consumed samples:       784640 | consumed tokens:   1606942720 | elapsed time per iteration (ms): 4191.9 | learning rate: 1.071E-04 | global batch size:   128 | lm loss: 1.524035E+00 | loss scale: 1048576.0 | grad norm: 0.456 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.535 | tokens per gpu per second (tgs): 1954.253 | TFLOPs: 15.73 |
g0113: [2024-08-03 01:54:18,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=6140, skipped=2, lr=[0.00010728680106666667, 0.00010728680106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6140 loss: 1.5092 iter time (s): 4.217 samples/sec: 30.355
g0133:  iteration     6140/10000000 | consumed samples:       785920 | consumed tokens:   1609564160 | elapsed time per iteration (ms): 4249.3 | learning rate: 1.073E-04 | global batch size:   128 | lm loss: 1.510664E+00 | loss scale: 1048576.0 | grad norm: 0.450 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.123 | tokens per gpu per second (tgs): 1927.856 | TFLOPs: 15.51 |
g0113: [2024-08-03 01:54:59,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=6150, skipped=2, lr=[0.00010746156373333334, 0.00010746156373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6150 loss: 1.4450 iter time (s): 4.091 samples/sec: 31.288
g0133:  iteration     6150/10000000 | consumed samples:       787200 | consumed tokens:   1612185600 | elapsed time per iteration (ms): 4123.3 | learning rate: 1.075E-04 | global batch size:   128 | lm loss: 1.487800E+00 | loss scale: 1048576.0 | grad norm: 0.453 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.043 | tokens per gpu per second (tgs): 1986.745 | TFLOPs: 15.99 |
g0113: [2024-08-03 01:55:40,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=6160, skipped=2, lr=[0.0001076363264, 0.0001076363264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6160 loss: 1.5006 iter time (s): 4.065 samples/sec: 31.487
g0133:  iteration     6160/10000000 | consumed samples:       788480 | consumed tokens:   1614807040 | elapsed time per iteration (ms): 4098.5 | learning rate: 1.076E-04 | global batch size:   128 | lm loss: 1.500134E+00 | loss scale: 1048576.0 | grad norm: 0.444 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.231 | tokens per gpu per second (tgs): 1998.802 | TFLOPs: 16.08 |
g0113: [2024-08-03 01:56:22,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=6170, skipped=2, lr=[0.00010781108906666667, 0.00010781108906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6170 loss: 1.4892 iter time (s): 4.128 samples/sec: 31.009
g0133:  iteration     6170/10000000 | consumed samples:       789760 | consumed tokens:   1617428480 | elapsed time per iteration (ms): 4161.5 | learning rate: 1.078E-04 | global batch size:   128 | lm loss: 1.491278E+00 | loss scale: 1048576.0 | grad norm: 0.496 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.758 | tokens per gpu per second (tgs): 1968.529 | TFLOPs: 15.84 |
g0113: [2024-08-03 01:57:01,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=6180, skipped=2, lr=[0.00010798585173333333, 0.00010798585173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6180 loss: 1.4736 iter time (s): 3.917 samples/sec: 32.680
g0133:  iteration     6180/10000000 | consumed samples:       791040 | consumed tokens:   1620049920 | elapsed time per iteration (ms): 3949.5 | learning rate: 1.080E-04 | global batch size:   128 | lm loss: 1.505602E+00 | loss scale: 1048576.0 | grad norm: 0.472 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.409 | tokens per gpu per second (tgs): 2074.179 | TFLOPs: 16.69 |
g0113: [2024-08-03 01:57:41,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=6190, skipped=2, lr=[0.0001081606144, 0.0001081606144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6190 loss: 1.4471 iter time (s): 3.923 samples/sec: 32.629
g0133:  iteration     6190/10000000 | consumed samples:       792320 | consumed tokens:   1622671360 | elapsed time per iteration (ms): 3958.4 | learning rate: 1.082E-04 | global batch size:   128 | lm loss: 1.489049E+00 | loss scale: 1048576.0 | grad norm: 0.448 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.337 | tokens per gpu per second (tgs): 2069.548 | TFLOPs: 16.65 |
g0113: [2024-08-03 01:58:23,274] [INFO] [logging.py:96:log_dist] [Rank 0] step=6200, skipped=2, lr=[0.00010833537706666667, 0.00010833537706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6200 loss: 1.4920 iter time (s): 4.184 samples/sec: 30.596
g0133:  iteration     6200/10000000 | consumed samples:       793600 | consumed tokens:   1625292800 | elapsed time per iteration (ms): 4217.8 | learning rate: 1.083E-04 | global batch size:   128 | lm loss: 1.506976E+00 | loss scale: 1048576.0 | grad norm: 0.457 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.347 | tokens per gpu per second (tgs): 1942.229 | TFLOPs: 15.63 |
g0113: [2024-08-03 01:59:05,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=6210, skipped=2, lr=[0.00010851013973333333, 0.00010851013973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6210 loss: 1.5236 iter time (s): 4.169 samples/sec: 30.705
g0133:  iteration     6210/10000000 | consumed samples:       794880 | consumed tokens:   1627914240 | elapsed time per iteration (ms): 4202.8 | learning rate: 1.085E-04 | global batch size:   128 | lm loss: 1.496418E+00 | loss scale: 1048576.0 | grad norm: 0.489 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.456 | tokens per gpu per second (tgs): 1949.166 | TFLOPs: 15.69 |
g0113: [2024-08-03 01:59:47,654] [INFO] [logging.py:96:log_dist] [Rank 0] step=6220, skipped=2, lr=[0.0001086849024, 0.0001086849024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6220 loss: 1.4757 iter time (s): 4.202 samples/sec: 30.465
g0133:  iteration     6220/10000000 | consumed samples:       796160 | consumed tokens:   1630535680 | elapsed time per iteration (ms): 4235.1 | learning rate: 1.087E-04 | global batch size:   128 | lm loss: 1.490081E+00 | loss scale: 1048576.0 | grad norm: 0.462 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.224 | tokens per gpu per second (tgs): 1934.307 | TFLOPs: 15.57 |
g0113: [2024-08-03 02:00:28,751] [INFO] [logging.py:96:log_dist] [Rank 0] step=6230, skipped=2, lr=[0.00010885966506666666, 0.00010885966506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6230 loss: 1.5162 iter time (s): 4.077 samples/sec: 31.397
g0133:  iteration     6230/10000000 | consumed samples:       797440 | consumed tokens:   1633157120 | elapsed time per iteration (ms): 4109.7 | learning rate: 1.089E-04 | global batch size:   128 | lm loss: 1.504720E+00 | loss scale: 1048576.0 | grad norm: 0.486 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.146 | tokens per gpu per second (tgs): 1993.322 | TFLOPs: 16.04 |
g0113: [2024-08-03 02:01:10,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=6240, skipped=2, lr=[0.00010903442773333333, 0.00010903442773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6240 loss: 1.4636 iter time (s): 4.179 samples/sec: 30.628
g0133:  iteration     6240/10000000 | consumed samples:       798720 | consumed tokens:   1635778560 | elapsed time per iteration (ms): 4211.6 | learning rate: 1.090E-04 | global batch size:   128 | lm loss: 1.489176E+00 | loss scale: 1048576.0 | grad norm: 0.449 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.392 | tokens per gpu per second (tgs): 1945.090 | TFLOPs: 15.65 |
g0113: [2024-08-03 02:01:51,953] [INFO] [logging.py:96:log_dist] [Rank 0] step=6250, skipped=2, lr=[0.0001092091904, 0.0001092091904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6250 loss: 1.4906 iter time (s): 4.076 samples/sec: 31.406
g0133:  iteration     6250/10000000 | consumed samples:       800000 | consumed tokens:   1638400000 | elapsed time per iteration (ms): 4108.5 | learning rate: 1.092E-04 | global batch size:   128 | lm loss: 1.492656E+00 | loss scale: 1048576.0 | grad norm: 0.464 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.155 | tokens per gpu per second (tgs): 1993.938 | TFLOPs: 16.05 |
g0113: [2024-08-03 02:02:32,243] [INFO] [logging.py:96:log_dist] [Rank 0] step=6260, skipped=2, lr=[0.00010938395306666666, 0.00010938395306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6260 loss: 1.4392 iter time (s): 3.997 samples/sec: 32.027
g0133:  iteration     6260/10000000 | consumed samples:       801280 | consumed tokens:   1641021440 | elapsed time per iteration (ms): 4028.9 | learning rate: 1.094E-04 | global batch size:   128 | lm loss: 1.479039E+00 | loss scale: 1048576.0 | grad norm: 0.437 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.770 | tokens per gpu per second (tgs): 2033.311 | TFLOPs: 16.36 |
g0113: [2024-08-03 02:03:11,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=6270, skipped=2, lr=[0.00010955871573333333, 0.00010955871573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6270 loss: 1.5146 iter time (s): 3.937 samples/sec: 32.512
g0133:  iteration     6270/10000000 | consumed samples:       802560 | consumed tokens:   1643642880 | elapsed time per iteration (ms): 3969.5 | learning rate: 1.096E-04 | global batch size:   128 | lm loss: 1.499548E+00 | loss scale: 1048576.0 | grad norm: 0.435 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.246 | tokens per gpu per second (tgs): 2063.744 | TFLOPs: 16.61 |
g0113: [2024-08-03 02:03:51,986] [INFO] [logging.py:96:log_dist] [Rank 0] step=6280, skipped=2, lr=[0.00010973347840000001, 0.00010973347840000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6280 loss: 1.4148 iter time (s): 3.972 samples/sec: 32.224
g0133:  iteration     6280/10000000 | consumed samples:       803840 | consumed tokens:   1646264320 | elapsed time per iteration (ms): 4004.9 | learning rate: 1.097E-04 | global batch size:   128 | lm loss: 1.480562E+00 | loss scale: 1048576.0 | grad norm: 0.456 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.961 | tokens per gpu per second (tgs): 2045.516 | TFLOPs: 16.46 |
g0113: [2024-08-03 02:04:30,672] [INFO] [logging.py:96:log_dist] [Rank 0] step=6290, skipped=2, lr=[0.00010990824106666667, 0.00010990824106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6290 loss: 1.4575 iter time (s): 3.835 samples/sec: 33.376
g0133:  iteration     6290/10000000 | consumed samples:       805120 | consumed tokens:   1648885760 | elapsed time per iteration (ms): 3868.5 | learning rate: 1.099E-04 | global batch size:   128 | lm loss: 1.486150E+00 | loss scale: 1048576.0 | grad norm: 0.424 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.088 | tokens per gpu per second (tgs): 2117.610 | TFLOPs: 17.04 |
g0113: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 6295
g0113: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 6295
g0113: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 6295
g0113: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0113: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0113: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 6295
g0125: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0125: Grad overflow on iteration 6295
g0133: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: Grad overflow on iteration 6295
g0125: Grad overflow on iteration 6295
g0129: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: Grad overflow on iteration 6295
g0133: Grad overflow on iteration 6295
g0128: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 6295
g0130: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 6295
g0130: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0129: Grad overflow on iteration 6295
g0130: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: Grad overflow on iteration 6295
g0130: Grad overflow on iteration 6295
g0129: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0130: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0130: Grad overflow on iteration 6295
g0129: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0125: Grad overflow on iteration 6295
g0128: Grad overflow on iteration 6295
g0125: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 6295
g0129: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0125: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0132: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0128: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: Grad overflow on iteration 6295
g0132: Grad overflow on iteration 6295
g0128: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: Grad overflow on iteration 6295
g0133: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0133: Grad overflow on iteration 6295
g0128: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: Grad overflow on iteration 6295
g0132: [2024-08-03 02:04:55,475] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0128: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0133: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0128: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0130: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0128: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0133: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0131: Grad overflow on iteration 6295
g0130: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0132: Grad overflow on iteration 6295
g0125: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0131: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0133: Grad overflow on iteration 6295
g0131: Grad overflow on iteration 6295
g0131: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0132: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: Grad overflow on iteration 6295
g0133: Grad overflow on iteration 6295
g0132: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0133: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0132: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0131: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 6295
g0131: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 6295
g0131: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0132: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0131: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0132: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: Grad overflow on iteration 6295
g0131: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0133: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0113: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0113: [2024-08-03 02:04:55,476] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
g0132: [2024-08-03 02:04:55,476] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0113: [2024-08-03 02:05:11,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=6300, skipped=3, lr=[0.00011008300373333334, 0.00011008300373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6300 loss: 1.4958 iter time (s): 4.033 samples/sec: 31.738
g0133:  iteration     6300/10000000 | consumed samples:       806400 | consumed tokens:   1651507200 | elapsed time per iteration (ms): 4065.6 | learning rate: 1.101E-04 | global batch size:   128 | lm loss: 1.484295E+00 | loss scale: 524288.0 | grad norm: 0.461 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.484 | tokens per gpu per second (tgs): 2014.967 | TFLOPs: 16.21 |
g0113: [2024-08-03 02:05:52,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=6310, skipped=3, lr=[0.00011025776640000001, 0.00011025776640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6310 loss: 1.4980 iter time (s): 4.064 samples/sec: 31.497
g0133:  iteration     6310/10000000 | consumed samples:       807680 | consumed tokens:   1654128640 | elapsed time per iteration (ms): 4096.2 | learning rate: 1.103E-04 | global batch size:   128 | lm loss: 1.474763E+00 | loss scale: 524288.0 | grad norm: 0.445 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.248 | tokens per gpu per second (tgs): 1999.888 | TFLOPs: 16.09 |
g0113: [2024-08-03 02:06:33,375] [INFO] [logging.py:96:log_dist] [Rank 0] step=6320, skipped=3, lr=[0.00011043252906666667, 0.00011043252906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6320 loss: 1.4777 iter time (s): 4.076 samples/sec: 31.404
g0133:  iteration     6320/10000000 | consumed samples:       808960 | consumed tokens:   1656750080 | elapsed time per iteration (ms): 4108.6 | learning rate: 1.104E-04 | global batch size:   128 | lm loss: 1.476526E+00 | loss scale: 524288.0 | grad norm: 0.432 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.154 | tokens per gpu per second (tgs): 1993.885 | TFLOPs: 16.05 |
g0113: [2024-08-03 02:07:15,200] [INFO] [logging.py:96:log_dist] [Rank 0] step=6330, skipped=3, lr=[0.00011060729173333334, 0.00011060729173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6330 loss: 1.4397 iter time (s): 4.150 samples/sec: 30.842
g0133:  iteration     6330/10000000 | consumed samples:       810240 | consumed tokens:   1659371520 | elapsed time per iteration (ms): 4182.4 | learning rate: 1.106E-04 | global batch size:   128 | lm loss: 1.475559E+00 | loss scale: 524288.0 | grad norm: 0.541 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.605 | tokens per gpu per second (tgs): 1958.695 | TFLOPs: 15.76 |
g0113: [2024-08-03 02:07:56,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=6340, skipped=3, lr=[0.0001107820544, 0.0001107820544], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6340 loss: 1.5015 iter time (s): 4.126 samples/sec: 31.019
g0133:  iteration     6340/10000000 | consumed samples:       811520 | consumed tokens:   1661992960 | elapsed time per iteration (ms): 4159.5 | learning rate: 1.108E-04 | global batch size:   128 | lm loss: 1.494103E+00 | loss scale: 524288.0 | grad norm: 0.482 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.773 | tokens per gpu per second (tgs): 1969.455 | TFLOPs: 15.85 |
g0113: [2024-08-03 02:08:38,494] [INFO] [logging.py:96:log_dist] [Rank 0] step=6350, skipped=3, lr=[0.00011095681706666667, 0.00011095681706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6350 loss: 1.4674 iter time (s): 4.138 samples/sec: 30.936
g0133:  iteration     6350/10000000 | consumed samples:       812800 | consumed tokens:   1664614400 | elapsed time per iteration (ms): 4170.2 | learning rate: 1.110E-04 | global batch size:   128 | lm loss: 1.457024E+00 | loss scale: 524288.0 | grad norm: 0.448 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.694 | tokens per gpu per second (tgs): 1964.413 | TFLOPs: 15.81 |
g0113: [2024-08-03 02:09:20,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=6360, skipped=3, lr=[0.00011113157973333334, 0.00011113157973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6360 loss: 1.4695 iter time (s): 4.195 samples/sec: 30.511
g0133:  iteration     6360/10000000 | consumed samples:       814080 | consumed tokens:   1667235840 | elapsed time per iteration (ms): 4227.5 | learning rate: 1.111E-04 | global batch size:   128 | lm loss: 1.476166E+00 | loss scale: 524288.0 | grad norm: 0.450 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.278 | tokens per gpu per second (tgs): 1937.793 | TFLOPs: 15.59 |
g0113: [2024-08-03 02:10:02,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=6370, skipped=3, lr=[0.0001113063424, 0.0001113063424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6370 loss: 1.5112 iter time (s): 4.170 samples/sec: 30.699
g0133:  iteration     6370/10000000 | consumed samples:       815360 | consumed tokens:   1669857280 | elapsed time per iteration (ms): 4202.1 | learning rate: 1.113E-04 | global batch size:   128 | lm loss: 1.474887E+00 | loss scale: 524288.0 | grad norm: 0.425 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.461 | tokens per gpu per second (tgs): 1949.518 | TFLOPs: 15.69 |
g0113: [2024-08-03 02:10:44,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=6380, skipped=3, lr=[0.00011148110506666667, 0.00011148110506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6380 loss: 1.3920 iter time (s): 4.174 samples/sec: 30.669
g0133:  iteration     6380/10000000 | consumed samples:       816640 | consumed tokens:   1672478720 | elapsed time per iteration (ms): 4206.0 | learning rate: 1.115E-04 | global batch size:   128 | lm loss: 1.476428E+00 | loss scale: 524288.0 | grad norm: 0.422 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.433 | tokens per gpu per second (tgs): 1947.709 | TFLOPs: 15.67 |
g0113: [2024-08-03 02:11:26,516] [INFO] [logging.py:96:log_dist] [Rank 0] step=6390, skipped=3, lr=[0.00011165586773333334, 0.00011165586773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6390 loss: 1.4992 iter time (s): 4.134 samples/sec: 30.961
g0133:  iteration     6390/10000000 | consumed samples:       817920 | consumed tokens:   1675100160 | elapsed time per iteration (ms): 4166.3 | learning rate: 1.117E-04 | global batch size:   128 | lm loss: 1.470858E+00 | loss scale: 524288.0 | grad norm: 0.482 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.723 | tokens per gpu per second (tgs): 1966.259 | TFLOPs: 15.82 |
g0113: [2024-08-03 02:12:06,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=6400, skipped=3, lr=[0.0001118306304, 0.0001118306304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6400 loss: 1.4647 iter time (s): 3.969 samples/sec: 32.254
g0133:  iteration     6400/10000000 | consumed samples:       819200 | consumed tokens:   1677721600 | elapsed time per iteration (ms): 4000.6 | learning rate: 1.118E-04 | global batch size:   128 | lm loss: 1.475715E+00 | loss scale: 524288.0 | grad norm: 0.477 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.996 | tokens per gpu per second (tgs): 2047.717 | TFLOPs: 16.48 |
g0113: [2024-08-03 02:12:45,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=6410, skipped=3, lr=[0.00011200539306666667, 0.00011200539306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6410 loss: 1.4292 iter time (s): 3.869 samples/sec: 33.083
g0133:  iteration     6410/10000000 | consumed samples:       820480 | consumed tokens:   1680343040 | elapsed time per iteration (ms): 3901.9 | learning rate: 1.120E-04 | global batch size:   128 | lm loss: 1.481402E+00 | loss scale: 524288.0 | grad norm: 0.431 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.805 | tokens per gpu per second (tgs): 2099.510 | TFLOPs: 16.90 |
g0113: [2024-08-03 02:13:26,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=6420, skipped=3, lr=[0.00011218015573333333, 0.00011218015573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6420 loss: 1.4371 iter time (s): 4.094 samples/sec: 31.263
g0133:  iteration     6420/10000000 | consumed samples:       821760 | consumed tokens:   1682964480 | elapsed time per iteration (ms): 4126.6 | learning rate: 1.122E-04 | global batch size:   128 | lm loss: 1.464997E+00 | loss scale: 524288.0 | grad norm: 0.413 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.018 | tokens per gpu per second (tgs): 1985.158 | TFLOPs: 15.97 |
g0113: [2024-08-03 02:14:08,717] [INFO] [logging.py:96:log_dist] [Rank 0] step=6430, skipped=3, lr=[0.0001123549184, 0.0001123549184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6430 loss: 1.4576 iter time (s): 4.155 samples/sec: 30.809
g0133:  iteration     6430/10000000 | consumed samples:       823040 | consumed tokens:   1685585920 | elapsed time per iteration (ms): 4190.9 | learning rate: 1.124E-04 | global batch size:   128 | lm loss: 1.466529E+00 | loss scale: 524288.0 | grad norm: 0.436 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.542 | tokens per gpu per second (tgs): 1954.695 | TFLOPs: 15.73 |
g0113: [2024-08-03 02:14:48,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=6440, skipped=3, lr=[0.00011252968106666667, 0.00011252968106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6440 loss: 1.5773 iter time (s): 3.985 samples/sec: 32.119
g0133:  iteration     6440/10000000 | consumed samples:       824320 | consumed tokens:   1688207360 | elapsed time per iteration (ms): 4017.3 | learning rate: 1.125E-04 | global batch size:   128 | lm loss: 1.494740E+00 | loss scale: 524288.0 | grad norm: 0.426 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.862 | tokens per gpu per second (tgs): 2039.159 | TFLOPs: 16.41 |
g0113: [2024-08-03 02:15:30,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=6450, skipped=3, lr=[0.00011270444373333333, 0.00011270444373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6450 loss: 1.4818 iter time (s): 4.176 samples/sec: 30.652
g0133:  iteration     6450/10000000 | consumed samples:       825600 | consumed tokens:   1690828800 | elapsed time per iteration (ms): 4208.5 | learning rate: 1.127E-04 | global batch size:   128 | lm loss: 1.458331E+00 | loss scale: 524288.0 | grad norm: 0.476 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.415 | tokens per gpu per second (tgs): 1946.556 | TFLOPs: 15.66 |
g0113: [2024-08-03 02:16:12,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=6460, skipped=3, lr=[0.00011287920640000001, 0.00011287920640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6460 loss: 1.5070 iter time (s): 4.144 samples/sec: 30.888
g0133:  iteration     6460/10000000 | consumed samples:       826880 | consumed tokens:   1693450240 | elapsed time per iteration (ms): 4176.5 | learning rate: 1.129E-04 | global batch size:   128 | lm loss: 1.466955E+00 | loss scale: 524288.0 | grad norm: 0.412 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.648 | tokens per gpu per second (tgs): 1961.441 | TFLOPs: 15.78 |
g0113: [2024-08-03 02:16:53,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=6470, skipped=3, lr=[0.00011305396906666668, 0.00011305396906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6470 loss: 1.4405 iter time (s): 4.076 samples/sec: 31.405
g0133:  iteration     6470/10000000 | consumed samples:       828160 | consumed tokens:   1696071680 | elapsed time per iteration (ms): 4108.4 | learning rate: 1.131E-04 | global batch size:   128 | lm loss: 1.478520E+00 | loss scale: 524288.0 | grad norm: 0.426 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.156 | tokens per gpu per second (tgs): 1993.969 | TFLOPs: 16.05 |
g0113: [2024-08-03 02:17:35,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=6480, skipped=3, lr=[0.00011322873173333334, 0.00011322873173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6480 loss: 1.5216 iter time (s): 4.159 samples/sec: 30.775
g0133:  iteration     6480/10000000 | consumed samples:       829440 | consumed tokens:   1698693120 | elapsed time per iteration (ms): 4193.4 | learning rate: 1.132E-04 | global batch size:   128 | lm loss: 1.470712E+00 | loss scale: 524288.0 | grad norm: 0.469 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.524 | tokens per gpu per second (tgs): 1953.560 | TFLOPs: 15.72 |
g0113: [2024-08-03 02:18:16,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=6490, skipped=3, lr=[0.00011340349440000001, 0.00011340349440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6490 loss: 1.4406 iter time (s): 4.002 samples/sec: 31.988
g0133:  iteration     6490/10000000 | consumed samples:       830720 | consumed tokens:   1701314560 | elapsed time per iteration (ms): 4034.2 | learning rate: 1.134E-04 | global batch size:   128 | lm loss: 1.454489E+00 | loss scale: 524288.0 | grad norm: 0.425 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.728 | tokens per gpu per second (tgs): 2030.619 | TFLOPs: 16.34 |
g0113: [2024-08-03 02:18:58,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=6500, skipped=3, lr=[0.00011357825706666668, 0.00011357825706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6500 loss: 1.4535 iter time (s): 4.163 samples/sec: 30.745
g0133:  iteration     6500/10000000 | consumed samples:       832000 | consumed tokens:   1703936000 | elapsed time per iteration (ms): 4196.6 | learning rate: 1.136E-04 | global batch size:   128 | lm loss: 1.467483E+00 | loss scale: 524288.0 | grad norm: 0.411 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.501 | tokens per gpu per second (tgs): 1952.068 | TFLOPs: 15.71 |
g0113: [2024-08-03 02:19:40,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=6510, skipped=3, lr=[0.00011375301973333334, 0.00011375301973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6510 loss: 1.4549 iter time (s): 4.191 samples/sec: 30.543
g0133:  iteration     6510/10000000 | consumed samples:       833280 | consumed tokens:   1706557440 | elapsed time per iteration (ms): 4224.4 | learning rate: 1.138E-04 | global batch size:   128 | lm loss: 1.446495E+00 | loss scale: 524288.0 | grad norm: 0.422 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.300 | tokens per gpu per second (tgs): 1939.228 | TFLOPs: 15.61 |
g0113: [2024-08-03 02:20:21,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=6520, skipped=3, lr=[0.00011392778240000001, 0.00011392778240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6520 loss: 1.4770 iter time (s): 4.136 samples/sec: 30.945
g0133:  iteration     6520/10000000 | consumed samples:       834560 | consumed tokens:   1709178880 | elapsed time per iteration (ms): 4168.6 | learning rate: 1.139E-04 | global batch size:   128 | lm loss: 1.465589E+00 | loss scale: 524288.0 | grad norm: 0.433 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.706 | tokens per gpu per second (tgs): 1965.190 | TFLOPs: 15.81 |
g0113: [2024-08-03 02:21:04,702] [INFO] [logging.py:96:log_dist] [Rank 0] step=6530, skipped=3, lr=[0.00011410254506666667, 0.00011410254506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6530 loss: 1.4293 iter time (s): 4.237 samples/sec: 30.212
g0133:  iteration     6530/10000000 | consumed samples:       835840 | consumed tokens:   1711800320 | elapsed time per iteration (ms): 4270.8 | learning rate: 1.141E-04 | global batch size:   128 | lm loss: 1.457642E+00 | loss scale: 524288.0 | grad norm: 0.424 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.971 | tokens per gpu per second (tgs): 1918.159 | TFLOPs: 15.44 |
g0113: [2024-08-03 02:21:47,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=6540, skipped=3, lr=[0.00011427730773333334, 0.00011427730773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6540 loss: 1.4243 iter time (s): 4.224 samples/sec: 30.301
g0133:  iteration     6540/10000000 | consumed samples:       837120 | consumed tokens:   1714421760 | elapsed time per iteration (ms): 4256.7 | learning rate: 1.143E-04 | global batch size:   128 | lm loss: 1.435683E+00 | loss scale: 524288.0 | grad norm: 0.418 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.070 | tokens per gpu per second (tgs): 1924.505 | TFLOPs: 15.49 |
g0113: [2024-08-03 02:22:29,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=6550, skipped=3, lr=[0.0001144520704, 0.0001144520704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6550 loss: 1.3945 iter time (s): 4.150 samples/sec: 30.844
g0133:  iteration     6550/10000000 | consumed samples:       838400 | consumed tokens:   1717043200 | elapsed time per iteration (ms): 4182.2 | learning rate: 1.145E-04 | global batch size:   128 | lm loss: 1.454691E+00 | loss scale: 524288.0 | grad norm: 0.423 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.606 | tokens per gpu per second (tgs): 1958.781 | TFLOPs: 15.76 |
g0113: [2024-08-03 02:23:11,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=6560, skipped=3, lr=[0.00011462683306666667, 0.00011462683306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6560 loss: 1.4506 iter time (s): 4.160 samples/sec: 30.771
g0133:  iteration     6560/10000000 | consumed samples:       839680 | consumed tokens:   1719664640 | elapsed time per iteration (ms): 4192.1 | learning rate: 1.146E-04 | global batch size:   128 | lm loss: 1.442476E+00 | loss scale: 524288.0 | grad norm: 0.438 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.534 | tokens per gpu per second (tgs): 1954.149 | TFLOPs: 15.73 |
g0113: [2024-08-03 02:23:52,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=6570, skipped=3, lr=[0.00011480159573333334, 0.00011480159573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6570 loss: 1.4702 iter time (s): 4.154 samples/sec: 30.812
g0133:  iteration     6570/10000000 | consumed samples:       840960 | consumed tokens:   1722286080 | elapsed time per iteration (ms): 4187.7 | learning rate: 1.148E-04 | global batch size:   128 | lm loss: 1.452963E+00 | loss scale: 524288.0 | grad norm: 0.413 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.566 | tokens per gpu per second (tgs): 1956.206 | TFLOPs: 15.74 |
g0113: [2024-08-03 02:24:33,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=6580, skipped=3, lr=[0.0001149763584, 0.0001149763584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6580 loss: 1.4949 iter time (s): 4.064 samples/sec: 31.496
g0133:  iteration     6580/10000000 | consumed samples:       842240 | consumed tokens:   1724907520 | elapsed time per iteration (ms): 4096.4 | learning rate: 1.150E-04 | global batch size:   128 | lm loss: 1.441517E+00 | loss scale: 524288.0 | grad norm: 0.410 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.247 | tokens per gpu per second (tgs): 1999.794 | TFLOPs: 16.09 |
g0113: [2024-08-03 02:25:14,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=6590, skipped=3, lr=[0.00011515112106666667, 0.00011515112106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6590 loss: 1.4759 iter time (s): 4.063 samples/sec: 31.501
g0133:  iteration     6590/10000000 | consumed samples:       843520 | consumed tokens:   1727528960 | elapsed time per iteration (ms): 4096.0 | learning rate: 1.152E-04 | global batch size:   128 | lm loss: 1.438300E+00 | loss scale: 524288.0 | grad norm: 0.398 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.250 | tokens per gpu per second (tgs): 2000.007 | TFLOPs: 16.09 |
g0113: [2024-08-03 02:25:56,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=6600, skipped=3, lr=[0.00011532588373333334, 0.00011532588373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6600 loss: 1.4768 iter time (s): 4.148 samples/sec: 30.857
g0133:  iteration     6600/10000000 | consumed samples:       844800 | consumed tokens:   1730150400 | elapsed time per iteration (ms): 4180.5 | learning rate: 1.153E-04 | global batch size:   128 | lm loss: 1.447857E+00 | loss scale: 524288.0 | grad norm: 0.418 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.618 | tokens per gpu per second (tgs): 1959.564 | TFLOPs: 15.77 |
g0113: [2024-08-03 02:26:39,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=6610, skipped=3, lr=[0.0001155006464, 0.0001155006464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6610 loss: 1.5114 iter time (s): 4.213 samples/sec: 30.384
g0133:  iteration     6610/10000000 | consumed samples:       846080 | consumed tokens:   1732771840 | elapsed time per iteration (ms): 4245.3 | learning rate: 1.155E-04 | global batch size:   128 | lm loss: 1.435139E+00 | loss scale: 524288.0 | grad norm: 0.426 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.151 | tokens per gpu per second (tgs): 1929.653 | TFLOPs: 15.53 |
g0113: [2024-08-03 02:27:20,924] [INFO] [logging.py:96:log_dist] [Rank 0] step=6620, skipped=3, lr=[0.00011567540906666667, 0.00011567540906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6620 loss: 1.5062 iter time (s): 4.153 samples/sec: 30.824
g0133:  iteration     6620/10000000 | consumed samples:       847360 | consumed tokens:   1735393280 | elapsed time per iteration (ms): 4184.9 | learning rate: 1.157E-04 | global batch size:   128 | lm loss: 1.457725E+00 | loss scale: 524288.0 | grad norm: 0.411 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.586 | tokens per gpu per second (tgs): 1957.498 | TFLOPs: 15.75 |
g0113: [2024-08-03 02:28:03,228] [INFO] [logging.py:96:log_dist] [Rank 0] step=6630, skipped=3, lr=[0.00011585017173333333, 0.00011585017173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6630 loss: 1.4172 iter time (s): 4.198 samples/sec: 30.491
g0133:  iteration     6630/10000000 | consumed samples:       848640 | consumed tokens:   1738014720 | elapsed time per iteration (ms): 4230.4 | learning rate: 1.159E-04 | global batch size:   128 | lm loss: 1.441639E+00 | loss scale: 524288.0 | grad norm: 0.405 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.257 | tokens per gpu per second (tgs): 1936.465 | TFLOPs: 15.58 |
g0113: [2024-08-03 02:28:44,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=6640, skipped=3, lr=[0.00011602493440000001, 0.00011602493440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6640 loss: 1.4170 iter time (s): 4.140 samples/sec: 30.921
g0133:  iteration     6640/10000000 | consumed samples:       849920 | consumed tokens:   1740636160 | elapsed time per iteration (ms): 4171.9 | learning rate: 1.160E-04 | global batch size:   128 | lm loss: 1.430208E+00 | loss scale: 524288.0 | grad norm: 0.416 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.681 | tokens per gpu per second (tgs): 1963.615 | TFLOPs: 15.80 |
g0113: [2024-08-03 02:29:26,710] [INFO] [logging.py:96:log_dist] [Rank 0] step=6650, skipped=3, lr=[0.00011619969706666668, 0.00011619969706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6650 loss: 1.4844 iter time (s): 4.144 samples/sec: 30.889
g0133:  iteration     6650/10000000 | consumed samples:       851200 | consumed tokens:   1743257600 | elapsed time per iteration (ms): 4176.9 | learning rate: 1.162E-04 | global batch size:   128 | lm loss: 1.438797E+00 | loss scale: 524288.0 | grad norm: 0.402 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.644 | tokens per gpu per second (tgs): 1961.245 | TFLOPs: 15.78 |
g0113: [2024-08-03 02:30:08,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=6660, skipped=3, lr=[0.00011637445973333335, 0.00011637445973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6660 loss: 1.4388 iter time (s): 4.128 samples/sec: 31.007
g0133:  iteration     6660/10000000 | consumed samples:       852480 | consumed tokens:   1745879040 | elapsed time per iteration (ms): 4160.1 | learning rate: 1.164E-04 | global batch size:   128 | lm loss: 1.447999E+00 | loss scale: 524288.0 | grad norm: 0.436 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.769 | tokens per gpu per second (tgs): 1969.189 | TFLOPs: 15.85 |
g0113: [2024-08-03 02:30:49,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=6670, skipped=3, lr=[0.00011654922240000001, 0.00011654922240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6670 loss: 1.4321 iter time (s): 4.110 samples/sec: 31.142
g0133:  iteration     6670/10000000 | consumed samples:       853760 | consumed tokens:   1748500480 | elapsed time per iteration (ms): 4142.5 | learning rate: 1.165E-04 | global batch size:   128 | lm loss: 1.431880E+00 | loss scale: 524288.0 | grad norm: 0.404 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.899 | tokens per gpu per second (tgs): 1977.540 | TFLOPs: 15.91 |
g0113: [2024-08-03 02:31:32,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=6680, skipped=3, lr=[0.00011672398506666668, 0.00011672398506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6680 loss: 1.4851 iter time (s): 4.232 samples/sec: 30.247
g0133:  iteration     6680/10000000 | consumed samples:       855040 | consumed tokens:   1751121920 | elapsed time per iteration (ms): 4264.3 | learning rate: 1.167E-04 | global batch size:   128 | lm loss: 1.457696E+00 | loss scale: 524288.0 | grad norm: 0.418 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.017 | tokens per gpu per second (tgs): 1921.067 | TFLOPs: 15.46 |
g0113: [2024-08-03 02:32:15,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=6690, skipped=3, lr=[0.00011689874773333334, 0.00011689874773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6690 loss: 1.4929 iter time (s): 4.248 samples/sec: 30.131
g0133:  iteration     6690/10000000 | consumed samples:       856320 | consumed tokens:   1753743360 | elapsed time per iteration (ms): 4280.4 | learning rate: 1.169E-04 | global batch size:   128 | lm loss: 1.415343E+00 | loss scale: 524288.0 | grad norm: 0.427 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.903 | tokens per gpu per second (tgs): 1913.821 | TFLOPs: 15.40 |
g0113: [2024-08-03 02:32:57,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=6700, skipped=3, lr=[0.00011707351040000001, 0.00011707351040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6700 loss: 1.4283 iter time (s): 4.212 samples/sec: 30.390
g0133:  iteration     6700/10000000 | consumed samples:       857600 | consumed tokens:   1756364800 | elapsed time per iteration (ms): 4244.0 | learning rate: 1.171E-04 | global batch size:   128 | lm loss: 1.441919E+00 | loss scale: 524288.0 | grad norm: 0.374 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.160 | tokens per gpu per second (tgs): 1930.254 | TFLOPs: 15.53 |
g0113: [2024-08-03 02:33:38,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=6710, skipped=3, lr=[0.00011724827306666668, 0.00011724827306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6710 loss: 1.4394 iter time (s): 4.096 samples/sec: 31.253
g0133:  iteration     6710/10000000 | consumed samples:       858880 | consumed tokens:   1758986240 | elapsed time per iteration (ms): 4128.7 | learning rate: 1.172E-04 | global batch size:   128 | lm loss: 1.421274E+00 | loss scale: 524288.0 | grad norm: 0.388 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.003 | tokens per gpu per second (tgs): 1984.166 | TFLOPs: 15.97 |
g0113: [2024-08-03 02:34:20,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=6720, skipped=3, lr=[0.00011742303573333334, 0.00011742303573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6720 loss: 1.3923 iter time (s): 4.108 samples/sec: 31.159
g0133:  iteration     6720/10000000 | consumed samples:       860160 | consumed tokens:   1761607680 | elapsed time per iteration (ms): 4140.2 | learning rate: 1.174E-04 | global batch size:   128 | lm loss: 1.414004E+00 | loss scale: 524288.0 | grad norm: 0.394 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.916 | tokens per gpu per second (tgs): 1978.655 | TFLOPs: 15.92 |
g0113: [2024-08-03 02:35:01,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=6730, skipped=3, lr=[0.00011759779840000001, 0.00011759779840000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6730 loss: 1.3667 iter time (s): 4.109 samples/sec: 31.155
g0133:  iteration     6730/10000000 | consumed samples:       861440 | consumed tokens:   1764229120 | elapsed time per iteration (ms): 4140.9 | learning rate: 1.176E-04 | global batch size:   128 | lm loss: 1.411560E+00 | loss scale: 524288.0 | grad norm: 0.400 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.911 | tokens per gpu per second (tgs): 1978.304 | TFLOPs: 15.92 |
g0113: [2024-08-03 02:35:42,775] [INFO] [logging.py:96:log_dist] [Rank 0] step=6740, skipped=3, lr=[0.00011777256106666667, 0.00011777256106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6740 loss: 1.3917 iter time (s): 4.072 samples/sec: 31.432
g0133:  iteration     6740/10000000 | consumed samples:       862720 | consumed tokens:   1766850560 | elapsed time per iteration (ms): 4105.3 | learning rate: 1.178E-04 | global batch size:   128 | lm loss: 1.435411E+00 | loss scale: 524288.0 | grad norm: 0.423 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.179 | tokens per gpu per second (tgs): 1995.485 | TFLOPs: 16.06 |
g0113: [2024-08-03 02:36:25,169] [INFO] [logging.py:96:log_dist] [Rank 0] step=6750, skipped=3, lr=[0.00011794732373333334, 0.00011794732373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6750 loss: 1.3909 iter time (s): 4.206 samples/sec: 30.430
g0133:  iteration     6750/10000000 | consumed samples:       864000 | consumed tokens:   1769472000 | elapsed time per iteration (ms): 4238.8 | learning rate: 1.179E-04 | global batch size:   128 | lm loss: 1.430883E+00 | loss scale: 524288.0 | grad norm: 0.411 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.198 | tokens per gpu per second (tgs): 1932.643 | TFLOPs: 15.55 |
g0113: [2024-08-03 02:37:07,646] [INFO] [logging.py:96:log_dist] [Rank 0] step=6760, skipped=3, lr=[0.00011812208640000001, 0.00011812208640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6760 loss: 1.4127 iter time (s): 4.215 samples/sec: 30.365
g0133:  iteration     6760/10000000 | consumed samples:       865280 | consumed tokens:   1772093440 | elapsed time per iteration (ms): 4247.7 | learning rate: 1.181E-04 | global batch size:   128 | lm loss: 1.419574E+00 | loss scale: 524288.0 | grad norm: 0.406 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.134 | tokens per gpu per second (tgs): 1928.594 | TFLOPs: 15.52 |
g0113: [2024-08-03 02:37:50,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=6770, skipped=3, lr=[0.00011829684906666667, 0.00011829684906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6770 loss: 1.4562 iter time (s): 4.224 samples/sec: 30.306
g0133:  iteration     6770/10000000 | consumed samples:       866560 | consumed tokens:   1774714880 | elapsed time per iteration (ms): 4256.0 | learning rate: 1.183E-04 | global batch size:   128 | lm loss: 1.405080E+00 | loss scale: 524288.0 | grad norm: 0.403 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.075 | tokens per gpu per second (tgs): 1924.807 | TFLOPs: 15.49 |
g0113: [2024-08-03 02:38:32,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=6780, skipped=3, lr=[0.00011847161173333334, 0.00011847161173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6780 loss: 1.4298 iter time (s): 4.220 samples/sec: 30.329
g0133:  iteration     6780/10000000 | consumed samples:       867840 | consumed tokens:   1777336320 | elapsed time per iteration (ms): 4252.6 | learning rate: 1.185E-04 | global batch size:   128 | lm loss: 1.424250E+00 | loss scale: 524288.0 | grad norm: 0.400 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.099 | tokens per gpu per second (tgs): 1926.337 | TFLOPs: 15.50 |
g0113: [2024-08-03 02:39:14,320] [INFO] [logging.py:96:log_dist] [Rank 0] step=6790, skipped=3, lr=[0.0001186463744, 0.0001186463744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6790 loss: 1.4359 iter time (s): 4.126 samples/sec: 31.019
g0133:  iteration     6790/10000000 | consumed samples:       869120 | consumed tokens:   1779957760 | elapsed time per iteration (ms): 4158.8 | learning rate: 1.186E-04 | global batch size:   128 | lm loss: 1.422561E+00 | loss scale: 524288.0 | grad norm: 0.382 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.778 | tokens per gpu per second (tgs): 1969.806 | TFLOPs: 15.85 |
g0113: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0131: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0130: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0130: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0129: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0129: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0130: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0131: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0125: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0131: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0129: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0130: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0130: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0132: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0131: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0132: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0113: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0132: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0131: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0133: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0113: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0131: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0125: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0129: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0132: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0133: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0130: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0128: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 02:39:44,003] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0128: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0125: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0128: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0128: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0133: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0132: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0128: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0133: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0133: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0133: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0131: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0129: [2024-08-03 02:39:44,004] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0113: [2024-08-03 02:39:56,939] [INFO] [logging.py:96:log_dist] [Rank 0] step=6800, skipped=3, lr=[0.00011882113706666667, 0.00011882113706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6800 loss: 1.3731 iter time (s): 4.230 samples/sec: 30.262
g0133:  iteration     6800/10000000 | consumed samples:       870400 | consumed tokens:   1782579200 | elapsed time per iteration (ms): 4261.9 | learning rate: 1.188E-04 | global batch size:   128 | lm loss: 1.409804E+00 | loss scale: 1048576.0 | grad norm: 0.394 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.034 | tokens per gpu per second (tgs): 1922.157 | TFLOPs: 15.47 |
g0113: [2024-08-03 02:40:38,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=6810, skipped=3, lr=[0.00011899589973333335, 0.00011899589973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6810 loss: 1.4212 iter time (s): 4.144 samples/sec: 30.884
g0133:  iteration     6810/10000000 | consumed samples:       871680 | consumed tokens:   1785200640 | elapsed time per iteration (ms): 4176.7 | learning rate: 1.190E-04 | global batch size:   128 | lm loss: 1.415141E+00 | loss scale: 1048576.0 | grad norm: 0.371 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.646 | tokens per gpu per second (tgs): 1961.353 | TFLOPs: 15.78 |
g0113: [2024-08-03 02:41:20,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=6820, skipped=3, lr=[0.00011917066240000002, 0.00011917066240000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6820 loss: 1.4561 iter time (s): 4.155 samples/sec: 30.806
g0133:  iteration     6820/10000000 | consumed samples:       872960 | consumed tokens:   1787822080 | elapsed time per iteration (ms): 4187.2 | learning rate: 1.192E-04 | global batch size:   128 | lm loss: 1.401512E+00 | loss scale: 1048576.0 | grad norm: 0.391 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.569 | tokens per gpu per second (tgs): 1956.416 | TFLOPs: 15.74 |
g0113: [2024-08-03 02:42:03,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=6830, skipped=3, lr=[0.00011934542506666668, 0.00011934542506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6830 loss: 1.4210 iter time (s): 4.233 samples/sec: 30.238
g0133:  iteration     6830/10000000 | consumed samples:       874240 | consumed tokens:   1790443520 | elapsed time per iteration (ms): 4265.2 | learning rate: 1.193E-04 | global batch size:   128 | lm loss: 1.401219E+00 | loss scale: 1048576.0 | grad norm: 0.391 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.010 | tokens per gpu per second (tgs): 1920.641 | TFLOPs: 15.46 |
g0113: [2024-08-03 02:42:45,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=6840, skipped=3, lr=[0.00011952018773333335, 0.00011952018773333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6840 loss: 1.3762 iter time (s): 4.225 samples/sec: 30.293
g0133:  iteration     6840/10000000 | consumed samples:       875520 | consumed tokens:   1793064960 | elapsed time per iteration (ms): 4257.8 | learning rate: 1.195E-04 | global batch size:   128 | lm loss: 1.405766E+00 | loss scale: 1048576.0 | grad norm: 0.385 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.063 | tokens per gpu per second (tgs): 1924.010 | TFLOPs: 15.48 |
g0113: [2024-08-03 02:43:27,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=6850, skipped=3, lr=[0.00011969495040000001, 0.00011969495040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6850 loss: 1.3818 iter time (s): 4.170 samples/sec: 30.692
g0133:  iteration     6850/10000000 | consumed samples:       876800 | consumed tokens:   1795686400 | elapsed time per iteration (ms): 4202.8 | learning rate: 1.197E-04 | global batch size:   128 | lm loss: 1.409261E+00 | loss scale: 1048576.0 | grad norm: 0.376 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.456 | tokens per gpu per second (tgs): 1949.160 | TFLOPs: 15.69 |
g0113: [2024-08-03 02:44:08,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=6860, skipped=3, lr=[0.00011986971306666668, 0.00011986971306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6860 loss: 1.3773 iter time (s): 4.075 samples/sec: 31.407
g0133:  iteration     6860/10000000 | consumed samples:       878080 | consumed tokens:   1798307840 | elapsed time per iteration (ms): 4107.7 | learning rate: 1.199E-04 | global batch size:   128 | lm loss: 1.392422E+00 | loss scale: 1048576.0 | grad norm: 0.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.161 | tokens per gpu per second (tgs): 1994.292 | TFLOPs: 16.05 |
g0113: [2024-08-03 02:44:49,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=6870, skipped=3, lr=[0.00012004447573333335, 0.00012004447573333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6870 loss: 1.4300 iter time (s): 4.005 samples/sec: 31.961
g0133:  iteration     6870/10000000 | consumed samples:       879360 | consumed tokens:   1800929280 | elapsed time per iteration (ms): 4037.7 | learning rate: 1.200E-04 | global batch size:   128 | lm loss: 1.414172E+00 | loss scale: 1048576.0 | grad norm: 0.396 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.701 | tokens per gpu per second (tgs): 2028.887 | TFLOPs: 16.33 |
g0113: [2024-08-03 02:45:30,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=6880, skipped=3, lr=[0.00012021923840000001, 0.00012021923840000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6880 loss: 1.4120 iter time (s): 4.058 samples/sec: 31.543
g0133:  iteration     6880/10000000 | consumed samples:       880640 | consumed tokens:   1803550720 | elapsed time per iteration (ms): 4090.6 | learning rate: 1.202E-04 | global batch size:   128 | lm loss: 1.406926E+00 | loss scale: 1048576.0 | grad norm: 0.398 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.291 | tokens per gpu per second (tgs): 2002.642 | TFLOPs: 16.12 |
g0113: [2024-08-03 02:46:11,018] [INFO] [logging.py:96:log_dist] [Rank 0] step=6890, skipped=3, lr=[0.00012039400106666668, 0.00012039400106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6890 loss: 1.3975 iter time (s): 4.050 samples/sec: 31.606
g0133:  iteration     6890/10000000 | consumed samples:       881920 | consumed tokens:   1806172160 | elapsed time per iteration (ms): 4082.0 | learning rate: 1.204E-04 | global batch size:   128 | lm loss: 1.390679E+00 | loss scale: 1048576.0 | grad norm: 0.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.358 | tokens per gpu per second (tgs): 2006.881 | TFLOPs: 16.15 |
g0113: [2024-08-03 02:46:52,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=6900, skipped=3, lr=[0.00012056876373333335, 0.00012056876373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6900 loss: 1.3960 iter time (s): 4.107 samples/sec: 31.164
g0133:  iteration     6900/10000000 | consumed samples:       883200 | consumed tokens:   1808793600 | elapsed time per iteration (ms): 4140.2 | learning rate: 1.206E-04 | global batch size:   128 | lm loss: 1.406855E+00 | loss scale: 1048576.0 | grad norm: 0.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.917 | tokens per gpu per second (tgs): 1978.672 | TFLOPs: 15.92 |
g0113: [2024-08-03 02:47:33,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=6910, skipped=3, lr=[0.00012074352640000001, 0.00012074352640000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6910 loss: 1.4414 iter time (s): 4.104 samples/sec: 31.186
g0133:  iteration     6910/10000000 | consumed samples:       884480 | consumed tokens:   1811415040 | elapsed time per iteration (ms): 4136.7 | learning rate: 1.207E-04 | global batch size:   128 | lm loss: 1.405578E+00 | loss scale: 1048576.0 | grad norm: 0.373 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.943 | tokens per gpu per second (tgs): 1980.342 | TFLOPs: 15.94 |
g0113: [2024-08-03 02:48:16,352] [INFO] [logging.py:96:log_dist] [Rank 0] step=6920, skipped=3, lr=[0.00012091828906666668, 0.00012091828906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6920 loss: 1.3636 iter time (s): 4.208 samples/sec: 30.415
g0133:  iteration     6920/10000000 | consumed samples:       885760 | consumed tokens:   1814036480 | elapsed time per iteration (ms): 4257.0 | learning rate: 1.209E-04 | global batch size:   128 | lm loss: 1.396363E+00 | loss scale: 1048576.0 | grad norm: 0.378 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.068 | tokens per gpu per second (tgs): 1924.372 | TFLOPs: 15.49 |
g0113: [2024-08-03 02:48:58,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=6930, skipped=3, lr=[0.00012109305173333334, 0.00012109305173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6930 loss: 1.4095 iter time (s): 4.228 samples/sec: 30.273
g0133:  iteration     6930/10000000 | consumed samples:       887040 | consumed tokens:   1816657920 | elapsed time per iteration (ms): 4263.2 | learning rate: 1.211E-04 | global batch size:   128 | lm loss: 1.395564E+00 | loss scale: 1048576.0 | grad norm: 0.372 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.024 | tokens per gpu per second (tgs): 1921.540 | TFLOPs: 15.46 |
g0113: [2024-08-03 02:49:41,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=6940, skipped=3, lr=[0.00012126781440000001, 0.00012126781440000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6940 loss: 1.3900 iter time (s): 4.169 samples/sec: 30.701
g0133:  iteration     6940/10000000 | consumed samples:       888320 | consumed tokens:   1819279360 | elapsed time per iteration (ms): 4207.6 | learning rate: 1.213E-04 | global batch size:   128 | lm loss: 1.397460E+00 | loss scale: 1048576.0 | grad norm: 0.374 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.421 | tokens per gpu per second (tgs): 1946.970 | TFLOPs: 15.67 |
g0113: [2024-08-03 02:50:22,231] [INFO] [logging.py:96:log_dist] [Rank 0] step=6950, skipped=3, lr=[0.00012144257706666668, 0.00012144257706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6950 loss: 1.3733 iter time (s): 4.084 samples/sec: 31.339
g0133:  iteration     6950/10000000 | consumed samples:       889600 | consumed tokens:   1821900800 | elapsed time per iteration (ms): 4116.5 | learning rate: 1.214E-04 | global batch size:   128 | lm loss: 1.400715E+00 | loss scale: 1048576.0 | grad norm: 0.379 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.094 | tokens per gpu per second (tgs): 1990.043 | TFLOPs: 16.01 |
g0113: [2024-08-03 02:51:04,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=6960, skipped=3, lr=[0.00012161733973333334, 0.00012161733973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6960 loss: 1.3625 iter time (s): 4.155 samples/sec: 30.810
g0133:  iteration     6960/10000000 | consumed samples:       890880 | consumed tokens:   1824522240 | elapsed time per iteration (ms): 4186.6 | learning rate: 1.216E-04 | global batch size:   128 | lm loss: 1.394215E+00 | loss scale: 1048576.0 | grad norm: 0.391 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.574 | tokens per gpu per second (tgs): 1956.706 | TFLOPs: 15.75 |
g0113: [2024-08-03 02:51:45,625] [INFO] [logging.py:96:log_dist] [Rank 0] step=6970, skipped=3, lr=[0.00012179210240000001, 0.00012179210240000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6970 loss: 1.3734 iter time (s): 4.120 samples/sec: 31.064
g0133:  iteration     6970/10000000 | consumed samples:       892160 | consumed tokens:   1827143680 | elapsed time per iteration (ms): 4152.7 | learning rate: 1.218E-04 | global batch size:   128 | lm loss: 1.391757E+00 | loss scale: 1048576.0 | grad norm: 0.372 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.823 | tokens per gpu per second (tgs): 1972.685 | TFLOPs: 15.87 |
g0113: [2024-08-03 02:52:26,620] [INFO] [logging.py:96:log_dist] [Rank 0] step=6980, skipped=3, lr=[0.00012196686506666667, 0.00012196686506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6980 loss: 1.3566 iter time (s): 4.067 samples/sec: 31.473
g0133:  iteration     6980/10000000 | consumed samples:       893440 | consumed tokens:   1829765120 | elapsed time per iteration (ms): 4099.6 | learning rate: 1.220E-04 | global batch size:   128 | lm loss: 1.392718E+00 | loss scale: 1048576.0 | grad norm: 0.368 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.223 | tokens per gpu per second (tgs): 1998.268 | TFLOPs: 16.08 |
g0113: [2024-08-03 02:53:08,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=6990, skipped=3, lr=[0.00012214162773333334, 0.00012214162773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 6990 loss: 1.4050 iter time (s): 4.134 samples/sec: 30.966
g0133:  iteration     6990/10000000 | consumed samples:       894720 | consumed tokens:   1832386560 | elapsed time per iteration (ms): 4166.1 | learning rate: 1.221E-04 | global batch size:   128 | lm loss: 1.390787E+00 | loss scale: 1048576.0 | grad norm: 0.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.724 | tokens per gpu per second (tgs): 1966.360 | TFLOPs: 15.82 |
g0113: [2024-08-03 02:53:49,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=7000, skipped=3, lr=[0.0001223163904, 0.0001223163904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7000 loss: 1.4044 iter time (s): 4.121 samples/sec: 31.058
g0133:  iteration     7000/10000000 | consumed samples:       896000 | consumed tokens:   1835008000 | elapsed time per iteration (ms): 4154.4 | learning rate: 1.223E-04 | global batch size:   128 | lm loss: 1.387778E+00 | loss scale: 1048576.0 | grad norm: 0.372 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.811 | tokens per gpu per second (tgs): 1971.888 | TFLOPs: 15.87 |
g0133: ------------------------------------------------------------------------------------------------
g0133:  validation loss at iteration 7000 | lm loss value: 1.386569E+00 | lm loss PPL: 4.001097E+00 | 
g0133: ------------------------------------------------------------------------------------------------
g0113: saving checkpoint at iteration    7000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: [2024-08-03 03:00:14,323] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step7000 is about to be saved!
g0133: [2024-08-03 03:00:14,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0133: [2024-08-03 03:00:14,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0133: [2024-08-03 03:00:14,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0113: [2024-08-03 03:00:14,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0113: [2024-08-03 03:00:14,329] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0113: [2024-08-03 03:00:14,329] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0128: [2024-08-03 03:00:14,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0128: [2024-08-03 03:00:14,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0128: [2024-08-03 03:00:14,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0131: [2024-08-03 03:00:14,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0131: [2024-08-03 03:00:14,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0125: [2024-08-03 03:00:14,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0125: [2024-08-03 03:00:14,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0125: [2024-08-03 03:00:14,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0131: [2024-08-03 03:00:14,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0129: [2024-08-03 03:00:14,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0129: [2024-08-03 03:00:14,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0129: [2024-08-03 03:00:14,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0132: [2024-08-03 03:00:14,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0132: [2024-08-03 03:00:14,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0132: [2024-08-03 03:00:14,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0130: [2024-08-03 03:00:14,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0130: [2024-08-03 03:00:14,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0130: [2024-08-03 03:00:14,333] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0133: [2024-08-03 03:00:14,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt...
g0132: [2024-08-03 03:00:14,363] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt...
g0125: [2024-08-03 03:00:14,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt...
g0129: [2024-08-03 03:00:14,366] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt...
g0130: [2024-08-03 03:00:14,366] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt...
g0128: [2024-08-03 03:00:14,367] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt...
g0131: [2024-08-03 03:00:14,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt...
g0113: [2024-08-03 03:00:14,374] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt...
g0133: [2024-08-03 03:00:14,473] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt.
g0133: [2024-08-03 03:00:14,474] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt...
g0133: [2024-08-03 03:00:14,477] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt.
g0130: [2024-08-03 03:00:14,493] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt.
g0128: [2024-08-03 03:00:14,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt.
g0133: [2024-08-03 03:00:14,521] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt...
g0130: [2024-08-03 03:00:14,527] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt...
g0128: [2024-08-03 03:00:14,548] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt...
g0132: [2024-08-03 03:00:14,581] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt.
g0132: [2024-08-03 03:00:14,616] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt...
g0131: [2024-08-03 03:00:14,631] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt.
g0129: [2024-08-03 03:00:14,640] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt.
g0131: [2024-08-03 03:00:14,670] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt...
g0129: [2024-08-03 03:00:14,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt...
g0128: [2024-08-03 03:00:14,694] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt.
g0113: [2024-08-03 03:00:14,718] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt.
g0128: [2024-08-03 03:00:14,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt...
g0113: [2024-08-03 03:00:14,744] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt...
g0129: [2024-08-03 03:00:14,817] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt.
g0129: [2024-08-03 03:00:14,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt...
g0133: [2024-08-03 03:00:14,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt.
g0133: [2024-08-03 03:00:14,848] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt...
g0131: [2024-08-03 03:00:14,855] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt.
g0113: [2024-08-03 03:00:14,871] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt.
g0131: [2024-08-03 03:00:14,890] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt...
g0128: [2024-08-03 03:00:14,894] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt.
g0128: [2024-08-03 03:00:14,896] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt...
g0113: [2024-08-03 03:00:14,897] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt...
g0129: [2024-08-03 03:00:14,934] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt.
g0129: [2024-08-03 03:00:14,936] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt...
g0131: [2024-08-03 03:00:15,034] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt.
g0131: [2024-08-03 03:00:15,036] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt...
g0132: [2024-08-03 03:00:15,084] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt.
g0132: [2024-08-03 03:00:15,114] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt...
g0132: [2024-08-03 03:00:15,277] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt.
g0132: [2024-08-03 03:00:15,279] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt...
g0113: [2024-08-03 03:00:15,544] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt.
g0113: [2024-08-03 03:00:15,568] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt...
g0113: [2024-08-03 03:00:15,707] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt.
g0113: [2024-08-03 03:00:15,708] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt
g0113: [2024-08-03 03:00:15,708] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0125: [2024-08-03 03:00:15,752] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt.
g0130: [2024-08-03 03:00:15,777] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt.
g0125: [2024-08-03 03:00:15,789] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt...
g0130: [2024-08-03 03:00:15,807] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt...
g0130: [2024-08-03 03:00:15,912] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt.
g0125: [2024-08-03 03:00:15,914] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt.
g0130: [2024-08-03 03:00:15,914] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt...
g0125: [2024-08-03 03:00:15,942] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt...
g0125: [2024-08-03 03:00:16,111] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt.
g0125: [2024-08-03 03:00:16,112] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt...
g0133: [2024-08-03 03:00:16,818] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt.
g0133: [2024-08-03 03:00:16,819] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0128: [2024-08-03 03:00:17,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt.
g0128: [2024-08-03 03:00:17,254] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0129: [2024-08-03 03:00:17,338] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt.
g0129: [2024-08-03 03:00:17,339] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0131: [2024-08-03 03:00:17,386] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt.
g0131: [2024-08-03 03:00:17,386] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0132: [2024-08-03 03:00:17,595] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt.
g0132: [2024-08-03 03:00:17,596] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0125: [2024-08-03 03:00:18,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt.
g0125: [2024-08-03 03:00:18,483] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0130: [2024-08-03 03:00:19,576] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt.
g0130: [2024-08-03 03:00:19,576] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0113: [2024-08-03 03:00:19,651] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0113: [2024-08-03 03:00:19,653] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7000 is ready now!
g0113:   successfully saved checkpoint at iteration    7000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_000100_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0113: Checkpoint Save GB: 22.521, GB/Sec: 4.2, Latency(second): 5.356
g0133: (min, max) time across ranks (ms):
g0133:     save-checkpoint ................................: (5355.39, 5356.27)
g0113: [2024-08-03 03:00:59,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=7010, skipped=3, lr=[0.00012249115306666667, 0.00012249115306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7010 loss: 1.3673 iter time (s): 3.998 samples/sec: 32.014
g0133:  iteration     7010/10000000 | consumed samples:       897280 | consumed tokens:   1837629440 | elapsed time per iteration (ms): 43013.9 | learning rate: 1.225E-04 | global batch size:   128 | lm loss: 1.384703E+00 | loss scale: 1048576.0 | grad norm: 0.373 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.976 | tokens per gpu per second (tgs): 190.450 | TFLOPs: 1.53 |
g0113: [2024-08-03 03:01:41,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=7020, skipped=3, lr=[0.00012266591573333334, 0.00012266591573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7020 loss: 1.4289 iter time (s): 4.128 samples/sec: 31.009
g0133:  iteration     7020/10000000 | consumed samples:       898560 | consumed tokens:   1840250880 | elapsed time per iteration (ms): 4160.1 | learning rate: 1.227E-04 | global batch size:   128 | lm loss: 1.384130E+00 | loss scale: 1048576.0 | grad norm: 0.402 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.769 | tokens per gpu per second (tgs): 1969.198 | TFLOPs: 15.85 |
g0128: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: Grad overflow on iteration 7021
g0133: Grad overflow on iteration 7021
g0133: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: Grad overflow on iteration 7021
g0113: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0128: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0132: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 7021
g0131: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 7021
g0131: Grad overflow on iteration 7021
g0113: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 7021
g0132: Grad overflow on iteration 7021
g0130: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 7021
g0129: Grad overflow on iteration 7021
g0132: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 7021
g0113: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: Grad overflow on iteration 7021
g0113: Grad overflow on iteration 7021
g0132: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: Grad overflow on iteration 7021
g0113: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0132: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0128: Grad overflow on iteration 7021
g0132: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0113: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0128: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0128: Grad overflow on iteration 7021
g0133: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: Grad overflow on iteration 7021
g0130: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: Grad overflow on iteration 7021
g0133: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0132: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0133: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: Grad overflow on iteration 7021
g0131: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0129: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: Grad overflow on iteration 7021
g0125: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0132: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0129: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0125: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0130: Grad overflow on iteration 7021
g0129: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0125: Grad overflow on iteration 7021
g0125: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 7021
g0113: [2024-08-03 03:01:49,481] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
g0125: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0129: Grad overflow on iteration 7021
g0128: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 7021
g0125: Grad overflow on iteration 7021
g0129: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: Grad overflow on iteration 7021
g0130: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0129: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0131: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0125: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0130: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 7021
g0130: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 7021
g0125: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0130: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0131: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0130: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0131: [2024-08-03 03:01:49,480] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 7021
g0130: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0131: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0128: Grad overflow on iteration 7021
g0128: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0128: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0131: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0133: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0129: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0113: [2024-08-03 03:01:49,481] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 7021
g0113: [2024-08-03 03:01:49,482] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0113: [2024-08-03 03:02:22,589] [INFO] [logging.py:96:log_dist] [Rank 0] step=7030, skipped=4, lr=[0.0001228406784, 0.0001228406784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7030 loss: 1.3990 iter time (s): 4.070 samples/sec: 31.450
g0133:  iteration     7030/10000000 | consumed samples:       899840 | consumed tokens:   1842872320 | elapsed time per iteration (ms): 4102.3 | learning rate: 1.228E-04 | global batch size:   128 | lm loss: 1.377665E+00 | loss scale: 524288.0 | grad norm: 0.385 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.202 | tokens per gpu per second (tgs): 1996.911 | TFLOPs: 16.07 |
g0113: [2024-08-03 03:03:02,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=7040, skipped=4, lr=[0.00012301544106666667, 0.00012301544106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7040 loss: 1.3646 iter time (s): 3.971 samples/sec: 32.236
g0133:  iteration     7040/10000000 | consumed samples:       901120 | consumed tokens:   1845493760 | elapsed time per iteration (ms): 4003.3 | learning rate: 1.230E-04 | global batch size:   128 | lm loss: 1.383188E+00 | loss scale: 524288.0 | grad norm: 0.375 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.973 | tokens per gpu per second (tgs): 2046.298 | TFLOPs: 16.47 |
g0113: [2024-08-03 03:03:42,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=7050, skipped=4, lr=[0.00012319020373333334, 0.00012319020373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7050 loss: 1.3516 iter time (s): 3.987 samples/sec: 32.101
g0133:  iteration     7050/10000000 | consumed samples:       902400 | consumed tokens:   1848115200 | elapsed time per iteration (ms): 4019.5 | learning rate: 1.232E-04 | global batch size:   128 | lm loss: 1.377084E+00 | loss scale: 524288.0 | grad norm: 0.415 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.845 | tokens per gpu per second (tgs): 2038.061 | TFLOPs: 16.40 |
g0113: [2024-08-03 03:04:23,723] [INFO] [logging.py:96:log_dist] [Rank 0] step=7060, skipped=4, lr=[0.0001233649664, 0.0001233649664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7060 loss: 1.4087 iter time (s): 4.058 samples/sec: 31.542
g0133:  iteration     7060/10000000 | consumed samples:       903680 | consumed tokens:   1850736640 | elapsed time per iteration (ms): 4090.5 | learning rate: 1.234E-04 | global batch size:   128 | lm loss: 1.358202E+00 | loss scale: 524288.0 | grad norm: 0.367 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.292 | tokens per gpu per second (tgs): 2002.684 | TFLOPs: 16.12 |
g0113: [2024-08-03 03:05:05,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=7070, skipped=4, lr=[0.00012353972906666667, 0.00012353972906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7070 loss: 1.3733 iter time (s): 4.115 samples/sec: 31.106
g0133:  iteration     7070/10000000 | consumed samples:       904960 | consumed tokens:   1853358080 | elapsed time per iteration (ms): 4148.2 | learning rate: 1.235E-04 | global batch size:   128 | lm loss: 1.392046E+00 | loss scale: 524288.0 | grad norm: 0.361 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.857 | tokens per gpu per second (tgs): 1974.851 | TFLOPs: 15.89 |
g0113: [2024-08-03 03:05:46,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=7080, skipped=4, lr=[0.00012371449173333336, 0.00012371449173333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7080 loss: 1.4098 iter time (s): 4.053 samples/sec: 31.581
g0133:  iteration     7080/10000000 | consumed samples:       906240 | consumed tokens:   1855979520 | elapsed time per iteration (ms): 4085.1 | learning rate: 1.237E-04 | global batch size:   128 | lm loss: 1.379117E+00 | loss scale: 524288.0 | grad norm: 0.359 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.333 | tokens per gpu per second (tgs): 2005.330 | TFLOPs: 16.14 |
g0113: [2024-08-03 03:06:27,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=7090, skipped=4, lr=[0.00012388925440000003, 0.00012388925440000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7090 loss: 1.3709 iter time (s): 4.151 samples/sec: 30.838
g0133:  iteration     7090/10000000 | consumed samples:       907520 | consumed tokens:   1858600960 | elapsed time per iteration (ms): 4183.0 | learning rate: 1.239E-04 | global batch size:   128 | lm loss: 1.377384E+00 | loss scale: 524288.0 | grad norm: 0.381 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.600 | tokens per gpu per second (tgs): 1958.398 | TFLOPs: 15.76 |
g0113: [2024-08-03 03:07:09,076] [INFO] [logging.py:96:log_dist] [Rank 0] step=7100, skipped=4, lr=[0.0001240640170666667, 0.0001240640170666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7100 loss: 1.3920 iter time (s): 4.087 samples/sec: 31.319
g0133:  iteration     7100/10000000 | consumed samples:       908800 | consumed tokens:   1861222400 | elapsed time per iteration (ms): 4118.9 | learning rate: 1.241E-04 | global batch size:   128 | lm loss: 1.367931E+00 | loss scale: 524288.0 | grad norm: 0.379 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.076 | tokens per gpu per second (tgs): 1988.857 | TFLOPs: 16.00 |
g0113: [2024-08-03 03:07:50,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=7110, skipped=4, lr=[0.00012423877973333336, 0.00012423877973333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7110 loss: 1.3873 iter time (s): 4.122 samples/sec: 31.056
g0133:  iteration     7110/10000000 | consumed samples:       910080 | consumed tokens:   1863843840 | elapsed time per iteration (ms): 4153.8 | learning rate: 1.242E-04 | global batch size:   128 | lm loss: 1.386093E+00 | loss scale: 524288.0 | grad norm: 0.389 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.815 | tokens per gpu per second (tgs): 1972.181 | TFLOPs: 15.87 |
g0113: [2024-08-03 03:08:33,342] [INFO] [logging.py:96:log_dist] [Rank 0] step=7120, skipped=4, lr=[0.00012441354240000003, 0.00012441354240000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7120 loss: 1.4118 iter time (s): 4.240 samples/sec: 30.188
g0133:  iteration     7120/10000000 | consumed samples:       911360 | consumed tokens:   1866465280 | elapsed time per iteration (ms): 4272.8 | learning rate: 1.244E-04 | global batch size:   128 | lm loss: 1.381188E+00 | loss scale: 524288.0 | grad norm: 0.356 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.957 | tokens per gpu per second (tgs): 1917.232 | TFLOPs: 15.43 |
g0113: [2024-08-03 03:09:15,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=7130, skipped=4, lr=[0.0001245883050666667, 0.0001245883050666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7130 loss: 1.3742 iter time (s): 4.174 samples/sec: 30.668
g0133:  iteration     7130/10000000 | consumed samples:       912640 | consumed tokens:   1869086720 | elapsed time per iteration (ms): 4208.3 | learning rate: 1.246E-04 | global batch size:   128 | lm loss: 1.382109E+00 | loss scale: 524288.0 | grad norm: 0.367 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.416 | tokens per gpu per second (tgs): 1946.643 | TFLOPs: 15.66 |
g0113: [2024-08-03 03:09:57,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=7140, skipped=4, lr=[0.00012476306773333333, 0.00012476306773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7140 loss: 1.3747 iter time (s): 4.126 samples/sec: 31.024
g0133:  iteration     7140/10000000 | consumed samples:       913920 | consumed tokens:   1871708160 | elapsed time per iteration (ms): 4158.3 | learning rate: 1.248E-04 | global batch size:   128 | lm loss: 1.383935E+00 | loss scale: 524288.0 | grad norm: 0.372 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.782 | tokens per gpu per second (tgs): 1970.018 | TFLOPs: 15.85 |
g0113: [2024-08-03 03:10:38,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=7150, skipped=4, lr=[0.0001249378304, 0.0001249378304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7150 loss: 1.3828 iter time (s): 4.100 samples/sec: 31.221
g0133:  iteration     7150/10000000 | consumed samples:       915200 | consumed tokens:   1874329600 | elapsed time per iteration (ms): 4132.3 | learning rate: 1.249E-04 | global batch size:   128 | lm loss: 1.360873E+00 | loss scale: 524288.0 | grad norm: 0.355 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.976 | tokens per gpu per second (tgs): 1982.433 | TFLOPs: 15.95 |
g0113: [2024-08-03 03:11:18,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=7160, skipped=4, lr=[0.00012511259306666666, 0.00012511259306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7160 loss: 1.3226 iter time (s): 4.011 samples/sec: 31.908
g0133:  iteration     7160/10000000 | consumed samples:       916480 | consumed tokens:   1876951040 | elapsed time per iteration (ms): 4043.6 | learning rate: 1.251E-04 | global batch size:   128 | lm loss: 1.342091E+00 | loss scale: 524288.0 | grad norm: 0.346 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.655 | tokens per gpu per second (tgs): 2025.920 | TFLOPs: 16.30 |
g0113: [2024-08-03 03:11:58,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=7170, skipped=4, lr=[0.00012528735573333333, 0.00012528735573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7170 loss: 1.3768 iter time (s): 3.943 samples/sec: 32.466
g0133:  iteration     7170/10000000 | consumed samples:       917760 | consumed tokens:   1879572480 | elapsed time per iteration (ms): 3975.1 | learning rate: 1.253E-04 | global batch size:   128 | lm loss: 1.348607E+00 | loss scale: 524288.0 | grad norm: 0.371 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.200 | tokens per gpu per second (tgs): 2060.818 | TFLOPs: 16.58 |
g0113: [2024-08-03 03:12:40,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=7180, skipped=4, lr=[0.0001254621184, 0.0001254621184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7180 loss: 1.3097 iter time (s): 4.195 samples/sec: 30.514
g0133:  iteration     7180/10000000 | consumed samples:       919040 | consumed tokens:   1882193920 | elapsed time per iteration (ms): 4227.3 | learning rate: 1.255E-04 | global batch size:   128 | lm loss: 1.355286E+00 | loss scale: 524288.0 | grad norm: 0.366 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.280 | tokens per gpu per second (tgs): 1937.890 | TFLOPs: 15.59 |
g0113: [2024-08-03 03:13:23,357] [INFO] [logging.py:96:log_dist] [Rank 0] step=7190, skipped=4, lr=[0.00012563688106666666, 0.00012563688106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7190 loss: 1.3519 iter time (s): 4.223 samples/sec: 30.309
g0133:  iteration     7190/10000000 | consumed samples:       920320 | consumed tokens:   1884815360 | elapsed time per iteration (ms): 4256.5 | learning rate: 1.256E-04 | global batch size:   128 | lm loss: 1.361371E+00 | loss scale: 524288.0 | grad norm: 0.368 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.072 | tokens per gpu per second (tgs): 1924.602 | TFLOPs: 15.49 |
g0113: [2024-08-03 03:14:05,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=7200, skipped=4, lr=[0.00012581164373333333, 0.00012581164373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7200 loss: 1.3293 iter time (s): 4.160 samples/sec: 30.769
g0133:  iteration     7200/10000000 | consumed samples:       921600 | consumed tokens:   1887436800 | elapsed time per iteration (ms): 4193.3 | learning rate: 1.258E-04 | global batch size:   128 | lm loss: 1.351349E+00 | loss scale: 524288.0 | grad norm: 0.351 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.525 | tokens per gpu per second (tgs): 1953.596 | TFLOPs: 15.72 |
g0113: [2024-08-03 03:14:48,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=7210, skipped=4, lr=[0.0001259864064, 0.0001259864064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7210 loss: 1.3309 iter time (s): 4.272 samples/sec: 29.962
g0133:  iteration     7210/10000000 | consumed samples:       922880 | consumed tokens:   1890058240 | elapsed time per iteration (ms): 4304.3 | learning rate: 1.260E-04 | global batch size:   128 | lm loss: 1.367499E+00 | loss scale: 524288.0 | grad norm: 0.367 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.738 | tokens per gpu per second (tgs): 1903.222 | TFLOPs: 15.32 |
g0113: [2024-08-03 03:15:30,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=7220, skipped=4, lr=[0.00012616116906666666, 0.00012616116906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7220 loss: 1.3581 iter time (s): 4.158 samples/sec: 30.787
g0133:  iteration     7220/10000000 | consumed samples:       924160 | consumed tokens:   1892679680 | elapsed time per iteration (ms): 4191.3 | learning rate: 1.262E-04 | global batch size:   128 | lm loss: 1.354105E+00 | loss scale: 524288.0 | grad norm: 0.354 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.539 | tokens per gpu per second (tgs): 1954.522 | TFLOPs: 15.73 |
g0113: [2024-08-03 03:16:11,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=7230, skipped=4, lr=[0.00012633593173333333, 0.00012633593173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7230 loss: 1.3284 iter time (s): 4.068 samples/sec: 31.461
g0133:  iteration     7230/10000000 | consumed samples:       925440 | consumed tokens:   1895301120 | elapsed time per iteration (ms): 4101.0 | learning rate: 1.263E-04 | global batch size:   128 | lm loss: 1.337481E+00 | loss scale: 524288.0 | grad norm: 0.345 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.212 | tokens per gpu per second (tgs): 1997.562 | TFLOPs: 16.07 |
g0113: [2024-08-03 03:16:53,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=7240, skipped=4, lr=[0.0001265106944, 0.0001265106944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7240 loss: 1.3269 iter time (s): 4.177 samples/sec: 30.642
g0133:  iteration     7240/10000000 | consumed samples:       926720 | consumed tokens:   1897922560 | elapsed time per iteration (ms): 4210.6 | learning rate: 1.265E-04 | global batch size:   128 | lm loss: 1.336075E+00 | loss scale: 524288.0 | grad norm: 0.381 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.400 | tokens per gpu per second (tgs): 1945.584 | TFLOPs: 15.66 |
g0113: [2024-08-03 03:17:35,801] [INFO] [logging.py:96:log_dist] [Rank 0] step=7250, skipped=4, lr=[0.00012668545706666666, 0.00012668545706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7250 loss: 1.3269 iter time (s): 4.211 samples/sec: 30.396
g0133:  iteration     7250/10000000 | consumed samples:       928000 | consumed tokens:   1900544000 | elapsed time per iteration (ms): 4243.9 | learning rate: 1.267E-04 | global batch size:   128 | lm loss: 1.350373E+00 | loss scale: 524288.0 | grad norm: 0.394 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.161 | tokens per gpu per second (tgs): 1930.288 | TFLOPs: 15.53 |
g0113: [2024-08-03 03:18:17,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=7260, skipped=4, lr=[0.00012686021973333332, 0.00012686021973333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7260 loss: 1.3348 iter time (s): 4.101 samples/sec: 31.211
g0133:  iteration     7260/10000000 | consumed samples:       929280 | consumed tokens:   1903165440 | elapsed time per iteration (ms): 4133.4 | learning rate: 1.269E-04 | global batch size:   128 | lm loss: 1.327709E+00 | loss scale: 524288.0 | grad norm: 0.346 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.967 | tokens per gpu per second (tgs): 1981.906 | TFLOPs: 15.95 |
g0113: [2024-08-03 03:18:58,364] [INFO] [logging.py:96:log_dist] [Rank 0] step=7270, skipped=4, lr=[0.0001270349824, 0.0001270349824], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7270 loss: 1.3182 iter time (s): 4.088 samples/sec: 31.310
g0133:  iteration     7270/10000000 | consumed samples:       930560 | consumed tokens:   1905786880 | elapsed time per iteration (ms): 4122.9 | learning rate: 1.270E-04 | global batch size:   128 | lm loss: 1.337040E+00 | loss scale: 524288.0 | grad norm: 0.336 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.046 | tokens per gpu per second (tgs): 1986.963 | TFLOPs: 15.99 |
g0113: [2024-08-03 03:19:40,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=7280, skipped=4, lr=[0.00012720974506666666, 0.00012720974506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7280 loss: 1.3457 iter time (s): 4.132 samples/sec: 30.974
g0133:  iteration     7280/10000000 | consumed samples:       931840 | consumed tokens:   1908408320 | elapsed time per iteration (ms): 4165.1 | learning rate: 1.272E-04 | global batch size:   128 | lm loss: 1.340683E+00 | loss scale: 524288.0 | grad norm: 0.351 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.732 | tokens per gpu per second (tgs): 1966.838 | TFLOPs: 15.83 |
g0113: [2024-08-03 03:20:21,479] [INFO] [logging.py:96:log_dist] [Rank 0] step=7290, skipped=4, lr=[0.00012738450773333332, 0.00012738450773333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7290 loss: 1.3476 iter time (s): 4.114 samples/sec: 31.113
g0133:  iteration     7290/10000000 | consumed samples:       933120 | consumed tokens:   1911029760 | elapsed time per iteration (ms): 4146.3 | learning rate: 1.274E-04 | global batch size:   128 | lm loss: 1.347730E+00 | loss scale: 524288.0 | grad norm: 0.370 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.871 | tokens per gpu per second (tgs): 1975.717 | TFLOPs: 15.90 |
g0113: [2024-08-03 03:21:01,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=7300, skipped=4, lr=[0.0001275592704, 0.0001275592704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7300 loss: 1.3084 iter time (s): 4.004 samples/sec: 31.965
g0133:  iteration     7300/10000000 | consumed samples:       934400 | consumed tokens:   1913651200 | elapsed time per iteration (ms): 4036.8 | learning rate: 1.276E-04 | global batch size:   128 | lm loss: 1.340384E+00 | loss scale: 524288.0 | grad norm: 0.345 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.708 | tokens per gpu per second (tgs): 2029.310 | TFLOPs: 16.33 |
g0113: [2024-08-03 03:21:43,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=7310, skipped=4, lr=[0.00012773403306666665, 0.00012773403306666665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7310 loss: 1.3514 iter time (s): 4.110 samples/sec: 31.147
g0133:  iteration     7310/10000000 | consumed samples:       935680 | consumed tokens:   1916272640 | elapsed time per iteration (ms): 4141.8 | learning rate: 1.277E-04 | global batch size:   128 | lm loss: 1.338392E+00 | loss scale: 524288.0 | grad norm: 0.343 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.904 | tokens per gpu per second (tgs): 1977.865 | TFLOPs: 15.92 |
g0113: [2024-08-03 03:22:24,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=7320, skipped=4, lr=[0.00012790879573333332, 0.00012790879573333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7320 loss: 1.4010 iter time (s): 4.088 samples/sec: 31.313
g0133:  iteration     7320/10000000 | consumed samples:       936960 | consumed tokens:   1918894080 | elapsed time per iteration (ms): 4121.5 | learning rate: 1.279E-04 | global batch size:   128 | lm loss: 1.339063E+00 | loss scale: 524288.0 | grad norm: 0.344 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.057 | tokens per gpu per second (tgs): 1987.623 | TFLOPs: 15.99 |
g0113: [2024-08-03 03:23:07,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=7330, skipped=4, lr=[0.00012808355839999999, 0.00012808355839999999], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7330 loss: 1.3733 iter time (s): 4.248 samples/sec: 30.132
g0133:  iteration     7330/10000000 | consumed samples:       938240 | consumed tokens:   1921515520 | elapsed time per iteration (ms): 4280.5 | learning rate: 1.281E-04 | global batch size:   128 | lm loss: 1.346917E+00 | loss scale: 524288.0 | grad norm: 0.345 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.903 | tokens per gpu per second (tgs): 1913.810 | TFLOPs: 15.40 |
g0113: [2024-08-03 03:23:49,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=7340, skipped=4, lr=[0.00012825832106666668, 0.00012825832106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7340 loss: 1.3219 iter time (s): 4.150 samples/sec: 30.840
g0133:  iteration     7340/10000000 | consumed samples:       939520 | consumed tokens:   1924136960 | elapsed time per iteration (ms): 4183.1 | learning rate: 1.283E-04 | global batch size:   128 | lm loss: 1.323474E+00 | loss scale: 524288.0 | grad norm: 0.344 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.599 | tokens per gpu per second (tgs): 1958.333 | TFLOPs: 15.76 |
g0113: [2024-08-03 03:24:30,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=7350, skipped=4, lr=[0.00012843308373333335, 0.00012843308373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7350 loss: 1.3351 iter time (s): 4.153 samples/sec: 30.820
g0133:  iteration     7350/10000000 | consumed samples:       940800 | consumed tokens:   1926758400 | elapsed time per iteration (ms): 4185.5 | learning rate: 1.284E-04 | global batch size:   128 | lm loss: 1.325922E+00 | loss scale: 524288.0 | grad norm: 0.352 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.582 | tokens per gpu per second (tgs): 1957.235 | TFLOPs: 15.75 |
g0129: [2024-08-03 03:24:43,652] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: Grad overflow on iteration 7352
g0129: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: [2024-08-03 03:24:43,652] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: Grad overflow on iteration 7352
g0129: Grad overflow on iteration 7352
g0129: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: Grad overflow on iteration 7352
g0129: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0129: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: [2024-08-03 03:24:43,652] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 7352
g0132: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: Grad overflow on iteration 7352
g0132: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: Grad overflow on iteration 7352
g0132: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-03 03:24:43,652] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 7352
g0113: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: Grad overflow on iteration 7352
g0113: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: Grad overflow on iteration 7352
g0113: Grad overflow on iteration 7352
g0129: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0132: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: Grad overflow on iteration 7352
g0113: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0129: Grad overflow on iteration 7352
g0125: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 7352
g0129: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0129: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0125: Grad overflow on iteration 7352
g0125: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0125: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0132: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0130: [2024-08-03 03:24:43,652] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 7352
g0128: Grad overflow on iteration 7352
g0130: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 7352
g0130: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: Grad overflow on iteration 7352
g0131: [2024-08-03 03:24:43,652] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 7352
g0131: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 7352
g0131: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: Grad overflow on iteration 7352
g0133: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 7352
g0131: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0131: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0128: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0113: Grad overflow on iteration 7352
g0128: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0113: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0128: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0133: Grad overflow on iteration 7352
g0133: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0130: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0132: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0130: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0113: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0113: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0130: Grad overflow on iteration 7352
g0113: [2024-08-03 03:24:43,653] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0130: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0130: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0131: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0131: Grad overflow on iteration 7352
g0131: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0132: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0131: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0128: Grad overflow on iteration 7352
g0133: Grad overflow on iteration 7352
g0128: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0128: Grad overflow on iteration 7352
g0128: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0133: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: Grad overflow on iteration 7352
g0133: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0133: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:344:_update_scale] 
g0133: Grad overflow on iteration 7352
g0133: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0133: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0128: [2024-08-03 03:24:43,653] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0133: [2024-08-03 03:24:43,654] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0113: [2024-08-03 03:25:12,364] [INFO] [logging.py:96:log_dist] [Rank 0] step=7360, skipped=5, lr=[0.0001286078464, 0.0001286078464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7360 loss: 1.3134 iter time (s): 4.107 samples/sec: 31.169
g0133:  iteration     7360/10000000 | consumed samples:       942080 | consumed tokens:   1929379840 | elapsed time per iteration (ms): 4139.1 | learning rate: 1.286E-04 | global batch size:   128 | lm loss: 1.323603E+00 | loss scale: 262144.0 | grad norm: 0.378 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.925 | tokens per gpu per second (tgs): 1979.193 | TFLOPs: 15.93 |
g0113: [2024-08-03 03:25:53,777] [INFO] [logging.py:96:log_dist] [Rank 0] step=7370, skipped=5, lr=[0.00012878260906666668, 0.00012878260906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7370 loss: 1.3390 iter time (s): 4.108 samples/sec: 31.157
g0133:  iteration     7370/10000000 | consumed samples:       943360 | consumed tokens:   1932001280 | elapsed time per iteration (ms): 4141.3 | learning rate: 1.288E-04 | global batch size:   128 | lm loss: 1.337526E+00 | loss scale: 262144.0 | grad norm: 0.354 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.908 | tokens per gpu per second (tgs): 1978.117 | TFLOPs: 15.92 |
g0113: [2024-08-03 03:26:36,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=7380, skipped=5, lr=[0.00012895737173333334, 0.00012895737173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0113: steps: 7380 loss: 1.3119 iter time (s): 4.243 samples/sec: 30.168
g0133:  iteration     7380/10000000 | consumed samples:       944640 | consumed tokens:   1934622720 | elapsed time per iteration (ms): 4275.3 | learning rate: 1.290E-04 | global batch size:   128 | lm loss: 1.333123E+00 | loss scale: 262144.0 | grad norm: 0.348 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.940 | tokens per gpu per second (tgs): 1916.132 | TFLOPs: 15.42 |
