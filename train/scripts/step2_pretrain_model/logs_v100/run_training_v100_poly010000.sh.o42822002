
ssh_config_file = /home/acf16449gb/.ssh/config

SSH configuration has been updated.
Host g0220
    HostName g0220
    Port 2222
    StrictHostKeyChecking no

Host g0225
    HostName g0225
    Port 2222
    StrictHostKeyChecking no

Host g0233
    HostName g0233
    Port 2222
    StrictHostKeyChecking no

Host g0234
    HostName g0234
    Port 2222
    StrictHostKeyChecking no

Host g0235
    HostName g0235
    Port 2222
    StrictHostKeyChecking no

Host g0236
    HostName g0236
    Port 2222
    StrictHostKeyChecking no

Host g0237
    HostName g0237
    Port 2222
    StrictHostKeyChecking no

Host g0238
    HostName g0238
    Port 2222
    StrictHostKeyChecking no



ucllm_nedo_dev_train_dir = /home/acf16449gb/ucllm_nedo_prod/train
megatron_deepspeed_dir = /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed

input_tokenizer_file = /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
output_model_dir = /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True
save_interval = 1000
wandb_entity = yohei-kobashi
wandb_project = encrypted_data_LLM
wandb_tag = other_gpu

Number of GPUs per node: 4
Both /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document.bin and /groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document.idx already exist.

hostfile = ./abci_node-8_gpu-32-v100/hostfile_jobid-42822002
g0220 slots=4
g0225 slots=4
g0233 slots=4
g0234 slots=4
g0235 slots=4
g0236 slots=4
g0237 slots=4
g0238 slots=4

[2024-08-09 11:57:22,922] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-08-09 11:57:28,443] [INFO] [runner.py:463:main] Using IP address of 10.1.7.16 for node g0220
[2024-08-09 11:57:28,445] [INFO] [multinode_runner.py:72:get_cmd] Running on the following workers: g0220,g0225,g0233,g0234,g0235,g0236,g0237,g0238
[2024-08-09 11:57:28,446] [INFO] [runner.py:570:main] cmd = pdsh -S -f 1024 -w g0220,g0225,g0233,g0234,g0235,g0236,g0237,g0238 export PYTHONPATH=/home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model;  cd /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model; /home/acf16449gb/crypto_llm/train/.venv_train/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJnMDIyMCI6IFswLCAxLCAyLCAzXSwgImcwMjI1IjogWzAsIDEsIDIsIDNdLCAiZzAyMzMiOiBbMCwgMSwgMiwgM10sICJnMDIzNCI6IFswLCAxLCAyLCAzXSwgImcwMjM1IjogWzAsIDEsIDIsIDNdLCAiZzAyMzYiOiBbMCwgMSwgMiwgM10sICJnMDIzNyI6IFswLCAxLCAyLCAzXSwgImcwMjM4IjogWzAsIDEsIDIsIDNdfQ== --node_rank=%n --master_addr=10.1.7.16 --master_port=29500 /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/pretrain_gpt.py --override-opt_param-scheduler --optimizer 'adam' --adam-beta1 '0.9' --adam-beta2 '0.95' --tensor-model-parallel-size '1' --init-method-std '0.013' --lr-decay-tokens '300000000000' --lr-warmup-tokens '3000000000' --micro-batch-size '1' --exit-duration-in-mins '30000000' --global-batch-size '128' --num-layers '22' --hidden-size '2048' --ffn-hidden-size '5632' --num-attention-heads '16' --num-key-value-heads '4' --no-query-key-layer-scaling --attention-dropout '0' --hidden-dropout '0' --use-rotary-position-embeddings --untie-embeddings-and-output-weights --swiglu --normalization 'rmsnorm' --disable-bias-linear --seq-length '2048' --max-position-embeddings '2048' --train-tokens '2621440000000' --train-samples '1280000000' --train-data-exact-num-epochs '1' --lr '2.0e-4' --min-lr '1.0e-5' --lr-decay-style 'cosine' --split '949,50,1' --log-interval '10' --eval-interval '1000' --eval-iters '100' --save-interval '1000' --weight-decay '0.1' --clip-grad '1.0' --hysteresis '2' --num-workers '0' --seed '1234' --load '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --save '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase' --no-async-tensor-model-parallel-allreduce --tensorboard-queue-size '1' --log-timers-to-tensorboard --log-batch-size-to-tensorboard --log-validation-ppl-to-tensorboard --tensorboard-dir '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True' --log-optimizer-states-to-tensorboard --tokenizer-type 'SentencePieceTokenizer' --tokenizer-model '/groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model' --data-path '/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document' --data-impl 'mmap' --deepspeed --deepspeed_config '/groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json' --zero-stage '0' --pipeline-model-parallel-size '8' --use_wandb --wandb_entity 'yohei-kobashi' --wandb_project 'encrypted_data_LLM' --wandb_group 'pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True' --wandb_tag 'other_gpu'
g0220: [2024-08-09 11:57:31,964] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0220: [2024-08-09 11:57:34,219] [INFO] [launch.py:138:main] 0 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0220: [2024-08-09 11:57:34,219] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3], 'g0237': [0, 1, 2, 3], 'g0238': [0, 1, 2, 3]}
g0220: [2024-08-09 11:57:34,219] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=0
g0220: [2024-08-09 11:57:34,219] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0220': [0, 1, 2, 3], 'g0225': [4, 5, 6, 7], 'g0233': [8, 9, 10, 11], 'g0234': [12, 13, 14, 15], 'g0235': [16, 17, 18, 19], 'g0236': [20, 21, 22, 23], 'g0237': [24, 25, 26, 27], 'g0238': [28, 29, 30, 31]})
g0220: [2024-08-09 11:57:34,219] [INFO] [launch.py:163:main] dist_world_size=32
g0220: [2024-08-09 11:57:34,219] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0220: [2024-08-09 11:57:37,642] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0220: [2024-08-09 11:57:37,642] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0220: [2024-08-09 11:57:37,653] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0220: [2024-08-09 11:57:37,745] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0233: [2024-08-09 11:57:39,951] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0236: [2024-08-09 11:57:40,023] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0238: [2024-08-09 11:57:40,600] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0234: [2024-08-09 11:57:40,664] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0235: [2024-08-09 11:57:40,852] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0225: [2024-08-09 11:57:40,999] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0237: [2024-08-09 11:57:41,094] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0220: --------------------------------------------------
g0220: DeepSpeed C++/CUDA extension op report
g0220: --------------------------------------------------
g0220: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0220:       runtime if needed. Op compatibility means that your system
g0220:       meet the required dependencies to JIT install the op.
g0220: --------------------------------------------------
g0220: JIT compiled ops requires ninja
g0220: --------------------------------------------------
g0220: DeepSpeed C++/CUDA extension op report
g0220: --------------------------------------------------
g0220: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0220:       runtime if needed. Op compatibility means that your system
g0220:       meet the required dependencies to JIT install the op.
g0220: --------------------------------------------------
g0220: JIT compiled ops requires ninja
g0220: --------------------------------------------------
g0220: DeepSpeed C++/CUDA extension op report
g0220: --------------------------------------------------
g0220: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0220:       runtime if needed. Op compatibility means that your system
g0220:       meet the required dependencies to JIT install the op.
g0220: --------------------------------------------------
g0220: JIT compiled ops requires ninja
g0220: --------------------------------------------------
g0220: DeepSpeed C++/CUDA extension op report
g0220: --------------------------------------------------
g0220: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0220:       runtime if needed. Op compatibility means that your system
g0220:       meet the required dependencies to JIT install the op.
g0220: --------------------------------------------------
g0220: JIT compiled ops requires ninja
g0220: ninjaninja ninjaninja ....................................    [92m[OKAY][0m....................................[92m[OKAY][0m
g0220:   
g0220: --------------------------------------------------[92m[OKAY][0m[92m[OKAY][0m--------------------------------------------------
g0220: 
g0220: 
g0220: 
g0220: op name--------------------------------------------------op name --------------------------------------------------................
g0220:  
g0220:  ................op nameinstalled  installedop name ................  installed.. ..   ..compatible................compatible
g0220:   
g0220: compatible--------------------------------------------------installed--------------------------------------------------
g0220: 
g0220: 
g0220:  --------------------------------------------------
g0220: .. compatible
g0220: --------------------------------------------------
g0220: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: async_iocpu_adagrad  ............ ...............[92m[YES][0m  [92m[YES][0m......  [92m[OKAY][0m......
g0220:  [92m[OKAY][0mcpu_lion
g0220:  ............... [92m[YES][0m ...... [92m[OKAY][0mfused_adam
g0220:  ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0220: cpu_adamevoformer_attn  ........................  [93m[NO][0m[92m[YES][0m  ....... ......[93m[NO][0m 
g0220: [92m[OKAY][0m
g0220: fused_lamb ............. cpu_adagrad[92m[YES][0m  .................. [92m[OKAY][0m 
g0220: [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_lion .............cpu_lion [92m[YES][0m  .....................  [92m[OKAY][0m[92m[YES][0m
g0220:  ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0220: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0220: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0220: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0220: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0220: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0220: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0220: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0220: inference_core_ops inference_core_ops.....  [92m[YES][0m.....  ......[92m[YES][0m  [92m[OKAY][0m......
g0220:  [92m[OKAY][0m
g0220: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: cutlass_ops ............cutlass_ops  [92m[YES][0m............  ......[92m[YES][0m  [92m[OKAY][0m......
g0220:  [92m[OKAY][0m
g0220: quantizer quantizer..............  ..............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0220: [92m[OKAY][0m
g0220: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0mragged_device_ops
g0220:  ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0220: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0220: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0220: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0220: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0220: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0220: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0220: ragged_opsragged_ops  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0220: 
g0220: random_ltdrandom_ltd  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0220: 
g0220: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0220: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0220: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0220: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0220: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0220: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0220: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0220: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0220: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0220: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0220: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0220: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0220: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0220: --------------------------------------------------
g0220: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0220: --------------------------------------------------
g0220: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0220: --------------------------------------------------
g0220: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0220: --------------------------------------------------
g0220: DeepSpeed general environment info:
g0220: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0220: torch version .................... 2.0.1+cu118
g0220: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0220: deepspeed info ................... 0.12.4, unknown, unknown
g0220: torch cuda version ............... 11.8
g0220: torch hip version ................ None
g0220: nvcc version ..................... 11.8
g0220: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0220: shared memory (/dev/shm) size .... 188.13 GB
g0220: DeepSpeed general environment info:
g0220: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0220: torch version .................... 2.0.1+cu118
g0220: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0220: deepspeed info ................... 0.12.4, unknown, unknown
g0220: torch cuda version ............... 11.8
g0220: torch hip version ................ None
g0220: nvcc version ..................... 11.8
g0220: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0220: shared memory (/dev/shm) size .... 188.13 GB
g0220: DeepSpeed general environment info:
g0220: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0220: torch version .................... 2.0.1+cu118
g0220: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0220: deepspeed info ................... 0.12.4, unknown, unknown
g0220: torch cuda version ............... 11.8
g0220: torch hip version ................ None
g0220: nvcc version ..................... 11.8
g0220: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0220: shared memory (/dev/shm) size .... 188.13 GB
g0220: DeepSpeed general environment info:
g0220: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0220: torch version .................... 2.0.1+cu118
g0220: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0220: deepspeed info ................... 0.12.4, unknown, unknown
g0220: torch cuda version ............... 11.8
g0220: torch hip version ................ None
g0220: nvcc version ..................... 11.8
g0220: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0220: shared memory (/dev/shm) size .... 188.13 GB
g0220: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0220: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0220: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0220: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0220: using world size: 32, data-parallel-size: 4, sequence-parallel size: 1, tensor-model-parallel size: 1, pipeline-model-parallel size: 8 
g0220: WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:SentencePieceTokenizer
g0220: using torch.float32 for parameters ...
g0220: ------------------------ arguments ------------------------
g0220:   accumulate_allreduce_grads_in_fp32 .............. False
g0220:   adam_beta1 ...................................... 0.9
g0220:   adam_beta2 ...................................... 0.95
g0220:   adam_eps ........................................ 1e-08
g0220:   add_bias_linear ................................. False
g0220:   add_position_embedding .......................... False
g0220:   adlr_autoresume ................................. False
g0220:   adlr_autoresume_interval ........................ 1000
g0220:   aml_data_download_path .......................... None
g0220:   apply_layernorm_1p .............................. False
g0220:   apply_query_key_layer_scaling ................... False
g0220:   apply_residual_connection_post_layernorm ........ False
g0220:   async_tensor_model_parallel_allreduce ........... False
g0220:   attention_dropout ............................... 0.0
g0220:   attention_softmax_in_fp32 ....................... False
g0220:   barrier_with_L1_time ............................ True
g0220:   bert_binary_head ................................ True
g0220:   bert_embedder_type .............................. megatron
g0220:   bert_load ....................................... None
g0220:   bf16 ............................................ False
g0220:   bias_dropout_fusion ............................. True
g0220:   bias_gelu_fusion ................................ False
g0220:   biencoder_projection_dim ........................ 0
g0220:   biencoder_shared_query_context_model ............ False
g0220:   block_data_path ................................. None
g0220:   checkpoint_activations .......................... False
g0220:   checkpoint_in_cpu ............................... False
g0220:   checkpoint_num_layers ........................... 1
g0220:   classes_fraction ................................ 1.0
g0220:   clip_grad ....................................... 1.0
g0220:   compression_training ............................ False
g0220:   consumed_train_samples .......................... 0
g0220:   consumed_train_tokens ........................... 0
g0220:   consumed_valid_samples .......................... 0
g0220:   contigious_checkpointing ........................ False
g0220:   cpu_optimizer ................................... False
g0220:   cpu_torch_adam .................................. False
g0220:   create_moe_param_group .......................... False
g0220:   curriculum_learning_legacy ...................... False
g0220:   data_cache_path ................................. None
g0220:   data_efficiency_curriculum_learning ............. False
g0220:   data_impl ....................................... mmap
g0220:   data_parallel_random_init ....................... False
g0220:   data_parallel_size .............................. 4
g0220:   data_path ....................................... ['/groups/gcf51099/crypto_llm/data/wikipedia_latin_poly_010000_1234_True_text_document']
g0220:   data_per_class_fraction ......................... 1.0
g0220:   data_sharding ................................... True
g0220:   dataloader_type ................................. single
g0220:   DDP_impl ........................................ local
g0220:   decoder_num_layers .............................. None
g0220:   decoder_seq_length .............................. None
g0220:   deepscale ....................................... False
g0220:   deepscale_config ................................ None
g0220:   deepspeed ....................................... True
g0220:   deepspeed_activation_checkpointing .............. False
g0220:   deepspeed_config ................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/deepspeed_config/ds_config_gbs128_mbs1_log10_zero0.json
g0220:   deepspeed_mpi ................................... False
g0220:   dino_bottleneck_size ............................ 256
g0220:   dino_freeze_last_layer .......................... 1
g0220:   dino_head_hidden_size ........................... 2048
g0220:   dino_local_crops_number ......................... 10
g0220:   dino_local_img_size ............................. 96
g0220:   dino_norm_last_layer ............................ False
g0220:   dino_teacher_temp ............................... 0.07
g0220:   dino_warmup_teacher_temp ........................ 0.04
g0220:   dino_warmup_teacher_temp_epochs ................. 30
g0220:   distribute_checkpointed_activations ............. False
g0220:   distribute_saved_activations .................... False
g0220:   distributed_backend ............................. nccl
g0220:   distributed_timeout_minutes ..................... 10
g0220:   ds_fused_adam ................................... False
g0220:   ds_inference .................................... False
g0220:   ds_pipeline_enabled ............................. True
g0220:   ds_sequence_parallel_size ....................... 1
g0220:   embedding_path .................................. None
g0220:   embedding_weights_in_fp32 ....................... False
g0220:   empty_unused_memory_level ....................... 0
g0220:   enable_expert_tensor_parallelism ................ False
g0220:   encoder_num_layers .............................. 22
g0220:   encoder_seq_length .............................. 2048
g0220:   end_weight_decay ................................ 0.1
g0220:   eod_mask_loss ................................... False
g0220:   eval_interval ................................... 1000
g0220:   eval_iters ...................................... 100
g0220:   evidence_data_path .............................. None
g0220:   exit_duration_in_mins ........................... 30000000
g0220:   exit_interval ................................... None
g0220:   exit_on_missing_checkpoint ...................... False
g0220:   exit_signal_handler ............................. False
g0220:   expert_interval ................................. 2
g0220:   ffn_hidden_size ................................. 5632
g0220:   finetune ........................................ False
g0220:   force_ds_sequence_parallel ...................... False
g0220:   fp16 ............................................ False
g0220:   fp16_lm_cross_entropy ........................... False
g0220:   fp32_residual_connection ........................ False
g0220:   fp8_amax_compute_algo ........................... most_recent
g0220:   fp8_amax_history_len ............................ 1
g0220:   fp8_e4m3 ........................................ False
g0220:   fp8_hybrid ...................................... False
g0220:   fp8_interval .................................... 1
g0220:   fp8_margin ...................................... 0
g0220:   fp8_wgrad ....................................... True
g0220:   global_batch_size ............................... 128
g0220:   gradient_accumulation_fusion .................... True
g0220:   head_lr_mult .................................... 1.0
g0220:   hidden_dropout .................................. 0.0
g0220:   hidden_size ..................................... 2048
g0220:   hidden_size_teacher ............................. None
g0220:   hysteresis ...................................... 2
g0220:   ict_head_size ................................... None
g0220:   ict_load ........................................ None
g0220:   img_h ........................................... 224
g0220:   img_w ........................................... 224
g0220:   indexer_batch_size .............................. 128
g0220:   indexer_log_interval ............................ 1000
g0220:   inference ....................................... False
g0220:   inference_batch_times_seqlen_threshold .......... 512
g0220:   init_method_std ................................. 0.013
g0220:   init_method_xavier_uniform ...................... False
g0220:   initial_loss_scale .............................. 4294967296
g0220:   iter_per_epoch .................................. 1250
g0220:   kd .............................................. False
g0220:   kd_alpha_ce ..................................... 1
g0220:   kd_beta_ce ...................................... 1
g0220:   kd_temp ......................................... 1.0
g0220:   kv_channels ..................................... 128
g0220:   layernorm_epsilon ............................... 1e-05
g0220:   lazy_mpu_init ................................... None
g0220:   load ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220:   load_teacher .................................... None
g0220:   local_rank ...................................... 0
g0220:   log_batch_size_to_tensorboard ................... True
g0220:   log_interval .................................... 10
g0220:   log_learning_rate_to_tensorboard ................ True
g0220:   log_loss_scale_to_tensorboard ................... True
g0220:   log_memory_to_tensorboard ....................... False
g0220:   log_num_zeros_in_grad ........................... False
g0220:   log_optimizer_states_to_tensorboard ............. True
g0220:   log_params_norm ................................. False
g0220:   log_timers_to_tensorboard ....................... True
g0220:   log_validation_ppl_to_tensorboard ............... True
g0220:   log_world_size_to_tensorboard ................... False
g0220:   loss_scale ...................................... None
g0220:   loss_scale_window ............................... 1000
g0220:   lr .............................................. 0.0002
g0220:   lr_decay_iters .................................. None
g0220:   lr_decay_samples ................................ None
g0220:   lr_decay_style .................................. cosine
g0220:   lr_decay_tokens ................................. 300000000000
g0220:   lr_warmup_fraction .............................. None
g0220:   lr_warmup_iters ................................. 0
g0220:   lr_warmup_samples ............................... 0
g0220:   lr_warmup_tokens ................................ 3000000000
g0220:   make_vocab_size_divisible_by .................... 128
g0220:   mask_factor ..................................... 1.0
g0220:   mask_prob ....................................... 0.15
g0220:   mask_type ....................................... random
g0220:   masked_softmax_fusion ........................... True
g0220:   max_position_embeddings ......................... 2048
g0220:   max_tokens_to_oom ............................... 12000
g0220:   mem_efficient_ln ................................ True
g0220:   memory_centric_tiled_linear ..................... False
g0220:   merge_file ...................................... None
g0220:   micro_batch_size ................................ 1
g0220:   min_loss_scale .................................. 1.0
g0220:   min_lr .......................................... 1e-05
g0220:   mlp_type ........................................ standard
g0220:   mmap_warmup ..................................... False
g0220:   moe_eval_capacity_factor ........................ 1.0
g0220:   moe_expert_parallel_size ........................ 1
g0220:   moe_loss_coeff .................................. 0.1
g0220:   moe_min_capacity ................................ 4
g0220:   moe_token_dropping .............................. True
g0220:   moe_train_capacity_factor ....................... 1.0
g0220:   mos ............................................. False
g0220:   no_load_lr_state ................................ False
g0220:   no_load_optim ................................... None
g0220:   no_load_rng ..................................... None
g0220:   no_persist_layer_norm ........................... False
g0220:   no_pipeline_parallel ............................ False
g0220:   no_save_optim ................................... None
g0220:   no_save_rng ..................................... None
g0220:   normalization ................................... rmsnorm
g0220:   num_attention_heads ............................. 16
g0220:   num_attention_heads_teacher ..................... None
g0220:   num_channels .................................... 3
g0220:   num_classes ..................................... 1000
g0220:   num_experts ..................................... [1]
g0220:   num_experts_switch .............................. None
g0220:   num_experts_teacher ............................. [1]
g0220:   num_key_value_heads ............................. 4
g0220:   num_layers ...................................... 22
g0220:   num_layers_per_virtual_pipeline_stage ........... None
g0220:   num_layers_teacher .............................. None
g0220:   num_workers ..................................... 0
g0220:   onnx_safe ....................................... None
g0220:   openai_gelu ..................................... False
g0220:   optimizer ....................................... adam
g0220:   output_bert_embeddings .......................... False
g0220:   overlap_p2p_comm ................................ False
g0220:   override_opt_param_scheduler .................... True
g0220:   params_dtype .................................... torch.float32
g0220:   partition_activations ........................... False
g0220:   patch_dim ....................................... 16
g0220:   perform_initialization .......................... True
g0220:   pipeline_model_parallel_size .................... 8
g0220:   pipeline_model_parallel_split_rank .............. None
g0220:   profile_backward ................................ False
g0220:   query_in_block_prob ............................. 0.1
g0220:   rampup_batch_size ............................... None
g0220:   random_ltd ...................................... False
g0220:   rank ............................................ 0
g0220:   recompute_granularity ........................... None
g0220:   recompute_method ................................ None
g0220:   recompute_num_layers ............................ 1
g0220:   remote_device ................................... none
g0220:   repeated_dataloader ............................. False
g0220:   reset_attention_mask ............................ False
g0220:   reset_iteration ................................. False
g0220:   reset_position_ids .............................. False
g0220:   retriever_report_topk_accuracies ................ []
g0220:   retriever_score_scaling ......................... False
g0220:   retriever_seq_length ............................ 256
g0220:   retro_add_retriever ............................. False
g0220:   retro_cyclic_train_iters ........................ None
g0220:   retro_encoder_attention_dropout ................. 0.1
g0220:   retro_encoder_hidden_dropout .................... 0.1
g0220:   retro_encoder_layers ............................ 2
g0220:   retro_num_neighbors ............................. 2
g0220:   retro_num_retrieved_chunks ...................... 2
g0220:   retro_return_doc_ids ............................ False
g0220:   retro_workdir ................................... None
g0220:   return_data_index ............................... False
g0220:   rotary_percent .................................. 1.0
g0220:   sample_rate ..................................... 1.0
g0220:   save ............................................ /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220:   save_interval ................................... 1000
g0220:   scatter_gather_tensors_in_pipeline .............. True
g0220:   scattered_embeddings ............................ False
g0220:   seed ............................................ 1234
g0220:   seq_length ...................................... 2048
g0220:   sequence_parallel ............................... False
g0220:   sgd_momentum .................................... 0.9
g0220:   short_seq_prob .................................. 0.1
g0220:   skip_train ...................................... False
g0220:   split ........................................... 949,50,1
g0220:   split_transformers .............................. False
g0220:   squared_relu .................................... False
g0220:   standalone_embedding_stage ...................... False
g0220:   start_weight_decay .............................. 0.1
g0220:   swiglu .......................................... True
g0220:   swin_backbone_type .............................. tiny
g0220:   synchronize_each_layer .......................... False
g0220:   tensor_model_parallel_size ...................... 1
g0220:   tensorboard_dir ................................. /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/tensorboard/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase_0.latin_wikipedia_poly_010000_1234_True
g0220:   tensorboard_log_interval ........................ 1
g0220:   tensorboard_queue_size .......................... 1
g0220:   test_data_path .................................. None
g0220:   tf32 ............................................ False
g0220:   tile_factor ..................................... 1
g0220:   timing_log_level ................................ 0
g0220:   timing_log_option ............................... minmax
g0220:   titles_data_path ................................ None
g0220:   tokenizer_model ................................. /groups/gcf51099/crypto_llm/tokenizers/tokenizer_wikipedia_latin_poly_010000_1234_True.model
g0220:   tokenizer_type .................................. SentencePieceTokenizer
g0220:   topk ............................................ 1
g0220:   train_data_exact_num_epochs ..................... 1
g0220:   train_data_path ................................. None
g0220:   train_desc_path ................................. None
g0220:   train_doc_idx_path .............................. None
g0220:   train_idx_path .................................. None
g0220:   train_iters ..................................... None
g0220:   train_sample_idx_path ........................... None
g0220:   train_samples ................................... 1280000000
g0220:   train_shuffle_idx_path .......................... None
g0220:   train_tokens .................................... 2621440000000
g0220:   transformer_impl ................................ local
g0220:   transformer_pipeline_model_parallel_size ........ 8
g0220:   universal_checkpoint ............................ False
g0220:   untie_embeddings_and_output_weights ............. True
g0220:   use_checkpoint_args ............................. False
g0220:   use_checkpoint_opt_param_scheduler .............. False
g0220:   use_contiguous_buffers_in_local_ddp ............. True
g0220:   use_cpu_initialization .......................... None
g0220:   use_dataset_only ................................ False
g0220:   use_distributed_optimizer ....................... False
g0220:   use_flash_attn .................................. False
g0220:   use_flash_attn_triton ........................... False
g0220:   use_flash_attn_v1 ............................... False
g0220:   use_flash_attn_v2 ............................... False
g0220:   use_one_sent_docs ............................... False
g0220:   use_pin_memory .................................. False
g0220:   use_ring_exchange_p2p ........................... False
g0220:   use_rotary_position_embeddings .................. True
g0220:   use_tutel ....................................... False
g0220:   use_wandb ....................................... True
g0220:   valid_data_path ................................. None
g0220:   variable_seq_lengths ............................ False
g0220:   virtual_pipeline_model_parallel_size ............ None
g0220:   vision_backbone_type ............................ vit
g0220:   vision_pretraining .............................. False
g0220:   vision_pretraining_type ......................... classify
g0220:   vocab_extra_ids ................................. 0
g0220:   vocab_file ...................................... None
g0220:   vocab_size ...................................... None
g0220:   wandb_entity .................................... yohei-kobashi
g0220:   wandb_group ..................................... pretrain_gpt_1.1B_0.latin_wikipedia_poly_010000_1234_True
g0220:   wandb_project ................................... encrypted_data_LLM
g0220:   wandb_tag ....................................... other_gpu
g0220:   weight_decay .................................... 0.1
g0220:   weight_decay_incr_style ......................... constant
g0220:   world_size ...................................... 32
g0220:   zero_allgather_bucket_size ...................... 0.0
g0220:   zero_contigious_gradients ....................... False
g0220:   zero_reduce_bucket_size ......................... 0.0
g0220:   zero_reduce_scatter ............................. False
g0220:   zero_stage ...................................... 0
g0220: -------------------- end of arguments ---------------------
g0220: setting number of micro-batches to constant 32
g0220: > building SentencePieceTokenizer tokenizer ...
g0220: [2024-08-09 11:57:42,559] [INFO] [comm.py:637:init_distributed] cdb=None
g0220:  > padded vocab (size: 32003) with 125 dummy tokens (new size: 32128)
g0220: > initializing torch distributed ...
g0220: [2024-08-09 11:57:42,559] [INFO] [comm.py:637:init_distributed] cdb=None
g0220: [2024-08-09 11:57:42,559] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
g0220: [2024-08-09 11:57:42,561] [INFO] [comm.py:637:init_distributed] cdb=None
g0220: [2024-08-09 11:57:42,563] [INFO] [comm.py:637:init_distributed] cdb=None
g0220: [W socket.cpp:426] [c10d] The server socket cannot be initialized on [::]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [2024-08-09 11:57:43,954] [INFO] [launch.py:138:main] 2 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0233: [2024-08-09 11:57:43,955] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3], 'g0237': [0, 1, 2, 3], 'g0238': [0, 1, 2, 3]}
g0233: [2024-08-09 11:57:43,955] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=2
g0233: [2024-08-09 11:57:43,955] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0220': [0, 1, 2, 3], 'g0225': [4, 5, 6, 7], 'g0233': [8, 9, 10, 11], 'g0234': [12, 13, 14, 15], 'g0235': [16, 17, 18, 19], 'g0236': [20, 21, 22, 23], 'g0237': [24, 25, 26, 27], 'g0238': [28, 29, 30, 31]})
g0233: [2024-08-09 11:57:43,955] [INFO] [launch.py:163:main] dist_world_size=32
g0233: [2024-08-09 11:57:43,955] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0236: [2024-08-09 11:57:44,142] [INFO] [launch.py:138:main] 5 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0236: [2024-08-09 11:57:44,143] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3], 'g0237': [0, 1, 2, 3], 'g0238': [0, 1, 2, 3]}
g0236: [2024-08-09 11:57:44,143] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=5
g0236: [2024-08-09 11:57:44,143] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0220': [0, 1, 2, 3], 'g0225': [4, 5, 6, 7], 'g0233': [8, 9, 10, 11], 'g0234': [12, 13, 14, 15], 'g0235': [16, 17, 18, 19], 'g0236': [20, 21, 22, 23], 'g0237': [24, 25, 26, 27], 'g0238': [28, 29, 30, 31]})
g0236: [2024-08-09 11:57:44,143] [INFO] [launch.py:163:main] dist_world_size=32
g0236: [2024-08-09 11:57:44,143] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0238: [2024-08-09 11:57:44,688] [INFO] [launch.py:138:main] 7 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0238: [2024-08-09 11:57:44,688] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3], 'g0237': [0, 1, 2, 3], 'g0238': [0, 1, 2, 3]}
g0238: [2024-08-09 11:57:44,688] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=7
g0238: [2024-08-09 11:57:44,688] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0220': [0, 1, 2, 3], 'g0225': [4, 5, 6, 7], 'g0233': [8, 9, 10, 11], 'g0234': [12, 13, 14, 15], 'g0235': [16, 17, 18, 19], 'g0236': [20, 21, 22, 23], 'g0237': [24, 25, 26, 27], 'g0238': [28, 29, 30, 31]})
g0238: [2024-08-09 11:57:44,688] [INFO] [launch.py:163:main] dist_world_size=32
g0238: [2024-08-09 11:57:44,688] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0234: [2024-08-09 11:57:44,779] [INFO] [launch.py:138:main] 3 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0234: [2024-08-09 11:57:44,779] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3], 'g0237': [0, 1, 2, 3], 'g0238': [0, 1, 2, 3]}
g0234: [2024-08-09 11:57:44,779] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=3
g0234: [2024-08-09 11:57:44,779] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0220': [0, 1, 2, 3], 'g0225': [4, 5, 6, 7], 'g0233': [8, 9, 10, 11], 'g0234': [12, 13, 14, 15], 'g0235': [16, 17, 18, 19], 'g0236': [20, 21, 22, 23], 'g0237': [24, 25, 26, 27], 'g0238': [28, 29, 30, 31]})
g0234: [2024-08-09 11:57:44,780] [INFO] [launch.py:163:main] dist_world_size=32
g0234: [2024-08-09 11:57:44,780] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0235: [2024-08-09 11:57:45,048] [INFO] [launch.py:138:main] 4 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0235: [2024-08-09 11:57:45,048] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3], 'g0237': [0, 1, 2, 3], 'g0238': [0, 1, 2, 3]}
g0235: [2024-08-09 11:57:45,048] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=4
g0235: [2024-08-09 11:57:45,048] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0220': [0, 1, 2, 3], 'g0225': [4, 5, 6, 7], 'g0233': [8, 9, 10, 11], 'g0234': [12, 13, 14, 15], 'g0235': [16, 17, 18, 19], 'g0236': [20, 21, 22, 23], 'g0237': [24, 25, 26, 27], 'g0238': [28, 29, 30, 31]})
g0235: [2024-08-09 11:57:45,048] [INFO] [launch.py:163:main] dist_world_size=32
g0235: [2024-08-09 11:57:45,048] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0225: [2024-08-09 11:57:45,111] [INFO] [launch.py:138:main] 1 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0225: [2024-08-09 11:57:45,111] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3], 'g0237': [0, 1, 2, 3], 'g0238': [0, 1, 2, 3]}
g0225: [2024-08-09 11:57:45,111] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=1
g0225: [2024-08-09 11:57:45,111] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0220': [0, 1, 2, 3], 'g0225': [4, 5, 6, 7], 'g0233': [8, 9, 10, 11], 'g0234': [12, 13, 14, 15], 'g0235': [16, 17, 18, 19], 'g0236': [20, 21, 22, 23], 'g0237': [24, 25, 26, 27], 'g0238': [28, 29, 30, 31]})
g0225: [2024-08-09 11:57:45,111] [INFO] [launch.py:163:main] dist_world_size=32
g0225: [2024-08-09 11:57:45,111] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0237: [2024-08-09 11:57:45,261] [INFO] [launch.py:138:main] 6 HPCX_NCCL_RDMA_SHARP_PLUGIN_DIR=/apps/rocky8/hpcx/2.12/nccl_rdma_sharp_plugin
g0237: [2024-08-09 11:57:45,262] [INFO] [launch.py:145:main] WORLD INFO DICT: {'g0220': [0, 1, 2, 3], 'g0225': [0, 1, 2, 3], 'g0233': [0, 1, 2, 3], 'g0234': [0, 1, 2, 3], 'g0235': [0, 1, 2, 3], 'g0236': [0, 1, 2, 3], 'g0237': [0, 1, 2, 3], 'g0238': [0, 1, 2, 3]}
g0237: [2024-08-09 11:57:45,262] [INFO] [launch.py:151:main] nnodes=8, num_local_procs=4, node_rank=6
g0237: [2024-08-09 11:57:45,262] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'g0220': [0, 1, 2, 3], 'g0225': [4, 5, 6, 7], 'g0233': [8, 9, 10, 11], 'g0234': [12, 13, 14, 15], 'g0235': [16, 17, 18, 19], 'g0236': [20, 21, 22, 23], 'g0237': [24, 25, 26, 27], 'g0238': [28, 29, 30, 31]})
g0237: [2024-08-09 11:57:45,262] [INFO] [launch.py:163:main] dist_world_size=32
g0237: [2024-08-09 11:57:45,262] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3
g0233: [2024-08-09 11:57:47,087] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0233: [2024-08-09 11:57:47,087] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0233: [2024-08-09 11:57:47,087] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0236: [2024-08-09 11:57:47,294] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0236: [2024-08-09 11:57:47,294] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0236: [2024-08-09 11:57:47,294] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0233: [2024-08-09 11:57:47,313] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0236: [2024-08-09 11:57:47,479] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0238: [2024-08-09 11:57:47,875] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0238: [2024-08-09 11:57:47,875] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0238: [2024-08-09 11:57:47,890] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0238: [2024-08-09 11:57:47,890] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0234: [2024-08-09 11:57:47,901] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0234: [2024-08-09 11:57:47,911] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0234: [2024-08-09 11:57:47,911] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0234: [2024-08-09 11:57:48,122] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0235: [2024-08-09 11:57:48,189] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0235: [2024-08-09 11:57:48,189] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0235: [2024-08-09 11:57:48,190] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0225: [2024-08-09 11:57:48,318] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0225: [2024-08-09 11:57:48,332] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0225: [2024-08-09 11:57:48,350] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0235: [2024-08-09 11:57:48,355] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0225: [2024-08-09 11:57:48,366] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0237: [2024-08-09 11:57:48,496] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0237: [2024-08-09 11:57:48,496] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0237: [2024-08-09 11:57:48,496] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0237: [2024-08-09 11:57:48,599] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
g0233: --------------------------------------------------
g0233: DeepSpeed C++/CUDA extension op report
g0233: --------------------------------------------------
g0233: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0233:       runtime if needed. Op compatibility means that your system
g0233:       meet the required dependencies to JIT install the op.
g0233: --------------------------------------------------
g0233: JIT compiled ops requires ninja
g0233: --------------------------------------------------
g0233: DeepSpeed C++/CUDA extension op report
g0233: --------------------------------------------------
g0233: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0233:       runtime if needed. Op compatibility means that your system
g0233:       meet the required dependencies to JIT install the op.
g0233: --------------------------------------------------
g0233: JIT compiled ops requires ninja
g0233: --------------------------------------------------
g0233: DeepSpeed C++/CUDA extension op report
g0233: --------------------------------------------------
g0233: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0233:       runtime if needed. Op compatibility means that your system
g0233:       meet the required dependencies to JIT install the op.
g0233: --------------------------------------------------
g0233: JIT compiled ops requires ninja
g0233: --------------------------------------------------
g0233: DeepSpeed C++/CUDA extension op report
g0233: --------------------------------------------------
g0233: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0233:       runtime if needed. Op compatibility means that your system
g0233:       meet the required dependencies to JIT install the op.
g0233: --------------------------------------------------
g0233: JIT compiled ops requires ninja
g0233: ninjaninjaninja  ....................................ninja   ..................[92m[OKAY][0m [92m[OKAY][0m 
g0233: ..................
g0233: [92m[OKAY][0m 
g0233: --------------------------------------------------[92m[OKAY][0m--------------------------------------------------
g0233: 
g0233: --------------------------------------------------
g0233: 
g0233: op name--------------------------------------------------op name 
g0233: op name ................ ................op name ................ installed  installed ................installed ..  ..installed..    compatiblecompatible..
g0233: compatible
g0233:  
g0233: --------------------------------------------------compatible----------------------------------------------------------------------------------------------------
g0233: 
g0233: 
g0233: 
g0233: --------------------------------------------------
g0236: ----------------------------------------------------------------------------------------------------
g0236: 
g0236: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0236: 
g0236: --------------------------------------------------
g0236: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0236:       runtime if needed. Op compatibility means that your system
g0236:       meet the required dependencies to JIT install the op.
g0236: 
g0236: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0236:       runtime if needed. Op compatibility means that your system
g0236:       meet the required dependencies to JIT install the op.
g0236: 
g0236: JIT compiled ops requires ninja
g0236: --------------------------------------------------
g0236: JIT compiled ops requires ninja--------------------------------------------------
g0236: 
g0236: DeepSpeed C++/CUDA extension op report
g0236: --------------------------------------------------
g0236: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0236:       runtime if needed. Op compatibility means that your system
g0236:       meet the required dependencies to JIT install the op.
g0236: ----------------------------------------------------------------------------------------------------
g0236: 
g0236: JIT compiled ops requires ninja
g0236: DeepSpeed C++/CUDA extension op report
g0236: --------------------------------------------------
g0236: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0236:       runtime if needed. Op compatibility means that your system
g0236:       meet the required dependencies to JIT install the op.
g0236: --------------------------------------------------
g0236: JIT compiled ops requires ninja
g0236: ninjaninja .................. ninjaninja .................. [92m[OKAY][0m  ..................
g0236: .................. [92m[OKAY][0m [92m[OKAY][0m--------------------------------------------------[92m[OKAY][0m
g0236: 
g0236: 
g0236: 
g0236: --------------------------------------------------op name
g0236: ---------------------------------------------------------------------------------------------------- 
g0236: ................
g0236: op name op name installed ................ op name..................    installedinstalled................ compatible  
g0236: ....installed  --------------------------------------------------compatible compatible
g0236: 
g0236: 
g0236: ..---------------------------------------------------------------------------------------------------- 
g0236: 
g0236: compatible
g0236: --------------------------------------------------
g0233: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0233: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0233: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0233: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0233: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_lion ............. [92m[YES][0m ...... async_io[92m[OKAY][0m
g0233:  ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0233: cpu_adagrad ............ [92m[YES][0m async_io......  [92m[OKAY][0m...............
g0233:  [92m[YES][0m cpu_lion......  ...............[92m[OKAY][0m [92m[YES][0m
g0233:  ...... [92m[OKAY][0m
g0233: fused_adam ............. [92m[YES][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH......
g0233:  [92m[OKAY][0m
g0233: evoformer_attn ......... cpu_adam[93m[NO][0m  ......................  [92m[YES][0m[93m[NO][0m 
g0233: ...... [92m[OKAY][0mfused_lamb
g0233:  ............. [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0m[92m[YES][0m
g0233:  ...... [92m[OKAY][0m
g0233: cpu_lion fused_lion...............  .............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0233: [92m[OKAY][0m
g0233: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0233: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0233: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0236: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0236: async_io ............... [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0236: ...... evoformer_attn[92m[OKAY][0m .........
g0236:  [93m[NO][0m ....... [93m[NO][0m
g0236: fused_adamfused_lamb  ..........................  [92m[YES][0m[92m[YES][0m ......  [92m[OKAY][0m......async_io 
g0236: [92m[OKAY][0m 
g0236: ............... [92m[YES][0m ...... [92m[OKAY][0mcpu_adamfused_lion 
g0236:  ............................  [92m[YES][0m [92m[YES][0m......  fused_adam[92m[OKAY][0m...... 
g0236:  ............. [92m[OKAY][0m[92m[YES][0m 
g0236: ...... async_io[92m[OKAY][0m
g0236:  cpu_adagrad............... cpu_adam  [92m[YES][0m...........................   [92m[YES][0m...... [92m[YES][0m ......[92m[OKAY][0m  
g0236: [92m[OKAY][0m......
g0236:  [92m[OKAY][0mcpu_adagrad
g0236:  fused_adam............  .............[92m[YES][0m  cpu_lion[92m[YES][0m......   ......[92m[OKAY][0m...............
g0236:   [92m[OKAY][0m
g0236: [92m[YES][0mcpu_lion  .....................cpu_adam   [92m[YES][0m............... [92m[OKAY][0m ......[92m[YES][0m
g0236:   [92m[OKAY][0m......
g0236:  [92m[OKAY][0m
g0236: cpu_adagrad[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH ............
g0236:  [92m[YES][0m evoformer_attn...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH .........
g0236: [92m[OKAY][0m 
g0236: evoformer_attn[93m[NO][0m  .........cpu_lion.......   [93m[NO][0m...............[93m[NO][0m  
g0236: .......[92m[YES][0m  [93m[NO][0m......
g0236:  fused_lamb[92m[OKAY][0m
g0236:  fused_lamb ..........................  [92m[YES][0m[92m[YES][0m ...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH ......
g0236: [92m[OKAY][0m 
g0236: [92m[OKAY][0mevoformer_attn 
g0236: ......... [93m[NO][0m ....... [93m[NO][0mfused_lion
g0236:  ............. [92m[YES][0mfused_lamb fused_lion ...................   [92m[OKAY][0m[92m[YES][0m
g0236:  ...................  [92m[OKAY][0m[92m[YES][0m
g0236:  ...... [92m[OKAY][0m
g0236: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: inference_core_ops inference_core_ops.....  [92m[YES][0m.....  ......[92m[YES][0m  [92m[OKAY][0m......
g0233:  [92m[OKAY][0m
g0233: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0233: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0233: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0236: inference_core_ops ..... [92m[YES][0m inference_core_ops......  ..... [92m[OKAY][0m[92m[YES][0m 
g0236: ...... [92m[OKAY][0m
g0236: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0233: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0233: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0233: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0233: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0233: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0233: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0233: sparse_attn ............ [93m[NO][0m .......ragged_ops [93m[NO][0m 
g0233: ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0233: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0233: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0233: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0233: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0233: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0236: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0233: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0233: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0233: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0233: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0233: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0236: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0236: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0236: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0233: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0233: --------------------------------------------------
g0233: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0233: --------------------------------------------------
g0233: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0233: --------------------------------------------------
g0233: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0233: --------------------------------------------------
g0236: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0236: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0236: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0236: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0236: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0236: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0236: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0236: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0236: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0236: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0236: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0233: DeepSpeed general environment info:
g0233: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0233: torch version .................... 2.0.1+cu118
g0233: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0233: deepspeed info ................... 0.12.4, unknown, unknown
g0233: torch cuda version ............... 11.8
g0233: torch hip version ................ None
g0233: nvcc version ..................... 11.8
g0233: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0233: shared memory (/dev/shm) size .... 188.13 GB
g0233: DeepSpeed general environment info:
g0233: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0233: torch version .................... 2.0.1+cu118
g0233: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0233: deepspeed info ................... 0.12.4, unknown, unknown
g0233: torch cuda version ............... 11.8
g0233: torch hip version ................ None
g0233: nvcc version ..................... 11.8
g0233: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0233: shared memory (/dev/shm) size .... 188.13 GB
g0233: DeepSpeed general environment info:
g0233: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0233: torch version .................... 2.0.1+cu118
g0233: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0233: deepspeed info ................... 0.12.4, unknown, unknown
g0233: torch cuda version ............... 11.8
g0233: torch hip version ................ None
g0233: nvcc version ..................... 11.8
g0233: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0233: shared memory (/dev/shm) size .... 188.13 GB
g0233: DeepSpeed general environment info:
g0233: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0233: torch version .................... 2.0.1+cu118
g0233: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0233: deepspeed info ................... 0.12.4, unknown, unknown
g0233: torch cuda version ............... 11.8
g0233: torch hip version ................ None
g0233: nvcc version ..................... 11.8
g0233: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0233: shared memory (/dev/shm) size .... 188.13 GB
g0236: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0236: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0236: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0236: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0236: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0236: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0236: --------------------------------------------------
g0236: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0236: --------------------------------------------------
g0236: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0236: --------------------------------------------------
g0236: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0236: --------------------------------------------------
g0236: DeepSpeed general environment info:
g0236: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0236: torch version .................... 2.0.1+cu118
g0236: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0236: deepspeed info ................... 0.12.4, unknown, unknown
g0236: torch cuda version ............... 11.8
g0236: torch hip version ................ None
g0236: nvcc version ..................... 11.8
g0236: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0236: shared memory (/dev/shm) size .... 188.13 GB
g0236: DeepSpeed general environment info:
g0236: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0236: torch version .................... 2.0.1+cu118
g0236: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0236: deepspeed info ................... 0.12.4, unknown, unknown
g0236: torch cuda version ............... 11.8
g0236: torch hip version ................ None
g0236: nvcc version ..................... 11.8
g0236: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0236: shared memory (/dev/shm) size .... 188.13 GB
g0236: DeepSpeed general environment info:
g0236: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0236: torch version .................... 2.0.1+cu118
g0236: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0236: deepspeed info ................... 0.12.4, unknown, unknown
g0236: torch cuda version ............... 11.8
g0236: torch hip version ................ None
g0236: nvcc version ..................... 11.8
g0236: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0236: shared memory (/dev/shm) size .... 188.13 GB
g0236: DeepSpeed general environment info:
g0236: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0236: torch version .................... 2.0.1+cu118
g0236: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0236: deepspeed info ................... 0.12.4, unknown, unknown
g0236: torch cuda version ............... 11.8
g0236: torch hip version ................ None
g0236: nvcc version ..................... 11.8
g0236: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0236: shared memory (/dev/shm) size .... 188.13 GB
g0233: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0233: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0233: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0233: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0236: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0236: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0236: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0236: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0233: [2024-08-09 11:57:51,909] [INFO] [comm.py:637:init_distributed] cdb=None
g0233: [2024-08-09 11:57:51,911] [INFO] [comm.py:637:init_distributed] cdb=None
g0236: [2024-08-09 11:57:51,911] [INFO] [comm.py:637:init_distributed] cdb=None
g0236: [2024-08-09 11:57:51,912] [INFO] [comm.py:637:init_distributed] cdb=None
g0236: [2024-08-09 11:57:51,912] [INFO] [comm.py:637:init_distributed] cdb=None
g0236: [2024-08-09 11:57:51,912] [INFO] [comm.py:637:init_distributed] cdb=None
g0233: [2024-08-09 11:57:51,912] [INFO] [comm.py:637:init_distributed] cdb=None
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0236: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [2024-08-09 11:57:51,921] [INFO] [comm.py:637:init_distributed] cdb=None
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0233: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: --------------------------------------------------
g0234: DeepSpeed C++/CUDA extension op report
g0234: --------------------------------------------------
g0234: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0234:       runtime if needed. Op compatibility means that your system
g0234:       meet the required dependencies to JIT install the op.
g0234: ------------------------------------------------------------------------------------------------------------------------------------------------------
g0234: JIT compiled ops requires ninja
g0234: 
g0234: 
g0234: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0234: 
g0234: ----------------------------------------------------------------------------------------------------
g0234: 
g0234: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0234:       runtime if needed. Op compatibility means that your system
g0234:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0234:       runtime if needed. Op compatibility means that your system
g0234:       meet the required dependencies to JIT install the op.
g0234: 
g0234: ----------------------------------------------------------------------------------------------------
g0234: 
g0234: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0234: 
g0234: --------------------------------------------------
g0234: DeepSpeed C++/CUDA extension op report
g0234: --------------------------------------------------
g0234: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0234:       runtime if needed. Op compatibility means that your system
g0234:       meet the required dependencies to JIT install the op.
g0234: --------------------------------------------------
g0234: JIT compiled ops requires ninja
g0234: ninja .................. [92m[OKAY][0m
g0234: --------------------------------------------------
g0234: op name ................ installed .. compatible
g0234: --------------------------------------------------
g0234: ninjaninja  ....................................  [92m[OKAY][0m[92m[OKAY][0m
g0234: 
g0234: ----------------------------------------------------------------------------------------------------
g0234: 
g0234: op nameop name  ................................  installedinstalled  ....  compatiblecompatible
g0234: 
g0234: ----------------------------------------------------------------------------------------------------
g0234: 
g0234: ninja .................. [92m[OKAY][0m
g0234: --------------------------------------------------
g0234: op name ................ installed .. compatible
g0234: --------------------------------------------------
g0238: ----------------------------------------------------------------------------------------------------
g0238: DeepSpeed C++/CUDA extension op report
g0238: 
g0238: --------------------------------------------------DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0238: 
g0238: 
g0238: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0238:       runtime if needed. Op compatibility means that your system
g0238:       meet the required dependencies to JIT install the op.DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0238: 
g0238: --------------------------------------------------
g0238: --------------------------------------------------
g0238: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0238:       runtime if needed. Op compatibility means that your system
g0238:       meet the required dependencies to JIT install the op.
g0238: JIT compiled ops requires ninja
g0238: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0238:       runtime if needed. Op compatibility means that your system
g0238:       meet the required dependencies to JIT install the op.
g0238: 
g0238: ----------------------------------------------------------------------------------------------------
g0238: 
g0238: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0238: 
g0238: --------------------------------------------------
g0238: DeepSpeed C++/CUDA extension op report
g0238: --------------------------------------------------
g0238: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0238:       runtime if needed. Op compatibility means that your system
g0238:       meet the required dependencies to JIT install the op.
g0238: --------------------------------------------------
g0238: JIT compiled ops requires ninja
g0238: ninjaninja  ninja....................................ninja   [92m[OKAY][0m.................. [92m[OKAY][0m
g0238:  
g0238: [92m[OKAY][0m..................--------------------------------------------------
g0238:  --------------------------------------------------
g0238: 
g0238: --------------------------------------------------[92m[OKAY][0mop name
g0238: op name 
g0238:  op name................................   ................installed  --------------------------------------------------installedinstalled.. 
g0238:   ....compatible  op name
g0238: compatiblecompatible 
g0238: 
g0238: --------------------------------------------------................
g0238: ---------------------------------------------------------------------------------------------------- 
g0238: 
g0238: installed .. compatible
g0238: --------------------------------------------------
g0234: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_adam ............... [92m[YES][0m ......async_io [92m[OKAY][0m 
g0234: ............... [92m[YES][0mcpu_adagrad  ..................  [92m[OKAY][0masync_io[92m[YES][0m
g0234:   .....................  [92m[OKAY][0m[92m[YES][0m
g0234: fused_adam  ................... cpu_lion [92m[OKAY][0m [92m[YES][0m
g0234: ...............  ......[92m[YES][0m  [92m[OKAY][0m......
g0234:  fused_adam[92m[OKAY][0m 
g0234: .............cpu_adam  [92m[YES][0m...............  ......[92m[YES][0m  [92m[OKAY][0m......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0234:  
g0234: [92m[OKAY][0mevoformer_attn
g0234: cpu_adam  ........................ cpu_adagrad [93m[NO][0m [92m[YES][0m ............ ....... ...... [92m[YES][0m [93m[NO][0m [92m[OKAY][0m
g0234: ......
g0234:  [92m[OKAY][0mfused_lamb
g0234: cpu_adagrad  .........................cpu_lion   [92m[YES][0m[92m[YES][0m...............   ............[92m[YES][0m   [92m[OKAY][0m[92m[OKAY][0m......
g0234: 
g0234:  [92m[OKAY][0m
g0234: cpu_lion ............... [92m[YES][0m ...... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHfused_lion[92m[OKAY][0m
g0234:  
g0234: ............. evoformer_attn[92m[YES][0m .........  ......[93m[NO][0m [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH [92m[OKAY][0m
g0234: 
g0234: ....... evoformer_attn[93m[NO][0m 
g0234: ......... [93m[NO][0mfused_lamb  ....................  [93m[NO][0m[92m[YES][0m
g0234:  ...... fused_lamb[92m[OKAY][0m 
g0234: ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: fused_lion ............. [92m[YES][0m ......fused_lion  [92m[OKAY][0m.............
g0234:  [92m[YES][0m ...... [92m[OKAY][0m
g0234: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0234: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0234: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0234: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0234: inference_core_opsinference_core_ops .....  .....[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0234: [92m[OKAY][0m
g0234: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0234: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: ragged_device_opsragged_device_ops  ............  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0234: 
g0234: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0234: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0234: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0234: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0234: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0234: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0234: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0234: ragged_opsragged_ops  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0234: 
g0234: random_ltdrandom_ltd  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0234: 
g0234: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0234: 
g0234: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0234: 
g0234: sparse_attnsparse_attn  ........................  [93m[NO][0m[93m[NO][0m  ..............  [93m[NO][0m[93m[NO][0m
g0234: 
g0234: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0234: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0234: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0234: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0234: spatial_inference ......transformer  [92m[YES][0m............  ......[92m[YES][0m  [92m[OKAY][0m......
g0234:  [92m[OKAY][0m
g0234: transformer stochastic_transformer............  .[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0234: [92m[OKAY][0m
g0234: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0234: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0234: --------------------------------------------------
g0234: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0234: --------------------------------------------------
g0234: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0234: --------------------------------------------------
g0234: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0234: --------------------------------------------------
g0234: DeepSpeed general environment info:
g0234: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0234: torch version .................... 2.0.1+cu118
g0234: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0234: deepspeed info ................... 0.12.4, unknown, unknown
g0234: torch cuda version ............... 11.8
g0234: torch hip version ................ None
g0234: nvcc version ..................... 11.8
g0234: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0234: shared memory (/dev/shm) size .... 188.13 GB
g0234: DeepSpeed general environment info:
g0234: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0234: torch version .................... 2.0.1+cu118
g0234: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0234: deepspeed info ................... 0.12.4, unknown, unknown
g0234: torch cuda version ............... 11.8
g0234: torch hip version ................ None
g0234: nvcc version ..................... 11.8
g0234: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0234: shared memory (/dev/shm) size .... 188.13 GB
g0234: DeepSpeed general environment info:
g0234: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0234: torch version .................... 2.0.1+cu118
g0234: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0234: deepspeed info ................... 0.12.4, unknown, unknown
g0234: torch cuda version ............... 11.8
g0234: torch hip version ................ None
g0234: nvcc version ..................... 11.8
g0234: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0234: shared memory (/dev/shm) size .... 188.13 GB
g0234: DeepSpeed general environment info:
g0234: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0234: torch version .................... 2.0.1+cu118
g0234: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0234: deepspeed info ................... 0.12.4, unknown, unknown
g0234: torch cuda version ............... 11.8
g0234: torch hip version ................ None
g0234: nvcc version ..................... 11.8
g0234: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0234: shared memory (/dev/shm) size .... 188.13 GB
g0234: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0234: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0234: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0234: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0234: [2024-08-09 11:57:52,456] [INFO] [comm.py:637:init_distributed] cdb=None
g0234: [2024-08-09 11:57:52,459] [INFO] [comm.py:637:init_distributed] cdb=None
g0234: [2024-08-09 11:57:52,460] [INFO] [comm.py:637:init_distributed] cdb=None
g0234: [2024-08-09 11:57:52,461] [INFO] [comm.py:637:init_distributed] cdb=None
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0234: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0238: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0238: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0238: cpu_adagrad async_io............  ...............[92m[YES][0m  [92m[YES][0m ............  [92m[OKAY][0m[92m[OKAY][0m
g0238: 
g0238: cpu_lionfused_adam  ............................  [92m[YES][0m[92m[YES][0m ......  ......[92m[OKAY][0m
g0238:  [92m[OKAY][0m
g0238: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0238: cpu_adagrad[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH ............
g0238:  [92m[YES][0m ......evoformer_attn [92m[OKAY][0m 
g0238: ......... cpu_lion [93m[NO][0m...............  [92m[YES][0m.......  ...... [93m[NO][0m[92m[OKAY][0m
g0238: 
g0238: async_io fused_lamb...............  [92m[YES][0m[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH............. 
g0238: ......  evoformer_attn[92m[YES][0m[92m[OKAY][0m .........
g0238:   ......[93m[NO][0m  ....... [92m[OKAY][0mfused_adam[93m[NO][0m
g0238:  
g0238: ............. async_io[92m[YES][0mfused_lamb   ................... ............... [92m[OKAY][0m [92m[YES][0m
g0238: [92m[YES][0m  ............ cpu_adam [92m[OKAY][0m 
g0238: [92m[OKAY][0mfused_lion............... 
g0238:  [92m[YES][0m ...................  [92m[OKAY][0m[92m[YES][0m
g0238: fused_lion  fused_adam...................cpu_adagrad    .............[92m[YES][0m[92m[OKAY][0m............   [92m[YES][0m......[92m[YES][0m
g0238:    ......[92m[OKAY][0m...... 
g0238:  [92m[OKAY][0m[92m[OKAY][0m
g0238: 
g0238: cpu_lion cpu_adam...............  ...............[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0238: [92m[OKAY][0m
g0238: cpu_adagrad ............ [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0238:  ...... evoformer_attn[92m[OKAY][0m 
g0238: ......... [93m[NO][0mcpu_lion  ......................  [93m[NO][0m[92m[YES][0m
g0238:  ...... fused_lamb[92m[OKAY][0m 
g0238: ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0238: evoformer_attn .........fused_lion  [93m[NO][0m.............  .......[92m[YES][0m  [93m[NO][0m......
g0238:  [92m[OKAY][0m
g0238: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0238: inference_core_ops ..... [92m[YES][0m inference_core_ops......  [92m[OKAY][0m.....
g0238:  [92m[YES][0m ...... [92m[OKAY][0m
g0238: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0238: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0238: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0238: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0238: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0238: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0238: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0238: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0238: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0238: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0238: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0238: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0238: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0238: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0238: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0238: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0238: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0238: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0238: 
g0238: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0238: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0238: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0238: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0238: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0238: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0238: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0238: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0238: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0238: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0238: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0238: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0238: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0238: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0238: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0238: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0238: --------------------------------------------------
g0238: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0238: --------------------------------------------------
g0238: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0238: --------------------------------------------------
g0238: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0238: --------------------------------------------------
g0238: DeepSpeed general environment info:
g0238: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0238: torch version .................... 2.0.1+cu118
g0238: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0238: deepspeed info ................... 0.12.4, unknown, unknown
g0238: torch cuda version ............... 11.8
g0238: torch hip version ................ None
g0238: nvcc version ..................... 11.8
g0238: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0238: shared memory (/dev/shm) size .... 188.13 GB
g0238: DeepSpeed general environment info:
g0238: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0238: torch version .................... 2.0.1+cu118
g0238: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0238: deepspeed info ................... 0.12.4, unknown, unknown
g0238: torch cuda version ............... 11.8
g0238: torch hip version ................ None
g0238: nvcc version ..................... 11.8
g0238: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0238: shared memory (/dev/shm) size .... 188.13 GB
g0238: DeepSpeed general environment info:
g0238: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0238: torch version .................... 2.0.1+cu118
g0238: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0238: deepspeed info ................... 0.12.4, unknown, unknown
g0238: torch cuda version ............... 11.8
g0238: torch hip version ................ None
g0238: nvcc version ..................... 11.8
g0238: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0238: shared memory (/dev/shm) size .... 188.13 GB
g0238: DeepSpeed general environment info:
g0238: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0238: torch version .................... 2.0.1+cu118
g0238: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0238: deepspeed info ................... 0.12.4, unknown, unknown
g0238: torch cuda version ............... 11.8
g0238: torch hip version ................ None
g0238: nvcc version ..................... 11.8
g0238: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0238: shared memory (/dev/shm) size .... 188.13 GB
g0238: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0238: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0238: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0238: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0235: --------------------------------------------------
g0235: DeepSpeed C++/CUDA extension op report
g0235: --------------------------------------------------
g0235: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0235:       runtime if needed. Op compatibility means that your system
g0235:       meet the required dependencies to JIT install the op.
g0235: --------------------------------------------------
g0235: JIT compiled ops requires ninja
g0235: --------------------------------------------------
g0235: DeepSpeed C++/CUDA extension op report
g0235: --------------------------------------------------
g0235: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0235:       runtime if needed. Op compatibility means that your system
g0235:       meet the required dependencies to JIT install the op.
g0235: 
g0235: --------------------------------------------------
g0235: DeepSpeed C++/CUDA extension op report
g0235: JIT compiled ops requires ninja
g0235: --------------------------------------------------
g0235: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0235:       runtime if needed. Op compatibility means that your system
g0235:       meet the required dependencies to JIT install the op.
g0235: --------------------------------------------------
g0235: JIT compiled ops requires ninja
g0235: --------------------------------------------------
g0235: DeepSpeed C++/CUDA extension op report
g0235: --------------------------------------------------
g0235: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0235:       runtime if needed. Op compatibility means that your system
g0235:       meet the required dependencies to JIT install the op.
g0235: --------------------------------------------------
g0235: JIT compiled ops requires ninja
g0235: ninjaninjaninja ninja .................. ..................  .................. ..................[92m[OKAY][0m [92m[OKAY][0m 
g0235: 
g0235: [92m[OKAY][0m[92m[OKAY][0m
g0235: 
g0235: ----------------------------------------------------------------------------------------------------
g0235: 
g0235: ----------------------------------------------------------------------------------------------------
g0235: 
g0235: op nameop name  op nameop name................................    ................................installedinstalled    installedinstalled....    ....compatible compatible
g0235:  compatible
g0235: compatible
g0235: --------------------------------------------------
g0235: --------------------------------------------------
g0235: --------------------------------------------------
g0235: --------------------------------------------------
g0235: 
g0238: [2024-08-09 11:57:52,664] [INFO] [comm.py:637:init_distributed] cdb=None
g0238: [2024-08-09 11:57:52,664] [INFO] [comm.py:637:init_distributed] cdb=None
g0238: [2024-08-09 11:57:52,665] [INFO] [comm.py:637:init_distributed] cdb=None
g0238: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0238: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0238: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0238: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0238: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0238: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: --------------------------------------------------
g0225: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0225: --------------------------------------------------
g0225: 
g0225: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0225:       runtime if needed. Op compatibility means that your system
g0225:       meet the required dependencies to JIT install the op.DeepSpeed C++/CUDA extension op report
g0225: 
g0225: ----------------------------------------------------------------------------------------------------
g0225: --------------------------------------------------
g0225: JIT compiled ops requires ninjaNOTE: Ops not installed will be just-in-time (JIT) compiled at
g0225:       runtime if needed. Op compatibility means that your system
g0225:       meet the required dependencies to JIT install the op.
g0225: 
g0225: 
g0225: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0225: 
g0225: JIT compiled ops requires ninja--------------------------------------------------
g0225: 
g0225: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0225:       runtime if needed. Op compatibility means that your system
g0225:       meet the required dependencies to JIT install the op.
g0225: --------------------------------------------------
g0225: JIT compiled ops requires ninja
g0225: --------------------------------------------------
g0225: DeepSpeed C++/CUDA extension op report
g0225: --------------------------------------------------
g0225: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0225:       runtime if needed. Op compatibility means that your system
g0225:       meet the required dependencies to JIT install the op.
g0225: --------------------------------------------------
g0225: JIT compiled ops requires ninja
g0225: ninjaninja  ninjaninja....................................    ..................[92m[OKAY][0m..................[92m[OKAY][0m 
g0225:  
g0225: [92m[OKAY][0m[92m[OKAY][0m--------------------------------------------------
g0225: --------------------------------------------------
g0225: 
g0225: 
g0225: ----------------------------------------------------------------------------------------------------op nameop name
g0225: 
g0225:   ................................op nameop name    installedinstalled................................    ....installedinstalled   compatible.. compatible
g0225:  ..
g0225: compatible-------------------------------------------------- --------------------------------------------------
g0225: 
g0225: compatible
g0225: 
g0225: --------------------------------------------------
g0225: --------------------------------------------------
g0235: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0235: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0235: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0235: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0235: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0235: async_iofused_adam  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0235: 
g0235: cpu_adam ............... [92m[YES][0mfused_adam  ...................  [92m[OKAY][0m[92m[YES][0m
g0235:  ...... cpu_adagrad[92m[OKAY][0m 
g0235: ............ [92m[YES][0m cpu_adam......  ...............[92m[OKAY][0m 
g0235: [92m[YES][0m ......cpu_lion  [92m[OKAY][0m...............
g0235:  [92m[YES][0m ......cpu_adagrad  [92m[OKAY][0m............
g0235:  [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATHcpu_lion
g0235:  ............... evoformer_attn[92m[YES][0m  ...............  [93m[NO][0m[92m[OKAY][0m 
g0235: ....... [93m[NO][0m
g0235: fused_lamb ............. [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[92m[YES][0m
g0235:  ...... evoformer_attn[92m[OKAY][0m .........
g0235:  [93m[NO][0m ....... [93m[NO][0m
g0235: fused_lionfused_lamb  ..........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0235: 
g0235: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: inference_core_ops inference_core_ops.....  [92m[YES][0m.....  ......[92m[YES][0m  [92m[OKAY][0m......
g0235:  [92m[OKAY][0m
g0235: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0235: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0235: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0235: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0235: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0235: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0235: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0[93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0235: 
g0235: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0235: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0235: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0235: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0235: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0235: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0235: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0235: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0235: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0235: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0235: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0235: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0235: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0235: --------------------------------------------------
g0235: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0235: --------------------------------------------------
g0235: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0235: --------------------------------------------------
g0235: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0235: --------------------------------------------------
g0235: DeepSpeed general environment info:
g0235: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0235: torch version .................... 2.0.1+cu118
g0235: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0235: deepspeed info ................... 0.12.4, unknown, unknown
g0235: torch cuda version ............... 11.8
g0235: torch hip version ................ None
g0235: nvcc version ..................... 11.8
g0235: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0235: shared memory (/dev/shm) size .... 188.13 GB
g0235: DeepSpeed general environment info:
g0235: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0235: torch version .................... 2.0.1+cu118
g0235: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0235: deepspeed info ................... 0.12.4, unknown, unknown
g0235: torch cuda version ............... 11.8
g0235: torch hip version ................ None
g0235: nvcc version ..................... 11.8
g0235: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0235: shared memory (/dev/shm) size .... 188.13 GB
g0235: DeepSpeed general environment info:
g0235: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0235: torch version .................... 2.0.1+cu118
g0235: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0235: deepspeed info ................... 0.12.4, unknown, unknown
g0235: torch cuda version ............... 11.8
g0235: torch hip version ................ None
g0235: nvcc version ..................... 11.8
g0235: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0235: shared memory (/dev/shm) size .... 188.13 GB
g0235: DeepSpeed general environment info:
g0235: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0235: torch version .................... 2.0.1+cu118
g0235: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0235: deepspeed info ................... 0.12.4, unknown, unknown
g0235: torch cuda version ............... 11.8
g0235: torch hip version ................ None
g0235: nvcc version ..................... 11.8
g0235: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0235: shared memory (/dev/shm) size .... 188.13 GB
g0225: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0225: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0225: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0225: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0225: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0225: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0225: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0225: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0225: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0235: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0235: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0235: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0235: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0225: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0225: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0225: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0225: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0225: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0225: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatibleragged_ops
g0225:  .............sparse_attn  [92m[YES][0m............  ......[93m[NO][0m  [92m[OKAY][0m.......
g0225:  [93m[NO][0m
g0225: random_ltd .............ragged_ops  [92m[YES][0m.............  ......[92m[YES][0m [92m[OKAY][0m 
g0225: ...... [92m[OKAY][0m
g0225: random_ltd ............. [92m[YES][0m [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0......
g0225:  [92m[OKAY][0m
g0225: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0225: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0225: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0225: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0225: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0225: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0225: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0225: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0225: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0225: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0225: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0225: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0225: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0225: spatial_inferencespatial_inference ......  ......[92m[YES][0m  [92m[YES][0m......  ......[92m[OKAY][0m 
g0225: [92m[OKAY][0m
g0225: transformertransformer  ........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0225: 
g0225: stochastic_transformerstochastic_transformer  ..  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0225: 
g0225: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0225: --------------------------------------------------
g0225: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0225: --------------------------------------------------
g0225: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0225: --------------------------------------------------
g0225: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0225: --------------------------------------------------
g0225: DeepSpeed general environment info:
g0225: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0225: torch version .................... 2.0.1+cu118
g0225: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0225: deepspeed info ................... 0.12.4, unknown, unknown
g0225: torch cuda version ............... 11.8
g0225: torch hip version ................ None
g0225: nvcc version ..................... 11.8
g0225: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0225: shared memory (/dev/shm) size .... 188.13 GB
g0225: DeepSpeed general environment info:
g0225: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0225: torch version .................... 2.0.1+cu118
g0225: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0225: deepspeed info ................... 0.12.4, unknown, unknown
g0225: torch cuda version ............... 11.8
g0225: torch hip version ................ None
g0225: nvcc version ..................... 11.8
g0225: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0225: shared memory (/dev/shm) size .... 188.13 GB
g0225: DeepSpeed general environment info:
g0225: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0225: torch version .................... 2.0.1+cu118
g0225: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0225: deepspeed info ................... 0.12.4, unknown, unknown
g0225: torch cuda version ............... 11.8
g0225: torch hip version ................ None
g0225: nvcc version ..................... 11.8
g0225: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0225: shared memory (/dev/shm) size .... 188.13 GB
g0225: DeepSpeed general environment info:
g0225: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0225: torch version .................... 2.0.1+cu118
g0225: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0225: deepspeed info ................... 0.12.4, unknown, unknown
g0225: torch cuda version ............... 11.8
g0225: torch hip version ................ None
g0225: nvcc version ..................... 11.8
g0225: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0225: shared memory (/dev/shm) size .... 188.13 GB
g0225: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0225: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0225: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0225: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0235: [2024-08-09 11:57:52,894] [INFO] [comm.py:637:init_distributed] cdb=None
g0235: [2024-08-09 11:57:52,895] [INFO] [comm.py:637:init_distributed] cdb=None
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [2024-08-09 11:57:52,901] [INFO] [comm.py:637:init_distributed] cdb=None
g0235: [2024-08-09 11:57:52,901] [INFO] [comm.py:637:init_distributed] cdb=None
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0235: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [2024-08-09 11:57:52,952] [INFO] [comm.py:637:init_distributed] cdb=None
g0225: [2024-08-09 11:57:52,952] [INFO] [comm.py:637:init_distributed] cdb=None
g0225: [2024-08-09 11:57:52,952] [INFO] [comm.py:637:init_distributed] cdb=None
g0225: [2024-08-09 11:57:52,953] [INFO] [comm.py:637:init_distributed] cdb=None
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0225: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0237: ----------------------------------------------------------------------------------------------------
g0237: 
g0237: DeepSpeed C++/CUDA extension op reportDeepSpeed C++/CUDA extension op report
g0237: 
g0237: ----------------------------------------------------------------------------------------------------
g0237: 
g0237: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0237:       runtime if needed. Op compatibility means that your system
g0237:       meet the required dependencies to JIT install the op.NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0237:       runtime if needed. Op compatibility means that your system
g0237:       meet the required dependencies to JIT install the op.
g0237: 
g0237: ----------------------------------------------------------------------------------------------------
g0237: 
g0237: JIT compiled ops requires ninjaJIT compiled ops requires ninja
g0237: 
g0237: ----------------------------------------------------------------------------------------------------
g0237: DeepSpeed C++/CUDA extension op report
g0237: 
g0237: DeepSpeed C++/CUDA extension op report--------------------------------------------------
g0237: 
g0237: --------------------------------------------------NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0237:       runtime if needed. Op compatibility means that your system
g0237:       meet the required dependencies to JIT install the op.
g0237: 
g0237: NOTE: Ops not installed will be just-in-time (JIT) compiled at
g0237:       runtime if needed. Op compatibility means that your system
g0237:       meet the required dependencies to JIT install the op.--------------------------------------------------
g0237: 
g0237: --------------------------------------------------JIT compiled ops requires ninja
g0237: 
g0237: JIT compiled ops requires ninja
g0237: ninjaninjaninja  ....................................   ..................[92m[OKAY][0m[92m[OKAY][0m 
g0237: 
g0237: [92m[OKAY][0m--------------------------------------------------
g0237: --------------------------------------------------
g0237: 
g0237: ninja--------------------------------------------------op nameop name
g0237:    ................................op name..................    installedinstalled................[92m[OKAY][0m   
g0237: ....installed   --------------------------------------------------compatiblecompatible..
g0237: 
g0237: 
g0237:  --------------------------------------------------op namecompatible--------------------------------------------------
g0237:  
g0237: 
g0237: ................ --------------------------------------------------installed
g0237:  .. compatible
g0237: --------------------------------------------------
g0238: > setting tensorboard ...
g0238: [2024-08-09 11:57:53,122] [INFO] [comm.py:637:init_distributed] cdb=None
g0238: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0238: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0237: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0237: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0237: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0237: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0237: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0237: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0237: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0237: fused_adam ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: cpu_adam ............... [92m[YES][0m ...... [92m[OKAY][0m
g0237: cpu_adagrad ............ [92m[YES][0m ...... [92m[OKAY][0m
g0237: cpu_lion ............... [92m[YES][0m ...... [92m[OKAY][0m
g0237: [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
g0237: evoformer_attn ......... [93m[NO][0m ....... [93m[NO][0m
g0237: fused_lamb ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: async_io ............... [92m[YES][0m ...... [92m[OKAY][0m
g0237: async_io ............... [92m[YES][0mfused_adam  ...................  [92m[OKAY][0m[92m[YES][0m
g0237:  ...... [92m[OKAY][0m
g0237: fused_adamcpu_adam  ............................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0237: 
g0237: cpu_adagradcpu_adam  ...........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0237: 
g0237: cpu_lioncpu_adagrad  ...........................  [92m[YES][0m[92m[YES][0m  ............  [92m[OKAY][0m[92m[OKAY][0m
g0237: 
g0237: cpu_lion ............... [92m[YES][0m ......[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH 
g0237: [92m[OKAY][0m
g0237: evoformer_attn ......... [93m[NO][0m ....... [93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m[NO][0m
g0237: 
g0237: evoformer_attn fused_lamb.........  .............[93m[NO][0m  [92m[YES][0m.......  ......[93m[NO][0m 
g0237: [92m[OKAY][0m
g0237: fused_lamb ............. [92m[YES][0m ...... fused_lion[92m[OKAY][0m 
g0237: ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: fused_lion ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: inference_core_ops .....inference_core_ops  [92m[YES][0m.....  ......[92m[YES][0m  [92m[OKAY][0m......
g0237:  [92m[OKAY][0m
g0237: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0237: inference_core_ops ..... [92m[YES][0m ...... [92m[OKAY][0m
g0237: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0237: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0237: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0237: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: cutlass_ops ............ [92m[YES][0m ...... [92m[OKAY][0m
g0237: quantizer .............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0237: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0237: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0237: ragged_device_ops ...... [92m[YES][0m ...... [92m[OKAY][0m
g0237: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0237: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0237: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0237: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0237: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0237: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0237: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0237: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0237: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0237: ragged_ops ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: random_ltd ............. [92m[YES][0m ...... [92m[OKAY][0m
g0237: [93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0
g0237: [93m [WARNING] [0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible
g0237: sparse_attn ............ [93m[NO][0m ....... [93m[NO][0m
g0237: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0237: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0237: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0237: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0237: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0237: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0237: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0237: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0237: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0237: spatial_inference ...... [92m[YES][0m ...... [92m[OKAY][0m
g0237: transformer ............ [92m[YES][0m ...... [92m[OKAY][0m
g0237: stochastic_transformer . [92m[YES][0m ...... [92m[OKAY][0m
g0237: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0237: --------------------------------------------------
g0237: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0237: --------------------------------------------------
g0237: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0237: --------------------------------------------------
g0237: transformer_inference .. [92m[YES][0m ...... [92m[OKAY][0m
g0237: --------------------------------------------------
g0237: DeepSpeed general environment info:
g0237: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0237: torch version .................... 2.0.1+cu118
g0237: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0237: deepspeed info ................... 0.12.4, unknown, unknown
g0237: torch cuda version ............... 11.8
g0237: torch hip version ................ None
g0237: nvcc version ..................... 11.8
g0237: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0237: shared memory (/dev/shm) size .... 188.13 GB
g0237: DeepSpeed general environment info:
g0237: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0237: torch version .................... 2.0.1+cu118
g0237: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0237: deepspeed info ................... 0.12.4, unknown, unknown
g0237: torch cuda version ............... 11.8
g0237: torch hip version ................ None
g0237: nvcc version ..................... 11.8
g0237: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0237: shared memory (/dev/shm) size .... 188.13 GB
g0237: DeepSpeed general environment info:
g0237: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0237: torch version .................... 2.0.1+cu118
g0237: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0237: deepspeed info ................... 0.12.4, unknown, unknown
g0237: torch cuda version ............... 11.8
g0237: torch hip version ................ None
g0237: nvcc version ..................... 11.8
g0237: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0237: shared memory (/dev/shm) size .... 188.13 GB
g0237: DeepSpeed general environment info:
g0237: torch install path ............... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/torch']
g0237: torch version .................... 2.0.1+cu118
g0237: deepspeed install path ........... ['/home/acf16449gb/crypto_llm/train/.venv_train/lib/python3.11/site-packages/deepspeed']
g0237: deepspeed info ................... 0.12.4, unknown, unknown
g0237: torch cuda version ............... 11.8
g0237: torch hip version ................ None
g0237: nvcc version ..................... 11.8
g0237: deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8
g0237: shared memory (/dev/shm) size .... 188.13 GB
g0237: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0237: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0237: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0237: **** Git info for Megatron: git_hash=26f6f9f git_branch=main ****
g0237: [2024-08-09 11:57:53,361] [INFO] [comm.py:637:init_distributed] cdb=None
g0237: [2024-08-09 11:57:53,365] [INFO] [comm.py:637:init_distributed] cdb=None
g0237: [2024-08-09 11:57:53,366] [INFO] [comm.py:637:init_distributed] cdb=None
g0237: [2024-08-09 11:57:53,367] [INFO] [comm.py:637:init_distributed] cdb=None
g0237: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0237: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0237: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0237: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0237: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0237: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0237: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0237: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: [W socket.cpp:601] [c10d] The client socket cannot be initialized to connect to [g0220-eth0]:29500 (errno: 97 - Address family not supported by protocol).
g0220: > initialized tensor model parallel with size 1
g0220: > initialized pipeline model parallel with size 8
g0220: > setting random seeds to 1234 ...
g0220: > initializing model parallel cuda seeds on global rank 0, model parallel rank 0, and data parallel rank 0 with model parallel seed: 3952 and data parallel seed: 1234
g0220: > compiling dataset index builder ...
g0220: make: Entering directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0220: make: Nothing to be done for 'default'.
g0220: make: Leaving directory '/home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/data'
g0220: >>> done with dataset index builder. Compilation time: 0.080 seconds
g0220: WARNING: constraints for invoking optimized fused softmax kernel are not met. We default back to unfused kernel invocations.
g0220: > compiling and loading fused kernels ...
g0220: Detected CUDA files, patching ldflags
g0220: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0220: Building extension module scaled_upper_triang_masked_softmax_cuda...
g0220: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0220: ninja: no work to do.
g0220: Loading extension module scaled_upper_triang_masked_softmax_cuda...
g0220: Detected CUDA files, patching ldflags
g0220: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0220: Building extension module scaled_masked_softmax_cuda...
g0220: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0220: ninja: no work to do.
g0220: Loading extension module scaled_masked_softmax_cuda...
g0220: Detected CUDA files, patching ldflags
g0220: Emitting ninja build file /home/acf16449gb/ucllm_nedo_prod/train/Megatron-DeepSpeed/megatron/fused_kernels/build/build.ninja...
g0220: Building extension module scaled_softmax_cuda...
g0220: Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
g0220: ninja: no work to do.
g0220: Loading extension module scaled_softmax_cuda...
g0220: >>> done with compiling and loading fused kernels. Compilation time: 7.828 seconds
g0220: time to initialize megatron (seconds): 23.504
g0220: [after megatron is initialized] datetime: 2024-08-09 11:58:03 
g0225: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0237: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0234: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0220: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0238: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0233: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0236: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0235: wandb: Currently logged in as: yohei-kobashi. Use `wandb login --relogin` to force relogin
g0238: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0238: wandb:  $ pip install wandb --upgrade
g0238: wandb: Tracking run with wandb version 0.17.5
g0238: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240809_115805-zcfd4hpc
g0238: wandb: Run `wandb offline` to turn off syncing.
g0238: wandb: Syncing run g0238.abci.local
g0238: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0238: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/zcfd4hpc
g0235: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0235: wandb:  $ pip install wandb --upgrade
g0235: wandb: Tracking run with wandb version 0.17.5
g0235: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240809_115805-j7j40lpx
g0235: wandb: Run `wandb offline` to turn off syncing.
g0235: wandb: Syncing run g0235.abci.local
g0235: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0235: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/j7j40lpx
g0236: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0236: wandb:  $ pip install wandb --upgrade
g0236: wandb: Tracking run with wandb version 0.17.5
g0236: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240809_115805-xl9lle5j
g0236: wandb: Run `wandb offline` to turn off syncing.
g0225: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0225: wandb:  $ pip install wandb --upgrade
g0225: wandb: Tracking run with wandb version 0.17.5
g0225: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240809_115805-8jg6jcoi
g0225: wandb: Run `wandb offline` to turn off syncing.
g0236: wandb: Syncing run g0236.abci.local
g0236: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0236: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/xl9lle5j
g0225: wandb: Syncing run g0225.abci.local
g0225: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0225: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/8jg6jcoi
g0233: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0233: wandb:  $ pip install wandb --upgrade
g0233: wandb: Tracking run with wandb version 0.17.5
g0233: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240809_115805-fmyzxv8w
g0233: wandb: Run `wandb offline` to turn off syncing.
g0237: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0237: wandb:  $ pip install wandb --upgrade
g0237: wandb: Tracking run with wandb version 0.17.5
g0237: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240809_115805-9ha0vias
g0237: wandb: Run `wandb offline` to turn off syncing.
g0233: wandb: Syncing run g0233.abci.local
g0233: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0233: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/fmyzxv8w
g0237: wandb: Syncing run g0237.abci.local
g0237: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0237: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/9ha0vias
g0234: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0234: wandb:  $ pip install wandb --upgrade
g0234: wandb: Tracking run with wandb version 0.17.5
g0234: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240809_115805-8jvle2hx
g0234: wandb: Run `wandb offline` to turn off syncing.
g0234: wandb: Syncing run g0234.abci.local
g0234: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0234: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/8jvle2hx
g0220: wandb: wandb version 0.17.6 is available!  To upgrade, please run:
g0220: wandb:  $ pip install wandb --upgrade
g0220: wandb: Tracking run with wandb version 0.17.5
g0220: wandb: Run data is saved locally in /home/acf16449gb/crypto_llm/train/scripts/step2_pretrain_model/wandb/run-20240809_115805-bdeh4piq
g0220: wandb: Run `wandb offline` to turn off syncing.
g0220: wandb: Syncing run g0220.abci.local
g0220: wandb: ⭐️ View project at https://wandb.ai/yohei-kobashi/encrypted_data_LLM
g0220: wandb: 🚀 View run at https://wandb.ai/yohei-kobashi/encrypted_data_LLM/runs/bdeh4piq
g0220: building GPT model ...
g0220: [2024-08-09 11:58:06,431] [INFO] [utils.py:795:see_memory_usage] Before Building Model
g0220: [2024-08-09 11:58:06,432] [INFO] [utils.py:796:see_memory_usage] MA 0.0 GB         Max_MA 0.73 GB         CA 0.0 GB         Max_CA 1 GB 
g0220: [2024-08-09 11:58:06,432] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 53.8 GB, percent = 14.3%
g0220: SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None
g0220: Using topology: {ProcessCoord(pipe=0, data=0, model=0): 0, ProcessCoord(pipe=0, data=1, model=0): 1, ProcessCoord(pipe=0, data=2, model=0): 2, ProcessCoord(pipe=0, data=3, model=0): 3, ProcessCoord(pipe=1, data=0, model=0): 4, ProcessCoord(pipe=1, data=1, model=0): 5, ProcessCoord(pipe=1, data=2, model=0): 6, ProcessCoord(pipe=1, data=3, model=0): 7, ProcessCoord(pipe=2, data=0, model=0): 8, ProcessCoord(pipe=2, data=1, model=0): 9, ProcessCoord(pipe=2, data=2, model=0): 10, ProcessCoord(pipe=2, data=3, model=0): 11, ProcessCoord(pipe=3, data=0, model=0): 12, ProcessCoord(pipe=3, data=1, model=0): 13, ProcessCoord(pipe=3, data=2, model=0): 14, ProcessCoord(pipe=3, data=3, model=0): 15, ProcessCoord(pipe=4, data=0, model=0): 16, ProcessCoord(pipe=4, data=1, model=0): 17, ProcessCoord(pipe=4, data=2, model=0): 18, ProcessCoord(pipe=4, data=3, model=0): 19, ProcessCoord(pipe=5, data=0, model=0): 20, ProcessCoord(pipe=5, data=1, model=0): 21, ProcessCoord(pipe=5, data=2, model=0): 22, ProcessCoord(pipe=5, data=3, model=0): 23, ProcessCoord(pipe=6, data=0, model=0): 24, ProcessCoord(pipe=6, data=1, model=0): 25, ProcessCoord(pipe=6, data=2, model=0): 26, ProcessCoord(pipe=6, data=3, model=0): 27, ProcessCoord(pipe=7, data=0, model=0): 28, ProcessCoord(pipe=7, data=1, model=0): 29, ProcessCoord(pipe=7, data=2, model=0): 30, ProcessCoord(pipe=7, data=3, model=0): 31}
g0220: [2024-08-09 11:58:06,955] [INFO] [module.py:375:_partition_layers] Partitioning pipeline stages with method type:transformer
g0220: stage=0 layers=5
g0220:      0: _to_float16
g0220:      1: EmbeddingPipe
g0220:      2: ParallelTransformerLayerPipe
g0220:      3: ParallelTransformerLayerPipe
g0220:      4: ParallelTransformerLayerPipe
g0220: stage=1 layers=3
g0220:      5: ParallelTransformerLayerPipe
g0220:      6: ParallelTransformerLayerPipe
g0220:      7: ParallelTransformerLayerPipe
g0220: stage=2 layers=3
g0220:      8: ParallelTransformerLayerPipe
g0220:      9: ParallelTransformerLayerPipe
g0220:     10: ParallelTransformerLayerPipe
g0220: stage=3 layers=3
g0220:     11: ParallelTransformerLayerPipe
g0220:     12: ParallelTransformerLayerPipe
g0220:     13: ParallelTransformerLayerPipe
g0220: stage=4 layers=3
g0220:     14: ParallelTransformerLayerPipe
g0220:     15: ParallelTransformerLayerPipe
g0220:     16: ParallelTransformerLayerPipe
g0220: stage=5 layers=3
g0220:     17: ParallelTransformerLayerPipe
g0220:     18: ParallelTransformerLayerPipe
g0220:     19: ParallelTransformerLayerPipe
g0220: stage=6 layers=3
g0220:     20: ParallelTransformerLayerPipe
g0220:     21: ParallelTransformerLayerPipe
g0220:     22: ParallelTransformerLayerPipe
g0220: stage=7 layers=3
g0220:     23: ParallelTransformerLayerPipe
g0220:     24: MixedFusedRMSNorm
g0220:     25: LMHeadPipe
g0220:   loss: CrossEntropy
g0235:  > number of parameters on (tensor, pipeline) model parallel rank (0, 4): 135278592
g0236:  > number of parameters on (tensor, pipeline) model parallel rank (0, 5): 135278592
g0233:  > number of parameters on (tensor, pipeline) model parallel rank (0, 2): 135278592
g0237:  > number of parameters on (tensor, pipeline) model parallel rank (0, 6): 135278592
g0225:  > number of parameters on (tensor, pipeline) model parallel rank (0, 1): 135278592
g0234:  > number of parameters on (tensor, pipeline) model parallel rank (0, 3): 135278592
g0238:  > number of parameters on (tensor, pipeline) model parallel rank (0, 7): 110893056
g0220: [2024-08-09 11:58:07,470] [INFO] [utils.py:795:see_memory_usage] After Building Model
g0220: [2024-08-09 11:58:07,471] [INFO] [utils.py:796:see_memory_usage] MA 0.75 GB         Max_MA 0.78 GB         CA 0.78 GB         Max_CA 1 GB 
g0220: [2024-08-09 11:58:07,471] [INFO] [utils.py:803:see_memory_usage] CPU Virtual Memory:  used = 53.87 GB, percent = 14.3%
g0220:  > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 201076736
g0220: setting training iterations to 10000000
g0220: > learning rate decay style: cosine
g0220: DeepSpeed is enabled.
g0220: [2024-08-09 11:58:07,473] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.12.4, git-hash=unknown, git-branch=unknown
g0225: [2024-08-09 11:58:07,583] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0225: [2024-08-09 11:58:07,583] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0225: [2024-08-09 11:58:07,583] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0225: [2024-08-09 11:58:07,583] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0233: [2024-08-09 11:58:07,598] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0233: [2024-08-09 11:58:07,598] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0233: [2024-08-09 11:58:07,598] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0233: [2024-08-09 11:58:07,598] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0238: [2024-08-09 11:58:07,600] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0238: [2024-08-09 11:58:07,600] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0238: [2024-08-09 11:58:07,600] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0238: [2024-08-09 11:58:07,600] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0237: [2024-08-09 11:58:07,609] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0237: [2024-08-09 11:58:07,609] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0237: [2024-08-09 11:58:07,609] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0237: [2024-08-09 11:58:07,609] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0235: [2024-08-09 11:58:07,610] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0235: [2024-08-09 11:58:07,610] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0235: [2024-08-09 11:58:07,610] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0235: [2024-08-09 11:58:07,611] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0236: [2024-08-09 11:58:07,617] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0236: [2024-08-09 11:58:07,617] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0236: [2024-08-09 11:58:07,617] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0236: [2024-08-09 11:58:07,617] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0234: [2024-08-09 11:58:07,634] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0234: [2024-08-09 11:58:07,634] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0234: [2024-08-09 11:58:07,634] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0234: [2024-08-09 11:58:07,634] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0220: [2024-08-09 11:58:07,666] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
g0220: [2024-08-09 11:58:07,667] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
g0220: [2024-08-09 11:58:07,667] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
g0220: [2024-08-09 11:58:07,667] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
g0220: [2024-08-09 11:58:07,668] [INFO] [logging.py:96:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale
g0220: [2024-08-09 11:58:07,693] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = FusedAdam
g0220: [2024-08-09 11:58:07,693] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
g0220: [2024-08-09 11:58:07,693] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0220: [2024-08-09 11:58:07,693] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <megatron.optimizer_param_scheduler.OptimizerParamScheduler object at 0x7f01bac86490>
g0220: [2024-08-09 11:58:07,693] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0220: [2024-08-09 11:58:07,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: [2024-08-09 11:58:07,694] [INFO] [config.py:979:print] DeepSpeedEngine configuration:
g0220: [2024-08-09 11:58:07,694] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0220: [2024-08-09 11:58:07,694] [INFO] [config.py:983:print]   activation_checkpointing_config  {
g0220:     "partition_activations": false, 
g0220:     "contiguous_memory_optimization": false, 
g0220:     "cpu_checkpointing": false, 
g0220:     "number_checkpoints": null, 
g0220:     "synchronize_checkpoint_boundary": false, 
g0220:     "profile": false
g0220: }
g0220: [2024-08-09 11:58:07,695] [INFO] [config.py:983:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
g0220: [2024-08-09 11:58:07,695] [INFO] [config.py:983:print]   amp_enabled .................. False
g0220: [2024-08-09 11:58:07,695] [INFO] [config.py:983:print]   amp_params ................... False
g0220: [2024-08-09 11:58:07,695] [INFO] [config.py:983:print]   autotuning_config ............ {
g0220:     "enabled": false, 
g0220:     "start_step": null, 
g0220:     "end_step": null, 
g0220:     "metric_path": null, 
g0220:     "arg_mappings": null, 
g0220:     "metric": "throughput", 
g0220:     "model_info": null, 
g0220:     "results_dir": "autotuning_results", 
g0220:     "exps_dir": "autotuning_exps", 
g0220:     "overwrite": true, 
g0220:     "fast": true, 
g0220:     "start_profile_step": 3, 
g0220:     "end_profile_step": 5, 
g0220:     "tuner_type": "gridsearch", 
g0220:     "tuner_early_stopping": 5, 
g0220:     "tuner_num_trials": 50, 
g0220:     "model_info_path": null, 
g0220:     "mp_size": 1, 
g0220:     "max_train_batch_size": null, 
g0220:     "min_train_batch_size": 1, 
g0220:     "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
g0220:     "min_train_micro_batch_size_per_gpu": 1, 
g0220:     "num_tuning_micro_batch_sizes": 3
g0220: }
g0220: [2024-08-09 11:58:07,695] [INFO] [config.py:983:print]   bfloat16_enabled ............. False
g0220: [2024-08-09 11:58:07,695] [INFO] [config.py:983:print]   checkpoint_parallel_write_pipeline  False
g0220: [2024-08-09 11:58:07,696] [INFO] [config.py:983:print]   checkpoint_tag_validation_enabled  True
g0220: [2024-08-09 11:58:07,696] [INFO] [config.py:983:print]   checkpoint_tag_validation_fail  False
g0220: [2024-08-09 11:58:07,696] [INFO] [config.py:983:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f01bac97750>
g0220: [2024-08-09 11:58:07,696] [INFO] [config.py:983:print]   communication_data_type ...... None
g0220: [2024-08-09 11:58:07,696] [INFO] [config.py:983:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
g0220: [2024-08-09 11:58:07,696] [INFO] [config.py:983:print]   curriculum_enabled_legacy .... False
g0220: [2024-08-09 11:58:07,696] [INFO] [config.py:983:print]   curriculum_params_legacy ..... False
g0220: [2024-08-09 11:58:07,696] [INFO] [config.py:983:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
g0220: [2024-08-09 11:58:07,697] [INFO] [config.py:983:print]   data_efficiency_enabled ...... False
g0220: [2024-08-09 11:58:07,697] [INFO] [config.py:983:print]   dataloader_drop_last ......... False
g0220: [2024-08-09 11:58:07,697] [INFO] [config.py:983:print]   disable_allgather ............ False
g0220: [2024-08-09 11:58:07,697] [INFO] [config.py:983:print]   dump_state ................... False
g0220: [2024-08-09 11:58:07,697] [INFO] [config.py:983:print]   dynamic_loss_scale_args ...... {'init_scale': 2048, 'scale_window': 500, 'delayed_shift': 2, 'consecutive_hysteresis': False, 'min_scale': 1}
g0220: [2024-08-09 11:58:07,697] [INFO] [config.py:983:print]   eigenvalue_enabled ........... False
g0220: [2024-08-09 11:58:07,697] [INFO] [config.py:983:print]   eigenvalue_gas_boundary_resolution  1
g0220: [2024-08-09 11:58:07,697] [INFO] [config.py:983:print]   eigenvalue_layer_name ........ bert.encoder.layer
g0220: [2024-08-09 11:58:07,697] [INFO] [config.py:983:print]   eigenvalue_layer_num ......... 0
g0220: [2024-08-09 11:58:07,698] [INFO] [config.py:983:print]   eigenvalue_max_iter .......... 100
g0220: [2024-08-09 11:58:07,698] [INFO] [config.py:983:print]   eigenvalue_stability ......... 1e-06
g0220: [2024-08-09 11:58:07,698] [INFO] [config.py:983:print]   eigenvalue_tol ............... 0.01
g0220: [2024-08-09 11:58:07,698] [INFO] [config.py:983:print]   eigenvalue_verbose ........... False
g0220: [2024-08-09 11:58:07,698] [INFO] [config.py:983:print]   elasticity_enabled ........... False
g0220: [2024-08-09 11:58:07,698] [INFO] [config.py:983:print]   flops_profiler_config ........ {
g0220:     "enabled": false, 
g0220:     "recompute_fwd_factor": 0.0, 
g0220:     "profile_step": 1, 
g0220:     "module_depth": -1, 
g0220:     "top_modules": 1, 
g0220:     "detailed": true, 
g0220:     "output_file": null
g0220: }
g0220: [2024-08-09 11:58:07,698] [INFO] [config.py:983:print]   fp16_auto_cast ............... False
g0220: [2024-08-09 11:58:07,698] [INFO] [config.py:983:print]   fp16_enabled ................. True
g0220: [2024-08-09 11:58:07,698] [INFO] [config.py:983:print]   fp16_master_weights_and_gradients  False
g0220: [2024-08-09 11:58:07,699] [INFO] [config.py:983:print]   global_rank .................. 0
g0220: [2024-08-09 11:58:07,699] [INFO] [config.py:983:print]   grad_accum_dtype ............. None
g0220: [2024-08-09 11:58:07,699] [INFO] [config.py:983:print]   gradient_accumulation_steps .. 32
g0220: [2024-08-09 11:58:07,699] [INFO] [config.py:983:print]   gradient_clipping ............ 1.0
g0220: [2024-08-09 11:58:07,699] [INFO] [config.py:983:print]   gradient_predivide_factor .... 1.0
g0220: [2024-08-09 11:58:07,699] [INFO] [config.py:983:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
g0220: [2024-08-09 11:58:07,699] [INFO] [config.py:983:print]   initial_dynamic_scale ........ 2048
g0220: [2024-08-09 11:58:07,699] [INFO] [config.py:983:print]   load_universal_checkpoint .... False
g0220: [2024-08-09 11:58:07,699] [INFO] [config.py:983:print]   loss_scale ................... 0
g0220: [2024-08-09 11:58:07,700] [INFO] [config.py:983:print]   memory_breakdown ............. False
g0220: [2024-08-09 11:58:07,700] [INFO] [config.py:983:print]   mics_hierarchial_params_gather  False
g0220: [2024-08-09 11:58:07,700] [INFO] [config.py:983:print]   mics_shard_size .............. -1
g0220: [2024-08-09 11:58:07,700] [INFO] [config.py:983:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
g0220: [2024-08-09 11:58:07,700] [INFO] [config.py:983:print]   nebula_config ................ {
g0220:     "enabled": false, 
g0220:     "persistent_storage_path": null, 
g0220:     "persistent_time_interval": 100, 
g0220:     "num_of_version_in_retention": 2, 
g0220:     "enable_nebula_load": true, 
g0220:     "load_path": null
g0220: }
g0220: [2024-08-09 11:58:07,700] [INFO] [config.py:983:print]   optimizer_legacy_fusion ...... False
g0220: [2024-08-09 11:58:07,700] [INFO] [config.py:983:print]   optimizer_name ............... None
g0220: [2024-08-09 11:58:07,701] [INFO] [config.py:983:print]   optimizer_params ............. None
g0220: [2024-08-09 11:58:07,701] [INFO] [config.py:983:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
g0220: [2024-08-09 11:58:07,701] [INFO] [config.py:983:print]   pld_enabled .................. False
g0220: [2024-08-09 11:58:07,701] [INFO] [config.py:983:print]   pld_params ................... False
g0220: [2024-08-09 11:58:07,701] [INFO] [config.py:983:print]   prescale_gradients ........... True
g0220: [2024-08-09 11:58:07,701] [INFO] [config.py:983:print]   scheduler_name ............... None
g0220: [2024-08-09 11:58:07,701] [INFO] [config.py:983:print]   scheduler_params ............. None
g0220: [2024-08-09 11:58:07,701] [INFO] [config.py:983:print]   seq_parallel_communication_data_type  torch.float32
g0220: [2024-08-09 11:58:07,701] [INFO] [config.py:983:print]   sparse_attention ............. None
g0220: [2024-08-09 11:58:07,702] [INFO] [config.py:983:print]   sparse_gradients_enabled ..... False
g0220: [2024-08-09 11:58:07,702] [INFO] [config.py:983:print]   steps_per_print .............. 10
g0220: [2024-08-09 11:58:07,702] [INFO] [config.py:983:print]   train_batch_size ............. 128
g0220: [2024-08-09 11:58:07,702] [INFO] [config.py:983:print]   train_micro_batch_size_per_gpu  1
g0220: [2024-08-09 11:58:07,702] [INFO] [config.py:983:print]   use_data_before_expert_parallel_  False
g0220: [2024-08-09 11:58:07,702] [INFO] [config.py:983:print]   use_node_local_storage ....... False
g0220: [2024-08-09 11:58:07,702] [INFO] [config.py:983:print]   wall_clock_breakdown ......... False
g0220: [2024-08-09 11:58:07,702] [INFO] [config.py:983:print]   weight_quantization_config ... None
g0220: [2024-08-09 11:58:07,702] [INFO] [config.py:983:print]   world_size ................... 4
g0220: [2024-08-09 11:58:07,703] [INFO] [config.py:983:print]   zero_allow_untested_optimizer  False
g0220: [2024-08-09 11:58:07,703] [INFO] [config.py:983:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
g0220: [2024-08-09 11:58:07,703] [INFO] [config.py:983:print]   zero_enabled ................. False
g0220: [2024-08-09 11:58:07,703] [INFO] [config.py:983:print]   zero_force_ds_cpu_optimizer .. True
g0220: [2024-08-09 11:58:07,703] [INFO] [config.py:983:print]   zero_optimization_stage ...... 0
g0220: [2024-08-09 11:58:07,703] [INFO] [config.py:969:print_user_config]   json = {
g0220:     "train_batch_size": 128, 
g0220:     "train_micro_batch_size_per_gpu": 1, 
g0220:     "steps_per_print": 10, 
g0220:     "zero_optimization": {
g0220:         "stage": 0
g0220:     }, 
g0220:     "gradient_clipping": 1.0, 
g0220:     "prescale_gradients": true, 
g0220:     "fp16": {
g0220:         "enabled": true, 
g0220:         "loss_scale": 0, 
g0220:         "loss_scale_window": 500, 
g0220:         "hysteresis": 2, 
g0220:         "min_loss_scale": 1, 
g0220:         "initial_scale_power": 11
g0220:     }, 
g0220:     "wall_clock_breakdown": false
g0220: }
g0220: [2024-08-09 11:58:07,703] [INFO] [engine.py:99:__init__] CONFIG: micro_batches=32 micro_batch_size=1
g0220: [2024-08-09 11:58:07,704] [INFO] [engine.py:139:__init__] is_pipe_partitioned= False is_grad_partitioned= False
g0220: [2024-08-09 11:58:08,405] [INFO] [engine.py:158:__init__] RANK=0 STAGE=0 LAYERS=5 [0, 5) STAGE_PARAMS=201076736 (201.077M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0235: [2024-08-09 11:58:08,406] [INFO] [engine.py:158:__init__] RANK=16 STAGE=4 LAYERS=3 [14, 17) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0233: [2024-08-09 11:58:08,406] [INFO] [engine.py:158:__init__] RANK=8 STAGE=2 LAYERS=3 [8, 11) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0237: [2024-08-09 11:58:08,407] [INFO] [engine.py:158:__init__] RANK=24 STAGE=6 LAYERS=3 [20, 23) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0234: [2024-08-09 11:58:08,407] [INFO] [engine.py:158:__init__] RANK=12 STAGE=3 LAYERS=3 [11, 14) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0236: [2024-08-09 11:58:08,407] [INFO] [engine.py:158:__init__] RANK=20 STAGE=5 LAYERS=3 [17, 20) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0238: [2024-08-09 11:58:08,407] [INFO] [engine.py:158:__init__] RANK=28 STAGE=7 LAYERS=3 [23, 26) STAGE_PARAMS=110893056 (110.893M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0225: [2024-08-09 11:58:08,407] [INFO] [engine.py:158:__init__] RANK=4 STAGE=1 LAYERS=3 [5, 8) STAGE_PARAMS=135278592 (135.279M) TOTAL_PARAMS=1123641344 (1123.641M) UNIQUE_PARAMS=1123641344 (1123.641M)
g0235: [2024-08-09 11:58:09,177] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0235: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0220: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0234: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0234: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0234: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0235: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0238: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0238: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0233: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0233: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0233: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0220: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0238: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0225: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0225: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0234: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0237: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0237: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0225: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0237: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0237: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0236: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0236: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0220: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0236: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0220: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0238: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0225: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0233: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0235: [2024-08-09 11:58:09,178] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0236: [2024-08-09 11:58:09,179] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0233: [2024-08-09 11:58:12,375] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0233: [2024-08-09 11:58:12,376] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0233: [2024-08-09 11:58:12,376] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0233: [2024-08-09 11:58:12,376] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 11:58:12,378] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 11:58:12,378] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 11:58:12,378] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 11:58:12,378] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0225: [2024-08-09 11:58:12,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0225: [2024-08-09 11:58:12,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0225: [2024-08-09 11:58:12,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0225: [2024-08-09 11:58:12,379] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0238: [2024-08-09 11:58:12,380] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0238: [2024-08-09 11:58:12,380] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0238: [2024-08-09 11:58:12,380] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0237: [2024-08-09 11:58:12,381] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0237: [2024-08-09 11:58:12,381] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0237: [2024-08-09 11:58:12,381] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0237: [2024-08-09 11:58:12,381] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0234: [2024-08-09 11:58:12,381] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0234: [2024-08-09 11:58:12,382] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0234: [2024-08-09 11:58:12,382] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0235: [2024-08-09 11:58:12,382] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0235: [2024-08-09 11:58:12,382] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0235: [2024-08-09 11:58:12,382] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0233: [2024-08-09 11:58:12,383] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt...
g0233: [2024-08-09 11:58:12,383] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt...
g0233: [2024-08-09 11:58:12,383] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt...
g0238: [2024-08-09 11:58:12,383] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0234: [2024-08-09 11:58:12,383] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0233: [2024-08-09 11:58:12,383] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt...
g0236: [2024-08-09 11:58:12,384] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0236: [2024-08-09 11:58:12,384] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0235: [2024-08-09 11:58:12,385] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 11:58:12,387] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0220: [2024-08-09 11:58:12,387] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0220: [2024-08-09 11:58:12,387] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0220: [2024-08-09 11:58:12,387] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt...
g0225: [2024-08-09 11:58:12,388] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt...
g0225: [2024-08-09 11:58:12,388] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt...
g0225: [2024-08-09 11:58:12,388] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt...
g0225: [2024-08-09 11:58:12,388] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt...
g0238: [2024-08-09 11:58:12,388] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt...
g0236: [2024-08-09 11:58:12,388] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0237: [2024-08-09 11:58:12,388] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt...
g0238: [2024-08-09 11:58:12,389] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt...
g0238: [2024-08-09 11:58:12,390] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt...
g0234: [2024-08-09 11:58:12,389] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt...
g0234: [2024-08-09 11:58:12,389] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt...
g0234: [2024-08-09 11:58:12,389] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt...
g0237: [2024-08-09 11:58:12,390] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt...
g0237: [2024-08-09 11:58:12,390] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt...
g0237: [2024-08-09 11:58:12,390] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt...
g0234: [2024-08-09 11:58:12,390] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt...
g0235: [2024-08-09 11:58:12,390] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt...
g0235: [2024-08-09 11:58:12,391] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt...
g0235: [2024-08-09 11:58:12,391] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt...
g0236: [2024-08-09 11:58:12,391] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt...
g0236: [2024-08-09 11:58:12,391] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt...
g0238: [2024-08-09 11:58:12,392] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt...
g0235: [2024-08-09 11:58:12,393] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt...
g0236: [2024-08-09 11:58:12,396] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0236: [2024-08-09 11:58:12,396] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt...
g0236: [2024-08-09 11:58:12,403] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt...
g0220: [2024-08-09 11:58:13,243] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 11:58:13,245] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt...
g0220: [2024-08-09 11:58:13,245] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 11:58:13,246] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt...
g0220: [2024-08-09 11:58:13,255] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 11:58:13,256] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt...
g0220: [2024-08-09 11:58:13,257] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 11:58:13,258] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt...
g0220: [2024-08-09 11:58:13,467] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt.
g0220: [2024-08-09 11:58:13,467] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt.
g0220: [2024-08-09 11:58:13,467] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt...
g0220: [2024-08-09 11:58:13,468] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt...
g0220: [2024-08-09 11:58:13,468] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt.
g0220: [2024-08-09 11:58:13,468] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt.
g0220: [2024-08-09 11:58:13,469] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt...
g0220: [2024-08-09 11:58:13,469] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt...
g0220: [2024-08-09 11:58:13,515] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt.
g0220: [2024-08-09 11:58:13,515] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt.
g0220: [2024-08-09 11:58:13,518] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt.
g0220: [2024-08-09 11:58:13,518] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_01-model_00-model_states.pt.
g0220: [2024-08-09 11:58:13,545] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt...
g0220: [2024-08-09 11:58:13,545] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt...
g0220: [2024-08-09 11:58:13,547] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt...
g0220: [2024-08-09 11:58:13,547] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,083] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 11:58:14,084] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 11:58:14,084] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 11:58:14,084] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,085] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,085] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,086] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 11:58:14,086] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,161] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,161] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,161] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,162] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,162] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,162] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,162] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,162] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,196] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,196] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,197] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,198] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_02-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,218] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,218] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,219] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,219] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,339] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 11:58:14,339] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 11:58:14,340] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,340] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,342] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 11:58:14,343] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,343] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 11:58:14,344] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt...
g0233: [2024-08-09 11:58:14,487] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 11:58:14,488] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 11:58:14,488] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 11:58:14,488] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt...
g0233: [2024-08-09 11:58:14,489] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt...
g0233: [2024-08-09 11:58:14,489] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt...
g0233: [2024-08-09 11:58:14,491] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 11:58:14,492] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt...
g0234: [2024-08-09 11:58:14,543] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 11:58:14,543] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 11:58:14,543] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 11:58:14,543] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 11:58:14,544] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt...
g0234: [2024-08-09 11:58:14,544] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt...
g0234: [2024-08-09 11:58:14,544] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt...
g0234: [2024-08-09 11:58:14,544] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt...
g0237: [2024-08-09 11:58:14,562] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 11:58:14,563] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 11:58:14,563] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 11:58:14,563] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 11:58:14,564] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt...
g0237: [2024-08-09 11:58:14,564] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt...
g0237: [2024-08-09 11:58:14,564] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt...
g0237: [2024-08-09 11:58:14,564] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,592] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,593] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,593] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,594] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,594] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,594] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,594] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,595] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,625] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,625] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,629] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,629] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,629] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,629] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,629] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,630] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,630] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,630] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,630] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,630] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,646] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,647] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,647] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,647] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt.
g0225: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt.
g0225: [2024-08-09 11:58:14,661] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt.
g0238: [2024-08-09 11:58:14,662] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_24-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,662] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,662] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt.
g0225: [2024-08-09 11:58:14,662] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt.
g0225: [2024-08-09 11:58:14,662] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_01_model_states.pt.
g0238: [2024-08-09 11:58:14,662] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,662] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt...
g0225: [2024-08-09 11:58:14,662] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt...
g0225: [2024-08-09 11:58:14,662] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,662] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt...
g0225: [2024-08-09 11:58:14,663] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt...
g0225: [2024-08-09 11:58:14,663] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,663] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,663] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,663] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 11:58:14,683] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,683] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,684] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt...
g0220: [2024-08-09 11:58:14,684] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt...
g0235: [2024-08-09 11:58:14,695] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 11:58:14,695] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 11:58:14,695] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 11:58:14,696] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 11:58:14,696] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt...
g0235: [2024-08-09 11:58:14,696] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt...
g0235: [2024-08-09 11:58:14,696] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt...
g0235: [2024-08-09 11:58:14,697] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,843] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt.
g0236: [2024-08-09 11:58:14,843] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt.
g0236: [2024-08-09 11:58:14,843] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,843] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt.
g0236: [2024-08-09 11:58:14,843] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt.
g0236: [2024-08-09 11:58:14,844] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,844] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,844] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,878] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt.
g0236: [2024-08-09 11:58:14,878] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt.
g0236: [2024-08-09 11:58:14,880] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt.
g0236: [2024-08-09 11:58:14,880] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_17-model_00-model_states.pt.
g0236: [2024-08-09 11:58:14,893] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,893] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,902] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt...
g0236: [2024-08-09 11:58:14,902] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt...
g0233: [2024-08-09 11:58:14,944] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt.
g0233: [2024-08-09 11:58:14,944] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt.
g0233: [2024-08-09 11:58:14,944] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt.
g0233: [2024-08-09 11:58:14,945] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt.
g0233: [2024-08-09 11:58:14,945] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt...
g0233: [2024-08-09 11:58:14,945] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt...
g0233: [2024-08-09 11:58:14,945] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt...
g0233: [2024-08-09 11:58:14,945] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,965] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,965] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,965] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,965] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,965] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,966] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt...
g0238: [2024-08-09 11:58:14,966] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 11:58:14,967] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt...
g0233: [2024-08-09 11:58:14,978] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt.
g0233: [2024-08-09 11:58:14,978] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt.
g0233: [2024-08-09 11:58:14,980] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt.
g0233: [2024-08-09 11:58:14,980] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_08-model_00-model_states.pt.
g0233: [2024-08-09 11:58:14,994] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt...
g0233: [2024-08-09 11:58:14,995] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,000] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,000] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,003] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,003] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,003] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,003] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,004] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,004] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,004] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,004] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt...
g0238: [2024-08-09 11:58:15,011] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 11:58:15,011] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 11:58:15,015] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 11:58:15,016] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_25-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,035] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,035] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,039] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,040] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_20-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,051] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,055] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,058] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,061] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt...
g0225: [2024-08-09 11:58:15,281] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt.
g0225: [2024-08-09 11:58:15,281] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt.
g0225: [2024-08-09 11:58:15,281] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt.
g0225: [2024-08-09 11:58:15,282] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt.
g0225: [2024-08-09 11:58:15,282] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt...
g0225: [2024-08-09 11:58:15,282] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt...
g0225: [2024-08-09 11:58:15,282] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt...
g0225: [2024-08-09 11:58:15,282] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt...
g0225: [2024-08-09 11:58:15,315] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt.
g0225: [2024-08-09 11:58:15,315] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt.
g0225: [2024-08-09 11:58:15,317] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt.
g0225: [2024-08-09 11:58:15,318] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_05-model_00-model_states.pt.
g0225: [2024-08-09 11:58:15,332] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt...
g0225: [2024-08-09 11:58:15,332] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,332] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,332] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,332] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,333] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,333] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,333] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,333] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,334] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt...
g0225: [2024-08-09 11:58:15,337] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt...
g0225: [2024-08-09 11:58:15,337] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,362] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,362] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,362] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,362] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,362] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,363] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,363] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,363] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,366] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,366] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,369] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,369] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_14-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,380] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,380] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,380] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,381] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,381] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,381] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,381] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,381] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,381] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,383] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,386] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,388] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,394] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,394] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,394] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,395] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_09-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,408] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,409] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,411] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,411] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,414] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt.
g0237: [2024-08-09 11:58:15,415] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_21-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,415] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,415] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt...
g0220: [2024-08-09 11:58:15,421] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 11:58:15,422] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 11:58:15,422] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 11:58:15,422] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt...
g0220: [2024-08-09 11:58:15,422] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt...
g0220: [2024-08-09 11:58:15,422] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 11:58:15,423] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt...
g0220: [2024-08-09 11:58:15,423] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,427] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,430] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,433] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt...
g0237: [2024-08-09 11:58:15,435] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt...
g0220: [2024-08-09 11:58:15,454] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 11:58:15,454] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 11:58:15,455] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 11:58:15,456] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_04-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,519] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,520] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,520] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,520] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,520] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,520] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,520] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,521] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,553] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,553] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,555] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,556] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,556] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt...
g0236: [2024-08-09 11:58:15,556] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt...
g0236: [2024-08-09 11:58:15,556] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt...
g0236: [2024-08-09 11:58:15,556] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,556] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,557] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_11-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,573] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,574] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,575] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,578] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt...
g0236: [2024-08-09 11:58:15,589] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,589] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,589] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,590] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_18-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,603] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt...
g0236: [2024-08-09 11:58:15,603] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt...
g0236: [2024-08-09 11:58:15,611] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt...
g0236: [2024-08-09 11:58:15,612] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,743] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,743] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,743] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,743] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,743] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,743] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,744] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,744] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt...
g0233: [2024-08-09 11:58:15,774] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,774] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,777] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 11:58:15,777] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_10-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,860] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,860] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,860] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,860] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt...
g0236: [2024-08-09 11:58:15,861] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt...
g0236: [2024-08-09 11:58:15,861] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt...
g0236: [2024-08-09 11:58:15,861] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,862] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt...
g0236: [2024-08-09 11:58:15,894] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,894] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,894] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 11:58:15,894] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_19-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,932] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,932] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,932] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,933] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,933] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,933] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,933] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,933] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,935] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,936] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,936] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,936] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,936] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,936] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,936] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,936] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,963] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,963] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,965] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,965] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_15-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,966] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,967] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,970] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt.
g0234: [2024-08-09 11:58:15,970] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_12-model_00-model_states.pt.
g0235: [2024-08-09 11:58:15,979] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,980] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,981] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt...
g0235: [2024-08-09 11:58:15,985] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,985] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,987] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,988] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt...
g0234: [2024-08-09 11:58:15,991] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,017] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,017] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,017] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,017] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,018] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,018] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,018] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,018] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,048] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,049] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,051] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,051] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_06-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,065] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,065] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,070] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,070] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt...
g0237: [2024-08-09 11:58:16,084] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 11:58:16,084] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 11:58:16,084] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 11:58:16,084] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 11:58:16,084] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt...
g0237: [2024-08-09 11:58:16,085] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt...
g0237: [2024-08-09 11:58:16,085] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt...
g0237: [2024-08-09 11:58:16,085] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt...
g0237: [2024-08-09 11:58:16,115] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 11:58:16,115] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 11:58:16,117] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 11:58:16,118] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_22-model_00-model_states.pt.
g0235: [2024-08-09 11:58:16,469] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 11:58:16,469] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 11:58:16,469] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 11:58:16,469] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 11:58:16,469] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt...
g0235: [2024-08-09 11:58:16,469] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt...
g0235: [2024-08-09 11:58:16,469] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt...
g0235: [2024-08-09 11:58:16,470] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt...
g0235: [2024-08-09 11:58:16,500] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 11:58:16,500] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 11:58:16,502] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 11:58:16,502] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_16-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,586] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,587] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,587] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,587] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,587] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,587] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,587] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,588] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt...
g0225: [2024-08-09 11:58:16,618] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,618] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,620] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 11:58:16,620] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_07-model_00-model_states.pt.
g0234: [2024-08-09 11:58:16,757] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 11:58:16,758] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 11:58:16,758] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 11:58:16,758] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt...
g0234: [2024-08-09 11:58:16,758] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt...
g0234: [2024-08-09 11:58:16,758] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt...
g0234: [2024-08-09 11:58:16,760] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 11:58:16,761] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt...
g0234: [2024-08-09 11:58:16,788] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 11:58:16,789] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 11:58:16,792] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 11:58:16,792] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step7000/layer_13-model_00-model_states.pt.
g0220:  > overriding learning rate value to 0.0002
g0220:  > overriding minimum learning rate value to 1e-05
g0220:  > overriding warmup iterations value to 0
g0220:  > overriding warmup tokens value to 3000000000
g0220:  > overriding total number of iterations value to 1280000000
g0220:  > overriding decay tokens value to 300000000000
g0220:  > overriding learning rate decay style value to cosine
g0220:  > overriding start weight decay value to 0.1
g0220:  > overriding end weight decay value to 0.1
g0220:  > overriding total number of weight decay iterations value to 1280000000
g0220:  > overriding weight decay incr style value to constant
g0220:  checkpoint version 3.0
g0220:   successfully loaded checkpoint from /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase at iteration 7000
g0238: (min, max) time across ranks (ms):
g0238:     load-checkpoint ................................: (8707.31, 8708.87)
g0220: [after model, optimizer, and learning rate scheduler are built] datetime: 2024-08-09 11:58:17 
g0220: > building train, validation, and test datasets ...
g0220:  > datasets target sizes (minimum size):
g0220:     train:      1280000000
g0220:     validation: 128012800
g0220:     test:       12800
g0220: > building train, validation, and test datasets for GPT ...
g0220: Single data path provided for train, valid & test
g0220:  > building dataset index ...
g0220:     reading sizes...
g0220:     reading pointers...
g0220:     reading document index...
g0220:     creating numpy buffer of mmap...
g0220:     creating memory view of numpy buffer...
g0220:  > finished creating indexed dataset in 0.030447 seconds
g0220:     number of documents: 2237032
g0220:  > dataset split:
g0220:     train:
g0220:      document indices in [0, 2122943) total of 2122943 documents
g0220:     validation:
g0220:      document indices in [2122943, 2234795) total of 111852 documents
g0220:     test:
g0220:      document indices in [2234795, 2237032) total of 2237 documents
g0220:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_doc_idx.npy
g0220:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_sample_idx.npy
g0220:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/04b634e751e149fefb06b13b6fcc5ec7_shuffle_idx.npy
g0220:     loaded indexed file in 0.121 seconds
g0220:     total number of samples: 10738039
g0220:     total number of epochs: 1
g0220:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_doc_idx.npy
g0220:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_sample_idx.npy
g0220:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/61f613d0ac885befefb938bdc8e6f878_shuffle_idx.npy
g0220:     loaded indexed file in 0.120 seconds
g0220:     total number of samples: 128391300
g0220:     total number of epochs: 228
g0220:  > loading doc-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_doc_idx.npy
g0220:  > loading sample-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_sample_idx.npy
g0220:  > loading shuffle-idx mapping from /groups/gcf51099/crypto_llm/data/index-cache/4287d16757d1b97e8079b6b58b70ad7b_shuffle_idx.npy
g0220:     loaded indexed file in 0.108 seconds
g0220:     total number of samples: 14458
g0220:     total number of epochs: 2
g0220: > finished creating GPT datasets ...
g0220: [after dataloaders are built] datetime: 2024-08-09 11:58:20 
g0220: done with setup ...
g0238: (min, max) time across ranks (ms):
g0238:     model-and-optimizer-setup ......................: (11568.18, 11584.78)
g0238:     train/valid/test-data-iterators-setup ..........: (2367.06, 2390.59)
g0220: training ...
g0220: [before the start of training step] datetime: 2024-08-09 11:58:20 
g0220: [2024-08-09 11:59:20,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=7010, skipped=4, lr=[0.00012249115306666667, 0.00012249115306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7010 loss: 1.1930 iter time (s): 5.930 samples/sec: 21.584
g0238:  iteration     7010/10000000 | consumed samples:       897280 | consumed tokens:   1837629440 | elapsed time per iteration (ms): 5966.8 | learning rate: 1.225E-04 | global batch size:   128 | lm loss: 1.183866E+00 | loss scale: 524288.0 | grad norm: 0.587 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.452 | tokens per gpu per second (tgs): 1372.928 | TFLOPs: 11.05 |
g0236: [Rank 20] (after 7010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 5159.57470703125 | reserved: 5434.0 | max reserved: 5434.0
g0238: [Rank 28] (after 7010 iterations) memory (MB) | allocated: 1924.90087890625 | max allocated: 2985.41162109375 | reserved: 3720.0 | max reserved: 3720.0
g0235: [Rank 16] (after 7010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6045.62158203125 | reserved: 6328.0 | max reserved: 6328.0
g0234: [Rank 12] (after 7010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 6931.66845703125 | reserved: 7222.0 | max reserved: 7222.0
g0225: [Rank 4] (after 7010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 8703.76220703125 | reserved: 9138.0 | max reserved: 9138.0
g0220: [Rank 0] (after 7010 iterations) memory (MB) | allocated: 2877.66943359375 | max allocated: 10557.68408203125 | reserved: 11010.0 | max reserved: 11010.0
g0233: [Rank 8] (after 7010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 7817.71533203125 | reserved: 8116.0 | max reserved: 8116.0
g0237: [Rank 24] (after 7010 iterations) memory (MB) | allocated: 1971.41943359375 | max allocated: 4273.52783203125 | reserved: 5054.0 | max reserved: 5054.0
g0220: [2024-08-09 12:00:07,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=7020, skipped=4, lr=[0.00012266591573333334, 0.00012266591573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7020 loss: 1.1868 iter time (s): 4.541 samples/sec: 28.189
g0238:  iteration     7020/10000000 | consumed samples:       898560 | consumed tokens:   1840250880 | elapsed time per iteration (ms): 4711.1 | learning rate: 1.227E-04 | global batch size:   128 | lm loss: 1.184973E+00 | loss scale: 524288.0 | grad norm: 0.605 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.170 | tokens per gpu per second (tgs): 1738.885 | TFLOPs: 13.99 |
g0220: [2024-08-09 12:00:52,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=7030, skipped=4, lr=[0.0001228406784, 0.0001228406784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7030 loss: 1.1599 iter time (s): 4.488 samples/sec: 28.520
g0238:  iteration     7030/10000000 | consumed samples:       899840 | consumed tokens:   1842872320 | elapsed time per iteration (ms): 4521.8 | learning rate: 1.228E-04 | global batch size:   128 | lm loss: 1.183926E+00 | loss scale: 524288.0 | grad norm: 0.664 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.307 | tokens per gpu per second (tgs): 1811.654 | TFLOPs: 14.58 |
g0220: [2024-08-09 12:01:36,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=7040, skipped=4, lr=[0.00012301544106666667, 0.00012301544106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7040 loss: 1.1914 iter time (s): 4.346 samples/sec: 29.449
g0238:  iteration     7040/10000000 | consumed samples:       901120 | consumed tokens:   1845493760 | elapsed time per iteration (ms): 4379.6 | learning rate: 1.230E-04 | global batch size:   128 | lm loss: 1.173222E+00 | loss scale: 524288.0 | grad norm: 0.521 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.226 | tokens per gpu per second (tgs): 1870.472 | TFLOPs: 15.05 |
g0220: [2024-08-09 12:02:19,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=7050, skipped=4, lr=[0.00012319020373333334, 0.00012319020373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7050 loss: 1.1713 iter time (s): 4.307 samples/sec: 29.722
g0238:  iteration     7050/10000000 | consumed samples:       902400 | consumed tokens:   1848115200 | elapsed time per iteration (ms): 4339.5 | learning rate: 1.232E-04 | global batch size:   128 | lm loss: 1.170256E+00 | loss scale: 524288.0 | grad norm: 0.530 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.496 | tokens per gpu per second (tgs): 1887.756 | TFLOPs: 15.19 |
g0220: [2024-08-09 12:03:05,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=7060, skipped=4, lr=[0.0001233649664, 0.0001233649664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7060 loss: 1.1708 iter time (s): 4.594 samples/sec: 27.863
g0238:  iteration     7060/10000000 | consumed samples:       903680 | consumed tokens:   1850736640 | elapsed time per iteration (ms): 4627.2 | learning rate: 1.234E-04 | global batch size:   128 | lm loss: 1.178157E+00 | loss scale: 524288.0 | grad norm: 0.536 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.662 | tokens per gpu per second (tgs): 1770.393 | TFLOPs: 14.25 |
g0220: [2024-08-09 12:03:58,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=7070, skipped=4, lr=[0.00012353972906666667, 0.00012353972906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7070 loss: 1.1438 iter time (s): 5.219 samples/sec: 24.526
g0238:  iteration     7070/10000000 | consumed samples:       904960 | consumed tokens:   1853358080 | elapsed time per iteration (ms): 5252.7 | learning rate: 1.235E-04 | global batch size:   128 | lm loss: 1.171031E+00 | loss scale: 524288.0 | grad norm: 0.522 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.369 | tokens per gpu per second (tgs): 1559.591 | TFLOPs: 12.55 |
g0220: [2024-08-09 12:04:46,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=7080, skipped=4, lr=[0.00012371449173333336, 0.00012371449173333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7080 loss: 1.1692 iter time (s): 4.782 samples/sec: 26.769
g0238:  iteration     7080/10000000 | consumed samples:       906240 | consumed tokens:   1855979520 | elapsed time per iteration (ms): 4814.9 | learning rate: 1.237E-04 | global batch size:   128 | lm loss: 1.178289E+00 | loss scale: 524288.0 | grad norm: 0.608 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.584 | tokens per gpu per second (tgs): 1701.384 | TFLOPs: 13.69 |
g0220: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0220: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0236: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0236: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0234: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0236: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0233: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0237: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0233: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0238: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0235: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0238: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0235: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0237: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0235: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0234: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0238: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0235: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0236: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0233: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0225: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0233: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0225: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0225: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0220: [2024-08-09 12:05:18,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0237: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0237: [2024-08-09 12:05:18,725] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0225: [2024-08-09 12:05:18,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0220: [2024-08-09 12:05:18,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0236: [2024-08-09 12:05:18,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0238: [2024-08-09 12:05:18,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0235: [2024-08-09 12:05:18,726] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 524288.0 to 1048576.0
g0220: [2024-08-09 12:05:31,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=7090, skipped=4, lr=[0.00012388925440000003, 0.00012388925440000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7090 loss: 1.1901 iter time (s): 4.498 samples/sec: 28.459
g0238:  iteration     7090/10000000 | consumed samples:       907520 | consumed tokens:   1858600960 | elapsed time per iteration (ms): 4530.5 | learning rate: 1.239E-04 | global batch size:   128 | lm loss: 1.171987E+00 | loss scale: 1048576.0 | grad norm: 0.492 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.253 | tokens per gpu per second (tgs): 1808.208 | TFLOPs: 14.55 |
g0220: [2024-08-09 12:06:17,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=7100, skipped=4, lr=[0.0001240640170666667, 0.0001240640170666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7100 loss: 1.1748 iter time (s): 4.570 samples/sec: 28.011
g0238:  iteration     7100/10000000 | consumed samples:       908800 | consumed tokens:   1861222400 | elapsed time per iteration (ms): 4603.2 | learning rate: 1.241E-04 | global batch size:   128 | lm loss: 1.170882E+00 | loss scale: 1048576.0 | grad norm: 0.620 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.807 | tokens per gpu per second (tgs): 1779.639 | TFLOPs: 14.32 |
g0220: [2024-08-09 12:07:02,652] [INFO] [logging.py:96:log_dist] [Rank 0] step=7110, skipped=4, lr=[0.00012423877973333336, 0.00012423877973333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7110 loss: 1.1872 iter time (s): 4.448 samples/sec: 28.776
g0238:  iteration     7110/10000000 | consumed samples:       910080 | consumed tokens:   1863843840 | elapsed time per iteration (ms): 4481.2 | learning rate: 1.242E-04 | global batch size:   128 | lm loss: 1.177209E+00 | loss scale: 1048576.0 | grad norm: 0.614 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.564 | tokens per gpu per second (tgs): 1828.092 | TFLOPs: 14.71 |
g0220: [2024-08-09 12:07:47,889] [INFO] [logging.py:96:log_dist] [Rank 0] step=7120, skipped=4, lr=[0.00012441354240000003, 0.00012441354240000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7120 loss: 1.1626 iter time (s): 4.490 samples/sec: 28.506
g0238:  iteration     7120/10000000 | consumed samples:       911360 | consumed tokens:   1866465280 | elapsed time per iteration (ms): 4523.5 | learning rate: 1.244E-04 | global batch size:   128 | lm loss: 1.170703E+00 | loss scale: 1048576.0 | grad norm: 0.522 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.297 | tokens per gpu per second (tgs): 1810.999 | TFLOPs: 14.57 |
g0237: [2024-08-09 12:08:26,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 12:08:26,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 7128
g0220: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 7128
g0220: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 7128
g0220: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0237: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 7128
g0236: Grad overflow on iteration 7128
g0237: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 7128
g0234: [2024-08-09 12:08:26,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 12:08:26,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0234: Grad overflow on iteration 7128
g0236: Grad overflow on iteration 7128
g0237: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0236: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 7128
g0235: Grad overflow on iteration 7128
g0235: [2024-08-09 12:08:26,042] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 7128
g0220: Grad overflow on iteration 7128
g0237: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0237: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0235: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0237: Grad overflow on iteration 7128
g0235: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0235: Grad overflow on iteration 7128
g0236: Grad overflow on iteration 7128
g0225: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 7128
g0234: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 7128
g0236: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0237: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0234: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0225: Grad overflow on iteration 7128
g0238: Grad overflow on iteration 7128
g0225: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 7128
g0238: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 7128
g0225: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0238: Grad overflow on iteration 7128
g0225: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0236: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0238: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0234: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0236: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 7128
g0220: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0220: Grad overflow on iteration 7128
g0236: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0238: Grad overflow on iteration 7128
g0238: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 7128
g0238: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0233: Grad overflow on iteration 7128
g0225: Grad overflow on iteration 7128
g0233: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 7128
g0235: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0225: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0235: Grad overflow on iteration 7128
g0225: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0234: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0220: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0233: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0233: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 7128
g0225: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0235: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0233: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 7128
g0233: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0234: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0234: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0220: [2024-08-09 12:08:26,043] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 1048576.0, reducing to 524288.0
g0235: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0233: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0238: [2024-08-09 12:08:26,043] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 1048576.0 to 524288.0
g0220: [2024-08-09 12:08:30,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=7130, skipped=5, lr=[0.0001245883050666667, 0.0001245883050666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7130 loss: 1.1617 iter time (s): 4.263 samples/sec: 30.024
g0238:  iteration     7130/10000000 | consumed samples:       912640 | consumed tokens:   1869086720 | elapsed time per iteration (ms): 4296.8 | learning rate: 1.246E-04 | global batch size:   128 | lm loss: 1.162929E+00 | loss scale: 524288.0 | grad norm: 0.498 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.790 | tokens per gpu per second (tgs): 1906.541 | TFLOPs: 15.34 |
g0220: [2024-08-09 12:09:14,702] [INFO] [logging.py:96:log_dist] [Rank 0] step=7140, skipped=5, lr=[0.00012476306773333333, 0.00012476306773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7140 loss: 1.1799 iter time (s): 4.351 samples/sec: 29.418
g0238:  iteration     7140/10000000 | consumed samples:       913920 | consumed tokens:   1871708160 | elapsed time per iteration (ms): 4384.4 | learning rate: 1.248E-04 | global batch size:   128 | lm loss: 1.167186E+00 | loss scale: 524288.0 | grad norm: 0.553 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.195 | tokens per gpu per second (tgs): 1868.456 | TFLOPs: 15.04 |
g0220: [2024-08-09 12:10:06,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=7150, skipped=5, lr=[0.0001249378304, 0.0001249378304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7150 loss: 1.1393 iter time (s): 5.159 samples/sec: 24.810
g0238:  iteration     7150/10000000 | consumed samples:       915200 | consumed tokens:   1874329600 | elapsed time per iteration (ms): 5191.9 | learning rate: 1.249E-04 | global batch size:   128 | lm loss: 1.165024E+00 | loss scale: 524288.0 | grad norm: 0.528 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.654 | tokens per gpu per second (tgs): 1577.852 | TFLOPs: 12.70 |
g0220: [2024-08-09 12:10:58,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=7160, skipped=5, lr=[0.00012511259306666666, 0.00012511259306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7160 loss: 1.1687 iter time (s): 5.107 samples/sec: 25.065
g0238:  iteration     7160/10000000 | consumed samples:       916480 | consumed tokens:   1876951040 | elapsed time per iteration (ms): 5139.6 | learning rate: 1.251E-04 | global batch size:   128 | lm loss: 1.165219E+00 | loss scale: 524288.0 | grad norm: 0.614 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.905 | tokens per gpu per second (tgs): 1593.907 | TFLOPs: 12.83 |
g0220: [2024-08-09 12:11:44,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=7170, skipped=5, lr=[0.00012528735573333333, 0.00012528735573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7170 loss: 1.1375 iter time (s): 4.654 samples/sec: 27.502
g0238:  iteration     7170/10000000 | consumed samples:       917760 | consumed tokens:   1879572480 | elapsed time per iteration (ms): 4686.9 | learning rate: 1.253E-04 | global batch size:   128 | lm loss: 1.157699E+00 | loss scale: 524288.0 | grad norm: 0.553 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.310 | tokens per gpu per second (tgs): 1747.834 | TFLOPs: 14.07 |
g0220: [2024-08-09 12:12:29,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=7180, skipped=5, lr=[0.0001254621184, 0.0001254621184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7180 loss: 1.1521 iter time (s): 4.435 samples/sec: 28.859
g0238:  iteration     7180/10000000 | consumed samples:       919040 | consumed tokens:   1882193920 | elapsed time per iteration (ms): 4467.9 | learning rate: 1.255E-04 | global batch size:   128 | lm loss: 1.161452E+00 | loss scale: 524288.0 | grad norm: 0.526 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.649 | tokens per gpu per second (tgs): 1833.522 | TFLOPs: 14.75 |
g0220: [2024-08-09 12:13:19,539] [INFO] [logging.py:96:log_dist] [Rank 0] step=7190, skipped=5, lr=[0.00012563688106666666, 0.00012563688106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7190 loss: 1.1785 iter time (s): 4.964 samples/sec: 25.785
g0238:  iteration     7190/10000000 | consumed samples:       920320 | consumed tokens:   1884815360 | elapsed time per iteration (ms): 4997.3 | learning rate: 1.256E-04 | global batch size:   128 | lm loss: 1.160725E+00 | loss scale: 524288.0 | grad norm: 0.810 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.614 | tokens per gpu per second (tgs): 1639.285 | TFLOPs: 13.19 |
g0220: [2024-08-09 12:14:04,867] [INFO] [logging.py:96:log_dist] [Rank 0] step=7200, skipped=5, lr=[0.00012581164373333333, 0.00012581164373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7200 loss: 1.1735 iter time (s): 4.500 samples/sec: 28.445
g0238:  iteration     7200/10000000 | consumed samples:       921600 | consumed tokens:   1887436800 | elapsed time per iteration (ms): 4532.8 | learning rate: 1.258E-04 | global batch size:   128 | lm loss: 1.165728E+00 | loss scale: 524288.0 | grad norm: 0.612 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.239 | tokens per gpu per second (tgs): 1807.278 | TFLOPs: 14.54 |
g0220: [2024-08-09 12:14:51,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=7210, skipped=5, lr=[0.0001259864064, 0.0001259864064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7210 loss: 1.1424 iter time (s): 4.598 samples/sec: 27.840
g0238:  iteration     7210/10000000 | consumed samples:       922880 | consumed tokens:   1890058240 | elapsed time per iteration (ms): 4630.4 | learning rate: 1.260E-04 | global batch size:   128 | lm loss: 1.169604E+00 | loss scale: 524288.0 | grad norm: 0.522 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.643 | tokens per gpu per second (tgs): 1769.170 | TFLOPs: 14.24 |
g0220: [2024-08-09 12:15:45,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=7220, skipped=5, lr=[0.00012616116906666666, 0.00012616116906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7220 loss: 1.1628 iter time (s): 5.429 samples/sec: 23.579
g0238:  iteration     7220/10000000 | consumed samples:       924160 | consumed tokens:   1892679680 | elapsed time per iteration (ms): 5462.0 | learning rate: 1.262E-04 | global batch size:   128 | lm loss: 1.164815E+00 | loss scale: 524288.0 | grad norm: 0.499 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.435 | tokens per gpu per second (tgs): 1499.811 | TFLOPs: 12.07 |
g0220: [2024-08-09 12:16:35,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=7230, skipped=5, lr=[0.00012633593173333333, 0.00012633593173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7230 loss: 1.1359 iter time (s): 4.958 samples/sec: 25.819
g0238:  iteration     7230/10000000 | consumed samples:       925440 | consumed tokens:   1895301120 | elapsed time per iteration (ms): 4990.2 | learning rate: 1.263E-04 | global batch size:   128 | lm loss: 1.157743E+00 | loss scale: 524288.0 | grad norm: 0.562 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.650 | tokens per gpu per second (tgs): 1641.607 | TFLOPs: 13.21 |
g0220: [2024-08-09 12:17:21,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=7240, skipped=5, lr=[0.0001265106944, 0.0001265106944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7240 loss: 1.1372 iter time (s): 4.530 samples/sec: 28.255
g0238:  iteration     7240/10000000 | consumed samples:       926720 | consumed tokens:   1897922560 | elapsed time per iteration (ms): 4563.3 | learning rate: 1.265E-04 | global batch size:   128 | lm loss: 1.156594E+00 | loss scale: 524288.0 | grad norm: 0.721 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.050 | tokens per gpu per second (tgs): 1795.199 | TFLOPs: 14.45 |
g0220: [2024-08-09 12:18:07,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=7250, skipped=5, lr=[0.00012668545706666666, 0.00012668545706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7250 loss: 1.1633 iter time (s): 4.575 samples/sec: 27.976
g0238:  iteration     7250/10000000 | consumed samples:       928000 | consumed tokens:   1900544000 | elapsed time per iteration (ms): 4608.4 | learning rate: 1.267E-04 | global batch size:   128 | lm loss: 1.148262E+00 | loss scale: 524288.0 | grad norm: 0.454 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.775 | tokens per gpu per second (tgs): 1777.621 | TFLOPs: 14.30 |
g0220: [2024-08-09 12:18:53,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=7260, skipped=5, lr=[0.00012686021973333332, 0.00012686021973333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7260 loss: 1.1340 iter time (s): 4.542 samples/sec: 28.184
g0238:  iteration     7260/10000000 | consumed samples:       929280 | consumed tokens:   1903165440 | elapsed time per iteration (ms): 4574.4 | learning rate: 1.269E-04 | global batch size:   128 | lm loss: 1.156653E+00 | loss scale: 524288.0 | grad norm: 0.507 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.982 | tokens per gpu per second (tgs): 1790.838 | TFLOPs: 14.41 |
g0220: [2024-08-09 12:19:44,420] [INFO] [logging.py:96:log_dist] [Rank 0] step=7270, skipped=5, lr=[0.0001270349824, 0.0001270349824], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7270 loss: 1.1773 iter time (s): 5.094 samples/sec: 25.129
g0238:  iteration     7270/10000000 | consumed samples:       930560 | consumed tokens:   1905786880 | elapsed time per iteration (ms): 5126.4 | learning rate: 1.270E-04 | global batch size:   128 | lm loss: 1.171359E+00 | loss scale: 524288.0 | grad norm: 0.723 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.969 | tokens per gpu per second (tgs): 1597.987 | TFLOPs: 12.86 |
g0220: [2024-08-09 12:20:32,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=7280, skipped=5, lr=[0.00012720974506666666, 0.00012720974506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7280 loss: 1.1700 iter time (s): 4.754 samples/sec: 26.926
g0238:  iteration     7280/10000000 | consumed samples:       931840 | consumed tokens:   1908408320 | elapsed time per iteration (ms): 4786.8 | learning rate: 1.272E-04 | global batch size:   128 | lm loss: 1.160385E+00 | loss scale: 524288.0 | grad norm: 0.545 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.740 | tokens per gpu per second (tgs): 1711.383 | TFLOPs: 13.77 |
g0220: [2024-08-09 12:21:19,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=7290, skipped=5, lr=[0.00012738450773333332, 0.00012738450773333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7290 loss: 1.1869 iter time (s): 4.696 samples/sec: 27.255
g0238:  iteration     7290/10000000 | consumed samples:       933120 | consumed tokens:   1911029760 | elapsed time per iteration (ms): 4729.2 | learning rate: 1.274E-04 | global batch size:   128 | lm loss: 1.165662E+00 | loss scale: 524288.0 | grad norm: 0.513 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.066 | tokens per gpu per second (tgs): 1732.210 | TFLOPs: 13.94 |
g0220: [2024-08-09 12:22:06,732] [INFO] [logging.py:96:log_dist] [Rank 0] step=7300, skipped=5, lr=[0.0001275592704, 0.0001275592704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7300 loss: 1.1420 iter time (s): 4.683 samples/sec: 27.335
g0238:  iteration     7300/10000000 | consumed samples:       934400 | consumed tokens:   1913651200 | elapsed time per iteration (ms): 4715.2 | learning rate: 1.276E-04 | global batch size:   128 | lm loss: 1.164561E+00 | loss scale: 524288.0 | grad norm: 0.605 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.146 | tokens per gpu per second (tgs): 1737.373 | TFLOPs: 13.98 |
g0220: [2024-08-09 12:22:54,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=7310, skipped=5, lr=[0.00012773403306666665, 0.00012773403306666665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7310 loss: 1.1479 iter time (s): 4.764 samples/sec: 26.870
g0238:  iteration     7310/10000000 | consumed samples:       935680 | consumed tokens:   1916272640 | elapsed time per iteration (ms): 4796.7 | learning rate: 1.277E-04 | global batch size:   128 | lm loss: 1.163425E+00 | loss scale: 524288.0 | grad norm: 0.609 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.685 | tokens per gpu per second (tgs): 1707.828 | TFLOPs: 13.74 |
g0220: [2024-08-09 12:23:39,778] [INFO] [logging.py:96:log_dist] [Rank 0] step=7320, skipped=5, lr=[0.00012790879573333332, 0.00012790879573333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7320 loss: 1.1539 iter time (s): 4.475 samples/sec: 28.605
g0238:  iteration     7320/10000000 | consumed samples:       936960 | consumed tokens:   1918894080 | elapsed time per iteration (ms): 4507.8 | learning rate: 1.279E-04 | global batch size:   128 | lm loss: 1.151443E+00 | loss scale: 524288.0 | grad norm: 0.525 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.395 | tokens per gpu per second (tgs): 1817.277 | TFLOPs: 14.62 |
g0220: [2024-08-09 12:24:24,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=7330, skipped=5, lr=[0.00012808355839999999, 0.00012808355839999999], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7330 loss: 1.1765 iter time (s): 4.416 samples/sec: 28.987
g0238:  iteration     7330/10000000 | consumed samples:       938240 | consumed tokens:   1921515520 | elapsed time per iteration (ms): 4448.9 | learning rate: 1.281E-04 | global batch size:   128 | lm loss: 1.153563E+00 | loss scale: 524288.0 | grad norm: 0.569 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.771 | tokens per gpu per second (tgs): 1841.370 | TFLOPs: 14.82 |
g0220: [2024-08-09 12:25:08,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=7340, skipped=5, lr=[0.00012825832106666668, 0.00012825832106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7340 loss: 1.1336 iter time (s): 4.350 samples/sec: 29.424
g0238:  iteration     7340/10000000 | consumed samples:       939520 | consumed tokens:   1924136960 | elapsed time per iteration (ms): 4383.5 | learning rate: 1.283E-04 | global batch size:   128 | lm loss: 1.151729E+00 | loss scale: 524288.0 | grad norm: 0.474 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.200 | tokens per gpu per second (tgs): 1868.817 | TFLOPs: 15.04 |
g0220: [2024-08-09 12:25:53,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=7350, skipped=5, lr=[0.00012843308373333335, 0.00012843308373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7350 loss: 1.0856 iter time (s): 4.490 samples/sec: 28.507
g0238:  iteration     7350/10000000 | consumed samples:       940800 | consumed tokens:   1926758400 | elapsed time per iteration (ms): 4522.8 | learning rate: 1.284E-04 | global batch size:   128 | lm loss: 1.143118E+00 | loss scale: 524288.0 | grad norm: 0.545 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.301 | tokens per gpu per second (tgs): 1811.284 | TFLOPs: 14.58 |
g0220: [2024-08-09 12:26:47,469] [INFO] [logging.py:96:log_dist] [Rank 0] step=7360, skipped=5, lr=[0.0001286078464, 0.0001286078464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7360 loss: 1.1275 iter time (s): 5.381 samples/sec: 23.785
g0238:  iteration     7360/10000000 | consumed samples:       942080 | consumed tokens:   1929379840 | elapsed time per iteration (ms): 5413.9 | learning rate: 1.286E-04 | global batch size:   128 | lm loss: 1.135183E+00 | loss scale: 524288.0 | grad norm: 0.569 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.643 | tokens per gpu per second (tgs): 1513.136 | TFLOPs: 12.18 |
g0220: [2024-08-09 12:27:34,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=7370, skipped=5, lr=[0.00012878260906666668, 0.00012878260906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7370 loss: 1.1579 iter time (s): 4.647 samples/sec: 27.542
g0238:  iteration     7370/10000000 | consumed samples:       943360 | consumed tokens:   1932001280 | elapsed time per iteration (ms): 4680.1 | learning rate: 1.288E-04 | global batch size:   128 | lm loss: 1.150120E+00 | loss scale: 524288.0 | grad norm: 0.580 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.350 | tokens per gpu per second (tgs): 1750.409 | TFLOPs: 14.09 |
g0220: [2024-08-09 12:28:22,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=7380, skipped=5, lr=[0.00012895737173333334, 0.00012895737173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7380 loss: 1.1303 iter time (s): 4.831 samples/sec: 26.493
g0238:  iteration     7380/10000000 | consumed samples:       944640 | consumed tokens:   1934622720 | elapsed time per iteration (ms): 4864.3 | learning rate: 1.290E-04 | global batch size:   128 | lm loss: 1.158980E+00 | loss scale: 524288.0 | grad norm: 0.512 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.314 | tokens per gpu per second (tgs): 1684.098 | TFLOPs: 13.55 |
g0220: [2024-08-09 12:29:09,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=7390, skipped=5, lr=[0.0001291321344, 0.0001291321344], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7390 loss: 1.1265 iter time (s): 4.667 samples/sec: 27.425
g0238:  iteration     7390/10000000 | consumed samples:       945920 | consumed tokens:   1937244160 | elapsed time per iteration (ms): 4699.9 | learning rate: 1.291E-04 | global batch size:   128 | lm loss: 1.157172E+00 | loss scale: 524288.0 | grad norm: 0.508 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.235 | tokens per gpu per second (tgs): 1743.014 | TFLOPs: 14.03 |
g0220: [2024-08-09 12:29:54,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=7400, skipped=5, lr=[0.00012930689706666668, 0.00012930689706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7400 loss: 1.1584 iter time (s): 4.407 samples/sec: 29.047
g0238:  iteration     7400/10000000 | consumed samples:       947200 | consumed tokens:   1939865600 | elapsed time per iteration (ms): 4439.4 | learning rate: 1.293E-04 | global batch size:   128 | lm loss: 1.138155E+00 | loss scale: 524288.0 | grad norm: 0.570 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.833 | tokens per gpu per second (tgs): 1845.291 | TFLOPs: 14.85 |
g0220: [2024-08-09 12:30:52,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=7410, skipped=5, lr=[0.00012948165973333334, 0.00012948165973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7410 loss: 1.1562 iter time (s): 5.738 samples/sec: 22.309
g0238:  iteration     7410/10000000 | consumed samples:       948480 | consumed tokens:   1942487040 | elapsed time per iteration (ms): 5770.7 | learning rate: 1.295E-04 | global batch size:   128 | lm loss: 1.142864E+00 | loss scale: 524288.0 | grad norm: 0.461 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.181 | tokens per gpu per second (tgs): 1419.597 | TFLOPs: 11.42 |
g0220: [2024-08-09 12:31:40,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=7420, skipped=5, lr=[0.0001296564224, 0.0001296564224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7420 loss: 1.1331 iter time (s): 4.810 samples/sec: 26.608
g0238:  iteration     7420/10000000 | consumed samples:       949760 | consumed tokens:   1945108480 | elapsed time per iteration (ms): 4843.2 | learning rate: 1.297E-04 | global batch size:   128 | lm loss: 1.141793E+00 | loss scale: 524288.0 | grad norm: 0.546 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.429 | tokens per gpu per second (tgs): 1691.449 | TFLOPs: 13.61 |
g0220: [2024-08-09 12:32:32,327] [INFO] [logging.py:96:log_dist] [Rank 0] step=7430, skipped=5, lr=[0.00012983118506666667, 0.00012983118506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7430 loss: 1.1291 iter time (s): 5.155 samples/sec: 24.828
g0238:  iteration     7430/10000000 | consumed samples:       951040 | consumed tokens:   1947729920 | elapsed time per iteration (ms): 5188.2 | learning rate: 1.298E-04 | global batch size:   128 | lm loss: 1.132327E+00 | loss scale: 524288.0 | grad norm: 0.592 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.671 | tokens per gpu per second (tgs): 1578.975 | TFLOPs: 12.71 |
g0220: [2024-08-09 12:33:22,566] [INFO] [logging.py:96:log_dist] [Rank 0] step=7440, skipped=5, lr=[0.00013000594773333334, 0.00013000594773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7440 loss: 1.1295 iter time (s): 4.991 samples/sec: 25.645
g0238:  iteration     7440/10000000 | consumed samples:       952320 | consumed tokens:   1950351360 | elapsed time per iteration (ms): 5024.0 | learning rate: 1.300E-04 | global batch size:   128 | lm loss: 1.146169E+00 | loss scale: 524288.0 | grad norm: 0.554 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.478 | tokens per gpu per second (tgs): 1630.586 | TFLOPs: 13.12 |
g0220: [2024-08-09 12:34:10,244] [INFO] [logging.py:96:log_dist] [Rank 0] step=7450, skipped=5, lr=[0.0001301807104, 0.0001301807104], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7450 loss: 1.1052 iter time (s): 4.735 samples/sec: 27.034
g0238:  iteration     7450/10000000 | consumed samples:       953600 | consumed tokens:   1952972800 | elapsed time per iteration (ms): 4767.7 | learning rate: 1.302E-04 | global batch size:   128 | lm loss: 1.131454E+00 | loss scale: 524288.0 | grad norm: 0.655 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.847 | tokens per gpu per second (tgs): 1718.221 | TFLOPs: 13.83 |
g0220: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 7450
g0225: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 7450
g0225: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 7450
g0225: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 7450
g0233: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 7450
g0234: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 7450
g0225: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 7450
g0233: Grad overflow on iteration 7450
g0238: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 7450
g0235: Grad overflow on iteration 7450
g0234: Grad overflow on iteration 7450
g0235: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 7450
g0237: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: Grad overflow on iteration 7450
g0237: Grad overflow on iteration 7450
g0237: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 7450
g0220: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 7450
g0225: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: Grad overflow on iteration 7450
g0220: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 7450
g0235: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: Grad overflow on iteration 7450
g0238: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 7450
g0237: Grad overflow on iteration 7450
g0235: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: Grad overflow on iteration 7450
g0220: Grad overflow on iteration 7450
g0237: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 7450
g0233: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: Grad overflow on iteration 7450
g0238: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 7450
g0238: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 7450
g0236: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 7450
g0238: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: Grad overflow on iteration 7450
g0233: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 7450
g0233: [2024-08-09 12:34:14,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: Grad overflow on iteration 7450
g0233: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 7450
g0233: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 12:34:14,398] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 12:34:14,398] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0220: [2024-08-09 12:34:53,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=7460, skipped=6, lr=[0.00013035547306666667, 0.00013035547306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7460 loss: 1.1514 iter time (s): 4.283 samples/sec: 29.884
g0238:  iteration     7460/10000000 | consumed samples:       954880 | consumed tokens:   1955594240 | elapsed time per iteration (ms): 4316.6 | learning rate: 1.304E-04 | global batch size:   128 | lm loss: 1.146049E+00 | loss scale: 262144.0 | grad norm: 0.489 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.653 | tokens per gpu per second (tgs): 1897.790 | TFLOPs: 15.27 |
g0220: [2024-08-09 12:35:38,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=7470, skipped=6, lr=[0.00013053023573333334, 0.00013053023573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7470 loss: 1.1636 iter time (s): 4.493 samples/sec: 28.487
g0238:  iteration     7470/10000000 | consumed samples:       956160 | consumed tokens:   1958215680 | elapsed time per iteration (ms): 4526.3 | learning rate: 1.305E-04 | global batch size:   128 | lm loss: 1.162525E+00 | loss scale: 262144.0 | grad norm: 0.525 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.279 | tokens per gpu per second (tgs): 1809.869 | TFLOPs: 14.56 |
g0220: [2024-08-09 12:36:28,326] [INFO] [logging.py:96:log_dist] [Rank 0] step=7480, skipped=6, lr=[0.0001307049984, 0.0001307049984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7480 loss: 1.1446 iter time (s): 4.933 samples/sec: 25.950
g0238:  iteration     7480/10000000 | consumed samples:       957440 | consumed tokens:   1960837120 | elapsed time per iteration (ms): 4965.3 | learning rate: 1.307E-04 | global batch size:   128 | lm loss: 1.135317E+00 | loss scale: 262144.0 | grad norm: 0.519 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.779 | tokens per gpu per second (tgs): 1649.865 | TFLOPs: 13.28 |
g0220: [2024-08-09 12:37:17,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=7490, skipped=6, lr=[0.00013087976106666667, 0.00013087976106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7490 loss: 1.1434 iter time (s): 4.853 samples/sec: 26.374
g0238:  iteration     7490/10000000 | consumed samples:       958720 | consumed tokens:   1963458560 | elapsed time per iteration (ms): 4893.8 | learning rate: 1.309E-04 | global batch size:   128 | lm loss: 1.135952E+00 | loss scale: 262144.0 | grad norm: 0.564 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.156 | tokens per gpu per second (tgs): 1673.963 | TFLOPs: 13.47 |
g0220: [2024-08-09 12:38:03,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=7500, skipped=6, lr=[0.00013105452373333334, 0.00013105452373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7500 loss: 1.1244 iter time (s): 4.582 samples/sec: 27.937
g0238:  iteration     7500/10000000 | consumed samples:       960000 | consumed tokens:   1966080000 | elapsed time per iteration (ms): 4614.8 | learning rate: 1.311E-04 | global batch size:   128 | lm loss: 1.129268E+00 | loss scale: 262144.0 | grad norm: 0.530 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.737 | tokens per gpu per second (tgs): 1775.146 | TFLOPs: 14.28 |
g0220: [2024-08-09 12:38:47,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=7510, skipped=6, lr=[0.0001312292864, 0.0001312292864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7510 loss: 1.1464 iter time (s): 4.357 samples/sec: 29.377
g0238:  iteration     7510/10000000 | consumed samples:       961280 | consumed tokens:   1968701440 | elapsed time per iteration (ms): 4390.2 | learning rate: 1.312E-04 | global batch size:   128 | lm loss: 1.132017E+00 | loss scale: 262144.0 | grad norm: 1.154 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.156 | tokens per gpu per second (tgs): 1865.964 | TFLOPs: 15.02 |
g0220: [2024-08-09 12:39:30,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=7520, skipped=6, lr=[0.00013140404906666667, 0.00013140404906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7520 loss: 1.1548 iter time (s): 4.312 samples/sec: 29.685
g0238:  iteration     7520/10000000 | consumed samples:       962560 | consumed tokens:   1971322880 | elapsed time per iteration (ms): 4344.5 | learning rate: 1.314E-04 | global batch size:   128 | lm loss: 1.124380E+00 | loss scale: 262144.0 | grad norm: 0.557 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.463 | tokens per gpu per second (tgs): 1885.602 | TFLOPs: 15.17 |
g0220: [2024-08-09 12:40:18,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=7530, skipped=6, lr=[0.00013157881173333333, 0.00013157881173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7530 loss: 1.1507 iter time (s): 4.762 samples/sec: 26.879
g0238:  iteration     7530/10000000 | consumed samples:       963840 | consumed tokens:   1973944320 | elapsed time per iteration (ms): 4794.9 | learning rate: 1.316E-04 | global batch size:   128 | lm loss: 1.135942E+00 | loss scale: 262144.0 | grad norm: 0.655 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.695 | tokens per gpu per second (tgs): 1708.498 | TFLOPs: 13.75 |
g0220: [2024-08-09 12:41:08,556] [INFO] [logging.py:96:log_dist] [Rank 0] step=7540, skipped=6, lr=[0.0001317535744, 0.0001317535744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7540 loss: 1.1370 iter time (s): 4.952 samples/sec: 25.848
g0238:  iteration     7540/10000000 | consumed samples:       965120 | consumed tokens:   1976565760 | elapsed time per iteration (ms): 4984.8 | learning rate: 1.318E-04 | global batch size:   128 | lm loss: 1.129264E+00 | loss scale: 262144.0 | grad norm: 0.627 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.678 | tokens per gpu per second (tgs): 1643.394 | TFLOPs: 13.22 |
g0220: [2024-08-09 12:41:54,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=7550, skipped=6, lr=[0.00013192833706666667, 0.00013192833706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7550 loss: 1.1071 iter time (s): 4.603 samples/sec: 27.811
g0238:  iteration     7550/10000000 | consumed samples:       966400 | consumed tokens:   1979187200 | elapsed time per iteration (ms): 4635.4 | learning rate: 1.319E-04 | global batch size:   128 | lm loss: 1.137656E+00 | loss scale: 262144.0 | grad norm: 0.501 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.614 | tokens per gpu per second (tgs): 1767.288 | TFLOPs: 14.22 |
g0220: [2024-08-09 12:42:37,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=7560, skipped=6, lr=[0.00013210309973333333, 0.00013210309973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7560 loss: 1.1267 iter time (s): 4.258 samples/sec: 30.059
g0238:  iteration     7560/10000000 | consumed samples:       967680 | consumed tokens:   1981808640 | elapsed time per iteration (ms): 4291.2 | learning rate: 1.321E-04 | global batch size:   128 | lm loss: 1.130718E+00 | loss scale: 262144.0 | grad norm: 0.479 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.828 | tokens per gpu per second (tgs): 1909.012 | TFLOPs: 15.36 |
g0220: [2024-08-09 12:43:20,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=7570, skipped=6, lr=[0.0001322778624, 0.0001322778624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7570 loss: 1.1168 iter time (s): 4.201 samples/sec: 30.467
g0238:  iteration     7570/10000000 | consumed samples:       968960 | consumed tokens:   1984430080 | elapsed time per iteration (ms): 4234.3 | learning rate: 1.323E-04 | global batch size:   128 | lm loss: 1.130603E+00 | loss scale: 262144.0 | grad norm: 0.620 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.229 | tokens per gpu per second (tgs): 1934.663 | TFLOPs: 15.57 |
g0220: [2024-08-09 12:44:02,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=7580, skipped=6, lr=[0.00013245262506666666, 0.00013245262506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7580 loss: 1.1184 iter time (s): 4.249 samples/sec: 30.122
g0238:  iteration     7580/10000000 | consumed samples:       970240 | consumed tokens:   1987051520 | elapsed time per iteration (ms): 4282.3 | learning rate: 1.325E-04 | global batch size:   128 | lm loss: 1.126682E+00 | loss scale: 262144.0 | grad norm: 0.449 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.890 | tokens per gpu per second (tgs): 1912.988 | TFLOPs: 15.39 |
g0220: [2024-08-09 12:44:43,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=7590, skipped=6, lr=[0.00013262738773333333, 0.00013262738773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7590 loss: 1.1410 iter time (s): 3.989 samples/sec: 32.091
g0238:  iteration     7590/10000000 | consumed samples:       971520 | consumed tokens:   1989672960 | elapsed time per iteration (ms): 4021.7 | learning rate: 1.326E-04 | global batch size:   128 | lm loss: 1.138292E+00 | loss scale: 262144.0 | grad norm: 0.533 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.828 | tokens per gpu per second (tgs): 2036.965 | TFLOPs: 16.39 |
g0220: [2024-08-09 12:45:24,243] [INFO] [logging.py:96:log_dist] [Rank 0] step=7600, skipped=6, lr=[0.0001328021504, 0.0001328021504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7600 loss: 1.1284 iter time (s): 4.070 samples/sec: 31.449
g0238:  iteration     7600/10000000 | consumed samples:       972800 | consumed tokens:   1992294400 | elapsed time per iteration (ms): 4103.8 | learning rate: 1.328E-04 | global batch size:   128 | lm loss: 1.133568E+00 | loss scale: 262144.0 | grad norm: 0.592 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.191 | tokens per gpu per second (tgs): 1996.206 | TFLOPs: 16.06 |
g0220: [2024-08-09 12:46:05,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=7610, skipped=6, lr=[0.00013297691306666666, 0.00013297691306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7610 loss: 1.1207 iter time (s): 4.066 samples/sec: 31.481
g0238:  iteration     7610/10000000 | consumed samples:       974080 | consumed tokens:   1994915840 | elapsed time per iteration (ms): 4099.2 | learning rate: 1.330E-04 | global batch size:   128 | lm loss: 1.126031E+00 | loss scale: 262144.0 | grad norm: 0.540 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.225 | tokens per gpu per second (tgs): 1998.414 | TFLOPs: 16.08 |
g0220: [2024-08-09 12:46:45,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=7620, skipped=6, lr=[0.00013315167573333333, 0.00013315167573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7620 loss: 1.1174 iter time (s): 4.001 samples/sec: 31.990
g0238:  iteration     7620/10000000 | consumed samples:       975360 | consumed tokens:   1997537280 | elapsed time per iteration (ms): 4040.4 | learning rate: 1.332E-04 | global batch size:   128 | lm loss: 1.125221E+00 | loss scale: 262144.0 | grad norm: 0.530 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.680 | tokens per gpu per second (tgs): 2027.501 | TFLOPs: 16.32 |
g0220: [2024-08-09 12:47:32,984] [INFO] [logging.py:96:log_dist] [Rank 0] step=7630, skipped=6, lr=[0.0001333264384, 0.0001333264384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7630 loss: 1.1464 iter time (s): 4.701 samples/sec: 27.226
g0238:  iteration     7630/10000000 | consumed samples:       976640 | consumed tokens:   2000158720 | elapsed time per iteration (ms): 4734.2 | learning rate: 1.333E-04 | global batch size:   128 | lm loss: 1.134746E+00 | loss scale: 262144.0 | grad norm: 0.605 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.037 | tokens per gpu per second (tgs): 1730.373 | TFLOPs: 13.92 |
g0220: [2024-08-09 12:48:19,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=7640, skipped=6, lr=[0.00013350120106666666, 0.00013350120106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7640 loss: 1.1243 iter time (s): 4.627 samples/sec: 27.662
g0238:  iteration     7640/10000000 | consumed samples:       977920 | consumed tokens:   2002780160 | elapsed time per iteration (ms): 4660.0 | learning rate: 1.335E-04 | global batch size:   128 | lm loss: 1.128977E+00 | loss scale: 262144.0 | grad norm: 0.538 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.468 | tokens per gpu per second (tgs): 1757.954 | TFLOPs: 14.15 |
g0220: [2024-08-09 12:49:02,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=7650, skipped=6, lr=[0.00013367596373333333, 0.00013367596373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7650 loss: 1.1152 iter time (s): 4.261 samples/sec: 30.042
g0238:  iteration     7650/10000000 | consumed samples:       979200 | consumed tokens:   2005401600 | elapsed time per iteration (ms): 4293.6 | learning rate: 1.337E-04 | global batch size:   128 | lm loss: 1.121004E+00 | loss scale: 262144.0 | grad norm: 0.446 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.812 | tokens per gpu per second (tgs): 1907.959 | TFLOPs: 15.35 |
g0220: [2024-08-09 12:49:46,079] [INFO] [logging.py:96:log_dist] [Rank 0] step=7660, skipped=6, lr=[0.0001338507264, 0.0001338507264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7660 loss: 1.1325 iter time (s): 4.323 samples/sec: 29.609
g0238:  iteration     7660/10000000 | consumed samples:       980480 | consumed tokens:   2008023040 | elapsed time per iteration (ms): 4356.1 | learning rate: 1.339E-04 | global batch size:   128 | lm loss: 1.136015E+00 | loss scale: 262144.0 | grad norm: 0.472 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.384 | tokens per gpu per second (tgs): 1880.600 | TFLOPs: 15.13 |
g0220: [2024-08-09 12:50:35,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=7670, skipped=6, lr=[0.00013402548906666666, 0.00013402548906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7670 loss: 1.1196 iter time (s): 4.931 samples/sec: 25.960
g0238:  iteration     7670/10000000 | consumed samples:       981760 | consumed tokens:   2010644480 | elapsed time per iteration (ms): 4964.8 | learning rate: 1.340E-04 | global batch size:   128 | lm loss: 1.135887E+00 | loss scale: 262144.0 | grad norm: 0.517 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.781 | tokens per gpu per second (tgs): 1650.000 | TFLOPs: 13.28 |
g0220: [2024-08-09 12:51:22,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=7680, skipped=6, lr=[0.00013420025173333333, 0.00013420025173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7680 loss: 1.1142 iter time (s): 4.615 samples/sec: 27.733
g0238:  iteration     7680/10000000 | consumed samples:       983040 | consumed tokens:   2013265920 | elapsed time per iteration (ms): 4648.1 | learning rate: 1.342E-04 | global batch size:   128 | lm loss: 1.119311E+00 | loss scale: 262144.0 | grad norm: 0.785 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.538 | tokens per gpu per second (tgs): 1762.454 | TFLOPs: 14.18 |
g0220: [2024-08-09 12:52:06,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=7690, skipped=6, lr=[0.0001343750144, 0.0001343750144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7690 loss: 1.1349 iter time (s): 4.356 samples/sec: 29.387
g0238:  iteration     7690/10000000 | consumed samples:       984320 | consumed tokens:   2015887360 | elapsed time per iteration (ms): 4388.4 | learning rate: 1.344E-04 | global batch size:   128 | lm loss: 1.138063E+00 | loss scale: 262144.0 | grad norm: 0.486 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.168 | tokens per gpu per second (tgs): 1866.722 | TFLOPs: 15.02 |
g0220: [2024-08-09 12:52:48,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=7700, skipped=6, lr=[0.00013454977706666668, 0.00013454977706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7700 loss: 1.1587 iter time (s): 4.251 samples/sec: 30.108
g0238:  iteration     7700/10000000 | consumed samples:       985600 | consumed tokens:   2018508800 | elapsed time per iteration (ms): 4284.5 | learning rate: 1.345E-04 | global batch size:   128 | lm loss: 1.135019E+00 | loss scale: 262144.0 | grad norm: 0.560 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.875 | tokens per gpu per second (tgs): 1912.009 | TFLOPs: 15.39 |
g0220: [2024-08-09 12:53:37,015] [INFO] [logging.py:96:log_dist] [Rank 0] step=7710, skipped=6, lr=[0.00013472453973333335, 0.00013472453973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7710 loss: 1.1335 iter time (s): 4.774 samples/sec: 26.811
g0238:  iteration     7710/10000000 | consumed samples:       986880 | consumed tokens:   2021130240 | elapsed time per iteration (ms): 4807.5 | learning rate: 1.347E-04 | global batch size:   128 | lm loss: 1.113583E+00 | loss scale: 262144.0 | grad norm: 0.456 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.625 | tokens per gpu per second (tgs): 1703.995 | TFLOPs: 13.71 |
g0220: [2024-08-09 12:54:23,734] [INFO] [logging.py:96:log_dist] [Rank 0] step=7720, skipped=6, lr=[0.00013489930240000002, 0.00013489930240000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7720 loss: 1.1531 iter time (s): 4.638 samples/sec: 27.597
g0238:  iteration     7720/10000000 | consumed samples:       988160 | consumed tokens:   2023751680 | elapsed time per iteration (ms): 4671.9 | learning rate: 1.349E-04 | global batch size:   128 | lm loss: 1.125721E+00 | loss scale: 262144.0 | grad norm: 0.744 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.398 | tokens per gpu per second (tgs): 1753.457 | TFLOPs: 14.11 |
g0220: [2024-08-09 12:55:10,453] [INFO] [logging.py:96:log_dist] [Rank 0] step=7730, skipped=6, lr=[0.00013507406506666668, 0.00013507406506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7730 loss: 1.1209 iter time (s): 4.639 samples/sec: 27.594
g0238:  iteration     7730/10000000 | consumed samples:       989440 | consumed tokens:   2026373120 | elapsed time per iteration (ms): 4671.9 | learning rate: 1.351E-04 | global batch size:   128 | lm loss: 1.126570E+00 | loss scale: 262144.0 | grad norm: 0.488 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.398 | tokens per gpu per second (tgs): 1753.478 | TFLOPs: 14.11 |
g0220: [2024-08-09 12:55:54,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=7740, skipped=6, lr=[0.00013524882773333335, 0.00013524882773333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7740 loss: 1.1015 iter time (s): 4.346 samples/sec: 29.454
g0238:  iteration     7740/10000000 | consumed samples:       990720 | consumed tokens:   2028994560 | elapsed time per iteration (ms): 4379.4 | learning rate: 1.352E-04 | global batch size:   128 | lm loss: 1.119114E+00 | loss scale: 262144.0 | grad norm: 0.388 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.228 | tokens per gpu per second (tgs): 1870.585 | TFLOPs: 15.05 |
g0220: [2024-08-09 12:56:41,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=7750, skipped=6, lr=[0.00013542359040000001, 0.00013542359040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7750 loss: 1.1081 iter time (s): 4.685 samples/sec: 27.321
g0238:  iteration     7750/10000000 | consumed samples:       992000 | consumed tokens:   2031616000 | elapsed time per iteration (ms): 4718.0 | learning rate: 1.354E-04 | global batch size:   128 | lm loss: 1.114612E+00 | loss scale: 262144.0 | grad norm: 0.426 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.130 | tokens per gpu per second (tgs): 1736.324 | TFLOPs: 13.97 |
g0220: [2024-08-09 12:57:27,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=7760, skipped=6, lr=[0.00013559835306666668, 0.00013559835306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7760 loss: 1.1150 iter time (s): 4.564 samples/sec: 28.044
g0238:  iteration     7760/10000000 | consumed samples:       993280 | consumed tokens:   2034237440 | elapsed time per iteration (ms): 4597.4 | learning rate: 1.356E-04 | global batch size:   128 | lm loss: 1.121785E+00 | loss scale: 262144.0 | grad norm: 0.446 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.842 | tokens per gpu per second (tgs): 1781.876 | TFLOPs: 14.34 |
g0220: [2024-08-09 12:58:13,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=7770, skipped=6, lr=[0.00013577311573333335, 0.00013577311573333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7770 loss: 1.1421 iter time (s): 4.529 samples/sec: 28.263
g0238:  iteration     7770/10000000 | consumed samples:       994560 | consumed tokens:   2036858880 | elapsed time per iteration (ms): 4563.0 | learning rate: 1.358E-04 | global batch size:   128 | lm loss: 1.121587E+00 | loss scale: 262144.0 | grad norm: 0.515 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.052 | tokens per gpu per second (tgs): 1795.324 | TFLOPs: 14.45 |
g0220: [2024-08-09 12:58:56,925] [INFO] [logging.py:96:log_dist] [Rank 0] step=7780, skipped=6, lr=[0.0001359478784, 0.0001359478784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7780 loss: 1.1220 iter time (s): 4.356 samples/sec: 29.383
g0238:  iteration     7780/10000000 | consumed samples:       995840 | consumed tokens:   2039480320 | elapsed time per iteration (ms): 4389.5 | learning rate: 1.359E-04 | global batch size:   128 | lm loss: 1.109794E+00 | loss scale: 262144.0 | grad norm: 0.540 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.161 | tokens per gpu per second (tgs): 1866.293 | TFLOPs: 15.02 |
g0220: [2024-08-09 12:59:44,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=7790, skipped=6, lr=[0.00013612264106666668, 0.00013612264106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7790 loss: 1.1718 iter time (s): 4.754 samples/sec: 26.923
g0238:  iteration     7790/10000000 | consumed samples:       997120 | consumed tokens:   2042101760 | elapsed time per iteration (ms): 4787.9 | learning rate: 1.361E-04 | global batch size:   128 | lm loss: 1.121504E+00 | loss scale: 262144.0 | grad norm: 0.419 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.734 | tokens per gpu per second (tgs): 1710.962 | TFLOPs: 13.77 |
g0220: [2024-08-09 13:00:31,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=7800, skipped=6, lr=[0.00013629740373333335, 0.00013629740373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7800 loss: 1.1209 iter time (s): 4.646 samples/sec: 27.553
g0238:  iteration     7800/10000000 | consumed samples:       998400 | consumed tokens:   2044723200 | elapsed time per iteration (ms): 4679.0 | learning rate: 1.363E-04 | global batch size:   128 | lm loss: 1.123492E+00 | loss scale: 262144.0 | grad norm: 0.558 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.357 | tokens per gpu per second (tgs): 1750.816 | TFLOPs: 14.09 |
g0220: [2024-08-09 13:01:16,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=7810, skipped=6, lr=[0.0001364721664, 0.0001364721664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7810 loss: 1.1433 iter time (s): 4.447 samples/sec: 28.786
g0238:  iteration     7810/10000000 | consumed samples:       999680 | consumed tokens:   2047344640 | elapsed time per iteration (ms): 4479.4 | learning rate: 1.365E-04 | global batch size:   128 | lm loss: 1.129573E+00 | loss scale: 262144.0 | grad norm: 0.485 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.576 | tokens per gpu per second (tgs): 1828.836 | TFLOPs: 14.72 |
g0220: [2024-08-09 13:02:01,359] [INFO] [logging.py:96:log_dist] [Rank 0] step=7820, skipped=6, lr=[0.00013664692906666668, 0.00013664692906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7820 loss: 1.0814 iter time (s): 4.464 samples/sec: 28.672
g0238:  iteration     7820/10000000 | consumed samples:      1000960 | consumed tokens:   2049966080 | elapsed time per iteration (ms): 4497.1 | learning rate: 1.366E-04 | global batch size:   128 | lm loss: 1.111789E+00 | loss scale: 262144.0 | grad norm: 0.537 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.463 | tokens per gpu per second (tgs): 1821.637 | TFLOPs: 14.66 |
g0220: [2024-08-09 13:02:48,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=7830, skipped=6, lr=[0.00013682169173333334, 0.00013682169173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7830 loss: 1.1275 iter time (s): 4.663 samples/sec: 27.448
g0238:  iteration     7830/10000000 | consumed samples:      1002240 | consumed tokens:   2052587520 | elapsed time per iteration (ms): 4696.8 | learning rate: 1.368E-04 | global batch size:   128 | lm loss: 1.119660E+00 | loss scale: 262144.0 | grad norm: 0.502 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.252 | tokens per gpu per second (tgs): 1744.149 | TFLOPs: 14.04 |
g0220: [2024-08-09 13:03:37,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=7840, skipped=6, lr=[0.0001369964544, 0.0001369964544], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7840 loss: 1.0929 iter time (s): 4.892 samples/sec: 26.166
g0238:  iteration     7840/10000000 | consumed samples:      1003520 | consumed tokens:   2055208960 | elapsed time per iteration (ms): 4959.9 | learning rate: 1.370E-04 | global batch size:   128 | lm loss: 1.116679E+00 | loss scale: 262144.0 | grad norm: 0.431 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.807 | tokens per gpu per second (tgs): 1651.641 | TFLOPs: 13.29 |
g0220: [2024-08-09 13:04:25,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=7850, skipped=6, lr=[0.00013717121706666668, 0.00013717121706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7850 loss: 1.1048 iter time (s): 4.684 samples/sec: 27.325
g0238:  iteration     7850/10000000 | consumed samples:      1004800 | consumed tokens:   2057830400 | elapsed time per iteration (ms): 4717.4 | learning rate: 1.372E-04 | global batch size:   128 | lm loss: 1.119446E+00 | loss scale: 262144.0 | grad norm: 0.694 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.134 | tokens per gpu per second (tgs): 1736.560 | TFLOPs: 13.97 |
g0220: [2024-08-09 13:05:25,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=7860, skipped=6, lr=[0.00013734597973333334, 0.00013734597973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7860 loss: 1.1449 iter time (s): 6.035 samples/sec: 21.209
g0238:  iteration     7860/10000000 | consumed samples:      1006080 | consumed tokens:   2060451840 | elapsed time per iteration (ms): 6069.0 | learning rate: 1.373E-04 | global batch size:   128 | lm loss: 1.109227E+00 | loss scale: 262144.0 | grad norm: 0.579 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.091 | tokens per gpu per second (tgs): 1349.811 | TFLOPs: 10.86 |
g0220: [2024-08-09 13:06:15,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=7870, skipped=6, lr=[0.0001375207424, 0.0001375207424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7870 loss: 1.1269 iter time (s): 4.971 samples/sec: 25.749
g0238:  iteration     7870/10000000 | consumed samples:      1007360 | consumed tokens:   2063073280 | elapsed time per iteration (ms): 5003.8 | learning rate: 1.375E-04 | global batch size:   128 | lm loss: 1.111418E+00 | loss scale: 262144.0 | grad norm: 0.464 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.580 | tokens per gpu per second (tgs): 1637.147 | TFLOPs: 13.17 |
g0220: [2024-08-09 13:07:06,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=7880, skipped=6, lr=[0.00013769550506666667, 0.00013769550506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7880 loss: 1.1039 iter time (s): 5.039 samples/sec: 25.402
g0238:  iteration     7880/10000000 | consumed samples:      1008640 | consumed tokens:   2065694720 | elapsed time per iteration (ms): 5071.8 | learning rate: 1.377E-04 | global batch size:   128 | lm loss: 1.108738E+00 | loss scale: 262144.0 | grad norm: 0.550 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.238 | tokens per gpu per second (tgs): 1615.211 | TFLOPs: 13.00 |
g0220: [2024-08-09 13:07:58,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=7890, skipped=6, lr=[0.00013787026773333334, 0.00013787026773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7890 loss: 1.0949 iter time (s): 5.193 samples/sec: 24.649
g0238:  iteration     7890/10000000 | consumed samples:      1009920 | consumed tokens:   2068316160 | elapsed time per iteration (ms): 5225.6 | learning rate: 1.379E-04 | global batch size:   128 | lm loss: 1.105360E+00 | loss scale: 262144.0 | grad norm: 0.389 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.495 | tokens per gpu per second (tgs): 1567.659 | TFLOPs: 12.62 |
g0220: [2024-08-09 13:08:50,395] [INFO] [logging.py:96:log_dist] [Rank 0] step=7900, skipped=6, lr=[0.0001380450304, 0.0001380450304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7900 loss: 1.0758 iter time (s): 5.123 samples/sec: 24.984
g0238:  iteration     7900/10000000 | consumed samples:      1011200 | consumed tokens:   2070937600 | elapsed time per iteration (ms): 5155.9 | learning rate: 1.380E-04 | global batch size:   128 | lm loss: 1.098392E+00 | loss scale: 262144.0 | grad norm: 0.634 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.826 | tokens per gpu per second (tgs): 1588.847 | TFLOPs: 12.79 |
g0220: [2024-08-09 13:10:03,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=7910, skipped=6, lr=[0.00013821979306666667, 0.00013821979306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7910 loss: 1.1323 iter time (s): 7.269 samples/sec: 17.609
g0238:  iteration     7910/10000000 | consumed samples:      1012480 | consumed tokens:   2073559040 | elapsed time per iteration (ms): 7301.8 | learning rate: 1.382E-04 | global batch size:   128 | lm loss: 1.122421E+00 | loss scale: 262144.0 | grad norm: 0.481 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.530 | tokens per gpu per second (tgs): 1121.914 | TFLOPs: 9.03 |
g0220: [2024-08-09 13:11:00,154] [INFO] [logging.py:96:log_dist] [Rank 0] step=7920, skipped=6, lr=[0.00013839455573333334, 0.00013839455573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7920 loss: 1.0993 iter time (s): 5.641 samples/sec: 22.691
g0238:  iteration     7920/10000000 | consumed samples:      1013760 | consumed tokens:   2076180480 | elapsed time per iteration (ms): 5674.1 | learning rate: 1.384E-04 | global batch size:   128 | lm loss: 1.110100E+00 | loss scale: 262144.0 | grad norm: 0.461 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.559 | tokens per gpu per second (tgs): 1443.763 | TFLOPs: 11.62 |
g0220: [2024-08-09 13:11:55,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=7930, skipped=6, lr=[0.0001385693184, 0.0001385693184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7930 loss: 1.0501 iter time (s): 5.487 samples/sec: 23.326
g0238:  iteration     7930/10000000 | consumed samples:      1015040 | consumed tokens:   2078801920 | elapsed time per iteration (ms): 5520.7 | learning rate: 1.386E-04 | global batch size:   128 | lm loss: 1.112640E+00 | loss scale: 262144.0 | grad norm: 0.487 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.185 | tokens per gpu per second (tgs): 1483.862 | TFLOPs: 11.94 |
g0220: [2024-08-09 13:12:43,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=7940, skipped=6, lr=[0.00013874408106666667, 0.00013874408106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7940 loss: 1.0857 iter time (s): 4.822 samples/sec: 26.543
g0238:  iteration     7940/10000000 | consumed samples:      1016320 | consumed tokens:   2081423360 | elapsed time per iteration (ms): 4855.6 | learning rate: 1.387E-04 | global batch size:   128 | lm loss: 1.097662E+00 | loss scale: 262144.0 | grad norm: 0.520 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.361 | tokens per gpu per second (tgs): 1687.107 | TFLOPs: 13.58 |
g0220: [2024-08-09 13:13:37,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=7950, skipped=6, lr=[0.00013891884373333334, 0.00013891884373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7950 loss: 1.0534 iter time (s): 5.329 samples/sec: 24.020
g0238:  iteration     7950/10000000 | consumed samples:      1017600 | consumed tokens:   2084044800 | elapsed time per iteration (ms): 5362.7 | learning rate: 1.389E-04 | global batch size:   128 | lm loss: 1.100183E+00 | loss scale: 262144.0 | grad norm: 0.635 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.868 | tokens per gpu per second (tgs): 1527.584 | TFLOPs: 12.29 |
g0220: [2024-08-09 13:13:48,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 13:13:48,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 13:13:48,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 13:13:48,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 13:13:48,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 13:13:48,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 13:13:48,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 13:13:48,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 13:13:48,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 13:13:48,641] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 13:13:48,642] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 13:14:28,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=7960, skipped=6, lr=[0.0001390936064, 0.0001390936064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7960 loss: 1.1146 iter time (s): 5.064 samples/sec: 25.279
g0238:  iteration     7960/10000000 | consumed samples:      1018880 | consumed tokens:   2086666240 | elapsed time per iteration (ms): 5096.8 | learning rate: 1.391E-04 | global batch size:   128 | lm loss: 1.112099E+00 | loss scale: 524288.0 | grad norm: 0.431 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.114 | tokens per gpu per second (tgs): 1607.291 | TFLOPs: 12.93 |
g0220: [2024-08-09 13:15:18,943] [INFO] [logging.py:96:log_dist] [Rank 0] step=7970, skipped=6, lr=[0.00013926836906666667, 0.00013926836906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7970 loss: 1.1412 iter time (s): 5.010 samples/sec: 25.550
g0238:  iteration     7970/10000000 | consumed samples:      1020160 | consumed tokens:   2089287680 | elapsed time per iteration (ms): 5043.0 | learning rate: 1.393E-04 | global batch size:   128 | lm loss: 1.116691E+00 | loss scale: 524288.0 | grad norm: 0.576 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.382 | tokens per gpu per second (tgs): 1624.429 | TFLOPs: 13.07 |
g0220: [2024-08-09 13:16:07,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=7980, skipped=6, lr=[0.00013944313173333333, 0.00013944313173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7980 loss: 1.1183 iter time (s): 4.773 samples/sec: 26.818
g0238:  iteration     7980/10000000 | consumed samples:      1021440 | consumed tokens:   2091909120 | elapsed time per iteration (ms): 4806.7 | learning rate: 1.394E-04 | global batch size:   128 | lm loss: 1.115384E+00 | loss scale: 524288.0 | grad norm: 0.499 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.629 | tokens per gpu per second (tgs): 1704.286 | TFLOPs: 13.71 |
g0220: [2024-08-09 13:17:07,481] [INFO] [logging.py:96:log_dist] [Rank 0] step=7990, skipped=6, lr=[0.0001396178944, 0.0001396178944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 7990 loss: 1.1473 iter time (s): 6.014 samples/sec: 21.285
g0238:  iteration     7990/10000000 | consumed samples:      1022720 | consumed tokens:   2094530560 | elapsed time per iteration (ms): 6047.0 | learning rate: 1.396E-04 | global batch size:   128 | lm loss: 1.107194E+00 | loss scale: 524288.0 | grad norm: 0.497 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.168 | tokens per gpu per second (tgs): 1354.728 | TFLOPs: 10.90 |
g0220: [2024-08-09 13:18:02,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=8000, skipped=6, lr=[0.00013979265706666667, 0.00013979265706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8000 loss: 1.1498 iter time (s): 5.438 samples/sec: 23.538
g0238:  iteration     8000/10000000 | consumed samples:      1024000 | consumed tokens:   2097152000 | elapsed time per iteration (ms): 5470.6 | learning rate: 1.398E-04 | global batch size:   128 | lm loss: 1.114846E+00 | loss scale: 524288.0 | grad norm: 0.671 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.398 | tokens per gpu per second (tgs): 1497.458 | TFLOPs: 12.05 |
g0238: ------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 8000 | lm loss value: 1.109919E+00 | lm loss PPL: 3.034113E+00 | 
g0238: ------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration    8000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-09 13:26:52,480] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step8000 is about to be saved!
g0220: [2024-08-09 13:26:52,490] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0220: [2024-08-09 13:26:52,490] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0220: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0238: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0238: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0237: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0237: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0238: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0236: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0225: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0236: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0225: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0225: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0236: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0235: [2024-08-09 13:26:52,491] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0235: [2024-08-09 13:26:52,492] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0235: [2024-08-09 13:26:52,492] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0237: [2024-08-09 13:26:52,492] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0233: [2024-08-09 13:26:52,494] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0233: [2024-08-09 13:26:52,495] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0233: [2024-08-09 13:26:52,495] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0234: [2024-08-09 13:26:52,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0234: [2024-08-09 13:26:52,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0234: [2024-08-09 13:26:52,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0238: [2024-08-09 13:26:52,516] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_23-model_00-model_states.pt...
g0235: [2024-08-09 13:26:52,529] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_14-model_00-model_states.pt...
g0236: [2024-08-09 13:26:52,530] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_17-model_00-model_states.pt...
g0225: [2024-08-09 13:26:52,530] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_05-model_00-model_states.pt...
g0237: [2024-08-09 13:26:52,530] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_20-model_00-model_states.pt...
g0233: [2024-08-09 13:26:52,533] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_08-model_00-model_states.pt...
g0234: [2024-08-09 13:26:52,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_11-model_00-model_states.pt...
g0220: [2024-08-09 13:26:52,544] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_01-model_00-model_states.pt...
g0233: [2024-08-09 13:26:52,643] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_08-model_00-model_states.pt.
g0225: [2024-08-09 13:26:52,678] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_05-model_00-model_states.pt.
g0233: [2024-08-09 13:26:52,681] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_09-model_00-model_states.pt...
g0234: [2024-08-09 13:26:52,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_11-model_00-model_states.pt.
g0238: [2024-08-09 13:26:52,687] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 13:26:52,688] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 13:26:52,690] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_24-model_00-model_states.pt.
g0236: [2024-08-09 13:26:52,700] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_17-model_00-model_states.pt.
g0225: [2024-08-09 13:26:52,719] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_06-model_00-model_states.pt...
g0234: [2024-08-09 13:26:52,725] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_12-model_00-model_states.pt...
g0220: [2024-08-09 13:26:52,733] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_01-model_00-model_states.pt.
g0238: [2024-08-09 13:26:52,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_25-model_00-model_states.pt...
g0237: [2024-08-09 13:26:52,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_20-model_00-model_states.pt.
g0236: [2024-08-09 13:26:52,740] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_18-model_00-model_states.pt...
g0235: [2024-08-09 13:26:52,749] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_14-model_00-model_states.pt.
g0220: [2024-08-09 13:26:52,768] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_02-model_00-model_states.pt...
g0237: [2024-08-09 13:26:52,778] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_21-model_00-model_states.pt...
g0235: [2024-08-09 13:26:52,788] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_15-model_00-model_states.pt...
g0233: [2024-08-09 13:26:52,816] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_09-model_00-model_states.pt.
g0225: [2024-08-09 13:26:52,840] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_06-model_00-model_states.pt.
g0233: [2024-08-09 13:26:52,851] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_10-model_00-model_states.pt...
g0225: [2024-08-09 13:26:52,875] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_07-model_00-model_states.pt...
g0237: [2024-08-09 13:26:52,891] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_21-model_00-model_states.pt.
g0236: [2024-08-09 13:26:52,914] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_18-model_00-model_states.pt.
g0237: [2024-08-09 13:26:52,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_22-model_00-model_states.pt...
g0235: [2024-08-09 13:26:52,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_15-model_00-model_states.pt.
g0236: [2024-08-09 13:26:52,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_19-model_00-model_states.pt...
g0220: [2024-08-09 13:26:52,975] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_02-model_00-model_states.pt.
g0235: [2024-08-09 13:26:52,982] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_16-model_00-model_states.pt...
g0220: [2024-08-09 13:26:53,012] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_03-model_00-model_states.pt...
g0233: [2024-08-09 13:26:53,028] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 13:26:53,029] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_02_model_states.pt...
g0225: [2024-08-09 13:26:53,038] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 13:26:53,040] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_01_model_states.pt...
g0220: [2024-08-09 13:26:53,102] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_03-model_00-model_states.pt.
g0235: [2024-08-09 13:26:53,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 13:26:53,109] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_04_model_states.pt...
g0237: [2024-08-09 13:26:53,129] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 13:26:53,130] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_06_model_states.pt...
g0238: [2024-08-09 13:26:53,135] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 13:26:53,137] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_07_model_states.pt...
g0220: [2024-08-09 13:26:53,138] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_04-model_00-model_states.pt...
g0236: [2024-08-09 13:26:53,207] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 13:26:53,209] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_05_model_states.pt...
g0220: [2024-08-09 13:26:53,260] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 13:26:53,262] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_00_model_states.pt
g0220: [2024-08-09 13:26:53,262] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_00_model_states.pt...
g0234: [2024-08-09 13:26:53,281] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_12-model_00-model_states.pt.
g0234: [2024-08-09 13:26:53,313] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_13-model_00-model_states.pt...
g0234: [2024-08-09 13:26:53,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 13:26:53,415] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_03_model_states.pt...
g0238: [2024-08-09 13:26:55,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 13:26:55,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0233: [2024-08-09 13:26:55,445] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 13:26:55,446] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0235: [2024-08-09 13:26:55,447] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 13:26:55,447] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0225: [2024-08-09 13:26:55,463] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_01_model_states.pt.
g0225: [2024-08-09 13:26:55,464] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0237: [2024-08-09 13:26:55,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 13:26:55,482] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0236: [2024-08-09 13:26:55,798] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 13:26:55,799] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0234: [2024-08-09 13:26:55,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 13:26:55,994] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0220: [2024-08-09 13:26:57,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step8000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 13:26:57,593] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8000 is ready now!
g0220:   successfully saved checkpoint at iteration    8000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 4.33, Latency(second): 5.205
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (5205.18, 5205.52)
g0220: [2024-08-09 13:28:04,030] [INFO] [logging.py:96:log_dist] [Rank 0] step=8010, skipped=6, lr=[0.00013996741973333333, 0.00013996741973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8010 loss: 1.0882 iter time (s): 6.605 samples/sec: 19.379
g0238:  iteration     8010/10000000 | consumed samples:      1025280 | consumed tokens:   2099773440 | elapsed time per iteration (ms): 60184.4 | learning rate: 1.400E-04 | global batch size:   128 | lm loss: 1.102551E+00 | loss scale: 524288.0 | grad norm: 0.473 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.127 | tokens per gpu per second (tgs): 136.115 | TFLOPs: 1.10 |
g0220: [2024-08-09 13:29:00,811] [INFO] [logging.py:96:log_dist] [Rank 0] step=8020, skipped=6, lr=[0.0001401421824, 0.0001401421824], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8020 loss: 1.1140 iter time (s): 5.645 samples/sec: 22.676
g0238:  iteration     8020/10000000 | consumed samples:      1026560 | consumed tokens:   2102394880 | elapsed time per iteration (ms): 5678.1 | learning rate: 1.401E-04 | global batch size:   128 | lm loss: 1.117482E+00 | loss scale: 524288.0 | grad norm: 0.512 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.543 | tokens per gpu per second (tgs): 1442.747 | TFLOPs: 11.61 |
g0220: [2024-08-09 13:29:57,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=8030, skipped=6, lr=[0.00014031694506666666, 0.00014031694506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8030 loss: 1.0906 iter time (s): 5.659 samples/sec: 22.618
g0238:  iteration     8030/10000000 | consumed samples:      1027840 | consumed tokens:   2105016320 | elapsed time per iteration (ms): 5692.9 | learning rate: 1.403E-04 | global batch size:   128 | lm loss: 1.116585E+00 | loss scale: 524288.0 | grad norm: 0.412 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.484 | tokens per gpu per second (tgs): 1438.979 | TFLOPs: 11.58 |
g0235: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 8034
g0235: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 8034
g0235: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 8034
g0236: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: Grad overflow on iteration 8034
g0237: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 8034
g0238: Grad overflow on iteration 8034
g0235: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 8034
g0236: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: Grad overflow on iteration 8034
g0237: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 8034
g0237: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: Grad overflow on iteration 8034
g0238: Grad overflow on iteration 8034
g0220: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 8034
g0234: Grad overflow on iteration 8034
g0220: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 8034
g0220: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: Grad overflow on iteration 8034
g0225: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 8034
g0238: Grad overflow on iteration 8034
g0225: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 8034
g0237: Grad overflow on iteration 8034
g0234: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 8034
g0236: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 13:30:29,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: Grad overflow on iteration 8034
g0236: Grad overflow on iteration 8034
g0236: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: Grad overflow on iteration 8034
g0237: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: Grad overflow on iteration 8034
g0235: [2024-08-09 13:30:29,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 13:30:29,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: Grad overflow on iteration 8034
g0220: [2024-08-09 13:30:29,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 8034
g0238: Grad overflow on iteration 8034
g0233: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 13:30:29,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 8034
g0225: Grad overflow on iteration 8034
g0233: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 8034
g0238: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: Grad overflow on iteration 8034
g0233: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 8034
g0233: [2024-08-09 13:30:29,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 13:30:29,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 13:30:29,992] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 13:30:29,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 13:30:29,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 13:30:29,993] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 13:30:29,993] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0220: [2024-08-09 13:30:59,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=8040, skipped=7, lr=[0.00014049170773333333, 0.00014049170773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8040 loss: 1.0985 iter time (s): 6.178 samples/sec: 20.719
g0238:  iteration     8040/10000000 | consumed samples:      1029120 | consumed tokens:   2107637760 | elapsed time per iteration (ms): 6212.3 | learning rate: 1.405E-04 | global batch size:   128 | lm loss: 1.111135E+00 | loss scale: 262144.0 | grad norm: 0.481 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.604 | tokens per gpu per second (tgs): 1318.664 | TFLOPs: 10.61 |
g0220: [2024-08-09 13:32:01,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=8050, skipped=7, lr=[0.00014066647040000002, 0.00014066647040000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8050 loss: 1.0777 iter time (s): 6.087 samples/sec: 21.030
g0238:  iteration     8050/10000000 | consumed samples:      1030400 | consumed tokens:   2110259200 | elapsed time per iteration (ms): 6119.3 | learning rate: 1.407E-04 | global batch size:   128 | lm loss: 1.109949E+00 | loss scale: 262144.0 | grad norm: 0.452 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.918 | tokens per gpu per second (tgs): 1338.724 | TFLOPs: 10.77 |
g0220: [2024-08-09 13:32:59,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=8060, skipped=7, lr=[0.0001408412330666667, 0.0001408412330666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8060 loss: 1.0642 iter time (s): 5.803 samples/sec: 22.058
g0238:  iteration     8060/10000000 | consumed samples:      1031680 | consumed tokens:   2112880640 | elapsed time per iteration (ms): 5835.9 | learning rate: 1.408E-04 | global batch size:   128 | lm loss: 1.100324E+00 | loss scale: 262144.0 | grad norm: 0.520 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.933 | tokens per gpu per second (tgs): 1403.722 | TFLOPs: 11.30 |
g0220: [2024-08-09 13:33:49,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=8070, skipped=7, lr=[0.00014101599573333336, 0.00014101599573333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8070 loss: 1.0591 iter time (s): 5.022 samples/sec: 25.486
g0238:  iteration     8070/10000000 | consumed samples:      1032960 | consumed tokens:   2115502080 | elapsed time per iteration (ms): 5055.1 | learning rate: 1.410E-04 | global batch size:   128 | lm loss: 1.098590E+00 | loss scale: 262144.0 | grad norm: 0.434 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.321 | tokens per gpu per second (tgs): 1620.531 | TFLOPs: 13.04 |
g0220: [2024-08-09 13:34:44,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=8080, skipped=7, lr=[0.00014119075840000002, 0.00014119075840000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8080 loss: 1.0797 iter time (s): 5.378 samples/sec: 23.802
g0238:  iteration     8080/10000000 | consumed samples:      1034240 | consumed tokens:   2118123520 | elapsed time per iteration (ms): 5410.3 | learning rate: 1.412E-04 | global batch size:   128 | lm loss: 1.098191E+00 | loss scale: 262144.0 | grad norm: 0.453 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.659 | tokens per gpu per second (tgs): 1514.150 | TFLOPs: 12.18 |
g0220: [2024-08-09 13:35:36,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=8090, skipped=7, lr=[0.0001413655210666667, 0.0001413655210666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8090 loss: 1.1208 iter time (s): 5.212 samples/sec: 24.558
g0238:  iteration     8090/10000000 | consumed samples:      1035520 | consumed tokens:   2120744960 | elapsed time per iteration (ms): 5244.8 | learning rate: 1.414E-04 | global batch size:   128 | lm loss: 1.106899E+00 | loss scale: 262144.0 | grad norm: 0.530 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.405 | tokens per gpu per second (tgs): 1561.925 | TFLOPs: 12.57 |
g0220: [2024-08-09 13:36:25,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=8100, skipped=7, lr=[0.00014154028373333335, 0.00014154028373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8100 loss: 1.1015 iter time (s): 4.894 samples/sec: 26.153
g0238:  iteration     8100/10000000 | consumed samples:      1036800 | consumed tokens:   2123366400 | elapsed time per iteration (ms): 4927.1 | learning rate: 1.415E-04 | global batch size:   128 | lm loss: 1.094960E+00 | loss scale: 262144.0 | grad norm: 0.495 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.979 | tokens per gpu per second (tgs): 1662.639 | TFLOPs: 13.38 |
g0220: [2024-08-09 13:37:19,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=8110, skipped=7, lr=[0.00014171504640000002, 0.00014171504640000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8110 loss: 1.0652 iter time (s): 5.355 samples/sec: 23.902
g0238:  iteration     8110/10000000 | consumed samples:      1038080 | consumed tokens:   2125987840 | elapsed time per iteration (ms): 5388.0 | learning rate: 1.417E-04 | global batch size:   128 | lm loss: 1.092372E+00 | loss scale: 262144.0 | grad norm: 0.434 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.756 | tokens per gpu per second (tgs): 1520.413 | TFLOPs: 12.24 |
g0220: [2024-08-09 13:38:14,478] [INFO] [logging.py:96:log_dist] [Rank 0] step=8120, skipped=7, lr=[0.00014188980906666669, 0.00014188980906666669], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8120 loss: 1.0843 iter time (s): 5.448 samples/sec: 23.493
g0238:  iteration     8120/10000000 | consumed samples:      1039360 | consumed tokens:   2128609280 | elapsed time per iteration (ms): 5480.8 | learning rate: 1.419E-04 | global batch size:   128 | lm loss: 1.097415E+00 | loss scale: 262144.0 | grad norm: 0.446 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.354 | tokens per gpu per second (tgs): 1494.673 | TFLOPs: 12.03 |
g0220: [2024-08-09 13:39:50,424] [INFO] [logging.py:96:log_dist] [Rank 0] step=8130, skipped=7, lr=[0.00014206457173333335, 0.00014206457173333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8130 loss: 1.1141 iter time (s): 9.562 samples/sec: 13.387
g0238:  iteration     8130/10000000 | consumed samples:      1040640 | consumed tokens:   2131230720 | elapsed time per iteration (ms): 9594.6 | learning rate: 1.421E-04 | global batch size:   128 | lm loss: 1.089230E+00 | loss scale: 262144.0 | grad norm: 0.407 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.341 | tokens per gpu per second (tgs): 853.817 | TFLOPs: 6.87 |
g0220: [2024-08-09 13:40:53,224] [INFO] [logging.py:96:log_dist] [Rank 0] step=8140, skipped=7, lr=[0.00014223933440000002, 0.00014223933440000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8140 loss: 1.0467 iter time (s): 6.246 samples/sec: 20.492
g0238:  iteration     8140/10000000 | consumed samples:      1041920 | consumed tokens:   2133852160 | elapsed time per iteration (ms): 6280.0 | learning rate: 1.422E-04 | global batch size:   128 | lm loss: 1.100066E+00 | loss scale: 262144.0 | grad norm: 0.451 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.382 | tokens per gpu per second (tgs): 1304.464 | TFLOPs: 10.50 |
g0220: [2024-08-09 13:41:50,917] [INFO] [logging.py:96:log_dist] [Rank 0] step=8150, skipped=7, lr=[0.00014241409706666668, 0.00014241409706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8150 loss: 1.0884 iter time (s): 5.736 samples/sec: 22.315
g0238:  iteration     8150/10000000 | consumed samples:      1043200 | consumed tokens:   2136473600 | elapsed time per iteration (ms): 5769.3 | learning rate: 1.424E-04 | global batch size:   128 | lm loss: 1.095104E+00 | loss scale: 262144.0 | grad norm: 0.596 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.186 | tokens per gpu per second (tgs): 1419.923 | TFLOPs: 11.43 |
g0220: [2024-08-09 13:42:44,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=8160, skipped=7, lr=[0.00014258885973333335, 0.00014258885973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8160 loss: 1.0949 iter time (s): 5.287 samples/sec: 24.209
g0238:  iteration     8160/10000000 | consumed samples:      1044480 | consumed tokens:   2139095040 | elapsed time per iteration (ms): 5321.0 | learning rate: 1.426E-04 | global batch size:   128 | lm loss: 1.090805E+00 | loss scale: 262144.0 | grad norm: 0.474 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.056 | tokens per gpu per second (tgs): 1539.570 | TFLOPs: 12.39 |
g0220: [2024-08-09 13:43:40,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=8170, skipped=7, lr=[0.00014276362240000002, 0.00014276362240000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8170 loss: 1.1011 iter time (s): 5.652 samples/sec: 22.646
g0238:  iteration     8170/10000000 | consumed samples:      1045760 | consumed tokens:   2141716480 | elapsed time per iteration (ms): 5685.0 | learning rate: 1.428E-04 | global batch size:   128 | lm loss: 1.104796E+00 | loss scale: 262144.0 | grad norm: 0.538 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.515 | tokens per gpu per second (tgs): 1440.991 | TFLOPs: 11.60 |
g0220: [2024-08-09 13:44:32,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=8180, skipped=7, lr=[0.00014293838506666668, 0.00014293838506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8180 loss: 1.1073 iter time (s): 5.131 samples/sec: 24.947
g0238:  iteration     8180/10000000 | consumed samples:      1047040 | consumed tokens:   2144337920 | elapsed time per iteration (ms): 5163.8 | learning rate: 1.429E-04 | global batch size:   128 | lm loss: 1.101118E+00 | loss scale: 262144.0 | grad norm: 0.492 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.788 | tokens per gpu per second (tgs): 1586.425 | TFLOPs: 12.77 |
g0220: [2024-08-09 13:45:25,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=8190, skipped=7, lr=[0.00014311314773333335, 0.00014311314773333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8190 loss: 1.0850 iter time (s): 5.244 samples/sec: 24.409
g0238:  iteration     8190/10000000 | consumed samples:      1048320 | consumed tokens:   2146959360 | elapsed time per iteration (ms): 5276.8 | learning rate: 1.431E-04 | global batch size:   128 | lm loss: 1.095827E+00 | loss scale: 262144.0 | grad norm: 0.465 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.257 | tokens per gpu per second (tgs): 1552.465 | TFLOPs: 12.49 |
g0220: [2024-08-09 13:46:17,563] [INFO] [logging.py:96:log_dist] [Rank 0] step=8200, skipped=7, lr=[0.00014328791040000001, 0.00014328791040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8200 loss: 1.0799 iter time (s): 5.185 samples/sec: 24.685
g0238:  iteration     8200/10000000 | consumed samples:      1049600 | consumed tokens:   2149580800 | elapsed time per iteration (ms): 5218.0 | learning rate: 1.433E-04 | global batch size:   128 | lm loss: 1.086839E+00 | loss scale: 262144.0 | grad norm: 0.510 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.530 | tokens per gpu per second (tgs): 1569.942 | TFLOPs: 12.63 |
g0220: [2024-08-09 13:47:15,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=8210, skipped=7, lr=[0.00014346267306666668, 0.00014346267306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8210 loss: 1.0848 iter time (s): 5.727 samples/sec: 22.348
g0238:  iteration     8210/10000000 | consumed samples:      1050880 | consumed tokens:   2152202240 | elapsed time per iteration (ms): 5761.3 | learning rate: 1.435E-04 | global batch size:   128 | lm loss: 1.101621E+00 | loss scale: 262144.0 | grad norm: 0.471 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.217 | tokens per gpu per second (tgs): 1421.900 | TFLOPs: 11.44 |
g0220: [2024-08-09 13:48:19,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=8220, skipped=7, lr=[0.00014363743573333335, 0.00014363743573333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8220 loss: 1.1227 iter time (s): 6.361 samples/sec: 20.124
g0238:  iteration     8220/10000000 | consumed samples:      1052160 | consumed tokens:   2154823680 | elapsed time per iteration (ms): 6393.9 | learning rate: 1.436E-04 | global batch size:   128 | lm loss: 1.103149E+00 | loss scale: 262144.0 | grad norm: 0.500 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.019 | tokens per gpu per second (tgs): 1281.217 | TFLOPs: 10.31 |
g0220: [2024-08-09 13:49:20,117] [INFO] [logging.py:96:log_dist] [Rank 0] step=8230, skipped=7, lr=[0.0001438121984, 0.0001438121984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8230 loss: 1.0500 iter time (s): 6.067 samples/sec: 21.098
g0238:  iteration     8230/10000000 | consumed samples:      1053440 | consumed tokens:   2157445120 | elapsed time per iteration (ms): 6100.3 | learning rate: 1.438E-04 | global batch size:   128 | lm loss: 1.081035E+00 | loss scale: 262144.0 | grad norm: 0.494 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.983 | tokens per gpu per second (tgs): 1342.884 | TFLOPs: 10.81 |
g0220: [2024-08-09 13:50:18,407] [INFO] [logging.py:96:log_dist] [Rank 0] step=8240, skipped=7, lr=[0.00014398696106666668, 0.00014398696106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8240 loss: 1.1169 iter time (s): 5.796 samples/sec: 22.085
g0238:  iteration     8240/10000000 | consumed samples:      1054720 | consumed tokens:   2160066560 | elapsed time per iteration (ms): 5829.0 | learning rate: 1.440E-04 | global batch size:   128 | lm loss: 1.098066E+00 | loss scale: 262144.0 | grad norm: 0.421 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.959 | tokens per gpu per second (tgs): 1405.395 | TFLOPs: 11.31 |
g0220: [2024-08-09 13:51:11,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=8250, skipped=7, lr=[0.00014416172373333335, 0.00014416172373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8250 loss: 1.1154 iter time (s): 5.316 samples/sec: 24.077
g0238:  iteration     8250/10000000 | consumed samples:      1056000 | consumed tokens:   2162688000 | elapsed time per iteration (ms): 5349.0 | learning rate: 1.442E-04 | global batch size:   128 | lm loss: 1.093843E+00 | loss scale: 262144.0 | grad norm: 0.418 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.930 | tokens per gpu per second (tgs): 1531.491 | TFLOPs: 12.32 |
g0220: [2024-08-09 13:52:08,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=8260, skipped=7, lr=[0.0001443364864, 0.0001443364864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8260 loss: 1.1264 iter time (s): 5.640 samples/sec: 22.696
g0238:  iteration     8260/10000000 | consumed samples:      1057280 | consumed tokens:   2165309440 | elapsed time per iteration (ms): 5673.3 | learning rate: 1.443E-04 | global batch size:   128 | lm loss: 1.099636E+00 | loss scale: 262144.0 | grad norm: 0.515 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.562 | tokens per gpu per second (tgs): 1443.949 | TFLOPs: 11.62 |
g0220: [2024-08-09 13:53:00,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=8270, skipped=7, lr=[0.00014451124906666668, 0.00014451124906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8270 loss: 1.0858 iter time (s): 5.134 samples/sec: 24.931
g0238:  iteration     8270/10000000 | consumed samples:      1058560 | consumed tokens:   2167930880 | elapsed time per iteration (ms): 5169.5 | learning rate: 1.445E-04 | global batch size:   128 | lm loss: 1.094165E+00 | loss scale: 262144.0 | grad norm: 0.456 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.761 | tokens per gpu per second (tgs): 1584.688 | TFLOPs: 12.75 |
g0220: [2024-08-09 13:53:52,921] [INFO] [logging.py:96:log_dist] [Rank 0] step=8280, skipped=7, lr=[0.00014468601173333334, 0.00014468601173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8280 loss: 1.0792 iter time (s): 5.226 samples/sec: 24.493
g0238:  iteration     8280/10000000 | consumed samples:      1059840 | consumed tokens:   2170552320 | elapsed time per iteration (ms): 5259.3 | learning rate: 1.447E-04 | global batch size:   128 | lm loss: 1.084811E+00 | loss scale: 262144.0 | grad norm: 0.421 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.338 | tokens per gpu per second (tgs): 1557.618 | TFLOPs: 12.53 |
g0220: [2024-08-09 13:54:41,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=8290, skipped=7, lr=[0.0001448607744, 0.0001448607744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8290 loss: 1.0728 iter time (s): 4.849 samples/sec: 26.399
g0238:  iteration     8290/10000000 | consumed samples:      1061120 | consumed tokens:   2173173760 | elapsed time per iteration (ms): 4881.6 | learning rate: 1.449E-04 | global batch size:   128 | lm loss: 1.079973E+00 | loss scale: 262144.0 | grad norm: 0.464 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.221 | tokens per gpu per second (tgs): 1678.139 | TFLOPs: 13.50 |
g0220: [2024-08-09 13:55:33,416] [INFO] [logging.py:96:log_dist] [Rank 0] step=8300, skipped=7, lr=[0.00014503553706666668, 0.00014503553706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8300 loss: 1.0912 iter time (s): 5.131 samples/sec: 24.944
g0238:  iteration     8300/10000000 | consumed samples:      1062400 | consumed tokens:   2175795200 | elapsed time per iteration (ms): 5167.8 | learning rate: 1.450E-04 | global batch size:   128 | lm loss: 1.086301E+00 | loss scale: 262144.0 | grad norm: 0.434 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.769 | tokens per gpu per second (tgs): 1585.202 | TFLOPs: 12.76 |
g0220: [2024-08-09 13:56:22,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=8310, skipped=7, lr=[0.00014521029973333334, 0.00014521029973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8310 loss: 1.0970 iter time (s): 4.858 samples/sec: 26.347
g0238:  iteration     8310/10000000 | consumed samples:      1063680 | consumed tokens:   2178416640 | elapsed time per iteration (ms): 4891.0 | learning rate: 1.452E-04 | global batch size:   128 | lm loss: 1.093883E+00 | loss scale: 262144.0 | grad norm: 0.441 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.170 | tokens per gpu per second (tgs): 1674.908 | TFLOPs: 13.48 |
g0220: [2024-08-09 13:57:11,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=8320, skipped=7, lr=[0.0001453850624, 0.0001453850624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8320 loss: 1.0945 iter time (s): 4.861 samples/sec: 26.333
g0238:  iteration     8320/10000000 | consumed samples:      1064960 | consumed tokens:   2181038080 | elapsed time per iteration (ms): 4893.7 | learning rate: 1.454E-04 | global batch size:   128 | lm loss: 1.096877E+00 | loss scale: 262144.0 | grad norm: 0.545 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.156 | tokens per gpu per second (tgs): 1673.987 | TFLOPs: 13.47 |
g0220: [2024-08-09 13:58:09,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=8330, skipped=7, lr=[0.00014555982506666667, 0.00014555982506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8330 loss: 1.0875 iter time (s): 5.741 samples/sec: 22.295
g0238:  iteration     8330/10000000 | consumed samples:      1066240 | consumed tokens:   2183659520 | elapsed time per iteration (ms): 5775.0 | learning rate: 1.456E-04 | global batch size:   128 | lm loss: 1.075069E+00 | loss scale: 262144.0 | grad norm: 0.408 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.164 | tokens per gpu per second (tgs): 1418.523 | TFLOPs: 11.42 |
g0220: [2024-08-09 13:59:02,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=8340, skipped=7, lr=[0.00014573458773333334, 0.00014573458773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8340 loss: 1.1032 iter time (s): 5.364 samples/sec: 23.861
g0238:  iteration     8340/10000000 | consumed samples:      1067520 | consumed tokens:   2186280960 | elapsed time per iteration (ms): 5398.4 | learning rate: 1.457E-04 | global batch size:   128 | lm loss: 1.088657E+00 | loss scale: 262144.0 | grad norm: 0.424 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.711 | tokens per gpu per second (tgs): 1517.486 | TFLOPs: 12.21 |
g0220: [2024-08-09 14:00:01,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=8350, skipped=7, lr=[0.0001459093504, 0.0001459093504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8350 loss: 1.1124 iter time (s): 5.798 samples/sec: 22.076
g0238:  iteration     8350/10000000 | consumed samples:      1068800 | consumed tokens:   2188902400 | elapsed time per iteration (ms): 5830.9 | learning rate: 1.459E-04 | global batch size:   128 | lm loss: 1.084341E+00 | loss scale: 262144.0 | grad norm: 0.399 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.952 | tokens per gpu per second (tgs): 1404.940 | TFLOPs: 11.31 |
g0220: [2024-08-09 14:00:52,374] [INFO] [logging.py:96:log_dist] [Rank 0] step=8360, skipped=7, lr=[0.00014608411306666667, 0.00014608411306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8360 loss: 1.0758 iter time (s): 5.074 samples/sec: 25.227
g0238:  iteration     8360/10000000 | consumed samples:      1070080 | consumed tokens:   2191523840 | elapsed time per iteration (ms): 5106.7 | learning rate: 1.461E-04 | global batch size:   128 | lm loss: 1.085546E+00 | loss scale: 262144.0 | grad norm: 0.478 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.065 | tokens per gpu per second (tgs): 1604.154 | TFLOPs: 12.91 |
g0220: [2024-08-09 14:01:49,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=8370, skipped=7, lr=[0.00014625887573333334, 0.00014625887573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8370 loss: 1.0781 iter time (s): 5.713 samples/sec: 22.404
g0238:  iteration     8370/10000000 | consumed samples:      1071360 | consumed tokens:   2194145280 | elapsed time per iteration (ms): 5746.9 | learning rate: 1.463E-04 | global batch size:   128 | lm loss: 1.084720E+00 | loss scale: 262144.0 | grad norm: 0.461 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.273 | tokens per gpu per second (tgs): 1425.463 | TFLOPs: 11.47 |
g0220: [2024-08-09 14:02:46,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=8380, skipped=7, lr=[0.0001464336384, 0.0001464336384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8380 loss: 1.0883 iter time (s): 5.659 samples/sec: 22.619
g0238:  iteration     8380/10000000 | consumed samples:      1072640 | consumed tokens:   2196766720 | elapsed time per iteration (ms): 5691.5 | learning rate: 1.464E-04 | global batch size:   128 | lm loss: 1.080865E+00 | loss scale: 262144.0 | grad norm: 0.448 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.490 | tokens per gpu per second (tgs): 1439.350 | TFLOPs: 11.58 |
g0220: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 8381
g0235: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 8381
g0225: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: Grad overflow on iteration 8381
g0220: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 8381
g0225: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 8381
g0225: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 8381
g0220: Grad overflow on iteration 8381
g0235: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 8381
g0235: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 8381
g0234: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 8381
g0233: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 8381
g0233: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: Grad overflow on iteration 8381
g0237: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 8381
g0234: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 8381
g0234: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 8381
g0220: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 8381
g0238: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: Grad overflow on iteration 8381
g0234: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 8381
g0233: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 8381
g0236: Grad overflow on iteration 8381
g0237: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 8381
g0235: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 8381
g0237: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 8381
g0233: Grad overflow on iteration 8381
g0237: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: Grad overflow on iteration 8381
g0237: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 8381
g0238: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 8381
g0237: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: Grad overflow on iteration 8381
g0220: [2024-08-09 14:02:58,080] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0238: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: Grad overflow on iteration 8381
g0236: [2024-08-09 14:02:58,079] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: Grad overflow on iteration 8381
g0238: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 8381
g0220: [2024-08-09 14:02:58,080] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 14:02:58,081] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 8381
g0220: [2024-08-09 14:02:58,082] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 14:03:43,737] [INFO] [logging.py:96:log_dist] [Rank 0] step=8390, skipped=8, lr=[0.00014660840106666667, 0.00014660840106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8390 loss: 1.0649 iter time (s): 5.665 samples/sec: 22.596
g0238:  iteration     8390/10000000 | consumed samples:      1073920 | consumed tokens:   2199388160 | elapsed time per iteration (ms): 5697.9 | learning rate: 1.466E-04 | global batch size:   128 | lm loss: 1.083416E+00 | loss scale: 131072.0 | grad norm: 0.487 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.464 | tokens per gpu per second (tgs): 1437.720 | TFLOPs: 11.57 |
g0220: [2024-08-09 14:04:44,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=8400, skipped=8, lr=[0.00014678316373333334, 0.00014678316373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8400 loss: 1.0719 iter time (s): 6.039 samples/sec: 21.194
g0238:  iteration     8400/10000000 | consumed samples:      1075200 | consumed tokens:   2202009600 | elapsed time per iteration (ms): 6072.4 | learning rate: 1.468E-04 | global batch size:   128 | lm loss: 1.076564E+00 | loss scale: 131072.0 | grad norm: 0.483 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.079 | tokens per gpu per second (tgs): 1349.061 | TFLOPs: 10.86 |
g0220: [2024-08-09 14:05:42,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=8410, skipped=8, lr=[0.00014695792640000003, 0.00014695792640000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8410 loss: 1.0661 iter time (s): 5.793 samples/sec: 22.096
g0238:  iteration     8410/10000000 | consumed samples:      1076480 | consumed tokens:   2204631040 | elapsed time per iteration (ms): 5825.3 | learning rate: 1.470E-04 | global batch size:   128 | lm loss: 1.078279E+00 | loss scale: 131072.0 | grad norm: 0.393 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.973 | tokens per gpu per second (tgs): 1406.281 | TFLOPs: 11.32 |
g0220: [2024-08-09 14:06:40,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=8420, skipped=8, lr=[0.0001471326890666667, 0.0001471326890666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8420 loss: 1.0648 iter time (s): 5.741 samples/sec: 22.296
g0238:  iteration     8420/10000000 | consumed samples:      1077760 | consumed tokens:   2207252480 | elapsed time per iteration (ms): 5773.6 | learning rate: 1.471E-04 | global batch size:   128 | lm loss: 1.088615E+00 | loss scale: 131072.0 | grad norm: 0.434 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.170 | tokens per gpu per second (tgs): 1418.861 | TFLOPs: 11.42 |
g0220: [2024-08-09 14:07:34,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=8430, skipped=8, lr=[0.00014730745173333336, 0.00014730745173333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8430 loss: 1.0817 iter time (s): 5.366 samples/sec: 23.852
g0238:  iteration     8430/10000000 | consumed samples:      1079040 | consumed tokens:   2209873920 | elapsed time per iteration (ms): 5398.9 | learning rate: 1.473E-04 | global batch size:   128 | lm loss: 1.073880E+00 | loss scale: 131072.0 | grad norm: 0.504 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.709 | tokens per gpu per second (tgs): 1517.352 | TFLOPs: 12.21 |
g0220: [2024-08-09 14:08:34,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=8440, skipped=8, lr=[0.00014748221440000003, 0.00014748221440000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8440 loss: 1.1240 iter time (s): 5.996 samples/sec: 21.349
g0238:  iteration     8440/10000000 | consumed samples:      1080320 | consumed tokens:   2212495360 | elapsed time per iteration (ms): 6047.7 | learning rate: 1.475E-04 | global batch size:   128 | lm loss: 1.081680E+00 | loss scale: 131072.0 | grad norm: 0.457 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.165 | tokens per gpu per second (tgs): 1354.555 | TFLOPs: 10.90 |
g0220: [2024-08-09 14:09:28,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=8450, skipped=8, lr=[0.0001476569770666667, 0.0001476569770666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8450 loss: 1.0976 iter time (s): 5.348 samples/sec: 23.933
g0238:  iteration     8450/10000000 | consumed samples:      1081600 | consumed tokens:   2215116800 | elapsed time per iteration (ms): 5381.2 | learning rate: 1.477E-04 | global batch size:   128 | lm loss: 1.088703E+00 | loss scale: 131072.0 | grad norm: 0.465 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.786 | tokens per gpu per second (tgs): 1522.329 | TFLOPs: 12.25 |
g0220: [2024-08-09 14:10:24,933] [INFO] [logging.py:96:log_dist] [Rank 0] step=8460, skipped=8, lr=[0.00014783173973333333, 0.00014783173973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8460 loss: 1.0588 iter time (s): 5.588 samples/sec: 22.907
g0238:  iteration     8460/10000000 | consumed samples:      1082880 | consumed tokens:   2217738240 | elapsed time per iteration (ms): 5620.4 | learning rate: 1.478E-04 | global batch size:   128 | lm loss: 1.089096E+00 | loss scale: 131072.0 | grad norm: 0.489 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.774 | tokens per gpu per second (tgs): 1457.560 | TFLOPs: 11.73 |
g0220: [2024-08-09 14:11:20,760] [INFO] [logging.py:96:log_dist] [Rank 0] step=8470, skipped=8, lr=[0.0001480065024, 0.0001480065024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8470 loss: 1.0798 iter time (s): 5.550 samples/sec: 23.063
g0238:  iteration     8470/10000000 | consumed samples:      1084160 | consumed tokens:   2220359680 | elapsed time per iteration (ms): 5582.7 | learning rate: 1.480E-04 | global batch size:   128 | lm loss: 1.073478E+00 | loss scale: 131072.0 | grad norm: 0.379 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.928 | tokens per gpu per second (tgs): 1467.403 | TFLOPs: 11.81 |
g0220: [2024-08-09 14:12:11,942] [INFO] [logging.py:96:log_dist] [Rank 0] step=8480, skipped=8, lr=[0.00014818126506666666, 0.00014818126506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8480 loss: 1.1138 iter time (s): 5.085 samples/sec: 25.172
g0238:  iteration     8480/10000000 | consumed samples:      1085440 | consumed tokens:   2222981120 | elapsed time per iteration (ms): 5118.2 | learning rate: 1.482E-04 | global batch size:   128 | lm loss: 1.084959E+00 | loss scale: 131072.0 | grad norm: 0.390 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.009 | tokens per gpu per second (tgs): 1600.562 | TFLOPs: 12.88 |
g0220: [2024-08-09 14:13:09,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=8490, skipped=8, lr=[0.00014835602773333333, 0.00014835602773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8490 loss: 1.0775 iter time (s): 5.741 samples/sec: 22.296
g0238:  iteration     8490/10000000 | consumed samples:      1086720 | consumed tokens:   2225602560 | elapsed time per iteration (ms): 5774.9 | learning rate: 1.484E-04 | global batch size:   128 | lm loss: 1.081781E+00 | loss scale: 131072.0 | grad norm: 0.408 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.165 | tokens per gpu per second (tgs): 1418.560 | TFLOPs: 11.42 |
g0220: [2024-08-09 14:14:03,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=8500, skipped=8, lr=[0.0001485307904, 0.0001485307904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8500 loss: 1.0936 iter time (s): 5.340 samples/sec: 23.971
g0238:  iteration     8500/10000000 | consumed samples:      1088000 | consumed tokens:   2228224000 | elapsed time per iteration (ms): 5373.1 | learning rate: 1.485E-04 | global batch size:   128 | lm loss: 1.080851E+00 | loss scale: 131072.0 | grad norm: 0.464 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.822 | tokens per gpu per second (tgs): 1524.624 | TFLOPs: 12.27 |
g0220: [2024-08-09 14:14:55,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=8510, skipped=8, lr=[0.00014870555306666666, 0.00014870555306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8510 loss: 1.0691 iter time (s): 5.221 samples/sec: 24.518
g0238:  iteration     8510/10000000 | consumed samples:      1089280 | consumed tokens:   2230845440 | elapsed time per iteration (ms): 5253.3 | learning rate: 1.487E-04 | global batch size:   128 | lm loss: 1.082542E+00 | loss scale: 131072.0 | grad norm: 0.432 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.366 | tokens per gpu per second (tgs): 1559.405 | TFLOPs: 12.55 |
g0220: [2024-08-09 14:15:48,408] [INFO] [logging.py:96:log_dist] [Rank 0] step=8520, skipped=8, lr=[0.00014888031573333333, 0.00014888031573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8520 loss: 1.0875 iter time (s): 5.212 samples/sec: 24.557
g0238:  iteration     8520/10000000 | consumed samples:      1090560 | consumed tokens:   2233466880 | elapsed time per iteration (ms): 5245.1 | learning rate: 1.489E-04 | global batch size:   128 | lm loss: 1.076982E+00 | loss scale: 131072.0 | grad norm: 0.385 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.404 | tokens per gpu per second (tgs): 1561.837 | TFLOPs: 12.57 |
g0220: [2024-08-09 14:16:37,834] [INFO] [logging.py:96:log_dist] [Rank 0] step=8530, skipped=8, lr=[0.0001490550784, 0.0001490550784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8530 loss: 1.0996 iter time (s): 4.910 samples/sec: 26.068
g0238:  iteration     8530/10000000 | consumed samples:      1091840 | consumed tokens:   2236088320 | elapsed time per iteration (ms): 4942.7 | learning rate: 1.491E-04 | global batch size:   128 | lm loss: 1.088325E+00 | loss scale: 131072.0 | grad norm: 0.443 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.897 | tokens per gpu per second (tgs): 1657.396 | TFLOPs: 13.34 |
g0220: [2024-08-09 14:17:40,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=8540, skipped=8, lr=[0.00014922984106666666, 0.00014922984106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8540 loss: 1.0954 iter time (s): 6.281 samples/sec: 20.380
g0238:  iteration     8540/10000000 | consumed samples:      1093120 | consumed tokens:   2238709760 | elapsed time per iteration (ms): 6313.5 | learning rate: 1.492E-04 | global batch size:   128 | lm loss: 1.070463E+00 | loss scale: 131072.0 | grad norm: 0.499 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.274 | tokens per gpu per second (tgs): 1297.546 | TFLOPs: 10.44 |
g0220: [2024-08-09 14:18:46,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=8550, skipped=8, lr=[0.00014940460373333333, 0.00014940460373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8550 loss: 1.0914 iter time (s): 6.498 samples/sec: 19.698
g0238:  iteration     8550/10000000 | consumed samples:      1094400 | consumed tokens:   2241331200 | elapsed time per iteration (ms): 6531.1 | learning rate: 1.494E-04 | global batch size:   128 | lm loss: 1.075356E+00 | loss scale: 131072.0 | grad norm: 0.436 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.599 | tokens per gpu per second (tgs): 1254.314 | TFLOPs: 10.09 |
g0220: [2024-08-09 14:19:41,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=8560, skipped=8, lr=[0.0001495793664, 0.0001495793664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8560 loss: 1.0858 iter time (s): 5.485 samples/sec: 23.338
g0238:  iteration     8560/10000000 | consumed samples:      1095680 | consumed tokens:   2243952640 | elapsed time per iteration (ms): 5518.8 | learning rate: 1.496E-04 | global batch size:   128 | lm loss: 1.079130E+00 | loss scale: 131072.0 | grad norm: 0.570 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.193 | tokens per gpu per second (tgs): 1484.384 | TFLOPs: 11.95 |
g0220: [2024-08-09 14:20:40,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=8570, skipped=8, lr=[0.00014975412906666666, 0.00014975412906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8570 loss: 1.0682 iter time (s): 5.915 samples/sec: 21.639
g0238:  iteration     8570/10000000 | consumed samples:      1096960 | consumed tokens:   2246574080 | elapsed time per iteration (ms): 5947.8 | learning rate: 1.498E-04 | global batch size:   128 | lm loss: 1.082522E+00 | loss scale: 131072.0 | grad norm: 0.378 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.521 | tokens per gpu per second (tgs): 1377.316 | TFLOPs: 11.08 |
g0220: [2024-08-09 14:21:39,357] [INFO] [logging.py:96:log_dist] [Rank 0] step=8580, skipped=8, lr=[0.00014992889173333333, 0.00014992889173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8580 loss: 1.0706 iter time (s): 5.809 samples/sec: 22.036
g0238:  iteration     8580/10000000 | consumed samples:      1098240 | consumed tokens:   2249195520 | elapsed time per iteration (ms): 5841.2 | learning rate: 1.499E-04 | global batch size:   128 | lm loss: 1.074529E+00 | loss scale: 131072.0 | grad norm: 0.381 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.913 | tokens per gpu per second (tgs): 1402.463 | TFLOPs: 11.29 |
g0220: [2024-08-09 14:22:37,089] [INFO] [logging.py:96:log_dist] [Rank 0] step=8590, skipped=8, lr=[0.0001501036544, 0.0001501036544], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8590 loss: 1.0352 iter time (s): 5.741 samples/sec: 22.298
g0238:  iteration     8590/10000000 | consumed samples:      1099520 | consumed tokens:   2251816960 | elapsed time per iteration (ms): 5773.2 | learning rate: 1.501E-04 | global batch size:   128 | lm loss: 1.070914E+00 | loss scale: 131072.0 | grad norm: 0.399 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.171 | tokens per gpu per second (tgs): 1418.969 | TFLOPs: 11.42 |
g0220: [2024-08-09 14:23:34,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=8600, skipped=8, lr=[0.00015027841706666666, 0.00015027841706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8600 loss: 1.0619 iter time (s): 5.741 samples/sec: 22.296
g0238:  iteration     8600/10000000 | consumed samples:      1100800 | consumed tokens:   2254438400 | elapsed time per iteration (ms): 5773.9 | learning rate: 1.503E-04 | global batch size:   128 | lm loss: 1.059471E+00 | loss scale: 131072.0 | grad norm: 0.406 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.169 | tokens per gpu per second (tgs): 1418.807 | TFLOPs: 11.42 |
g0220: [2024-08-09 14:24:30,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=8610, skipped=8, lr=[0.00015045317973333332, 0.00015045317973333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8610 loss: 1.0742 iter time (s): 5.551 samples/sec: 23.058
g0238:  iteration     8610/10000000 | consumed samples:      1102080 | consumed tokens:   2257059840 | elapsed time per iteration (ms): 5583.8 | learning rate: 1.505E-04 | global batch size:   128 | lm loss: 1.068069E+00 | loss scale: 131072.0 | grad norm: 0.460 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.923 | tokens per gpu per second (tgs): 1467.095 | TFLOPs: 11.81 |
g0220: [2024-08-09 14:25:29,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=8620, skipped=8, lr=[0.0001506279424, 0.0001506279424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8620 loss: 1.0671 iter time (s): 5.804 samples/sec: 22.055
g0238:  iteration     8620/10000000 | consumed samples:      1103360 | consumed tokens:   2259681280 | elapsed time per iteration (ms): 5836.8 | learning rate: 1.506E-04 | global batch size:   128 | lm loss: 1.083107E+00 | loss scale: 131072.0 | grad norm: 0.480 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.930 | tokens per gpu per second (tgs): 1403.505 | TFLOPs: 11.29 |
g0220: [2024-08-09 14:26:28,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=8630, skipped=8, lr=[0.00015080270506666666, 0.00015080270506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8630 loss: 1.0924 iter time (s): 5.905 samples/sec: 21.678
g0238:  iteration     8630/10000000 | consumed samples:      1104640 | consumed tokens:   2262302720 | elapsed time per iteration (ms): 5937.5 | learning rate: 1.508E-04 | global batch size:   128 | lm loss: 1.080801E+00 | loss scale: 131072.0 | grad norm: 0.438 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.558 | tokens per gpu per second (tgs): 1379.697 | TFLOPs: 11.10 |
g0220: [2024-08-09 14:27:26,204] [INFO] [logging.py:96:log_dist] [Rank 0] step=8640, skipped=8, lr=[0.00015097746773333332, 0.00015097746773333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8640 loss: 1.0846 iter time (s): 5.746 samples/sec: 22.278
g0238:  iteration     8640/10000000 | consumed samples:      1105920 | consumed tokens:   2264924160 | elapsed time per iteration (ms): 5779.4 | learning rate: 1.510E-04 | global batch size:   128 | lm loss: 1.077804E+00 | loss scale: 131072.0 | grad norm: 0.403 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.148 | tokens per gpu per second (tgs): 1417.456 | TFLOPs: 11.41 |
g0220: [2024-08-09 14:28:25,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=8650, skipped=8, lr=[0.0001511522304, 0.0001511522304], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8650 loss: 1.1058 iter time (s): 5.898 samples/sec: 21.702
g0238:  iteration     8650/10000000 | consumed samples:      1107200 | consumed tokens:   2267545600 | elapsed time per iteration (ms): 5930.6 | learning rate: 1.512E-04 | global batch size:   128 | lm loss: 1.082669E+00 | loss scale: 131072.0 | grad norm: 0.945 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.583 | tokens per gpu per second (tgs): 1381.304 | TFLOPs: 11.12 |
g0220: [2024-08-09 14:29:22,169] [INFO] [logging.py:96:log_dist] [Rank 0] step=8660, skipped=8, lr=[0.00015132699306666665, 0.00015132699306666665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8660 loss: 1.0489 iter time (s): 5.632 samples/sec: 22.725
g0238:  iteration     8660/10000000 | consumed samples:      1108480 | consumed tokens:   2270167040 | elapsed time per iteration (ms): 5665.8 | learning rate: 1.513E-04 | global batch size:   128 | lm loss: 1.072421E+00 | loss scale: 131072.0 | grad norm: 0.432 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.592 | tokens per gpu per second (tgs): 1445.857 | TFLOPs: 11.64 |
g0220: [2024-08-09 14:30:15,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=8670, skipped=8, lr=[0.00015150175573333335, 0.00015150175573333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8670 loss: 1.0534 iter time (s): 5.341 samples/sec: 23.966
g0238:  iteration     8670/10000000 | consumed samples:      1109760 | consumed tokens:   2272788480 | elapsed time per iteration (ms): 5374.2 | learning rate: 1.515E-04 | global batch size:   128 | lm loss: 1.071222E+00 | loss scale: 131072.0 | grad norm: 0.362 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.817 | tokens per gpu per second (tgs): 1524.317 | TFLOPs: 12.27 |
g0220: [2024-08-09 14:31:10,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=8680, skipped=8, lr=[0.0001516765184, 0.0001516765184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8680 loss: 1.0728 iter time (s): 5.473 samples/sec: 23.386
g0238:  iteration     8680/10000000 | consumed samples:      1111040 | consumed tokens:   2275409920 | elapsed time per iteration (ms): 5506.0 | learning rate: 1.517E-04 | global batch size:   128 | lm loss: 1.084557E+00 | loss scale: 131072.0 | grad norm: 0.379 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.247 | tokens per gpu per second (tgs): 1487.825 | TFLOPs: 11.97 |
g0220: [2024-08-09 14:32:03,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=8690, skipped=8, lr=[0.00015185128106666668, 0.00015185128106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8690 loss: 1.0897 iter time (s): 5.224 samples/sec: 24.501
g0238:  iteration     8690/10000000 | consumed samples:      1112320 | consumed tokens:   2278031360 | elapsed time per iteration (ms): 5257.2 | learning rate: 1.519E-04 | global batch size:   128 | lm loss: 1.072116E+00 | loss scale: 131072.0 | grad norm: 0.419 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.347 | tokens per gpu per second (tgs): 1558.235 | TFLOPs: 12.54 |
g0220: [2024-08-09 14:32:55,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=8700, skipped=8, lr=[0.00015202604373333335, 0.00015202604373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8700 loss: 1.1462 iter time (s): 5.197 samples/sec: 24.631
g0238:  iteration     8700/10000000 | consumed samples:      1113600 | consumed tokens:   2280652800 | elapsed time per iteration (ms): 5229.6 | learning rate: 1.520E-04 | global batch size:   128 | lm loss: 1.082541E+00 | loss scale: 131072.0 | grad norm: 2.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.476 | tokens per gpu per second (tgs): 1566.465 | TFLOPs: 12.61 |
g0220: [2024-08-09 14:33:55,202] [INFO] [logging.py:96:log_dist] [Rank 0] step=8710, skipped=8, lr=[0.0001522008064, 0.0001522008064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8710 loss: 1.1125 iter time (s): 5.902 samples/sec: 21.686
g0238:  iteration     8710/10000000 | consumed samples:      1114880 | consumed tokens:   2283274240 | elapsed time per iteration (ms): 5936.2 | learning rate: 1.522E-04 | global batch size:   128 | lm loss: 1.076110E+00 | loss scale: 131072.0 | grad norm: 0.657 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.563 | tokens per gpu per second (tgs): 1380.012 | TFLOPs: 11.11 |
g0220: [2024-08-09 14:34:57,691] [INFO] [logging.py:96:log_dist] [Rank 0] step=8720, skipped=8, lr=[0.00015237556906666668, 0.00015237556906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8720 loss: 1.0601 iter time (s): 6.216 samples/sec: 20.593
g0238:  iteration     8720/10000000 | consumed samples:      1116160 | consumed tokens:   2285895680 | elapsed time per iteration (ms): 6248.8 | learning rate: 1.524E-04 | global batch size:   128 | lm loss: 1.071185E+00 | loss scale: 131072.0 | grad norm: 0.506 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.484 | tokens per gpu per second (tgs): 1310.966 | TFLOPs: 10.55 |
g0220: [2024-08-09 14:36:05,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=8730, skipped=8, lr=[0.00015255033173333334, 0.00015255033173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8730 loss: 1.0641 iter time (s): 6.766 samples/sec: 18.919
g0238:  iteration     8730/10000000 | consumed samples:      1117440 | consumed tokens:   2288517120 | elapsed time per iteration (ms): 6798.5 | learning rate: 1.526E-04 | global batch size:   128 | lm loss: 1.074181E+00 | loss scale: 131072.0 | grad norm: 0.387 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.828 | tokens per gpu per second (tgs): 1204.967 | TFLOPs: 9.70 |
g0220: [2024-08-09 14:36:57,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=8740, skipped=8, lr=[0.0001527250944, 0.0001527250944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8740 loss: 1.0790 iter time (s): 5.149 samples/sec: 24.860
g0238:  iteration     8740/10000000 | consumed samples:      1118720 | consumed tokens:   2291138560 | elapsed time per iteration (ms): 5181.5 | learning rate: 1.527E-04 | global batch size:   128 | lm loss: 1.073904E+00 | loss scale: 131072.0 | grad norm: 0.429 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.703 | tokens per gpu per second (tgs): 1580.996 | TFLOPs: 12.72 |
g0220: [2024-08-09 14:37:54,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=8750, skipped=8, lr=[0.00015289985706666668, 0.00015289985706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8750 loss: 1.0631 iter time (s): 5.657 samples/sec: 22.627
g0238:  iteration     8750/10000000 | consumed samples:      1120000 | consumed tokens:   2293760000 | elapsed time per iteration (ms): 5690.2 | learning rate: 1.529E-04 | global batch size:   128 | lm loss: 1.074535E+00 | loss scale: 131072.0 | grad norm: 0.471 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.495 | tokens per gpu per second (tgs): 1439.666 | TFLOPs: 11.59 |
g0220: [2024-08-09 14:38:49,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=8760, skipped=8, lr=[0.00015307461973333334, 0.00015307461973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8760 loss: 1.0350 iter time (s): 5.509 samples/sec: 23.237
g0238:  iteration     8760/10000000 | consumed samples:      1121280 | consumed tokens:   2296381440 | elapsed time per iteration (ms): 5541.7 | learning rate: 1.531E-04 | global batch size:   128 | lm loss: 1.063053E+00 | loss scale: 131072.0 | grad norm: 0.453 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.098 | tokens per gpu per second (tgs): 1478.248 | TFLOPs: 11.90 |
g0220: [2024-08-09 14:39:45,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=8770, skipped=8, lr=[0.0001532493824, 0.0001532493824], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8770 loss: 1.0827 iter time (s): 5.489 samples/sec: 23.318
g0238:  iteration     8770/10000000 | consumed samples:      1122560 | consumed tokens:   2299002880 | elapsed time per iteration (ms): 5522.3 | learning rate: 1.532E-04 | global batch size:   128 | lm loss: 1.067935E+00 | loss scale: 131072.0 | grad norm: 0.458 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.179 | tokens per gpu per second (tgs): 1483.434 | TFLOPs: 11.94 |
g0220: [2024-08-09 14:40:56,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=8780, skipped=8, lr=[0.00015342414506666667, 0.00015342414506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8780 loss: 1.0649 iter time (s): 7.081 samples/sec: 18.076
g0238:  iteration     8780/10000000 | consumed samples:      1123840 | consumed tokens:   2301624320 | elapsed time per iteration (ms): 7114.8 | learning rate: 1.534E-04 | global batch size:   128 | lm loss: 1.066381E+00 | loss scale: 131072.0 | grad norm: 0.387 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.991 | tokens per gpu per second (tgs): 1151.410 | TFLOPs: 9.27 |
g0220: [2024-08-09 14:41:57,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=8790, skipped=8, lr=[0.00015359890773333334, 0.00015359890773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8790 loss: 1.0639 iter time (s): 6.061 samples/sec: 21.117
g0238:  iteration     8790/10000000 | consumed samples:      1125120 | consumed tokens:   2304245760 | elapsed time per iteration (ms): 6095.4 | learning rate: 1.536E-04 | global batch size:   128 | lm loss: 1.069353E+00 | loss scale: 131072.0 | grad norm: 0.524 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.999 | tokens per gpu per second (tgs): 1343.961 | TFLOPs: 10.82 |
g0220: [2024-08-09 14:42:55,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=8800, skipped=8, lr=[0.0001537736704, 0.0001537736704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8800 loss: 1.0689 iter time (s): 5.763 samples/sec: 22.212
g0238:  iteration     8800/10000000 | consumed samples:      1126400 | consumed tokens:   2306867200 | elapsed time per iteration (ms): 5798.7 | learning rate: 1.538E-04 | global batch size:   128 | lm loss: 1.065667E+00 | loss scale: 131072.0 | grad norm: 0.401 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.074 | tokens per gpu per second (tgs): 1412.720 | TFLOPs: 11.37 |
g0220: [2024-08-09 14:43:54,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=8810, skipped=8, lr=[0.00015394843306666667, 0.00015394843306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8810 loss: 1.0925 iter time (s): 5.929 samples/sec: 21.588
g0238:  iteration     8810/10000000 | consumed samples:      1127680 | consumed tokens:   2309488640 | elapsed time per iteration (ms): 5962.0 | learning rate: 1.539E-04 | global batch size:   128 | lm loss: 1.070010E+00 | loss scale: 131072.0 | grad norm: 0.398 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.469 | tokens per gpu per second (tgs): 1374.046 | TFLOPs: 11.06 |
g0220: [2024-08-09 14:45:00,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=8820, skipped=8, lr=[0.00015412319573333334, 0.00015412319573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8820 loss: 1.0781 iter time (s): 6.582 samples/sec: 19.446
g0238:  iteration     8820/10000000 | consumed samples:      1128960 | consumed tokens:   2312110080 | elapsed time per iteration (ms): 6615.3 | learning rate: 1.541E-04 | global batch size:   128 | lm loss: 1.065844E+00 | loss scale: 131072.0 | grad norm: 0.441 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.349 | tokens per gpu per second (tgs): 1238.334 | TFLOPs: 9.97 |
g0220: [2024-08-09 14:46:03,383] [INFO] [logging.py:96:log_dist] [Rank 0] step=8830, skipped=8, lr=[0.0001542979584, 0.0001542979584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8830 loss: 1.0324 iter time (s): 6.216 samples/sec: 20.593
g0238:  iteration     8830/10000000 | consumed samples:      1130240 | consumed tokens:   2314731520 | elapsed time per iteration (ms): 6248.6 | learning rate: 1.543E-04 | global batch size:   128 | lm loss: 1.049257E+00 | loss scale: 131072.0 | grad norm: 0.378 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.485 | tokens per gpu per second (tgs): 1311.011 | TFLOPs: 10.55 |
g0220: [2024-08-09 14:46:59,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=8840, skipped=8, lr=[0.00015447272106666667, 0.00015447272106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8840 loss: 1.0733 iter time (s): 5.584 samples/sec: 22.924
g0238:  iteration     8840/10000000 | consumed samples:      1131520 | consumed tokens:   2317352960 | elapsed time per iteration (ms): 5616.6 | learning rate: 1.545E-04 | global batch size:   128 | lm loss: 1.066654E+00 | loss scale: 131072.0 | grad norm: 0.417 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.790 | tokens per gpu per second (tgs): 1458.532 | TFLOPs: 11.74 |
g0220: [2024-08-09 14:47:58,092] [INFO] [logging.py:96:log_dist] [Rank 0] step=8850, skipped=8, lr=[0.00015464748373333334, 0.00015464748373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8850 loss: 1.0704 iter time (s): 5.822 samples/sec: 21.987
g0238:  iteration     8850/10000000 | consumed samples:      1132800 | consumed tokens:   2319974400 | elapsed time per iteration (ms): 5854.2 | learning rate: 1.546E-04 | global batch size:   128 | lm loss: 1.064987E+00 | loss scale: 131072.0 | grad norm: 0.415 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.864 | tokens per gpu per second (tgs): 1399.327 | TFLOPs: 11.26 |
g0220: [2024-08-09 14:49:04,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=8860, skipped=8, lr=[0.0001548222464, 0.0001548222464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8860 loss: 1.0482 iter time (s): 6.625 samples/sec: 19.320
g0238:  iteration     8860/10000000 | consumed samples:      1134080 | consumed tokens:   2322595840 | elapsed time per iteration (ms): 6657.8 | learning rate: 1.548E-04 | global batch size:   128 | lm loss: 1.060692E+00 | loss scale: 131072.0 | grad norm: 0.442 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.226 | tokens per gpu per second (tgs): 1230.436 | TFLOPs: 9.90 |
g0220: [2024-08-09 14:50:06,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=8870, skipped=8, lr=[0.00015499700906666667, 0.00015499700906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8870 loss: 1.0739 iter time (s): 6.129 samples/sec: 20.885
g0238:  iteration     8870/10000000 | consumed samples:      1135360 | consumed tokens:   2325217280 | elapsed time per iteration (ms): 6162.5 | learning rate: 1.550E-04 | global batch size:   128 | lm loss: 1.064449E+00 | loss scale: 131072.0 | grad norm: 0.350 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.771 | tokens per gpu per second (tgs): 1329.323 | TFLOPs: 10.70 |
g0220: [2024-08-09 14:51:02,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=8880, skipped=8, lr=[0.00015517177173333333, 0.00015517177173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8880 loss: 1.0675 iter time (s): 5.572 samples/sec: 22.970
g0238:  iteration     8880/10000000 | consumed samples:      1136640 | consumed tokens:   2327838720 | elapsed time per iteration (ms): 5605.1 | learning rate: 1.552E-04 | global batch size:   128 | lm loss: 1.064379E+00 | loss scale: 131072.0 | grad norm: 0.394 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.836 | tokens per gpu per second (tgs): 1461.517 | TFLOPs: 11.76 |
g0220: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 14:51:17,905] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 14:51:17,905] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 14:51:17,905] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 14:51:17,905] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 14:51:17,905] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 14:51:17,905] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 14:51:17,905] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 14:51:17,905] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 14:51:17,905] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 14:51:17,906] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 14:51:58,600] [INFO] [logging.py:96:log_dist] [Rank 0] step=8890, skipped=8, lr=[0.0001553465344, 0.0001553465344], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8890 loss: 1.0515 iter time (s): 5.591 samples/sec: 22.894
g0238:  iteration     8890/10000000 | consumed samples:      1137920 | consumed tokens:   2330460160 | elapsed time per iteration (ms): 5625.3 | learning rate: 1.553E-04 | global batch size:   128 | lm loss: 1.060402E+00 | loss scale: 262144.0 | grad norm: 0.396 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.754 | tokens per gpu per second (tgs): 1456.286 | TFLOPs: 11.72 |
g0220: [2024-08-09 14:53:03,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=8900, skipped=8, lr=[0.00015552129706666667, 0.00015552129706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8900 loss: 1.0725 iter time (s): 6.459 samples/sec: 19.816
g0238:  iteration     8900/10000000 | consumed samples:      1139200 | consumed tokens:   2333081600 | elapsed time per iteration (ms): 6492.2 | learning rate: 1.555E-04 | global batch size:   128 | lm loss: 1.066330E+00 | loss scale: 262144.0 | grad norm: 0.425 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.716 | tokens per gpu per second (tgs): 1261.816 | TFLOPs: 10.15 |
g0220: [2024-08-09 14:54:17,956] [INFO] [logging.py:96:log_dist] [Rank 0] step=8910, skipped=8, lr=[0.00015569605973333333, 0.00015569605973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8910 loss: 1.0396 iter time (s): 7.410 samples/sec: 17.275
g0238:  iteration     8910/10000000 | consumed samples:      1140480 | consumed tokens:   2335703040 | elapsed time per iteration (ms): 7443.3 | learning rate: 1.557E-04 | global batch size:   128 | lm loss: 1.059863E+00 | loss scale: 262144.0 | grad norm: 0.491 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.197 | tokens per gpu per second (tgs): 1100.587 | TFLOPs: 8.86 |
g0220: [2024-08-09 14:55:16,180] [INFO] [logging.py:96:log_dist] [Rank 0] step=8920, skipped=8, lr=[0.0001558708224, 0.0001558708224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8920 loss: 1.1025 iter time (s): 5.789 samples/sec: 22.109
g0238:  iteration     8920/10000000 | consumed samples:      1141760 | consumed tokens:   2338324480 | elapsed time per iteration (ms): 5822.4 | learning rate: 1.559E-04 | global batch size:   128 | lm loss: 1.055307E+00 | loss scale: 262144.0 | grad norm: 0.419 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.984 | tokens per gpu per second (tgs): 1406.975 | TFLOPs: 11.32 |
g0220: [2024-08-09 14:56:18,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=8930, skipped=8, lr=[0.00015604558506666666, 0.00015604558506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8930 loss: 1.0161 iter time (s): 6.179 samples/sec: 20.716
g0238:  iteration     8930/10000000 | consumed samples:      1143040 | consumed tokens:   2340945920 | elapsed time per iteration (ms): 6211.7 | learning rate: 1.560E-04 | global batch size:   128 | lm loss: 1.052696E+00 | loss scale: 262144.0 | grad norm: 0.369 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.606 | tokens per gpu per second (tgs): 1318.802 | TFLOPs: 10.61 |
g0220: [2024-08-09 14:57:15,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=8940, skipped=8, lr=[0.00015622034773333333, 0.00015622034773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8940 loss: 1.0811 iter time (s): 5.708 samples/sec: 22.423
g0238:  iteration     8940/10000000 | consumed samples:      1144320 | consumed tokens:   2343567360 | elapsed time per iteration (ms): 5742.0 | learning rate: 1.562E-04 | global batch size:   128 | lm loss: 1.060018E+00 | loss scale: 262144.0 | grad norm: 0.412 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.292 | tokens per gpu per second (tgs): 1426.675 | TFLOPs: 11.48 |
g0220: [2024-08-09 14:58:13,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=8950, skipped=8, lr=[0.0001563951104, 0.0001563951104], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8950 loss: 1.0535 iter time (s): 5.775 samples/sec: 22.165
g0238:  iteration     8950/10000000 | consumed samples:      1145600 | consumed tokens:   2346188800 | elapsed time per iteration (ms): 5807.5 | learning rate: 1.564E-04 | global batch size:   128 | lm loss: 1.066928E+00 | loss scale: 262144.0 | grad norm: 0.406 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.040 | tokens per gpu per second (tgs): 1410.585 | TFLOPs: 11.35 |
g0220: [2024-08-09 14:59:13,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=8960, skipped=8, lr=[0.00015656987306666666, 0.00015656987306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8960 loss: 1.0649 iter time (s): 5.920 samples/sec: 21.620
g0238:  iteration     8960/10000000 | consumed samples:      1146880 | consumed tokens:   2348810240 | elapsed time per iteration (ms): 5954.2 | learning rate: 1.566E-04 | global batch size:   128 | lm loss: 1.058726E+00 | loss scale: 262144.0 | grad norm: 0.459 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.497 | tokens per gpu per second (tgs): 1375.836 | TFLOPs: 11.07 |
g0220: [2024-08-09 15:00:13,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=8970, skipped=8, lr=[0.00015674463573333333, 0.00015674463573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8970 loss: 1.0718 iter time (s): 5.934 samples/sec: 21.570
g0238:  iteration     8970/10000000 | consumed samples:      1148160 | consumed tokens:   2351431680 | elapsed time per iteration (ms): 5967.3 | learning rate: 1.567E-04 | global batch size:   128 | lm loss: 1.062374E+00 | loss scale: 262144.0 | grad norm: 0.428 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.450 | tokens per gpu per second (tgs): 1372.809 | TFLOPs: 11.05 |
g0220: [2024-08-09 15:01:06,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=8980, skipped=8, lr=[0.0001569193984, 0.0001569193984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8980 loss: 1.0230 iter time (s): 5.334 samples/sec: 23.998
g0238:  iteration     8980/10000000 | consumed samples:      1149440 | consumed tokens:   2354053120 | elapsed time per iteration (ms): 5366.7 | learning rate: 1.569E-04 | global batch size:   128 | lm loss: 1.063349E+00 | loss scale: 262144.0 | grad norm: 0.354 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.851 | tokens per gpu per second (tgs): 1526.450 | TFLOPs: 12.28 |
g0220: [2024-08-09 15:02:09,757] [INFO] [logging.py:96:log_dist] [Rank 0] step=8990, skipped=8, lr=[0.00015709416106666666, 0.00015709416106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 8990 loss: 1.0521 iter time (s): 6.275 samples/sec: 20.397
g0238:  iteration     8990/10000000 | consumed samples:      1150720 | consumed tokens:   2356674560 | elapsed time per iteration (ms): 6308.4 | learning rate: 1.571E-04 | global batch size:   128 | lm loss: 1.055571E+00 | loss scale: 262144.0 | grad norm: 0.366 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.290 | tokens per gpu per second (tgs): 1298.592 | TFLOPs: 10.45 |
g0220: [2024-08-09 15:03:09,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=9000, skipped=8, lr=[0.00015726892373333333, 0.00015726892373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9000 loss: 1.0638 iter time (s): 5.956 samples/sec: 21.492
g0238:  iteration     9000/10000000 | consumed samples:      1152000 | consumed tokens:   2359296000 | elapsed time per iteration (ms): 5988.1 | learning rate: 1.573E-04 | global batch size:   128 | lm loss: 1.054216E+00 | loss scale: 262144.0 | grad norm: 0.722 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.376 | tokens per gpu per second (tgs): 1368.040 | TFLOPs: 11.01 |
g0238: ------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 9000 | lm loss value: 1.056352E+00 | lm loss PPL: 2.875860E+00 | 
g0238: ------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration    9000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-09 15:12:08,974] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step9000 is about to be saved!
g0220: [2024-08-09 15:12:08,980] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0220: [2024-08-09 15:12:08,980] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0220: [2024-08-09 15:12:08,980] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0238: [2024-08-09 15:12:08,980] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0237: [2024-08-09 15:12:08,980] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0237: [2024-08-09 15:12:08,980] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0237: [2024-08-09 15:12:08,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0238: [2024-08-09 15:12:08,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0238: [2024-08-09 15:12:08,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0233: [2024-08-09 15:12:08,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0233: [2024-08-09 15:12:08,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0233: [2024-08-09 15:12:08,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0236: [2024-08-09 15:12:08,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0236: [2024-08-09 15:12:08,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0236: [2024-08-09 15:12:08,981] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0235: [2024-08-09 15:12:08,982] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0235: [2024-08-09 15:12:08,982] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0235: [2024-08-09 15:12:08,983] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0225: [2024-08-09 15:12:08,983] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0225: [2024-08-09 15:12:08,983] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0225: [2024-08-09 15:12:08,983] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0234: [2024-08-09 15:12:08,984] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0234: [2024-08-09 15:12:08,984] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0234: [2024-08-09 15:12:08,984] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0238: [2024-08-09 15:12:09,005] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_23-model_00-model_states.pt...
g0236: [2024-08-09 15:12:09,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_17-model_00-model_states.pt...
g0233: [2024-08-09 15:12:09,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_08-model_00-model_states.pt...
g0237: [2024-08-09 15:12:09,018] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_20-model_00-model_states.pt...
g0235: [2024-08-09 15:12:09,019] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_14-model_00-model_states.pt...
g0225: [2024-08-09 15:12:09,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_05-model_00-model_states.pt...
g0234: [2024-08-09 15:12:09,021] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_11-model_00-model_states.pt...
g0220: [2024-08-09 15:12:09,035] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_01-model_00-model_states.pt...
g0238: [2024-08-09 15:12:09,158] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 15:12:09,160] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 15:12:09,163] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_24-model_00-model_states.pt.
g0234: [2024-08-09 15:12:09,181] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_11-model_00-model_states.pt.
g0236: [2024-08-09 15:12:09,195] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_17-model_00-model_states.pt.
g0238: [2024-08-09 15:12:09,210] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_25-model_00-model_states.pt...
g0234: [2024-08-09 15:12:09,220] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_12-model_00-model_states.pt...
g0236: [2024-08-09 15:12:09,236] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_18-model_00-model_states.pt...
g0237: [2024-08-09 15:12:09,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_20-model_00-model_states.pt.
g0233: [2024-08-09 15:12:09,250] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_08-model_00-model_states.pt.
g0225: [2024-08-09 15:12:09,258] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_05-model_00-model_states.pt.
g0235: [2024-08-09 15:12:09,268] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_14-model_00-model_states.pt.
g0233: [2024-08-09 15:12:09,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_09-model_00-model_states.pt...
g0237: [2024-08-09 15:12:09,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_21-model_00-model_states.pt...
g0225: [2024-08-09 15:12:09,297] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_06-model_00-model_states.pt...
g0235: [2024-08-09 15:12:09,307] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_15-model_00-model_states.pt...
g0220: [2024-08-09 15:12:09,361] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_01-model_00-model_states.pt.
g0220: [2024-08-09 15:12:09,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_02-model_00-model_states.pt...
g0234: [2024-08-09 15:12:09,420] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_12-model_00-model_states.pt.
g0233: [2024-08-09 15:12:09,428] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_09-model_00-model_states.pt.
g0234: [2024-08-09 15:12:09,455] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_13-model_00-model_states.pt...
g0233: [2024-08-09 15:12:09,458] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_10-model_00-model_states.pt...
g0225: [2024-08-09 15:12:09,529] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_06-model_00-model_states.pt.
g0236: [2024-08-09 15:12:09,539] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_18-model_00-model_states.pt.
g0238: [2024-08-09 15:12:09,559] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 15:12:09,560] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_07_model_states.pt...
g0237: [2024-08-09 15:12:09,561] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_21-model_00-model_states.pt.
g0225: [2024-08-09 15:12:09,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_07-model_00-model_states.pt...
g0236: [2024-08-09 15:12:09,574] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_19-model_00-model_states.pt...
g0237: [2024-08-09 15:12:09,595] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_22-model_00-model_states.pt...
g0235: [2024-08-09 15:12:09,599] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_15-model_00-model_states.pt.
g0220: [2024-08-09 15:12:09,603] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_02-model_00-model_states.pt.
g0233: [2024-08-09 15:12:09,630] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 15:12:09,631] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_02_model_states.pt...
g0220: [2024-08-09 15:12:09,632] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_03-model_00-model_states.pt...
g0235: [2024-08-09 15:12:09,633] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_16-model_00-model_states.pt...
g0234: [2024-08-09 15:12:09,651] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 15:12:09,653] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_03_model_states.pt...
g0236: [2024-08-09 15:12:09,784] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 15:12:09,786] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_05_model_states.pt...
g0225: [2024-08-09 15:12:09,806] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 15:12:09,808] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_01_model_states.pt...
g0235: [2024-08-09 15:12:09,878] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 15:12:09,880] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_04_model_states.pt...
g0237: [2024-08-09 15:12:09,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 15:12:09,896] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_06_model_states.pt...
g0220: [2024-08-09 15:12:09,900] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 15:12:09,929] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_04-model_00-model_states.pt...
g0220: [2024-08-09 15:12:10,096] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 15:12:10,097] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_00_model_states.pt
g0220: [2024-08-09 15:12:10,097] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_00_model_states.pt...
g0238: [2024-08-09 15:12:11,550] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 15:12:11,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0234: [2024-08-09 15:12:12,160] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 15:12:12,161] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0236: [2024-08-09 15:12:12,237] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 15:12:12,238] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0235: [2024-08-09 15:12:12,412] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 15:12:12,413] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0233: [2024-08-09 15:12:12,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 15:12:12,709] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0237: [2024-08-09 15:12:12,769] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 15:12:12,769] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0225: [2024-08-09 15:12:13,471] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_01_model_states.pt.
g0225: [2024-08-09 15:12:13,471] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0220: [2024-08-09 15:12:15,429] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step9000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 15:12:15,430] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9000 is ready now!
g0220:   successfully saved checkpoint at iteration    9000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 3.47, Latency(second): 6.495
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (6495.01, 6495.71)
g0220: [2024-08-09 15:13:32,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=9010, skipped=8, lr=[0.0001574436864, 0.0001574436864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9010 loss: 1.0789 iter time (s): 7.591 samples/sec: 16.863
g0238:  iteration     9010/10000000 | consumed samples:      1153280 | consumed tokens:   2361917440 | elapsed time per iteration (ms): 62262.4 | learning rate: 1.574E-04 | global batch size:   128 | lm loss: 1.061337E+00 | loss scale: 262144.0 | grad norm: 0.583 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.056 | tokens per gpu per second (tgs): 131.572 | TFLOPs: 1.06 |
g0220: [2024-08-09 15:14:37,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=9020, skipped=8, lr=[0.00015761844906666669, 0.00015761844906666669], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9020 loss: 1.0635 iter time (s): 6.520 samples/sec: 19.632
g0238:  iteration     9020/10000000 | consumed samples:      1154560 | consumed tokens:   2364538880 | elapsed time per iteration (ms): 6552.7 | learning rate: 1.576E-04 | global batch size:   128 | lm loss: 1.063052E+00 | loss scale: 262144.0 | grad norm: 0.426 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.534 | tokens per gpu per second (tgs): 1250.166 | TFLOPs: 10.06 |
g0220: [2024-08-09 15:15:39,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=9030, skipped=8, lr=[0.00015779321173333335, 0.00015779321173333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9030 loss: 1.0166 iter time (s): 6.093 samples/sec: 21.009
g0238:  iteration     9030/10000000 | consumed samples:      1155840 | consumed tokens:   2367160320 | elapsed time per iteration (ms): 6125.3 | learning rate: 1.578E-04 | global batch size:   128 | lm loss: 1.058278E+00 | loss scale: 262144.0 | grad norm: 0.399 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.897 | tokens per gpu per second (tgs): 1337.398 | TFLOPs: 10.76 |
g0220: [2024-08-09 15:16:50,735] [INFO] [logging.py:96:log_dist] [Rank 0] step=9040, skipped=8, lr=[0.00015796797440000002, 0.00015796797440000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9040 loss: 1.0576 iter time (s): 7.136 samples/sec: 17.938
g0238:  iteration     9040/10000000 | consumed samples:      1157120 | consumed tokens:   2369781760 | elapsed time per iteration (ms): 7169.0 | learning rate: 1.580E-04 | global batch size:   128 | lm loss: 1.060094E+00 | loss scale: 262144.0 | grad norm: 0.503 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.855 | tokens per gpu per second (tgs): 1142.705 | TFLOPs: 9.20 |
g0220: [2024-08-09 15:18:00,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=9050, skipped=8, lr=[0.00015814273706666668, 0.00015814273706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9050 loss: 1.1006 iter time (s): 6.926 samples/sec: 18.482
g0238:  iteration     9050/10000000 | consumed samples:      1158400 | consumed tokens:   2372403200 | elapsed time per iteration (ms): 6958.7 | learning rate: 1.581E-04 | global batch size:   128 | lm loss: 1.063399E+00 | loss scale: 262144.0 | grad norm: 0.398 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.394 | tokens per gpu per second (tgs): 1177.225 | TFLOPs: 9.47 |
g0220: [2024-08-09 15:18:58,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=9060, skipped=8, lr=[0.00015831749973333335, 0.00015831749973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9060 loss: 1.0552 iter time (s): 5.748 samples/sec: 22.267
g0238:  iteration     9060/10000000 | consumed samples:      1159680 | consumed tokens:   2375024640 | elapsed time per iteration (ms): 5781.5 | learning rate: 1.583E-04 | global batch size:   128 | lm loss: 1.046387E+00 | loss scale: 262144.0 | grad norm: 0.422 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.140 | tokens per gpu per second (tgs): 1416.935 | TFLOPs: 11.40 |
g0220: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 9066
g0220: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 9066
g0220: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 9066
g0220: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 9066
g0220: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 9066
g0237: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 9066
g0237: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 9066
g0237: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 9066
g0225: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: Grad overflow on iteration 9066
g0225: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 9066
g0225: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 9066
g0225: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 9066
g0237: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 9066
g0236: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 9066
g0235: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 9066
g0238: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 9066
g0235: Grad overflow on iteration 9066
g0234: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 9066
g0237: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: Grad overflow on iteration 9066
g0235: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: Grad overflow on iteration 9066
g0238: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 9066
g0238: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 9066
g0233: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 9066
g0236: Grad overflow on iteration 9066
g0236: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: Grad overflow on iteration 9066
g0236: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 9066
g0234: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 9066
g0234: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: Grad overflow on iteration 9066
g0234: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 9066
g0235: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 9066
g0233: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 9066
g0233: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 9066
g0233: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 15:19:41,518] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0234: [2024-08-09 15:19:41,518] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 15:19:56,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=9070, skipped=9, lr=[0.00015849226240000002, 0.00015849226240000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9070 loss: 1.0394 iter time (s): 5.852 samples/sec: 21.874
g0238:  iteration     9070/10000000 | consumed samples:      1160960 | consumed tokens:   2377646080 | elapsed time per iteration (ms): 5885.2 | learning rate: 1.585E-04 | global batch size:   128 | lm loss: 1.061420E+00 | loss scale: 131072.0 | grad norm: 0.408 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.749 | tokens per gpu per second (tgs): 1391.964 | TFLOPs: 11.20 |
g0220: [2024-08-09 15:21:03,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=9080, skipped=9, lr=[0.00015866702506666668, 0.00015866702506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9080 loss: 1.0552 iter time (s): 6.599 samples/sec: 19.397
g0238:  iteration     9080/10000000 | consumed samples:      1162240 | consumed tokens:   2380267520 | elapsed time per iteration (ms): 6632.3 | learning rate: 1.587E-04 | global batch size:   128 | lm loss: 1.064938E+00 | loss scale: 131072.0 | grad norm: 0.454 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.299 | tokens per gpu per second (tgs): 1235.161 | TFLOPs: 9.94 |
g0220: [2024-08-09 15:22:04,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=9090, skipped=9, lr=[0.00015884178773333335, 0.00015884178773333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9090 loss: 1.0474 iter time (s): 6.117 samples/sec: 20.924
g0238:  iteration     9090/10000000 | consumed samples:      1163520 | consumed tokens:   2382888960 | elapsed time per iteration (ms): 6150.6 | learning rate: 1.588E-04 | global batch size:   128 | lm loss: 1.050427E+00 | loss scale: 131072.0 | grad norm: 0.501 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.811 | tokens per gpu per second (tgs): 1331.913 | TFLOPs: 10.72 |
g0220: [2024-08-09 15:23:01,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=9100, skipped=9, lr=[0.00015901655040000001, 0.00015901655040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9100 loss: 1.0568 iter time (s): 5.640 samples/sec: 22.696
g0238:  iteration     9100/10000000 | consumed samples:      1164800 | consumed tokens:   2385510400 | elapsed time per iteration (ms): 5673.4 | learning rate: 1.590E-04 | global batch size:   128 | lm loss: 1.051124E+00 | loss scale: 131072.0 | grad norm: 0.392 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.561 | tokens per gpu per second (tgs): 1443.930 | TFLOPs: 11.62 |
g0220: [2024-08-09 15:24:01,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=9110, skipped=9, lr=[0.00015919131306666668, 0.00015919131306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9110 loss: 1.0492 iter time (s): 5.983 samples/sec: 21.394
g0238:  iteration     9110/10000000 | consumed samples:      1166080 | consumed tokens:   2388131840 | elapsed time per iteration (ms): 6015.7 | learning rate: 1.592E-04 | global batch size:   128 | lm loss: 1.043371E+00 | loss scale: 131072.0 | grad norm: 0.410 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.278 | tokens per gpu per second (tgs): 1361.765 | TFLOPs: 10.96 |
g0220: [2024-08-09 15:25:10,906] [INFO] [logging.py:96:log_dist] [Rank 0] step=9120, skipped=9, lr=[0.00015936607573333335, 0.00015936607573333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9120 loss: 1.0514 iter time (s): 6.887 samples/sec: 18.586
g0238:  iteration     9120/10000000 | consumed samples:      1167360 | consumed tokens:   2390753280 | elapsed time per iteration (ms): 6919.4 | learning rate: 1.594E-04 | global batch size:   128 | lm loss: 1.056199E+00 | loss scale: 131072.0 | grad norm: 0.546 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.499 | tokens per gpu per second (tgs): 1183.921 | TFLOPs: 9.53 |
g0220: [2024-08-09 15:26:09,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=9130, skipped=9, lr=[0.0001595408384, 0.0001595408384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9130 loss: 1.0383 iter time (s): 5.863 samples/sec: 21.833
g0238:  iteration     9130/10000000 | consumed samples:      1168640 | consumed tokens:   2393374720 | elapsed time per iteration (ms): 5895.5 | learning rate: 1.595E-04 | global batch size:   128 | lm loss: 1.043809E+00 | loss scale: 131072.0 | grad norm: 1.130 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.711 | tokens per gpu per second (tgs): 1389.531 | TFLOPs: 11.18 |
g0220: [2024-08-09 15:27:15,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=9140, skipped=9, lr=[0.00015971560106666668, 0.00015971560106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9140 loss: 1.0458 iter time (s): 6.529 samples/sec: 19.604
g0238:  iteration     9140/10000000 | consumed samples:      1169920 | consumed tokens:   2395996160 | elapsed time per iteration (ms): 6562.2 | learning rate: 1.597E-04 | global batch size:   128 | lm loss: 1.047873E+00 | loss scale: 131072.0 | grad norm: 0.512 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.506 | tokens per gpu per second (tgs): 1248.369 | TFLOPs: 10.05 |
g0220: [2024-08-09 15:28:20,245] [INFO] [logging.py:96:log_dist] [Rank 0] step=9150, skipped=9, lr=[0.00015989036373333334, 0.00015989036373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9150 loss: 1.0500 iter time (s): 6.444 samples/sec: 19.865
g0238:  iteration     9150/10000000 | consumed samples:      1171200 | consumed tokens:   2398617600 | elapsed time per iteration (ms): 6476.2 | learning rate: 1.599E-04 | global batch size:   128 | lm loss: 1.045669E+00 | loss scale: 131072.0 | grad norm: 0.458 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.765 | tokens per gpu per second (tgs): 1264.936 | TFLOPs: 10.18 |
g0220: [2024-08-09 15:29:22,685] [INFO] [logging.py:96:log_dist] [Rank 0] step=9160, skipped=9, lr=[0.0001600651264, 0.0001600651264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9160 loss: 1.0734 iter time (s): 6.211 samples/sec: 20.607
g0238:  iteration     9160/10000000 | consumed samples:      1172480 | consumed tokens:   2401239040 | elapsed time per iteration (ms): 6243.9 | learning rate: 1.601E-04 | global batch size:   128 | lm loss: 1.050957E+00 | loss scale: 131072.0 | grad norm: 0.409 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.500 | tokens per gpu per second (tgs): 1311.994 | TFLOPs: 10.56 |
g0220: [2024-08-09 15:30:37,083] [INFO] [logging.py:96:log_dist] [Rank 0] step=9170, skipped=9, lr=[0.00016023988906666668, 0.00016023988906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9170 loss: 1.0219 iter time (s): 7.407 samples/sec: 17.280
g0238:  iteration     9170/10000000 | consumed samples:      1173760 | consumed tokens:   2403860480 | elapsed time per iteration (ms): 7439.8 | learning rate: 1.602E-04 | global batch size:   128 | lm loss: 1.037934E+00 | loss scale: 131072.0 | grad norm: 0.388 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.205 | tokens per gpu per second (tgs): 1101.099 | TFLOPs: 8.86 |
g0220: [2024-08-09 15:31:55,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=9180, skipped=9, lr=[0.00016041465173333334, 0.00016041465173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9180 loss: 1.0618 iter time (s): 7.829 samples/sec: 16.349
g0238:  iteration     9180/10000000 | consumed samples:      1175040 | consumed tokens:   2406481920 | elapsed time per iteration (ms): 7863.6 | learning rate: 1.604E-04 | global batch size:   128 | lm loss: 1.053351E+00 | loss scale: 131072.0 | grad norm: 0.387 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 16.278 | tokens per gpu per second (tgs): 1041.768 | TFLOPs: 8.38 |
g0220: [2024-08-09 15:32:57,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=9190, skipped=9, lr=[0.0001605894144, 0.0001605894144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9190 loss: 1.0589 iter time (s): 6.177 samples/sec: 20.724
g0238:  iteration     9190/10000000 | consumed samples:      1176320 | consumed tokens:   2409103360 | elapsed time per iteration (ms): 6209.1 | learning rate: 1.606E-04 | global batch size:   128 | lm loss: 1.051479E+00 | loss scale: 131072.0 | grad norm: 0.469 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.615 | tokens per gpu per second (tgs): 1319.346 | TFLOPs: 10.62 |
g0220: [2024-08-09 15:34:02,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=9200, skipped=9, lr=[0.00016076417706666668, 0.00016076417706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9200 loss: 1.0076 iter time (s): 6.392 samples/sec: 20.025
g0238:  iteration     9200/10000000 | consumed samples:      1177600 | consumed tokens:   2411724800 | elapsed time per iteration (ms): 6425.3 | learning rate: 1.608E-04 | global batch size:   128 | lm loss: 1.044896E+00 | loss scale: 131072.0 | grad norm: 0.401 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.921 | tokens per gpu per second (tgs): 1274.955 | TFLOPs: 10.26 |
g0220: [2024-08-09 15:35:08,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=9210, skipped=9, lr=[0.00016093893973333334, 0.00016093893973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9210 loss: 1.0489 iter time (s): 6.638 samples/sec: 19.282
g0238:  iteration     9210/10000000 | consumed samples:      1178880 | consumed tokens:   2414346240 | elapsed time per iteration (ms): 6671.7 | learning rate: 1.609E-04 | global batch size:   128 | lm loss: 1.043516E+00 | loss scale: 131072.0 | grad norm: 0.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.186 | tokens per gpu per second (tgs): 1227.877 | TFLOPs: 9.88 |
g0220: [2024-08-09 15:36:06,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=9220, skipped=9, lr=[0.0001611137024, 0.0001611137024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9220 loss: 1.0310 iter time (s): 5.777 samples/sec: 22.155
g0238:  iteration     9220/10000000 | consumed samples:      1180160 | consumed tokens:   2416967680 | elapsed time per iteration (ms): 5810.9 | learning rate: 1.611E-04 | global batch size:   128 | lm loss: 1.045800E+00 | loss scale: 131072.0 | grad norm: 0.444 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.027 | tokens per gpu per second (tgs): 1409.758 | TFLOPs: 11.34 |
g0220: [2024-08-09 15:37:05,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=9230, skipped=9, lr=[0.00016128846506666667, 0.00016128846506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9230 loss: 1.0157 iter time (s): 5.793 samples/sec: 22.094
g0238:  iteration     9230/10000000 | consumed samples:      1181440 | consumed tokens:   2419589120 | elapsed time per iteration (ms): 5826.1 | learning rate: 1.613E-04 | global batch size:   128 | lm loss: 1.037443E+00 | loss scale: 131072.0 | grad norm: 0.438 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.970 | tokens per gpu per second (tgs): 1406.093 | TFLOPs: 11.32 |
g0220: [2024-08-09 15:38:08,580] [INFO] [logging.py:96:log_dist] [Rank 0] step=9240, skipped=9, lr=[0.00016146322773333334, 0.00016146322773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9240 loss: 1.0053 iter time (s): 6.310 samples/sec: 20.286
g0238:  iteration     9240/10000000 | consumed samples:      1182720 | consumed tokens:   2422210560 | elapsed time per iteration (ms): 6343.0 | learning rate: 1.615E-04 | global batch size:   128 | lm loss: 1.057374E+00 | loss scale: 131072.0 | grad norm: 0.445 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.180 | tokens per gpu per second (tgs): 1291.501 | TFLOPs: 10.39 |
g0220: [2024-08-09 15:39:12,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=9250, skipped=9, lr=[0.0001616379904, 0.0001616379904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9250 loss: 1.0463 iter time (s): 6.352 samples/sec: 20.150
g0238:  iteration     9250/10000000 | consumed samples:      1184000 | consumed tokens:   2424832000 | elapsed time per iteration (ms): 6385.1 | learning rate: 1.616E-04 | global batch size:   128 | lm loss: 1.036706E+00 | loss scale: 131072.0 | grad norm: 0.411 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.047 | tokens per gpu per second (tgs): 1282.978 | TFLOPs: 10.32 |
g0220: [2024-08-09 15:40:18,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=9260, skipped=9, lr=[0.00016181275306666667, 0.00016181275306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9260 loss: 1.0322 iter time (s): 6.551 samples/sec: 19.540
g0238:  iteration     9260/10000000 | consumed samples:      1185280 | consumed tokens:   2427453440 | elapsed time per iteration (ms): 6583.8 | learning rate: 1.618E-04 | global batch size:   128 | lm loss: 1.053372E+00 | loss scale: 131072.0 | grad norm: 0.367 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.442 | tokens per gpu per second (tgs): 1244.265 | TFLOPs: 10.01 |
g0220: [2024-08-09 15:41:12,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=9270, skipped=9, lr=[0.00016198751573333334, 0.00016198751573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9270 loss: 1.0437 iter time (s): 5.367 samples/sec: 23.848
g0238:  iteration     9270/10000000 | consumed samples:      1186560 | consumed tokens:   2430074880 | elapsed time per iteration (ms): 5400.0 | learning rate: 1.620E-04 | global batch size:   128 | lm loss: 1.039405E+00 | loss scale: 131072.0 | grad norm: 0.443 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.704 | tokens per gpu per second (tgs): 1517.039 | TFLOPs: 12.21 |
g0220: [2024-08-09 15:42:14,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=9280, skipped=9, lr=[0.0001621622784, 0.0001621622784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9280 loss: 1.0549 iter time (s): 6.215 samples/sec: 20.594
g0238:  iteration     9280/10000000 | consumed samples:      1187840 | consumed tokens:   2432696320 | elapsed time per iteration (ms): 6253.0 | learning rate: 1.622E-04 | global batch size:   128 | lm loss: 1.053944E+00 | loss scale: 131072.0 | grad norm: 0.463 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.470 | tokens per gpu per second (tgs): 1310.086 | TFLOPs: 10.54 |
g0220: [2024-08-09 15:43:16,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=9290, skipped=9, lr=[0.00016233704106666667, 0.00016233704106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9290 loss: 1.0402 iter time (s): 6.181 samples/sec: 20.709
g0238:  iteration     9290/10000000 | consumed samples:      1189120 | consumed tokens:   2435317760 | elapsed time per iteration (ms): 6213.6 | learning rate: 1.623E-04 | global batch size:   128 | lm loss: 1.043484E+00 | loss scale: 131072.0 | grad norm: 0.475 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.600 | tokens per gpu per second (tgs): 1318.408 | TFLOPs: 10.61 |
g0220: [2024-08-09 15:44:17,973] [INFO] [logging.py:96:log_dist] [Rank 0] step=9300, skipped=9, lr=[0.00016251180373333334, 0.00016251180373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9300 loss: 1.0200 iter time (s): 6.071 samples/sec: 21.085
g0238:  iteration     9300/10000000 | consumed samples:      1190400 | consumed tokens:   2437939200 | elapsed time per iteration (ms): 6104.0 | learning rate: 1.625E-04 | global batch size:   128 | lm loss: 1.035255E+00 | loss scale: 131072.0 | grad norm: 0.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.970 | tokens per gpu per second (tgs): 1342.060 | TFLOPs: 10.80 |
g0220: [2024-08-09 15:45:18,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=9310, skipped=9, lr=[0.0001626865664, 0.0001626865664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9310 loss: 1.0414 iter time (s): 6.017 samples/sec: 21.272
g0238:  iteration     9310/10000000 | consumed samples:      1191680 | consumed tokens:   2440560640 | elapsed time per iteration (ms): 6050.6 | learning rate: 1.627E-04 | global batch size:   128 | lm loss: 1.042027E+00 | loss scale: 131072.0 | grad norm: 0.392 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.155 | tokens per gpu per second (tgs): 1353.922 | TFLOPs: 10.90 |
g0220: [2024-08-09 15:46:19,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=9320, skipped=9, lr=[0.00016286132906666667, 0.00016286132906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9320 loss: 1.0643 iter time (s): 6.109 samples/sec: 20.954
g0238:  iteration     9320/10000000 | consumed samples:      1192960 | consumed tokens:   2443182080 | elapsed time per iteration (ms): 6141.1 | learning rate: 1.629E-04 | global batch size:   128 | lm loss: 1.045386E+00 | loss scale: 131072.0 | grad norm: 0.472 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.843 | tokens per gpu per second (tgs): 1333.965 | TFLOPs: 10.73 |
g0220: [2024-08-09 15:47:20,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=9330, skipped=9, lr=[0.00016303609173333333, 0.00016303609173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9330 loss: 1.0006 iter time (s): 6.000 samples/sec: 21.332
g0238:  iteration     9330/10000000 | consumed samples:      1194240 | consumed tokens:   2445803520 | elapsed time per iteration (ms): 6033.9 | learning rate: 1.630E-04 | global batch size:   128 | lm loss: 1.035600E+00 | loss scale: 131072.0 | grad norm: 0.459 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.213 | tokens per gpu per second (tgs): 1357.653 | TFLOPs: 10.93 |
g0220: [2024-08-09 15:48:16,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=9340, skipped=9, lr=[0.0001632108544, 0.0001632108544], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9340 loss: 1.0810 iter time (s): 5.635 samples/sec: 22.716
g0238:  iteration     9340/10000000 | consumed samples:      1195520 | consumed tokens:   2448424960 | elapsed time per iteration (ms): 5667.8 | learning rate: 1.632E-04 | global batch size:   128 | lm loss: 1.042952E+00 | loss scale: 131072.0 | grad norm: 0.444 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.584 | tokens per gpu per second (tgs): 1445.351 | TFLOPs: 11.63 |
g0220: [2024-08-09 15:49:13,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=9350, skipped=9, lr=[0.00016338561706666667, 0.00016338561706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9350 loss: 1.0628 iter time (s): 5.674 samples/sec: 22.560
g0238:  iteration     9350/10000000 | consumed samples:      1196800 | consumed tokens:   2451046400 | elapsed time per iteration (ms): 5707.3 | learning rate: 1.634E-04 | global batch size:   128 | lm loss: 1.047259E+00 | loss scale: 131072.0 | grad norm: 0.465 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.428 | tokens per gpu per second (tgs): 1435.361 | TFLOPs: 11.55 |
g0220: [2024-08-09 15:50:17,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=9360, skipped=9, lr=[0.00016356037973333333, 0.00016356037973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9360 loss: 1.0149 iter time (s): 6.284 samples/sec: 20.369
g0238:  iteration     9360/10000000 | consumed samples:      1198080 | consumed tokens:   2453667840 | elapsed time per iteration (ms): 6317.5 | learning rate: 1.636E-04 | global batch size:   128 | lm loss: 1.046092E+00 | loss scale: 131072.0 | grad norm: 0.329 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.261 | tokens per gpu per second (tgs): 1296.707 | TFLOPs: 10.43 |
g0220: [2024-08-09 15:51:15,743] [INFO] [logging.py:96:log_dist] [Rank 0] step=9370, skipped=9, lr=[0.0001637351424, 0.0001637351424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9370 loss: 1.0453 iter time (s): 5.826 samples/sec: 21.972
g0238:  iteration     9370/10000000 | consumed samples:      1199360 | consumed tokens:   2456289280 | elapsed time per iteration (ms): 5858.2 | learning rate: 1.637E-04 | global batch size:   128 | lm loss: 1.033961E+00 | loss scale: 131072.0 | grad norm: 0.367 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.850 | tokens per gpu per second (tgs): 1398.387 | TFLOPs: 11.25 |
g0220: [2024-08-09 15:52:16,806] [INFO] [logging.py:96:log_dist] [Rank 0] step=9380, skipped=9, lr=[0.0001639099050666667, 0.0001639099050666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9380 loss: 1.0515 iter time (s): 6.074 samples/sec: 21.074
g0238:  iteration     9380/10000000 | consumed samples:      1200640 | consumed tokens:   2458910720 | elapsed time per iteration (ms): 6106.3 | learning rate: 1.639E-04 | global batch size:   128 | lm loss: 1.050564E+00 | loss scale: 131072.0 | grad norm: 0.400 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.962 | tokens per gpu per second (tgs): 1341.561 | TFLOPs: 10.80 |
g0220: [2024-08-09 15:53:13,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=9390, skipped=9, lr=[0.00016408466773333336, 0.00016408466773333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9390 loss: 1.0322 iter time (s): 5.648 samples/sec: 22.663
g0238:  iteration     9390/10000000 | consumed samples:      1201920 | consumed tokens:   2461532160 | elapsed time per iteration (ms): 5680.6 | learning rate: 1.641E-04 | global batch size:   128 | lm loss: 1.043017E+00 | loss scale: 131072.0 | grad norm: 0.433 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.533 | tokens per gpu per second (tgs): 1442.113 | TFLOPs: 11.60 |
g0220: [2024-08-09 15:54:08,773] [INFO] [logging.py:96:log_dist] [Rank 0] step=9400, skipped=9, lr=[0.00016425943040000002, 0.00016425943040000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9400 loss: 1.0346 iter time (s): 5.483 samples/sec: 23.343
g0238:  iteration     9400/10000000 | consumed samples:      1203200 | consumed tokens:   2464153600 | elapsed time per iteration (ms): 5516.0 | learning rate: 1.643E-04 | global batch size:   128 | lm loss: 1.033489E+00 | loss scale: 131072.0 | grad norm: 0.359 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.205 | tokens per gpu per second (tgs): 1485.123 | TFLOPs: 11.95 |
g0220: [2024-08-09 15:55:13,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=9410, skipped=9, lr=[0.0001644341930666667, 0.0001644341930666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9410 loss: 1.0113 iter time (s): 6.409 samples/sec: 19.973
g0238:  iteration     9410/10000000 | consumed samples:      1204480 | consumed tokens:   2466775040 | elapsed time per iteration (ms): 6442.7 | learning rate: 1.644E-04 | global batch size:   128 | lm loss: 1.045943E+00 | loss scale: 131072.0 | grad norm: 0.381 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.868 | tokens per gpu per second (tgs): 1271.521 | TFLOPs: 10.23 |
g0220: [2024-08-09 15:56:20,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=9420, skipped=9, lr=[0.00016460895573333336, 0.00016460895573333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9420 loss: 1.0212 iter time (s): 6.716 samples/sec: 19.058
g0238:  iteration     9420/10000000 | consumed samples:      1205760 | consumed tokens:   2469396480 | elapsed time per iteration (ms): 6750.2 | learning rate: 1.646E-04 | global batch size:   128 | lm loss: 1.037286E+00 | loss scale: 131072.0 | grad norm: 0.343 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.963 | tokens per gpu per second (tgs): 1213.600 | TFLOPs: 9.77 |
g0220: [2024-08-09 15:57:22,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=9430, skipped=9, lr=[0.00016478371840000002, 0.00016478371840000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9430 loss: 1.0352 iter time (s): 6.120 samples/sec: 20.915
g0238:  iteration     9430/10000000 | consumed samples:      1207040 | consumed tokens:   2472017920 | elapsed time per iteration (ms): 6153.2 | learning rate: 1.648E-04 | global batch size:   128 | lm loss: 1.039997E+00 | loss scale: 131072.0 | grad norm: 0.367 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.802 | tokens per gpu per second (tgs): 1331.349 | TFLOPs: 10.71 |
g0220: [2024-08-09 15:58:17,129] [INFO] [logging.py:96:log_dist] [Rank 0] step=9440, skipped=9, lr=[0.0001649584810666667, 0.0001649584810666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9440 loss: 1.0653 iter time (s): 5.456 samples/sec: 23.461
g0238:  iteration     9440/10000000 | consumed samples:      1208320 | consumed tokens:   2474639360 | elapsed time per iteration (ms): 5490.9 | learning rate: 1.650E-04 | global batch size:   128 | lm loss: 1.036050E+00 | loss scale: 131072.0 | grad norm: 0.466 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.311 | tokens per gpu per second (tgs): 1491.924 | TFLOPs: 12.01 |
g0220: [2024-08-09 15:59:17,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=9450, skipped=9, lr=[0.00016513324373333335, 0.00016513324373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9450 loss: 1.0328 iter time (s): 5.964 samples/sec: 21.461
g0238:  iteration     9450/10000000 | consumed samples:      1209600 | consumed tokens:   2477260800 | elapsed time per iteration (ms): 5997.6 | learning rate: 1.651E-04 | global batch size:   128 | lm loss: 1.034337E+00 | loss scale: 131072.0 | grad norm: 0.380 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.342 | tokens per gpu per second (tgs): 1365.877 | TFLOPs: 10.99 |
g0220: [2024-08-09 16:00:21,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=9460, skipped=9, lr=[0.00016530800640000002, 0.00016530800640000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9460 loss: 1.0188 iter time (s): 6.428 samples/sec: 19.913
g0238:  iteration     9460/10000000 | consumed samples:      1210880 | consumed tokens:   2479882240 | elapsed time per iteration (ms): 6461.9 | learning rate: 1.653E-04 | global batch size:   128 | lm loss: 1.035329E+00 | loss scale: 131072.0 | grad norm: 0.392 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.808 | tokens per gpu per second (tgs): 1267.729 | TFLOPs: 10.20 |
g0220: [2024-08-09 16:01:21,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=9470, skipped=9, lr=[0.00016548276906666669, 0.00016548276906666669], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9470 loss: 1.0224 iter time (s): 5.899 samples/sec: 21.699
g0238:  iteration     9470/10000000 | consumed samples:      1212160 | consumed tokens:   2482503680 | elapsed time per iteration (ms): 5931.9 | learning rate: 1.655E-04 | global batch size:   128 | lm loss: 1.037424E+00 | loss scale: 131072.0 | grad norm: 0.414 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.578 | tokens per gpu per second (tgs): 1381.005 | TFLOPs: 11.11 |
g0220: [2024-08-09 16:02:13,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=9480, skipped=9, lr=[0.00016565753173333335, 0.00016565753173333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9480 loss: 1.0306 iter time (s): 5.192 samples/sec: 24.652
g0238:  iteration     9480/10000000 | consumed samples:      1213440 | consumed tokens:   2485125120 | elapsed time per iteration (ms): 5225.2 | learning rate: 1.657E-04 | global batch size:   128 | lm loss: 1.042490E+00 | loss scale: 131072.0 | grad norm: 0.379 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.497 | tokens per gpu per second (tgs): 1567.801 | TFLOPs: 12.62 |
g0220: [2024-08-09 16:03:14,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=9490, skipped=9, lr=[0.00016583229440000002, 0.00016583229440000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9490 loss: 1.0379 iter time (s): 6.056 samples/sec: 21.136
g0238:  iteration     9490/10000000 | consumed samples:      1214720 | consumed tokens:   2487746560 | elapsed time per iteration (ms): 6089.6 | learning rate: 1.658E-04 | global batch size:   128 | lm loss: 1.035328E+00 | loss scale: 131072.0 | grad norm: 0.445 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.020 | tokens per gpu per second (tgs): 1345.252 | TFLOPs: 10.83 |
g0220: [2024-08-09 16:04:02,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=9500, skipped=9, lr=[0.00016600705706666668, 0.00016600705706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9500 loss: 1.0273 iter time (s): 4.798 samples/sec: 26.680
g0238:  iteration     9500/10000000 | consumed samples:      1216000 | consumed tokens:   2490368000 | elapsed time per iteration (ms): 4830.9 | learning rate: 1.660E-04 | global batch size:   128 | lm loss: 1.038042E+00 | loss scale: 131072.0 | grad norm: 0.372 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.496 | tokens per gpu per second (tgs): 1695.767 | TFLOPs: 13.65 |
g0220: [2024-08-09 16:04:51,150] [INFO] [logging.py:96:log_dist] [Rank 0] step=9510, skipped=9, lr=[0.00016618181973333335, 0.00016618181973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9510 loss: 1.0360 iter time (s): 4.831 samples/sec: 26.496
g0238:  iteration     9510/10000000 | consumed samples:      1217280 | consumed tokens:   2492989440 | elapsed time per iteration (ms): 4863.7 | learning rate: 1.662E-04 | global batch size:   128 | lm loss: 1.035709E+00 | loss scale: 131072.0 | grad norm: 0.379 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.318 | tokens per gpu per second (tgs): 1684.330 | TFLOPs: 13.55 |
g0220: [2024-08-09 16:05:39,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=9520, skipped=9, lr=[0.00016635658240000002, 0.00016635658240000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9520 loss: 1.0573 iter time (s): 4.768 samples/sec: 26.846
g0238:  iteration     9520/10000000 | consumed samples:      1218560 | consumed tokens:   2495610880 | elapsed time per iteration (ms): 4800.7 | learning rate: 1.664E-04 | global batch size:   128 | lm loss: 1.037592E+00 | loss scale: 131072.0 | grad norm: 0.505 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.663 | tokens per gpu per second (tgs): 1706.429 | TFLOPs: 13.73 |
g0220: [2024-08-09 16:06:38,658] [INFO] [logging.py:96:log_dist] [Rank 0] step=9530, skipped=9, lr=[0.00016653134506666668, 0.00016653134506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9530 loss: 1.0510 iter time (s): 5.916 samples/sec: 21.635
g0238:  iteration     9530/10000000 | consumed samples:      1219840 | consumed tokens:   2498232320 | elapsed time per iteration (ms): 5950.1 | learning rate: 1.665E-04 | global batch size:   128 | lm loss: 1.034750E+00 | loss scale: 131072.0 | grad norm: 0.482 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.512 | tokens per gpu per second (tgs): 1376.782 | TFLOPs: 11.08 |
g0220: [2024-08-09 16:07:31,155] [INFO] [logging.py:96:log_dist] [Rank 0] step=9540, skipped=9, lr=[0.00016670610773333335, 0.00016670610773333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9540 loss: 1.0543 iter time (s): 5.215 samples/sec: 24.543
g0238:  iteration     9540/10000000 | consumed samples:      1221120 | consumed tokens:   2500853760 | elapsed time per iteration (ms): 5249.7 | learning rate: 1.667E-04 | global batch size:   128 | lm loss: 1.035789E+00 | loss scale: 131072.0 | grad norm: 0.406 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.382 | tokens per gpu per second (tgs): 1560.461 | TFLOPs: 12.56 |
g0220: [2024-08-09 16:08:21,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=9550, skipped=9, lr=[0.00016688087040000001, 0.00016688087040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9550 loss: 1.0643 iter time (s): 4.983 samples/sec: 25.690
g0238:  iteration     9550/10000000 | consumed samples:      1222400 | consumed tokens:   2503475200 | elapsed time per iteration (ms): 5015.6 | learning rate: 1.669E-04 | global batch size:   128 | lm loss: 1.044398E+00 | loss scale: 131072.0 | grad norm: 0.394 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.521 | tokens per gpu per second (tgs): 1633.316 | TFLOPs: 13.14 |
g0220: [2024-08-09 16:09:18,091] [INFO] [logging.py:96:log_dist] [Rank 0] step=9560, skipped=9, lr=[0.00016705563306666668, 0.00016705563306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9560 loss: 1.0095 iter time (s): 5.645 samples/sec: 22.674
g0238:  iteration     9560/10000000 | consumed samples:      1223680 | consumed tokens:   2506096640 | elapsed time per iteration (ms): 5678.0 | learning rate: 1.671E-04 | global batch size:   128 | lm loss: 1.033166E+00 | loss scale: 131072.0 | grad norm: 0.334 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.543 | tokens per gpu per second (tgs): 1442.773 | TFLOPs: 11.61 |
g0233: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 16:10:04,737] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 16:10:04,738] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 16:10:04,738] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 16:10:04,738] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 16:10:16,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=9570, skipped=9, lr=[0.00016723039573333335, 0.00016723039573333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9570 loss: 1.0117 iter time (s): 5.769 samples/sec: 22.187
g0238:  iteration     9570/10000000 | consumed samples:      1224960 | consumed tokens:   2508718080 | elapsed time per iteration (ms): 5802.3 | learning rate: 1.672E-04 | global batch size:   128 | lm loss: 1.026697E+00 | loss scale: 262144.0 | grad norm: 0.478 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.060 | tokens per gpu per second (tgs): 1411.850 | TFLOPs: 11.36 |
g0220: [2024-08-09 16:11:14,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=9580, skipped=9, lr=[0.0001674051584, 0.0001674051584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9580 loss: 1.0312 iter time (s): 5.771 samples/sec: 22.179
g0238:  iteration     9580/10000000 | consumed samples:      1226240 | consumed tokens:   2511339520 | elapsed time per iteration (ms): 5804.1 | learning rate: 1.674E-04 | global batch size:   128 | lm loss: 1.026625E+00 | loss scale: 262144.0 | grad norm: 0.338 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.053 | tokens per gpu per second (tgs): 1411.422 | TFLOPs: 11.36 |
g0220: [2024-08-09 16:12:08,036] [INFO] [logging.py:96:log_dist] [Rank 0] step=9590, skipped=9, lr=[0.00016757992106666668, 0.00016757992106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9590 loss: 1.0457 iter time (s): 5.355 samples/sec: 23.902
g0238:  iteration     9590/10000000 | consumed samples:      1227520 | consumed tokens:   2513960960 | elapsed time per iteration (ms): 5388.2 | learning rate: 1.676E-04 | global batch size:   128 | lm loss: 1.043371E+00 | loss scale: 262144.0 | grad norm: 0.338 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.755 | tokens per gpu per second (tgs): 1520.348 | TFLOPs: 12.23 |
g0220: [2024-08-09 16:13:04,967] [INFO] [logging.py:96:log_dist] [Rank 0] step=9600, skipped=9, lr=[0.00016775468373333334, 0.00016775468373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9600 loss: 1.0272 iter time (s): 5.660 samples/sec: 22.614
g0238:  iteration     9600/10000000 | consumed samples:      1228800 | consumed tokens:   2516582400 | elapsed time per iteration (ms): 5693.0 | learning rate: 1.678E-04 | global batch size:   128 | lm loss: 1.026950E+00 | loss scale: 262144.0 | grad norm: 0.336 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.484 | tokens per gpu per second (tgs): 1438.971 | TFLOPs: 11.58 |
g0220: [2024-08-09 16:13:57,959] [INFO] [logging.py:96:log_dist] [Rank 0] step=9610, skipped=9, lr=[0.0001679294464, 0.0001679294464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9610 loss: 1.0296 iter time (s): 5.267 samples/sec: 24.304
g0238:  iteration     9610/10000000 | consumed samples:      1230080 | consumed tokens:   2519203840 | elapsed time per iteration (ms): 5299.1 | learning rate: 1.679E-04 | global batch size:   128 | lm loss: 1.030553E+00 | loss scale: 262144.0 | grad norm: 0.435 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.155 | tokens per gpu per second (tgs): 1545.911 | TFLOPs: 12.44 |
g0220: [2024-08-09 16:14:53,158] [INFO] [logging.py:96:log_dist] [Rank 0] step=9620, skipped=9, lr=[0.00016810420906666668, 0.00016810420906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9620 loss: 1.0152 iter time (s): 5.487 samples/sec: 23.326
g0238:  iteration     9620/10000000 | consumed samples:      1231360 | consumed tokens:   2521825280 | elapsed time per iteration (ms): 5520.0 | learning rate: 1.681E-04 | global batch size:   128 | lm loss: 1.039063E+00 | loss scale: 262144.0 | grad norm: 0.383 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.189 | tokens per gpu per second (tgs): 1484.065 | TFLOPs: 11.94 |
g0220: [2024-08-09 16:15:47,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=9630, skipped=9, lr=[0.00016827897173333334, 0.00016827897173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9630 loss: 1.0428 iter time (s): 5.437 samples/sec: 23.544
g0238:  iteration     9630/10000000 | consumed samples:      1232640 | consumed tokens:   2524446720 | elapsed time per iteration (ms): 5469.8 | learning rate: 1.683E-04 | global batch size:   128 | lm loss: 1.036463E+00 | loss scale: 262144.0 | grad norm: 0.349 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.401 | tokens per gpu per second (tgs): 1497.684 | TFLOPs: 12.05 |
g0220: [2024-08-09 16:16:42,376] [INFO] [logging.py:96:log_dist] [Rank 0] step=9640, skipped=9, lr=[0.0001684537344, 0.0001684537344], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9640 loss: 1.0245 iter time (s): 5.419 samples/sec: 23.621
g0238:  iteration     9640/10000000 | consumed samples:      1233920 | consumed tokens:   2527068160 | elapsed time per iteration (ms): 5452.0 | learning rate: 1.685E-04 | global batch size:   128 | lm loss: 1.037293E+00 | loss scale: 262144.0 | grad norm: 0.383 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.477 | tokens per gpu per second (tgs): 1502.557 | TFLOPs: 12.09 |
g0220: [2024-08-09 16:17:32,027] [INFO] [logging.py:96:log_dist] [Rank 0] step=9650, skipped=9, lr=[0.00016862849706666668, 0.00016862849706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9650 loss: 1.0208 iter time (s): 4.932 samples/sec: 25.954
g0238:  iteration     9650/10000000 | consumed samples:      1235200 | consumed tokens:   2529689600 | elapsed time per iteration (ms): 4965.0 | learning rate: 1.686E-04 | global batch size:   128 | lm loss: 1.026810E+00 | loss scale: 262144.0 | grad norm: 0.401 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.781 | tokens per gpu per second (tgs): 1649.954 | TFLOPs: 13.28 |
g0220: [2024-08-09 16:18:22,813] [INFO] [logging.py:96:log_dist] [Rank 0] step=9660, skipped=9, lr=[0.00016880325973333334, 0.00016880325973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9660 loss: 0.9876 iter time (s): 5.040 samples/sec: 25.397
g0238:  iteration     9660/10000000 | consumed samples:      1236480 | consumed tokens:   2532311040 | elapsed time per iteration (ms): 5079.0 | learning rate: 1.688E-04 | global batch size:   128 | lm loss: 1.031727E+00 | loss scale: 262144.0 | grad norm: 0.379 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.202 | tokens per gpu per second (tgs): 1612.919 | TFLOPs: 12.98 |
g0220: [2024-08-09 16:19:11,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=9670, skipped=9, lr=[0.0001689780224, 0.0001689780224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9670 loss: 1.0494 iter time (s): 4.808 samples/sec: 26.624
g0238:  iteration     9670/10000000 | consumed samples:      1237760 | consumed tokens:   2534932480 | elapsed time per iteration (ms): 4858.5 | learning rate: 1.690E-04 | global batch size:   128 | lm loss: 1.034144E+00 | loss scale: 262144.0 | grad norm: 0.438 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.346 | tokens per gpu per second (tgs): 1686.134 | TFLOPs: 13.57 |
g0220: [2024-08-09 16:19:55,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=9680, skipped=9, lr=[0.00016915278506666667, 0.00016915278506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9680 loss: 1.0581 iter time (s): 4.411 samples/sec: 29.016
g0238:  iteration     9680/10000000 | consumed samples:      1239040 | consumed tokens:   2537553920 | elapsed time per iteration (ms): 4444.1 | learning rate: 1.692E-04 | global batch size:   128 | lm loss: 1.037419E+00 | loss scale: 262144.0 | grad norm: 0.343 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.802 | tokens per gpu per second (tgs): 1843.359 | TFLOPs: 14.83 |
g0220: [2024-08-09 16:20:49,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=9690, skipped=9, lr=[0.00016932754773333334, 0.00016932754773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9690 loss: 1.0364 iter time (s): 5.301 samples/sec: 24.145
g0238:  iteration     9690/10000000 | consumed samples:      1240320 | consumed tokens:   2540175360 | elapsed time per iteration (ms): 5335.1 | learning rate: 1.693E-04 | global batch size:   128 | lm loss: 1.032873E+00 | loss scale: 262144.0 | grad norm: 0.385 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.992 | tokens per gpu per second (tgs): 1535.501 | TFLOPs: 12.36 |
g0220: [2024-08-09 16:21:45,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=9700, skipped=9, lr=[0.0001695023104, 0.0001695023104], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9700 loss: 1.0373 iter time (s): 5.576 samples/sec: 22.955
g0238:  iteration     9700/10000000 | consumed samples:      1241600 | consumed tokens:   2542796800 | elapsed time per iteration (ms): 5608.8 | learning rate: 1.695E-04 | global batch size:   128 | lm loss: 1.013092E+00 | loss scale: 262144.0 | grad norm: 0.475 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.821 | tokens per gpu per second (tgs): 1460.572 | TFLOPs: 11.75 |
g0220: [2024-08-09 16:22:39,438] [INFO] [logging.py:96:log_dist] [Rank 0] step=9710, skipped=9, lr=[0.00016967707306666667, 0.00016967707306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9710 loss: 1.0093 iter time (s): 5.383 samples/sec: 23.778
g0238:  iteration     9710/10000000 | consumed samples:      1242880 | consumed tokens:   2545418240 | elapsed time per iteration (ms): 5415.6 | learning rate: 1.697E-04 | global batch size:   128 | lm loss: 1.021800E+00 | loss scale: 262144.0 | grad norm: 0.368 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.635 | tokens per gpu per second (tgs): 1512.659 | TFLOPs: 12.17 |
g0220: [2024-08-09 16:23:33,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=9720, skipped=9, lr=[0.00016985183573333334, 0.00016985183573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9720 loss: 1.0434 iter time (s): 5.361 samples/sec: 23.874
g0238:  iteration     9720/10000000 | consumed samples:      1244160 | consumed tokens:   2548039680 | elapsed time per iteration (ms): 5394.2 | learning rate: 1.699E-04 | global batch size:   128 | lm loss: 1.018558E+00 | loss scale: 262144.0 | grad norm: 0.368 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.729 | tokens per gpu per second (tgs): 1518.668 | TFLOPs: 12.22 |
g0220: [2024-08-09 16:24:40,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=9730, skipped=9, lr=[0.0001700265984, 0.0001700265984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9730 loss: 1.0466 iter time (s): 6.690 samples/sec: 19.133
g0238:  iteration     9730/10000000 | consumed samples:      1245440 | consumed tokens:   2550661120 | elapsed time per iteration (ms): 6723.2 | learning rate: 1.700E-04 | global batch size:   128 | lm loss: 1.030045E+00 | loss scale: 262144.0 | grad norm: 0.336 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.039 | tokens per gpu per second (tgs): 1218.470 | TFLOPs: 9.81 |
g0220: [2024-08-09 16:25:37,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=9740, skipped=9, lr=[0.0001702013610666667, 0.0001702013610666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9740 loss: 1.0237 iter time (s): 5.703 samples/sec: 22.444
g0238:  iteration     9740/10000000 | consumed samples:      1246720 | consumed tokens:   2553282560 | elapsed time per iteration (ms): 5736.4 | learning rate: 1.702E-04 | global batch size:   128 | lm loss: 1.022925E+00 | loss scale: 262144.0 | grad norm: 0.378 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.314 | tokens per gpu per second (tgs): 1428.083 | TFLOPs: 11.49 |
g0220: [2024-08-09 16:26:33,621] [INFO] [logging.py:96:log_dist] [Rank 0] step=9750, skipped=9, lr=[0.00017037612373333336, 0.00017037612373333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9750 loss: 1.0203 iter time (s): 5.531 samples/sec: 23.141
g0238:  iteration     9750/10000000 | consumed samples:      1248000 | consumed tokens:   2555904000 | elapsed time per iteration (ms): 5564.5 | learning rate: 1.704E-04 | global batch size:   128 | lm loss: 1.016832E+00 | loss scale: 262144.0 | grad norm: 0.377 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.003 | tokens per gpu per second (tgs): 1472.192 | TFLOPs: 11.85 |
g0220: [2024-08-09 16:27:33,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=9760, skipped=9, lr=[0.00017055088640000003, 0.00017055088640000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9760 loss: 1.0446 iter time (s): 5.904 samples/sec: 21.679
g0238:  iteration     9760/10000000 | consumed samples:      1249280 | consumed tokens:   2558525440 | elapsed time per iteration (ms): 5941.3 | learning rate: 1.706E-04 | global batch size:   128 | lm loss: 1.036292E+00 | loss scale: 262144.0 | grad norm: 0.397 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.544 | tokens per gpu per second (tgs): 1378.834 | TFLOPs: 11.10 |
g0220: [2024-08-09 16:28:42,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=9770, skipped=9, lr=[0.0001707256490666667, 0.0001707256490666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9770 loss: 1.0507 iter time (s): 6.927 samples/sec: 18.479
g0238:  iteration     9770/10000000 | consumed samples:      1250560 | consumed tokens:   2561146880 | elapsed time per iteration (ms): 6959.6 | learning rate: 1.707E-04 | global batch size:   128 | lm loss: 1.034409E+00 | loss scale: 262144.0 | grad norm: 0.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 18.392 | tokens per gpu per second (tgs): 1177.087 | TFLOPs: 9.47 |
g0220: [2024-08-09 16:29:41,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=9780, skipped=9, lr=[0.00017090041173333333, 0.00017090041173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9780 loss: 1.0347 iter time (s): 5.886 samples/sec: 21.747
g0238:  iteration     9780/10000000 | consumed samples:      1251840 | consumed tokens:   2563768320 | elapsed time per iteration (ms): 5918.5 | learning rate: 1.709E-04 | global batch size:   128 | lm loss: 1.026174E+00 | loss scale: 262144.0 | grad norm: 0.334 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.627 | tokens per gpu per second (tgs): 1384.136 | TFLOPs: 11.14 |
g0220: [2024-08-09 16:30:34,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=9790, skipped=9, lr=[0.0001710751744, 0.0001710751744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9790 loss: 1.0023 iter time (s): 5.187 samples/sec: 24.678
g0238:  iteration     9790/10000000 | consumed samples:      1253120 | consumed tokens:   2566389760 | elapsed time per iteration (ms): 5220.2 | learning rate: 1.711E-04 | global batch size:   128 | lm loss: 1.023366E+00 | loss scale: 262144.0 | grad norm: 0.406 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.520 | tokens per gpu per second (tgs): 1569.280 | TFLOPs: 12.63 |
g0220: [2024-08-09 16:31:25,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=9800, skipped=9, lr=[0.00017124993706666667, 0.00017124993706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9800 loss: 0.9884 iter time (s): 5.078 samples/sec: 25.208
g0238:  iteration     9800/10000000 | consumed samples:      1254400 | consumed tokens:   2569011200 | elapsed time per iteration (ms): 5111.0 | learning rate: 1.712E-04 | global batch size:   128 | lm loss: 1.009960E+00 | loss scale: 262144.0 | grad norm: 0.365 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.044 | tokens per gpu per second (tgs): 1602.819 | TFLOPs: 12.90 |
g0220: [2024-08-09 16:32:15,887] [INFO] [logging.py:96:log_dist] [Rank 0] step=9810, skipped=9, lr=[0.00017142469973333333, 0.00017142469973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9810 loss: 0.9992 iter time (s): 5.043 samples/sec: 25.383
g0238:  iteration     9810/10000000 | consumed samples:      1255680 | consumed tokens:   2571632640 | elapsed time per iteration (ms): 5075.9 | learning rate: 1.714E-04 | global batch size:   128 | lm loss: 1.019311E+00 | loss scale: 262144.0 | grad norm: 0.366 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.217 | tokens per gpu per second (tgs): 1613.900 | TFLOPs: 12.99 |
g0220: [2024-08-09 16:33:17,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=9820, skipped=9, lr=[0.0001715994624, 0.0001715994624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9820 loss: 1.0308 iter time (s): 6.142 samples/sec: 20.840
g0238:  iteration     9820/10000000 | consumed samples:      1256960 | consumed tokens:   2574254080 | elapsed time per iteration (ms): 6174.7 | learning rate: 1.716E-04 | global batch size:   128 | lm loss: 1.026672E+00 | loss scale: 262144.0 | grad norm: 0.337 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.730 | tokens per gpu per second (tgs): 1326.701 | TFLOPs: 10.68 |
g0220: [2024-08-09 16:34:15,029] [INFO] [logging.py:96:log_dist] [Rank 0] step=9830, skipped=9, lr=[0.00017177422506666666, 0.00017177422506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9830 loss: 1.0243 iter time (s): 5.707 samples/sec: 22.430
g0238:  iteration     9830/10000000 | consumed samples:      1258240 | consumed tokens:   2576875520 | elapsed time per iteration (ms): 5739.5 | learning rate: 1.718E-04 | global batch size:   128 | lm loss: 1.020785E+00 | loss scale: 262144.0 | grad norm: 0.319 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.302 | tokens per gpu per second (tgs): 1427.302 | TFLOPs: 11.49 |
g0220: [2024-08-09 16:35:07,651] [INFO] [logging.py:96:log_dist] [Rank 0] step=9840, skipped=9, lr=[0.00017194898773333333, 0.00017194898773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9840 loss: 1.0255 iter time (s): 5.230 samples/sec: 24.475
g0238:  iteration     9840/10000000 | consumed samples:      1259520 | consumed tokens:   2579496960 | elapsed time per iteration (ms): 5262.2 | learning rate: 1.719E-04 | global batch size:   128 | lm loss: 1.029806E+00 | loss scale: 262144.0 | grad norm: 0.387 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.325 | tokens per gpu per second (tgs): 1556.770 | TFLOPs: 12.53 |
g0220: [2024-08-09 16:35:57,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=9850, skipped=9, lr=[0.0001721237504, 0.0001721237504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9850 loss: 1.0168 iter time (s): 4.986 samples/sec: 25.671
g0238:  iteration     9850/10000000 | consumed samples:      1260800 | consumed tokens:   2582118400 | elapsed time per iteration (ms): 5019.0 | learning rate: 1.721E-04 | global batch size:   128 | lm loss: 1.008641E+00 | loss scale: 262144.0 | grad norm: 0.445 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.503 | tokens per gpu per second (tgs): 1632.198 | TFLOPs: 13.13 |
g0220: [2024-08-09 16:36:47,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=9860, skipped=9, lr=[0.00017229851306666666, 0.00017229851306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9860 loss: 1.0100 iter time (s): 4.912 samples/sec: 26.061
g0238:  iteration     9860/10000000 | consumed samples:      1262080 | consumed tokens:   2584739840 | elapsed time per iteration (ms): 4943.8 | learning rate: 1.723E-04 | global batch size:   128 | lm loss: 1.016822E+00 | loss scale: 262144.0 | grad norm: 0.360 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.891 | tokens per gpu per second (tgs): 1657.033 | TFLOPs: 13.33 |
g0220: [2024-08-09 16:37:43,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=9870, skipped=9, lr=[0.00017247327573333333, 0.00017247327573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9870 loss: 1.0415 iter time (s): 5.567 samples/sec: 22.991
g0238:  iteration     9870/10000000 | consumed samples:      1263360 | consumed tokens:   2587361280 | elapsed time per iteration (ms): 5600.2 | learning rate: 1.725E-04 | global batch size:   128 | lm loss: 1.026237E+00 | loss scale: 262144.0 | grad norm: 0.400 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.856 | tokens per gpu per second (tgs): 1462.794 | TFLOPs: 11.77 |
g0220: [2024-08-09 16:38:34,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=9880, skipped=9, lr=[0.0001726480384, 0.0001726480384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9880 loss: 1.0307 iter time (s): 5.048 samples/sec: 25.356
g0238:  iteration     9880/10000000 | consumed samples:      1264640 | consumed tokens:   2589982720 | elapsed time per iteration (ms): 5081.4 | learning rate: 1.726E-04 | global batch size:   128 | lm loss: 1.021872E+00 | loss scale: 262144.0 | grad norm: 0.346 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.190 | tokens per gpu per second (tgs): 1612.167 | TFLOPs: 12.97 |
g0220: [2024-08-09 16:39:22,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=9890, skipped=9, lr=[0.00017282280106666666, 0.00017282280106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9890 loss: 1.0271 iter time (s): 4.807 samples/sec: 26.628
g0238:  iteration     9890/10000000 | consumed samples:      1265920 | consumed tokens:   2592604160 | elapsed time per iteration (ms): 4839.7 | learning rate: 1.728E-04 | global batch size:   128 | lm loss: 1.015601E+00 | loss scale: 262144.0 | grad norm: 0.417 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.448 | tokens per gpu per second (tgs): 1692.676 | TFLOPs: 13.62 |
g0220: [2024-08-09 16:40:10,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=9900, skipped=9, lr=[0.00017299756373333333, 0.00017299756373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9900 loss: 1.0094 iter time (s): 4.777 samples/sec: 26.795
g0238:  iteration     9900/10000000 | consumed samples:      1267200 | consumed tokens:   2595225600 | elapsed time per iteration (ms): 4809.8 | learning rate: 1.730E-04 | global batch size:   128 | lm loss: 1.021644E+00 | loss scale: 262144.0 | grad norm: 0.320 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.612 | tokens per gpu per second (tgs): 1703.200 | TFLOPs: 13.71 |
g0220: [2024-08-09 16:40:53,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=9910, skipped=9, lr=[0.0001731723264, 0.0001731723264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9910 loss: 1.0095 iter time (s): 4.265 samples/sec: 30.010
g0238:  iteration     9910/10000000 | consumed samples:      1268480 | consumed tokens:   2597847040 | elapsed time per iteration (ms): 4299.0 | learning rate: 1.732E-04 | global batch size:   128 | lm loss: 1.015394E+00 | loss scale: 262144.0 | grad norm: 0.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.774 | tokens per gpu per second (tgs): 1905.539 | TFLOPs: 15.33 |
g0220: [2024-08-09 16:41:44,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=9920, skipped=9, lr=[0.00017334708906666666, 0.00017334708906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9920 loss: 1.0148 iter time (s): 5.108 samples/sec: 25.060
g0238:  iteration     9920/10000000 | consumed samples:      1269760 | consumed tokens:   2600468480 | elapsed time per iteration (ms): 5140.8 | learning rate: 1.733E-04 | global batch size:   128 | lm loss: 1.023059E+00 | loss scale: 262144.0 | grad norm: 0.379 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.899 | tokens per gpu per second (tgs): 1593.534 | TFLOPs: 12.82 |
g0220: [2024-08-09 16:42:34,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=9930, skipped=9, lr=[0.00017352185173333333, 0.00017352185173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9930 loss: 0.9799 iter time (s): 4.950 samples/sec: 25.857
g0238:  iteration     9930/10000000 | consumed samples:      1271040 | consumed tokens:   2603089920 | elapsed time per iteration (ms): 4982.6 | learning rate: 1.735E-04 | global batch size:   128 | lm loss: 1.012740E+00 | loss scale: 262144.0 | grad norm: 0.347 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.690 | tokens per gpu per second (tgs): 1644.135 | TFLOPs: 13.23 |
g0220: [2024-08-09 16:43:20,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=9940, skipped=9, lr=[0.0001736966144, 0.0001736966144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9940 loss: 1.0178 iter time (s): 4.512 samples/sec: 28.369
g0238:  iteration     9940/10000000 | consumed samples:      1272320 | consumed tokens:   2605711360 | elapsed time per iteration (ms): 4545.0 | learning rate: 1.737E-04 | global batch size:   128 | lm loss: 1.019378E+00 | loss scale: 262144.0 | grad norm: 0.341 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.163 | tokens per gpu per second (tgs): 1802.414 | TFLOPs: 14.50 |
g0220: [2024-08-09 16:44:06,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=9950, skipped=9, lr=[0.00017387137706666666, 0.00017387137706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9950 loss: 1.0179 iter time (s): 4.552 samples/sec: 28.121
g0238:  iteration     9950/10000000 | consumed samples:      1273600 | consumed tokens:   2608332800 | elapsed time per iteration (ms): 4585.0 | learning rate: 1.739E-04 | global batch size:   128 | lm loss: 1.017460E+00 | loss scale: 262144.0 | grad norm: 0.370 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.917 | tokens per gpu per second (tgs): 1786.693 | TFLOPs: 14.38 |
g0220: [2024-08-09 16:44:50,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=9960, skipped=9, lr=[0.00017404613973333332, 0.00017404613973333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9960 loss: 0.9936 iter time (s): 4.389 samples/sec: 29.161
g0238:  iteration     9960/10000000 | consumed samples:      1274880 | consumed tokens:   2610954240 | elapsed time per iteration (ms): 4422.1 | learning rate: 1.740E-04 | global batch size:   128 | lm loss: 1.024596E+00 | loss scale: 262144.0 | grad norm: 0.508 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.945 | tokens per gpu per second (tgs): 1852.502 | TFLOPs: 14.91 |
g0220: [2024-08-09 16:45:34,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=9970, skipped=9, lr=[0.0001742209024, 0.0001742209024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9970 loss: 1.0010 iter time (s): 4.380 samples/sec: 29.224
g0238:  iteration     9970/10000000 | consumed samples:      1276160 | consumed tokens:   2613575680 | elapsed time per iteration (ms): 4412.5 | learning rate: 1.742E-04 | global batch size:   128 | lm loss: 1.024125E+00 | loss scale: 262144.0 | grad norm: 0.375 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.008 | tokens per gpu per second (tgs): 1856.530 | TFLOPs: 14.94 |
g0220: [2024-08-09 16:46:18,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=9980, skipped=9, lr=[0.00017439566506666666, 0.00017439566506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9980 loss: 1.0247 iter time (s): 4.355 samples/sec: 29.391
g0238:  iteration     9980/10000000 | consumed samples:      1277440 | consumed tokens:   2616197120 | elapsed time per iteration (ms): 4387.7 | learning rate: 1.744E-04 | global batch size:   128 | lm loss: 1.024372E+00 | loss scale: 262144.0 | grad norm: 0.356 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.173 | tokens per gpu per second (tgs): 1867.055 | TFLOPs: 15.02 |
g0220: [2024-08-09 16:47:06,336] [INFO] [logging.py:96:log_dist] [Rank 0] step=9990, skipped=9, lr=[0.00017457042773333335, 0.00017457042773333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 9990 loss: 1.0309 iter time (s): 4.767 samples/sec: 26.851
g0238:  iteration     9990/10000000 | consumed samples:      1278720 | consumed tokens:   2618818560 | elapsed time per iteration (ms): 4799.8 | learning rate: 1.746E-04 | global batch size:   128 | lm loss: 1.030715E+00 | loss scale: 262144.0 | grad norm: 0.447 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.668 | tokens per gpu per second (tgs): 1706.745 | TFLOPs: 13.73 |
g0220: [2024-08-09 16:47:57,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=10000, skipped=9, lr=[0.00017474519040000001, 0.00017474519040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10000 loss: 1.0152 iter time (s): 5.122 samples/sec: 24.991
g0238:  iteration    10000/10000000 | consumed samples:      1280000 | consumed tokens:   2621440000 | elapsed time per iteration (ms): 5154.1 | learning rate: 1.747E-04 | global batch size:   128 | lm loss: 1.022442E+00 | loss scale: 262144.0 | grad norm: 0.383 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.834 | tokens per gpu per second (tgs): 1589.405 | TFLOPs: 12.79 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 10000 | lm loss value: 1.017741E+00 | lm loss PPL: 2.766938E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   10000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-09 16:54:48,129] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step10000 is about to be saved!
g0238: [2024-08-09 16:54:48,136] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0238: [2024-08-09 16:54:48,136] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0238: [2024-08-09 16:54:48,136] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0220: [2024-08-09 16:54:48,137] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0220: [2024-08-09 16:54:48,137] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0220: [2024-08-09 16:54:48,137] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0233: [2024-08-09 16:54:48,137] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0233: [2024-08-09 16:54:48,137] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0233: [2024-08-09 16:54:48,138] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0225: [2024-08-09 16:54:48,138] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0225: [2024-08-09 16:54:48,138] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0225: [2024-08-09 16:54:48,138] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0236: [2024-08-09 16:54:48,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0236: [2024-08-09 16:54:48,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0236: [2024-08-09 16:54:48,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0235: [2024-08-09 16:54:48,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0235: [2024-08-09 16:54:48,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0235: [2024-08-09 16:54:48,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0237: [2024-08-09 16:54:48,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0237: [2024-08-09 16:54:48,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0237: [2024-08-09 16:54:48,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0234: [2024-08-09 16:54:48,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0234: [2024-08-09 16:54:48,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0234: [2024-08-09 16:54:48,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0238: [2024-08-09 16:54:48,160] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_23-model_00-model_states.pt...
g0233: [2024-08-09 16:54:48,176] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_08-model_00-model_states.pt...
g0225: [2024-08-09 16:54:48,176] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_05-model_00-model_states.pt...
g0235: [2024-08-09 16:54:48,177] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_14-model_00-model_states.pt...
g0236: [2024-08-09 16:54:48,177] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_17-model_00-model_states.pt...
g0237: [2024-08-09 16:54:48,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_20-model_00-model_states.pt...
g0234: [2024-08-09 16:54:48,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_11-model_00-model_states.pt...
g0220: [2024-08-09 16:54:48,191] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_01-model_00-model_states.pt...
g0225: [2024-08-09 16:54:48,288] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_05-model_00-model_states.pt.
g0225: [2024-08-09 16:54:48,328] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_06-model_00-model_states.pt...
g0236: [2024-08-09 16:54:48,330] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_17-model_00-model_states.pt.
g0237: [2024-08-09 16:54:48,339] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_20-model_00-model_states.pt.
g0235: [2024-08-09 16:54:48,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_14-model_00-model_states.pt.
g0236: [2024-08-09 16:54:48,370] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_18-model_00-model_states.pt...
g0237: [2024-08-09 16:54:48,377] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_21-model_00-model_states.pt...
g0235: [2024-08-09 16:54:48,386] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_15-model_00-model_states.pt...
g0233: [2024-08-09 16:54:48,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_08-model_00-model_states.pt.
g0234: [2024-08-09 16:54:48,399] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_11-model_00-model_states.pt.
g0233: [2024-08-09 16:54:48,436] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_09-model_00-model_states.pt...
g0234: [2024-08-09 16:54:48,439] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_12-model_00-model_states.pt...
g0237: [2024-08-09 16:54:48,505] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_21-model_00-model_states.pt.
g0220: [2024-08-09 16:54:48,522] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_01-model_00-model_states.pt.
g0237: [2024-08-09 16:54:48,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_22-model_00-model_states.pt...
g0236: [2024-08-09 16:54:48,544] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_18-model_00-model_states.pt.
g0235: [2024-08-09 16:54:48,551] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_15-model_00-model_states.pt.
g0220: [2024-08-09 16:54:48,552] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_02-model_00-model_states.pt...
g0233: [2024-08-09 16:54:48,555] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_09-model_00-model_states.pt.
g0234: [2024-08-09 16:54:48,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_12-model_00-model_states.pt.
g0236: [2024-08-09 16:54:48,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_19-model_00-model_states.pt...
g0235: [2024-08-09 16:54:48,585] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_16-model_00-model_states.pt...
g0238: [2024-08-09 16:54:48,587] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 16:54:48,588] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 16:54:48,589] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_24-model_00-model_states.pt.
g0233: [2024-08-09 16:54:48,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_10-model_00-model_states.pt...
g0234: [2024-08-09 16:54:48,610] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_13-model_00-model_states.pt...
g0225: [2024-08-09 16:54:48,615] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_06-model_00-model_states.pt.
g0237: [2024-08-09 16:54:48,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 16:54:48,631] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_06_model_states.pt...
g0238: [2024-08-09 16:54:48,635] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_25-model_00-model_states.pt...
g0225: [2024-08-09 16:54:48,650] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_07-model_00-model_states.pt...
g0233: [2024-08-09 16:54:48,681] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 16:54:48,683] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_02_model_states.pt...
g0220: [2024-08-09 16:54:48,698] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_02-model_00-model_states.pt.
g0234: [2024-08-09 16:54:48,700] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 16:54:48,702] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_03_model_states.pt...
g0220: [2024-08-09 16:54:48,728] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_03-model_00-model_states.pt...
g0236: [2024-08-09 16:54:48,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 16:54:48,767] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_05_model_states.pt...
g0235: [2024-08-09 16:54:48,790] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 16:54:48,792] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_04_model_states.pt...
g0225: [2024-08-09 16:54:48,825] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 16:54:48,827] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_01_model_states.pt...
g0238: [2024-08-09 16:54:48,909] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 16:54:48,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_07_model_states.pt...
g0220: [2024-08-09 16:54:49,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 16:54:49,138] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_04-model_00-model_states.pt...
g0220: [2024-08-09 16:54:49,310] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 16:54:49,311] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_00_model_states.pt
g0220: [2024-08-09 16:54:49,312] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_00_model_states.pt...
g0238: [2024-08-09 16:54:50,839] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 16:54:50,839] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0237: [2024-08-09 16:54:50,987] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 16:54:50,988] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0225: [2024-08-09 16:54:51,226] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_01_model_states.pt.
g0225: [2024-08-09 16:54:51,227] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0236: [2024-08-09 16:54:51,230] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 16:54:51,231] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0233: [2024-08-09 16:54:51,330] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 16:54:51,330] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0235: [2024-08-09 16:54:51,402] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 16:54:51,403] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0234: [2024-08-09 16:54:51,460] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 16:54:51,460] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0220: [2024-08-09 16:54:52,837] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step10000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 16:54:52,838] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10000 is ready now!
g0220:   successfully saved checkpoint at iteration   10000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 4.72, Latency(second): 4.767
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (4766.82, 4767.28)
g0220: [2024-08-09 16:56:01,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=10010, skipped=9, lr=[0.00017491995306666668, 0.00017491995306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10010 loss: 0.9988 iter time (s): 6.801 samples/sec: 18.820
g0238:  iteration    10010/10000000 | consumed samples:      1281280 | consumed tokens:   2624061440 | elapsed time per iteration (ms): 48330.4 | learning rate: 1.749E-04 | global batch size:   128 | lm loss: 1.025680E+00 | loss scale: 262144.0 | grad norm: 0.349 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.648 | tokens per gpu per second (tgs): 169.500 | TFLOPs: 1.36 |
g0220: [2024-08-09 16:56:50,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=10020, skipped=9, lr=[0.00017509471573333335, 0.00017509471573333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10020 loss: 1.0287 iter time (s): 4.889 samples/sec: 26.179
g0238:  iteration    10020/10000000 | consumed samples:      1282560 | consumed tokens:   2626682880 | elapsed time per iteration (ms): 4921.6 | learning rate: 1.751E-04 | global batch size:   128 | lm loss: 1.027646E+00 | loss scale: 262144.0 | grad norm: 0.328 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.008 | tokens per gpu per second (tgs): 1664.484 | TFLOPs: 13.39 |
g0220: [2024-08-09 16:57:40,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=10030, skipped=9, lr=[0.0001752694784, 0.0001752694784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10030 loss: 1.0193 iter time (s): 4.992 samples/sec: 25.642
g0238:  iteration    10030/10000000 | consumed samples:      1283840 | consumed tokens:   2629304320 | elapsed time per iteration (ms): 5024.4 | learning rate: 1.753E-04 | global batch size:   128 | lm loss: 1.000544E+00 | loss scale: 262144.0 | grad norm: 0.376 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.476 | tokens per gpu per second (tgs): 1630.436 | TFLOPs: 13.12 |
g0220: [2024-08-09 16:58:28,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=10040, skipped=9, lr=[0.00017544424106666668, 0.00017544424106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10040 loss: 0.9917 iter time (s): 4.802 samples/sec: 26.657
g0238:  iteration    10040/10000000 | consumed samples:      1285120 | consumed tokens:   2631925760 | elapsed time per iteration (ms): 4834.2 | learning rate: 1.754E-04 | global batch size:   128 | lm loss: 1.013636E+00 | loss scale: 262144.0 | grad norm: 0.324 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.478 | tokens per gpu per second (tgs): 1694.597 | TFLOPs: 13.64 |
g0220: [2024-08-09 16:59:15,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=10050, skipped=9, lr=[0.00017561900373333334, 0.00017561900373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10050 loss: 1.0433 iter time (s): 4.583 samples/sec: 27.932
g0238:  iteration    10050/10000000 | consumed samples:      1286400 | consumed tokens:   2634547200 | elapsed time per iteration (ms): 4616.0 | learning rate: 1.756E-04 | global batch size:   128 | lm loss: 1.019320E+00 | loss scale: 262144.0 | grad norm: 0.327 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.730 | tokens per gpu per second (tgs): 1774.693 | TFLOPs: 14.28 |
g0220: [2024-08-09 17:00:06,098] [INFO] [logging.py:96:log_dist] [Rank 0] step=10060, skipped=9, lr=[0.0001757937664, 0.0001757937664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10060 loss: 1.0251 iter time (s): 5.061 samples/sec: 25.294
g0238:  iteration    10060/10000000 | consumed samples:      1287680 | consumed tokens:   2637168640 | elapsed time per iteration (ms): 5095.3 | learning rate: 1.758E-04 | global batch size:   128 | lm loss: 1.021126E+00 | loss scale: 262144.0 | grad norm: 0.353 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.121 | tokens per gpu per second (tgs): 1607.766 | TFLOPs: 12.94 |
g0233: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 17:01:02,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 17:01:02,838] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 17:01:02,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 17:01:02,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 17:01:02,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 17:01:02,839] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 17:01:02,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 17:01:02,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 17:01:02,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 17:01:02,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 17:01:02,839] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 17:01:11,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=10070, skipped=9, lr=[0.00017596852906666668, 0.00017596852906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10070 loss: 1.0214 iter time (s): 6.546 samples/sec: 19.553
g0238:  iteration    10070/10000000 | consumed samples:      1288960 | consumed tokens:   2639790080 | elapsed time per iteration (ms): 6580.2 | learning rate: 1.760E-04 | global batch size:   128 | lm loss: 1.022185E+00 | loss scale: 524288.0 | grad norm: 0.323 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.452 | tokens per gpu per second (tgs): 1244.939 | TFLOPs: 10.02 |
g0220: [2024-08-09 17:02:07,629] [INFO] [logging.py:96:log_dist] [Rank 0] step=10080, skipped=9, lr=[0.00017614329173333334, 0.00017614329173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10080 loss: 1.0243 iter time (s): 5.539 samples/sec: 23.109
g0238:  iteration    10080/10000000 | consumed samples:      1290240 | consumed tokens:   2642411520 | elapsed time per iteration (ms): 5573.3 | learning rate: 1.761E-04 | global batch size:   128 | lm loss: 1.024209E+00 | loss scale: 524288.0 | grad norm: 0.331 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.967 | tokens per gpu per second (tgs): 1469.861 | TFLOPs: 11.83 |
g0220: [2024-08-09 17:02:55,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=10090, skipped=9, lr=[0.00017631805440000004, 0.00017631805440000004], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10090 loss: 1.0058 iter time (s): 4.728 samples/sec: 27.074
g0238:  iteration    10090/10000000 | consumed samples:      1291520 | consumed tokens:   2645032960 | elapsed time per iteration (ms): 4761.4 | learning rate: 1.763E-04 | global batch size:   128 | lm loss: 1.023931E+00 | loss scale: 524288.0 | grad norm: 0.343 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.883 | tokens per gpu per second (tgs): 1720.507 | TFLOPs: 13.85 |
g0220: [2024-08-09 17:03:39,816] [INFO] [logging.py:96:log_dist] [Rank 0] step=10100, skipped=9, lr=[0.00017649281706666668, 0.00017649281706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10100 loss: 1.0209 iter time (s): 4.423 samples/sec: 28.937
g0238:  iteration    10100/10000000 | consumed samples:      1292800 | consumed tokens:   2647654400 | elapsed time per iteration (ms): 4457.1 | learning rate: 1.765E-04 | global batch size:   128 | lm loss: 1.014402E+00 | loss scale: 524288.0 | grad norm: 0.324 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.718 | tokens per gpu per second (tgs): 1837.976 | TFLOPs: 14.79 |
g0220: [2024-08-09 17:04:23,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=10110, skipped=9, lr=[0.00017666757973333337, 0.00017666757973333337], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10110 loss: 1.0207 iter time (s): 4.364 samples/sec: 29.330
g0238:  iteration    10110/10000000 | consumed samples:      1294080 | consumed tokens:   2650275840 | elapsed time per iteration (ms): 4397.8 | learning rate: 1.767E-04 | global batch size:   128 | lm loss: 9.987759E-01 | loss scale: 524288.0 | grad norm: 0.489 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.105 | tokens per gpu per second (tgs): 1862.747 | TFLOPs: 14.99 |
g0220: [2024-08-09 17:05:30,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=10120, skipped=9, lr=[0.0001768423424, 0.0001768423424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10120 loss: 0.9892 iter time (s): 6.625 samples/sec: 19.322
g0238:  iteration    10120/10000000 | consumed samples:      1295360 | consumed tokens:   2652897280 | elapsed time per iteration (ms): 6657.3 | learning rate: 1.768E-04 | global batch size:   128 | lm loss: 1.009886E+00 | loss scale: 524288.0 | grad norm: 0.326 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.227 | tokens per gpu per second (tgs): 1230.520 | TFLOPs: 9.90 |
g0220: [2024-08-09 17:06:19,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=10130, skipped=9, lr=[0.0001770171050666667, 0.0001770171050666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10130 loss: 1.0254 iter time (s): 4.914 samples/sec: 26.046
g0238:  iteration    10130/10000000 | consumed samples:      1296640 | consumed tokens:   2655518720 | elapsed time per iteration (ms): 4947.1 | learning rate: 1.770E-04 | global batch size:   128 | lm loss: 1.009385E+00 | loss scale: 524288.0 | grad norm: 0.356 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.874 | tokens per gpu per second (tgs): 1655.920 | TFLOPs: 13.33 |
g0220: [2024-08-09 17:07:05,185] [INFO] [logging.py:96:log_dist] [Rank 0] step=10140, skipped=9, lr=[0.00017719186773333334, 0.00017719186773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10140 loss: 1.0414 iter time (s): 4.501 samples/sec: 28.436
g0238:  iteration    10140/10000000 | consumed samples:      1297920 | consumed tokens:   2658140160 | elapsed time per iteration (ms): 4534.6 | learning rate: 1.772E-04 | global batch size:   128 | lm loss: 1.024028E+00 | loss scale: 524288.0 | grad norm: 0.394 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.227 | tokens per gpu per second (tgs): 1806.536 | TFLOPs: 14.54 |
g0220: [2024-08-09 17:07:53,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=10150, skipped=9, lr=[0.00017736663040000003, 0.00017736663040000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10150 loss: 1.0107 iter time (s): 4.776 samples/sec: 26.799
g0238:  iteration    10150/10000000 | consumed samples:      1299200 | consumed tokens:   2660761600 | elapsed time per iteration (ms): 4810.7 | learning rate: 1.774E-04 | global batch size:   128 | lm loss: 1.009204E+00 | loss scale: 524288.0 | grad norm: 0.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.607 | tokens per gpu per second (tgs): 1702.878 | TFLOPs: 13.70 |
g0220: [2024-08-09 17:08:43,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=10160, skipped=9, lr=[0.00017754139306666667, 0.00017754139306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10160 loss: 0.9918 iter time (s): 5.028 samples/sec: 25.456
g0238:  iteration    10160/10000000 | consumed samples:      1300480 | consumed tokens:   2663383040 | elapsed time per iteration (ms): 5063.3 | learning rate: 1.775E-04 | global batch size:   128 | lm loss: 1.004816E+00 | loss scale: 524288.0 | grad norm: 0.325 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.280 | tokens per gpu per second (tgs): 1617.923 | TFLOPs: 13.02 |
g0220: [2024-08-09 17:09:50,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=10170, skipped=9, lr=[0.00017771615573333336, 0.00017771615573333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10170 loss: 1.0356 iter time (s): 6.523 samples/sec: 19.623
g0238:  iteration    10170/10000000 | consumed samples:      1301760 | consumed tokens:   2666004480 | elapsed time per iteration (ms): 6669.6 | learning rate: 1.777E-04 | global batch size:   128 | lm loss: 1.012257E+00 | loss scale: 524288.0 | grad norm: 0.318 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.191 | tokens per gpu per second (tgs): 1228.251 | TFLOPs: 9.88 |
g0220: [2024-08-09 17:10:43,348] [INFO] [logging.py:96:log_dist] [Rank 0] step=10180, skipped=9, lr=[0.0001778909184, 0.0001778909184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10180 loss: 1.0440 iter time (s): 5.240 samples/sec: 24.428
g0238:  iteration    10180/10000000 | consumed samples:      1303040 | consumed tokens:   2668625920 | elapsed time per iteration (ms): 5272.3 | learning rate: 1.779E-04 | global batch size:   128 | lm loss: 1.010847E+00 | loss scale: 524288.0 | grad norm: 0.354 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.278 | tokens per gpu per second (tgs): 1553.776 | TFLOPs: 12.50 |
g0220: [2024-08-09 17:11:31,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=10190, skipped=9, lr=[0.0001780656810666667, 0.0001780656810666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10190 loss: 1.0245 iter time (s): 4.790 samples/sec: 26.721
g0238:  iteration    10190/10000000 | consumed samples:      1304320 | consumed tokens:   2671247360 | elapsed time per iteration (ms): 4823.0 | learning rate: 1.781E-04 | global batch size:   128 | lm loss: 1.008778E+00 | loss scale: 524288.0 | grad norm: 0.325 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.540 | tokens per gpu per second (tgs): 1698.529 | TFLOPs: 13.67 |
g0220: [2024-08-09 17:12:18,080] [INFO] [logging.py:96:log_dist] [Rank 0] step=10200, skipped=9, lr=[0.00017824044373333334, 0.00017824044373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10200 loss: 1.0187 iter time (s): 4.618 samples/sec: 27.720
g0238:  iteration    10200/10000000 | consumed samples:      1305600 | consumed tokens:   2673868800 | elapsed time per iteration (ms): 4650.2 | learning rate: 1.782E-04 | global batch size:   128 | lm loss: 1.015422E+00 | loss scale: 524288.0 | grad norm: 0.347 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.526 | tokens per gpu per second (tgs): 1761.643 | TFLOPs: 14.18 |
g0220: [2024-08-09 17:13:03,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=10210, skipped=9, lr=[0.00017841520640000003, 0.00017841520640000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10210 loss: 1.0032 iter time (s): 4.484 samples/sec: 28.545
g0238:  iteration    10210/10000000 | consumed samples:      1306880 | consumed tokens:   2676490240 | elapsed time per iteration (ms): 4516.8 | learning rate: 1.784E-04 | global batch size:   128 | lm loss: 1.012965E+00 | loss scale: 524288.0 | grad norm: 0.394 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.339 | tokens per gpu per second (tgs): 1813.673 | TFLOPs: 14.59 |
g0220: [2024-08-09 17:14:06,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=10220, skipped=9, lr=[0.00017858996906666667, 0.00017858996906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10220 loss: 0.9965 iter time (s): 6.255 samples/sec: 20.464
g0238:  iteration    10220/10000000 | consumed samples:      1308160 | consumed tokens:   2679111680 | elapsed time per iteration (ms): 6287.4 | learning rate: 1.786E-04 | global batch size:   128 | lm loss: 1.012692E+00 | loss scale: 524288.0 | grad norm: 0.334 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.358 | tokens per gpu per second (tgs): 1302.917 | TFLOPs: 10.48 |
g0220: [2024-08-09 17:14:58,137] [INFO] [logging.py:96:log_dist] [Rank 0] step=10230, skipped=9, lr=[0.00017876473173333336, 0.00017876473173333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10230 loss: 0.9956 iter time (s): 5.169 samples/sec: 24.765
g0238:  iteration    10230/10000000 | consumed samples:      1309440 | consumed tokens:   2681733120 | elapsed time per iteration (ms): 5201.4 | learning rate: 1.788E-04 | global batch size:   128 | lm loss: 1.016125E+00 | loss scale: 524288.0 | grad norm: 0.335 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.609 | tokens per gpu per second (tgs): 1574.967 | TFLOPs: 12.67 |
g0220: [2024-08-09 17:15:41,798] [INFO] [logging.py:96:log_dist] [Rank 0] step=10240, skipped=9, lr=[0.0001789394944, 0.0001789394944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10240 loss: 1.0293 iter time (s): 4.334 samples/sec: 29.536
g0238:  iteration    10240/10000000 | consumed samples:      1310720 | consumed tokens:   2684354560 | elapsed time per iteration (ms): 4366.1 | learning rate: 1.789E-04 | global batch size:   128 | lm loss: 1.024572E+00 | loss scale: 524288.0 | grad norm: 0.335 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.316 | tokens per gpu per second (tgs): 1876.254 | TFLOPs: 15.10 |
g0220: [2024-08-09 17:16:26,593] [INFO] [logging.py:96:log_dist] [Rank 0] step=10250, skipped=9, lr=[0.0001791142570666667, 0.0001791142570666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10250 loss: 0.9892 iter time (s): 4.447 samples/sec: 28.786
g0238:  iteration    10250/10000000 | consumed samples:      1312000 | consumed tokens:   2686976000 | elapsed time per iteration (ms): 4479.5 | learning rate: 1.791E-04 | global batch size:   128 | lm loss: 1.002899E+00 | loss scale: 524288.0 | grad norm: 0.388 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.575 | tokens per gpu per second (tgs): 1828.793 | TFLOPs: 14.72 |
g0220: [2024-08-09 17:17:13,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=10260, skipped=9, lr=[0.00017928901973333333, 0.00017928901973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10260 loss: 1.0146 iter time (s): 4.638 samples/sec: 27.600
g0238:  iteration    10260/10000000 | consumed samples:      1313280 | consumed tokens:   2689597440 | elapsed time per iteration (ms): 4670.1 | learning rate: 1.793E-04 | global batch size:   128 | lm loss: 1.004440E+00 | loss scale: 524288.0 | grad norm: 0.298 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.408 | tokens per gpu per second (tgs): 1754.133 | TFLOPs: 14.12 |
g0220: [2024-08-09 17:18:04,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=10270, skipped=9, lr=[0.00017946378240000003, 0.00017946378240000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10270 loss: 1.0253 iter time (s): 5.045 samples/sec: 25.374
g0238:  iteration    10270/10000000 | consumed samples:      1314560 | consumed tokens:   2692218880 | elapsed time per iteration (ms): 5077.2 | learning rate: 1.795E-04 | global batch size:   128 | lm loss: 9.941453E-01 | loss scale: 524288.0 | grad norm: 0.331 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.211 | tokens per gpu per second (tgs): 1613.479 | TFLOPs: 12.98 |
g0220: [2024-08-09 17:19:04,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=10280, skipped=9, lr=[0.00017963854506666666, 0.00017963854506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10280 loss: 1.0002 iter time (s): 6.030 samples/sec: 21.227
g0238:  iteration    10280/10000000 | consumed samples:      1315840 | consumed tokens:   2694840320 | elapsed time per iteration (ms): 6064.1 | learning rate: 1.796E-04 | global batch size:   128 | lm loss: 1.004942E+00 | loss scale: 524288.0 | grad norm: 0.362 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.108 | tokens per gpu per second (tgs): 1350.898 | TFLOPs: 10.87 |
g0220: [2024-08-09 17:19:53,411] [INFO] [logging.py:96:log_dist] [Rank 0] step=10290, skipped=9, lr=[0.00017981330773333336, 0.00017981330773333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10290 loss: 1.0258 iter time (s): 4.837 samples/sec: 26.460
g0238:  iteration    10290/10000000 | consumed samples:      1317120 | consumed tokens:   2697461760 | elapsed time per iteration (ms): 4870.2 | learning rate: 1.798E-04 | global batch size:   128 | lm loss: 1.017649E+00 | loss scale: 524288.0 | grad norm: 0.385 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.282 | tokens per gpu per second (tgs): 1682.076 | TFLOPs: 13.54 |
g0220: [2024-08-09 17:20:39,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=10300, skipped=9, lr=[0.0001799880704, 0.0001799880704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10300 loss: 1.0363 iter time (s): 4.560 samples/sec: 28.070
g0238:  iteration    10300/10000000 | consumed samples:      1318400 | consumed tokens:   2700083200 | elapsed time per iteration (ms): 4592.9 | learning rate: 1.800E-04 | global batch size:   128 | lm loss: 1.014494E+00 | loss scale: 524288.0 | grad norm: 0.362 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.869 | tokens per gpu per second (tgs): 1783.615 | TFLOPs: 14.35 |
g0220: [2024-08-09 17:21:40,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=10310, skipped=9, lr=[0.0001801628330666667, 0.0001801628330666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10310 loss: 1.0013 iter time (s): 6.098 samples/sec: 20.990
g0238:  iteration    10310/10000000 | consumed samples:      1319680 | consumed tokens:   2702704640 | elapsed time per iteration (ms): 6132.4 | learning rate: 1.802E-04 | global batch size:   128 | lm loss: 1.018531E+00 | loss scale: 524288.0 | grad norm: 0.395 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.873 | tokens per gpu per second (tgs): 1335.866 | TFLOPs: 10.75 |
g0220: [2024-08-09 17:22:25,332] [INFO] [logging.py:96:log_dist] [Rank 0] step=10320, skipped=9, lr=[0.00018033759573333333, 0.00018033759573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10320 loss: 1.0221 iter time (s): 4.434 samples/sec: 28.870
g0238:  iteration    10320/10000000 | consumed samples:      1320960 | consumed tokens:   2705326080 | elapsed time per iteration (ms): 4466.8 | learning rate: 1.803E-04 | global batch size:   128 | lm loss: 1.009965E+00 | loss scale: 524288.0 | grad norm: 0.324 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.656 | tokens per gpu per second (tgs): 1833.980 | TFLOPs: 14.76 |
g0220: [2024-08-09 17:23:09,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=10330, skipped=9, lr=[0.00018051235840000002, 0.00018051235840000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10330 loss: 1.0584 iter time (s): 4.398 samples/sec: 29.104
g0238:  iteration    10330/10000000 | consumed samples:      1322240 | consumed tokens:   2707947520 | elapsed time per iteration (ms): 4442.0 | learning rate: 1.805E-04 | global batch size:   128 | lm loss: 1.011744E+00 | loss scale: 524288.0 | grad norm: 0.562 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.816 | tokens per gpu per second (tgs): 1844.226 | TFLOPs: 14.84 |
g0220: [2024-08-09 17:24:02,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=10340, skipped=9, lr=[0.00018068712106666666, 0.00018068712106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10340 loss: 1.0155 iter time (s): 5.196 samples/sec: 24.636
g0238:  iteration    10340/10000000 | consumed samples:      1323520 | consumed tokens:   2710568960 | elapsed time per iteration (ms): 5253.2 | learning rate: 1.807E-04 | global batch size:   128 | lm loss: 1.015358E+00 | loss scale: 524288.0 | grad norm: 0.360 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.366 | tokens per gpu per second (tgs): 1559.419 | TFLOPs: 12.55 |
g0220: [2024-08-09 17:24:49,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=10350, skipped=9, lr=[0.00018086188373333335, 0.00018086188373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10350 loss: 0.9651 iter time (s): 4.720 samples/sec: 27.120
g0238:  iteration    10350/10000000 | consumed samples:      1324800 | consumed tokens:   2713190400 | elapsed time per iteration (ms): 4752.4 | learning rate: 1.809E-04 | global batch size:   128 | lm loss: 1.006614E+00 | loss scale: 524288.0 | grad norm: 0.318 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.934 | tokens per gpu per second (tgs): 1723.774 | TFLOPs: 13.87 |
g0236: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 10350
g0236: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 10350
g0236: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 10350
g0220: Grad overflow on iteration 10350
g0236: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 10350
g0236: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 10350
g0234: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 10350
g0234: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 10350
g0234: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 10350
g0235: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 10350
g0234: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 10350
g0233: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 10350
g0233: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 10350
g0233: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 10350
g0233: Grad overflow on iteration 10350
g0233: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 10350
g0238: Grad overflow on iteration 10350
g0237: Grad overflow on iteration 10350
g0220: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: Grad overflow on iteration 10350
g0233: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 10350
g0235: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 10350
g0233: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 10350
g0225: Grad overflow on iteration 10350
g0234: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 17:24:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: Grad overflow on iteration 10350
g0238: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: Grad overflow on iteration 10350
g0238: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 10350
g0237: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 10350
g0220: Grad overflow on iteration 10350
g0237: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 10350
g0237: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 10350
g0220: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: Grad overflow on iteration 10350
g0225: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 10350
g0225: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 17:24:54,034] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0238: [2024-08-09 17:24:54,034] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 17:25:30,143] [INFO] [logging.py:96:log_dist] [Rank 0] step=10360, skipped=10, lr=[0.00018103664640000002, 0.00018103664640000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10360 loss: 0.9875 iter time (s): 4.001 samples/sec: 31.993
g0238:  iteration    10360/10000000 | consumed samples:      1326080 | consumed tokens:   2715811840 | elapsed time per iteration (ms): 4033.5 | learning rate: 1.810E-04 | global batch size:   128 | lm loss: 1.013208E+00 | loss scale: 262144.0 | grad norm: 0.338 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.734 | tokens per gpu per second (tgs): 2030.995 | TFLOPs: 16.34 |
g0220: [2024-08-09 17:26:14,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=10370, skipped=10, lr=[0.00018121140906666669, 0.00018121140906666669], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10370 loss: 1.0090 iter time (s): 4.367 samples/sec: 29.309
g0238:  iteration    10370/10000000 | consumed samples:      1327360 | consumed tokens:   2718433280 | elapsed time per iteration (ms): 4400.0 | learning rate: 1.812E-04 | global batch size:   128 | lm loss: 1.010067E+00 | loss scale: 262144.0 | grad norm: 0.328 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.091 | tokens per gpu per second (tgs): 1861.804 | TFLOPs: 14.98 |
g0220: [2024-08-09 17:26:57,462] [INFO] [logging.py:96:log_dist] [Rank 0] step=10380, skipped=10, lr=[0.00018138617173333335, 0.00018138617173333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10380 loss: 1.0195 iter time (s): 4.299 samples/sec: 29.775
g0238:  iteration    10380/10000000 | consumed samples:      1328640 | consumed tokens:   2721054720 | elapsed time per iteration (ms): 4331.8 | learning rate: 1.814E-04 | global batch size:   128 | lm loss: 1.005747E+00 | loss scale: 262144.0 | grad norm: 0.361 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.549 | tokens per gpu per second (tgs): 1891.126 | TFLOPs: 15.22 |
g0220: [2024-08-09 17:27:39,987] [INFO] [logging.py:96:log_dist] [Rank 0] step=10390, skipped=10, lr=[0.00018156093440000002, 0.00018156093440000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10390 loss: 1.0382 iter time (s): 4.220 samples/sec: 30.334
g0238:  iteration    10390/10000000 | consumed samples:      1329920 | consumed tokens:   2723676160 | elapsed time per iteration (ms): 4252.5 | learning rate: 1.816E-04 | global batch size:   128 | lm loss: 1.004054E+00 | loss scale: 262144.0 | grad norm: 0.324 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.100 | tokens per gpu per second (tgs): 1926.416 | TFLOPs: 15.50 |
g0220: [2024-08-09 17:28:20,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=10400, skipped=10, lr=[0.00018173569706666668, 0.00018173569706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10400 loss: 0.9893 iter time (s): 4.066 samples/sec: 31.481
g0238:  iteration    10400/10000000 | consumed samples:      1331200 | consumed tokens:   2726297600 | elapsed time per iteration (ms): 4099.0 | learning rate: 1.817E-04 | global batch size:   128 | lm loss: 1.001538E+00 | loss scale: 262144.0 | grad norm: 0.361 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.228 | tokens per gpu per second (tgs): 1998.561 | TFLOPs: 16.08 |
g0220: [2024-08-09 17:29:06,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=10410, skipped=10, lr=[0.00018191045973333335, 0.00018191045973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10410 loss: 0.9850 iter time (s): 4.561 samples/sec: 28.066
g0238:  iteration    10410/10000000 | consumed samples:      1332480 | consumed tokens:   2728919040 | elapsed time per iteration (ms): 4593.0 | learning rate: 1.819E-04 | global batch size:   128 | lm loss: 1.012751E+00 | loss scale: 262144.0 | grad norm: 0.332 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.869 | tokens per gpu per second (tgs): 1783.594 | TFLOPs: 14.35 |
g0220: [2024-08-09 17:29:54,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=10420, skipped=10, lr=[0.00018208522240000002, 0.00018208522240000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10420 loss: 1.0282 iter time (s): 4.714 samples/sec: 27.152
g0238:  iteration    10420/10000000 | consumed samples:      1333760 | consumed tokens:   2731540480 | elapsed time per iteration (ms): 4747.3 | learning rate: 1.821E-04 | global batch size:   128 | lm loss: 1.011833E+00 | loss scale: 262144.0 | grad norm: 0.515 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.963 | tokens per gpu per second (tgs): 1725.630 | TFLOPs: 13.89 |
g0220: [2024-08-09 17:30:40,673] [INFO] [logging.py:96:log_dist] [Rank 0] step=10430, skipped=10, lr=[0.00018225998506666668, 0.00018225998506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10430 loss: 1.0350 iter time (s): 4.597 samples/sec: 27.846
g0238:  iteration    10430/10000000 | consumed samples:      1335040 | consumed tokens:   2734161920 | elapsed time per iteration (ms): 4629.6 | learning rate: 1.823E-04 | global batch size:   128 | lm loss: 1.013451E+00 | loss scale: 262144.0 | grad norm: 0.407 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.648 | tokens per gpu per second (tgs): 1769.493 | TFLOPs: 14.24 |
g0220: [2024-08-09 17:31:24,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=10440, skipped=10, lr=[0.00018243474773333335, 0.00018243474773333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10440 loss: 0.9977 iter time (s): 4.327 samples/sec: 29.584
g0238:  iteration    10440/10000000 | consumed samples:      1336320 | consumed tokens:   2736783360 | elapsed time per iteration (ms): 4359.5 | learning rate: 1.824E-04 | global batch size:   128 | lm loss: 1.006900E+00 | loss scale: 262144.0 | grad norm: 0.351 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.361 | tokens per gpu per second (tgs): 1879.117 | TFLOPs: 15.12 |
g0220: [2024-08-09 17:32:10,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=10450, skipped=10, lr=[0.0001826095104, 0.0001826095104], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10450 loss: 1.0090 iter time (s): 4.570 samples/sec: 28.009
g0238:  iteration    10450/10000000 | consumed samples:      1337600 | consumed tokens:   2739404800 | elapsed time per iteration (ms): 4602.8 | learning rate: 1.826E-04 | global batch size:   128 | lm loss: 1.000113E+00 | loss scale: 262144.0 | grad norm: 0.373 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.809 | tokens per gpu per second (tgs): 1779.794 | TFLOPs: 14.32 |
g0220: [2024-08-09 17:32:56,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=10460, skipped=10, lr=[0.00018278427306666668, 0.00018278427306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10460 loss: 1.0042 iter time (s): 4.593 samples/sec: 27.868
g0238:  iteration    10460/10000000 | consumed samples:      1338880 | consumed tokens:   2742026240 | elapsed time per iteration (ms): 4625.5 | learning rate: 1.828E-04 | global batch size:   128 | lm loss: 1.000169E+00 | loss scale: 262144.0 | grad norm: 0.320 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.673 | tokens per gpu per second (tgs): 1771.042 | TFLOPs: 14.25 |
g0220: [2024-08-09 17:33:41,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=10470, skipped=10, lr=[0.00018295903573333332, 0.00018295903573333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10470 loss: 0.9942 iter time (s): 4.453 samples/sec: 28.745
g0238:  iteration    10470/10000000 | consumed samples:      1340160 | consumed tokens:   2744647680 | elapsed time per iteration (ms): 4485.5 | learning rate: 1.830E-04 | global batch size:   128 | lm loss: 1.006441E+00 | loss scale: 262144.0 | grad norm: 0.359 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.536 | tokens per gpu per second (tgs): 1826.326 | TFLOPs: 14.70 |
g0220: [2024-08-09 17:34:34,285] [INFO] [logging.py:96:log_dist] [Rank 0] step=10480, skipped=10, lr=[0.0001831337984, 0.0001831337984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10480 loss: 1.0040 iter time (s): 5.254 samples/sec: 24.362
g0238:  iteration    10480/10000000 | consumed samples:      1341440 | consumed tokens:   2747269120 | elapsed time per iteration (ms): 5287.6 | learning rate: 1.831E-04 | global batch size:   128 | lm loss: 1.005329E+00 | loss scale: 262144.0 | grad norm: 0.309 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.208 | tokens per gpu per second (tgs): 1549.299 | TFLOPs: 12.47 |
g0220: [2024-08-09 17:35:20,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=10490, skipped=10, lr=[0.00018330856106666665, 0.00018330856106666665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10490 loss: 1.0159 iter time (s): 4.573 samples/sec: 27.987
g0238:  iteration    10490/10000000 | consumed samples:      1342720 | consumed tokens:   2749890560 | elapsed time per iteration (ms): 4607.1 | learning rate: 1.833E-04 | global batch size:   128 | lm loss: 9.991026E-01 | loss scale: 262144.0 | grad norm: 0.344 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.783 | tokens per gpu per second (tgs): 1778.138 | TFLOPs: 14.31 |
g0220: [2024-08-09 17:36:03,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=10500, skipped=10, lr=[0.00018348332373333334, 0.00018348332373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10500 loss: 0.9821 iter time (s): 4.305 samples/sec: 29.733
g0238:  iteration    10500/10000000 | consumed samples:      1344000 | consumed tokens:   2752512000 | elapsed time per iteration (ms): 4338.0 | learning rate: 1.835E-04 | global batch size:   128 | lm loss: 9.936480E-01 | loss scale: 262144.0 | grad norm: 0.344 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.506 | tokens per gpu per second (tgs): 1888.406 | TFLOPs: 15.20 |
g0220: [2024-08-09 17:36:50,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=10510, skipped=10, lr=[0.00018365808639999998, 0.00018365808639999998], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10510 loss: 1.0278 iter time (s): 4.691 samples/sec: 27.287
g0238:  iteration    10510/10000000 | consumed samples:      1345280 | consumed tokens:   2755133440 | elapsed time per iteration (ms): 4724.9 | learning rate: 1.837E-04 | global batch size:   128 | lm loss: 1.003571E+00 | loss scale: 262144.0 | grad norm: 0.728 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.091 | tokens per gpu per second (tgs): 1733.801 | TFLOPs: 13.95 |
g0220: [2024-08-09 17:37:41,782] [INFO] [logging.py:96:log_dist] [Rank 0] step=10520, skipped=10, lr=[0.00018383284906666668, 0.00018383284906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10520 loss: 0.9778 iter time (s): 5.046 samples/sec: 25.365
g0238:  iteration    10520/10000000 | consumed samples:      1346560 | consumed tokens:   2757754880 | elapsed time per iteration (ms): 5079.6 | learning rate: 1.838E-04 | global batch size:   128 | lm loss: 1.001112E+00 | loss scale: 262144.0 | grad norm: 0.292 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.199 | tokens per gpu per second (tgs): 1612.711 | TFLOPs: 12.98 |
g0220: [2024-08-09 17:38:27,831] [INFO] [logging.py:96:log_dist] [Rank 0] step=10530, skipped=10, lr=[0.00018400761173333332, 0.00018400761173333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10530 loss: 0.9810 iter time (s): 4.572 samples/sec: 27.996
g0238:  iteration    10530/10000000 | consumed samples:      1347840 | consumed tokens:   2760376320 | elapsed time per iteration (ms): 4605.0 | learning rate: 1.840E-04 | global batch size:   128 | lm loss: 9.903990E-01 | loss scale: 262144.0 | grad norm: 0.325 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.796 | tokens per gpu per second (tgs): 1778.952 | TFLOPs: 14.32 |
g0220: [2024-08-09 17:39:10,021] [INFO] [logging.py:96:log_dist] [Rank 0] step=10540, skipped=10, lr=[0.0001841823744, 0.0001841823744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10540 loss: 1.0024 iter time (s): 4.186 samples/sec: 30.578
g0238:  iteration    10540/10000000 | consumed samples:      1349120 | consumed tokens:   2762997760 | elapsed time per iteration (ms): 4219.1 | learning rate: 1.842E-04 | global batch size:   128 | lm loss: 1.005141E+00 | loss scale: 262144.0 | grad norm: 0.326 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.338 | tokens per gpu per second (tgs): 1941.634 | TFLOPs: 15.62 |
g0220: [2024-08-09 17:39:59,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=10550, skipped=10, lr=[0.00018435713706666665, 0.00018435713706666665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10550 loss: 1.0273 iter time (s): 4.963 samples/sec: 25.789
g0238:  iteration    10550/10000000 | consumed samples:      1350400 | consumed tokens:   2765619200 | elapsed time per iteration (ms): 4996.4 | learning rate: 1.844E-04 | global batch size:   128 | lm loss: 1.006875E+00 | loss scale: 262144.0 | grad norm: 0.402 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.618 | tokens per gpu per second (tgs): 1639.567 | TFLOPs: 13.19 |
g0220: [2024-08-09 17:40:44,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=10560, skipped=10, lr=[0.00018453189973333334, 0.00018453189973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10560 loss: 0.9789 iter time (s): 4.439 samples/sec: 28.835
g0238:  iteration    10560/10000000 | consumed samples:      1351680 | consumed tokens:   2768240640 | elapsed time per iteration (ms): 4471.8 | learning rate: 1.845E-04 | global batch size:   128 | lm loss: 1.000809E+00 | loss scale: 262144.0 | grad norm: 0.389 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.624 | tokens per gpu per second (tgs): 1831.943 | TFLOPs: 14.74 |
g0220: [2024-08-09 17:41:31,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=10570, skipped=10, lr=[0.00018470666239999998, 0.00018470666239999998], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10570 loss: 1.0105 iter time (s): 4.610 samples/sec: 27.764
g0238:  iteration    10570/10000000 | consumed samples:      1352960 | consumed tokens:   2770862080 | elapsed time per iteration (ms): 4643.5 | learning rate: 1.847E-04 | global batch size:   128 | lm loss: 9.935757E-01 | loss scale: 262144.0 | grad norm: 0.361 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.566 | tokens per gpu per second (tgs): 1764.204 | TFLOPs: 14.20 |
g0220: [2024-08-09 17:42:20,533] [INFO] [logging.py:96:log_dist] [Rank 0] step=10580, skipped=10, lr=[0.00018488142506666667, 0.00018488142506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10580 loss: 1.0187 iter time (s): 4.906 samples/sec: 26.088
g0238:  iteration    10580/10000000 | consumed samples:      1354240 | consumed tokens:   2773483520 | elapsed time per iteration (ms): 4939.5 | learning rate: 1.849E-04 | global batch size:   128 | lm loss: 1.003966E+00 | loss scale: 262144.0 | grad norm: 0.326 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.914 | tokens per gpu per second (tgs): 1658.472 | TFLOPs: 13.35 |
g0220: [2024-08-09 17:43:02,456] [INFO] [logging.py:96:log_dist] [Rank 0] step=10590, skipped=10, lr=[0.0001850561877333333, 0.0001850561877333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10590 loss: 1.0054 iter time (s): 4.159 samples/sec: 30.774
g0238:  iteration    10590/10000000 | consumed samples:      1355520 | consumed tokens:   2776104960 | elapsed time per iteration (ms): 4192.2 | learning rate: 1.851E-04 | global batch size:   128 | lm loss: 1.002222E+00 | loss scale: 262144.0 | grad norm: 0.329 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.533 | tokens per gpu per second (tgs): 1954.128 | TFLOPs: 15.73 |
g0220: [2024-08-09 17:43:47,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=10600, skipped=10, lr=[0.0001852309504, 0.0001852309504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10600 loss: 0.9696 iter time (s): 4.427 samples/sec: 28.914
g0238:  iteration    10600/10000000 | consumed samples:      1356800 | consumed tokens:   2778726400 | elapsed time per iteration (ms): 4460.0 | learning rate: 1.852E-04 | global batch size:   128 | lm loss: 9.943450E-01 | loss scale: 262144.0 | grad norm: 0.306 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.700 | tokens per gpu per second (tgs): 1836.775 | TFLOPs: 14.78 |
g0220: [2024-08-09 17:44:31,292] [INFO] [logging.py:96:log_dist] [Rank 0] step=10610, skipped=10, lr=[0.00018540571306666667, 0.00018540571306666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10610 loss: 1.0281 iter time (s): 4.391 samples/sec: 29.154
g0238:  iteration    10610/10000000 | consumed samples:      1358080 | consumed tokens:   2781347840 | elapsed time per iteration (ms): 4423.6 | learning rate: 1.854E-04 | global batch size:   128 | lm loss: 9.916764E-01 | loss scale: 262144.0 | grad norm: 0.376 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.936 | tokens per gpu per second (tgs): 1851.906 | TFLOPs: 14.90 |
g0220: [2024-08-09 17:45:13,976] [INFO] [logging.py:96:log_dist] [Rank 0] step=10620, skipped=10, lr=[0.00018558047573333334, 0.00018558047573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10620 loss: 0.9692 iter time (s): 4.236 samples/sec: 30.217
g0238:  iteration    10620/10000000 | consumed samples:      1359360 | consumed tokens:   2783969280 | elapsed time per iteration (ms): 4268.4 | learning rate: 1.856E-04 | global batch size:   128 | lm loss: 9.950277E-01 | loss scale: 262144.0 | grad norm: 0.352 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.988 | tokens per gpu per second (tgs): 1919.235 | TFLOPs: 15.44 |
g0220: [2024-08-09 17:45:58,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=10630, skipped=10, lr=[0.0001857552384, 0.0001857552384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10630 loss: 1.0246 iter time (s): 4.372 samples/sec: 29.280
g0238:  iteration    10630/10000000 | consumed samples:      1360640 | consumed tokens:   2786590720 | elapsed time per iteration (ms): 4405.2 | learning rate: 1.858E-04 | global batch size:   128 | lm loss: 1.003631E+00 | loss scale: 262144.0 | grad norm: 0.363 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.057 | tokens per gpu per second (tgs): 1859.617 | TFLOPs: 14.96 |
g0220: [2024-08-09 17:47:10,082] [INFO] [logging.py:96:log_dist] [Rank 0] step=10640, skipped=10, lr=[0.00018593000106666667, 0.00018593000106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10640 loss: 1.0001 iter time (s): 7.173 samples/sec: 17.846
g0238:  iteration    10640/10000000 | consumed samples:      1361920 | consumed tokens:   2789212160 | elapsed time per iteration (ms): 7205.5 | learning rate: 1.859E-04 | global batch size:   128 | lm loss: 9.984179E-01 | loss scale: 262144.0 | grad norm: 0.299 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 17.764 | tokens per gpu per second (tgs): 1136.903 | TFLOPs: 9.15 |
g0220: [2024-08-09 17:47:57,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=10650, skipped=10, lr=[0.00018610476373333334, 0.00018610476373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10650 loss: 1.0081 iter time (s): 4.680 samples/sec: 27.348
g0238:  iteration    10650/10000000 | consumed samples:      1363200 | consumed tokens:   2791833600 | elapsed time per iteration (ms): 4713.5 | learning rate: 1.861E-04 | global batch size:   128 | lm loss: 1.000497E+00 | loss scale: 262144.0 | grad norm: 0.359 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.156 | tokens per gpu per second (tgs): 1737.985 | TFLOPs: 13.99 |
g0220: [2024-08-09 17:48:38,759] [INFO] [logging.py:96:log_dist] [Rank 0] step=10660, skipped=10, lr=[0.0001862795264, 0.0001862795264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10660 loss: 0.9705 iter time (s): 4.121 samples/sec: 31.059
g0238:  iteration    10660/10000000 | consumed samples:      1364480 | consumed tokens:   2794455040 | elapsed time per iteration (ms): 4154.0 | learning rate: 1.863E-04 | global batch size:   128 | lm loss: 9.923011E-01 | loss scale: 262144.0 | grad norm: 0.338 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.814 | tokens per gpu per second (tgs): 1972.081 | TFLOPs: 15.87 |
g0220: [2024-08-09 17:49:19,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=10670, skipped=10, lr=[0.00018645428906666667, 0.00018645428906666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10670 loss: 1.0118 iter time (s): 4.051 samples/sec: 31.596
g0238:  iteration    10670/10000000 | consumed samples:      1365760 | consumed tokens:   2797076480 | elapsed time per iteration (ms): 4084.2 | learning rate: 1.865E-04 | global batch size:   128 | lm loss: 9.940725E-01 | loss scale: 262144.0 | grad norm: 0.344 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.340 | tokens per gpu per second (tgs): 2005.782 | TFLOPs: 16.14 |
g0220: [2024-08-09 17:50:04,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=10680, skipped=10, lr=[0.00018662905173333333, 0.00018662905173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10680 loss: 1.0009 iter time (s): 4.434 samples/sec: 28.870
g0238:  iteration    10680/10000000 | consumed samples:      1367040 | consumed tokens:   2799697920 | elapsed time per iteration (ms): 4466.8 | learning rate: 1.866E-04 | global batch size:   128 | lm loss: 1.000243E+00 | loss scale: 262144.0 | grad norm: 0.307 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.656 | tokens per gpu per second (tgs): 1833.971 | TFLOPs: 14.76 |
g0220: [2024-08-09 17:50:49,958] [INFO] [logging.py:96:log_dist] [Rank 0] step=10690, skipped=10, lr=[0.0001868038144, 0.0001868038144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10690 loss: 0.9875 iter time (s): 4.536 samples/sec: 28.218
g0238:  iteration    10690/10000000 | consumed samples:      1368320 | consumed tokens:   2802319360 | elapsed time per iteration (ms): 4569.0 | learning rate: 1.868E-04 | global batch size:   128 | lm loss: 9.931727E-01 | loss scale: 262144.0 | grad norm: 0.345 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.015 | tokens per gpu per second (tgs): 1792.940 | TFLOPs: 14.43 |
g0220: [2024-08-09 17:51:42,935] [INFO] [logging.py:96:log_dist] [Rank 0] step=10700, skipped=10, lr=[0.00018697857706666667, 0.00018697857706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10700 loss: 1.0019 iter time (s): 5.265 samples/sec: 24.312
g0238:  iteration    10700/10000000 | consumed samples:      1369600 | consumed tokens:   2804940800 | elapsed time per iteration (ms): 5297.5 | learning rate: 1.870E-04 | global batch size:   128 | lm loss: 1.008400E+00 | loss scale: 262144.0 | grad norm: 0.622 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.162 | tokens per gpu per second (tgs): 1546.399 | TFLOPs: 12.44 |
g0220: [2024-08-09 17:52:33,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=10710, skipped=10, lr=[0.00018715333973333333, 0.00018715333973333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10710 loss: 0.9516 iter time (s): 4.986 samples/sec: 25.674
g0238:  iteration    10710/10000000 | consumed samples:      1370880 | consumed tokens:   2807562240 | elapsed time per iteration (ms): 5018.1 | learning rate: 1.872E-04 | global batch size:   128 | lm loss: 9.871196E-01 | loss scale: 262144.0 | grad norm: 0.295 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.508 | tokens per gpu per second (tgs): 1632.483 | TFLOPs: 13.14 |
g0220: [2024-08-09 17:53:26,846] [INFO] [logging.py:96:log_dist] [Rank 0] step=10720, skipped=10, lr=[0.00018732810240000003, 0.00018732810240000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10720 loss: 1.0010 iter time (s): 5.340 samples/sec: 23.970
g0238:  iteration    10720/10000000 | consumed samples:      1372160 | consumed tokens:   2810183680 | elapsed time per iteration (ms): 5373.5 | learning rate: 1.873E-04 | global batch size:   128 | lm loss: 9.872791E-01 | loss scale: 262144.0 | grad norm: 0.337 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.821 | tokens per gpu per second (tgs): 1524.531 | TFLOPs: 12.27 |
g0220: [2024-08-09 17:54:10,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=10730, skipped=10, lr=[0.00018750286506666666, 0.00018750286506666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10730 loss: 0.9949 iter time (s): 4.292 samples/sec: 29.820
g0238:  iteration    10730/10000000 | consumed samples:      1373440 | consumed tokens:   2812805120 | elapsed time per iteration (ms): 4325.0 | learning rate: 1.875E-04 | global batch size:   128 | lm loss: 9.966048E-01 | loss scale: 262144.0 | grad norm: 0.296 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.595 | tokens per gpu per second (tgs): 1894.105 | TFLOPs: 15.24 |
g0220: [2024-08-09 17:54:50,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=10740, skipped=10, lr=[0.00018767762773333336, 0.00018767762773333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10740 loss: 0.9990 iter time (s): 3.983 samples/sec: 32.140
g0238:  iteration    10740/10000000 | consumed samples:      1374720 | consumed tokens:   2815426560 | elapsed time per iteration (ms): 4015.7 | learning rate: 1.877E-04 | global batch size:   128 | lm loss: 9.863111E-01 | loss scale: 262144.0 | grad norm: 0.353 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.875 | tokens per gpu per second (tgs): 2040.018 | TFLOPs: 16.42 |
g0220: [2024-08-09 17:55:33,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=10750, skipped=10, lr=[0.0001878523904, 0.0001878523904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10750 loss: 1.0097 iter time (s): 4.301 samples/sec: 29.760
g0238:  iteration    10750/10000000 | consumed samples:      1376000 | consumed tokens:   2818048000 | elapsed time per iteration (ms): 4333.7 | learning rate: 1.879E-04 | global batch size:   128 | lm loss: 1.003011E+00 | loss scale: 262144.0 | grad norm: 0.815 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.536 | tokens per gpu per second (tgs): 1890.318 | TFLOPs: 15.21 |
g0220: [2024-08-09 17:56:16,875] [INFO] [logging.py:96:log_dist] [Rank 0] step=10760, skipped=10, lr=[0.0001880271530666667, 0.0001880271530666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10760 loss: 0.9805 iter time (s): 4.295 samples/sec: 29.800
g0238:  iteration    10760/10000000 | consumed samples:      1377280 | consumed tokens:   2820669440 | elapsed time per iteration (ms): 4328.1 | learning rate: 1.880E-04 | global batch size:   128 | lm loss: 9.920284E-01 | loss scale: 262144.0 | grad norm: 0.301 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.574 | tokens per gpu per second (tgs): 1892.732 | TFLOPs: 15.23 |
g0220: [2024-08-09 17:57:22,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=10770, skipped=10, lr=[0.00018820191573333333, 0.00018820191573333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10770 loss: 1.0167 iter time (s): 6.490 samples/sec: 19.724
g0238:  iteration    10770/10000000 | consumed samples:      1378560 | consumed tokens:   2823290880 | elapsed time per iteration (ms): 6522.6 | learning rate: 1.882E-04 | global batch size:   128 | lm loss: 9.874585E-01 | loss scale: 262144.0 | grad norm: 0.306 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 19.624 | tokens per gpu per second (tgs): 1255.944 | TFLOPs: 10.11 |
g0220: [2024-08-09 17:58:11,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=10780, skipped=10, lr=[0.00018837667840000002, 0.00018837667840000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10780 loss: 0.9668 iter time (s): 4.886 samples/sec: 26.196
g0238:  iteration    10780/10000000 | consumed samples:      1379840 | consumed tokens:   2825912320 | elapsed time per iteration (ms): 4919.8 | learning rate: 1.884E-04 | global batch size:   128 | lm loss: 9.881822E-01 | loss scale: 262144.0 | grad norm: 0.302 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.017 | tokens per gpu per second (tgs): 1665.113 | TFLOPs: 13.40 |
g0220: [2024-08-09 17:58:53,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=10790, skipped=10, lr=[0.00018855144106666666, 0.00018855144106666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10790 loss: 0.9544 iter time (s): 4.196 samples/sec: 30.505
g0238:  iteration    10790/10000000 | consumed samples:      1381120 | consumed tokens:   2828533760 | elapsed time per iteration (ms): 4229.2 | learning rate: 1.886E-04 | global batch size:   128 | lm loss: 9.815668E-01 | loss scale: 262144.0 | grad norm: 0.315 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.266 | tokens per gpu per second (tgs): 1937.005 | TFLOPs: 15.59 |
g0220: [2024-08-09 17:59:35,064] [INFO] [logging.py:96:log_dist] [Rank 0] step=10800, skipped=10, lr=[0.00018872620373333335, 0.00018872620373333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10800 loss: 0.9716 iter time (s): 4.115 samples/sec: 31.108
g0238:  iteration    10800/10000000 | consumed samples:      1382400 | consumed tokens:   2831155200 | elapsed time per iteration (ms): 4147.2 | learning rate: 1.887E-04 | global batch size:   128 | lm loss: 9.918981E-01 | loss scale: 262144.0 | grad norm: 0.314 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.864 | tokens per gpu per second (tgs): 1975.327 | TFLOPs: 15.90 |
g0220: [2024-08-09 18:00:20,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=10810, skipped=10, lr=[0.0001889009664, 0.0001889009664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10810 loss: 0.9818 iter time (s): 4.484 samples/sec: 28.546
g0238:  iteration    10810/10000000 | consumed samples:      1383680 | consumed tokens:   2833776640 | elapsed time per iteration (ms): 4517.8 | learning rate: 1.889E-04 | global batch size:   128 | lm loss: 9.905684E-01 | loss scale: 262144.0 | grad norm: 0.278 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.333 | tokens per gpu per second (tgs): 1813.287 | TFLOPs: 14.59 |
g0220: [2024-08-09 18:01:02,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=10820, skipped=10, lr=[0.00018907572906666669, 0.00018907572906666669], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10820 loss: 1.0067 iter time (s): 4.171 samples/sec: 30.686
g0238:  iteration    10820/10000000 | consumed samples:      1384960 | consumed tokens:   2836398080 | elapsed time per iteration (ms): 4205.2 | learning rate: 1.891E-04 | global batch size:   128 | lm loss: 9.844044E-01 | loss scale: 262144.0 | grad norm: 0.300 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.439 | tokens per gpu per second (tgs): 1948.077 | TFLOPs: 15.68 |
g0220: [2024-08-09 18:01:43,299] [INFO] [logging.py:96:log_dist] [Rank 0] step=10830, skipped=10, lr=[0.00018925049173333332, 0.00018925049173333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10830 loss: 0.9909 iter time (s): 4.068 samples/sec: 31.468
g0238:  iteration    10830/10000000 | consumed samples:      1386240 | consumed tokens:   2839019520 | elapsed time per iteration (ms): 4100.7 | learning rate: 1.893E-04 | global batch size:   128 | lm loss: 9.975581E-01 | loss scale: 262144.0 | grad norm: 0.389 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.214 | tokens per gpu per second (tgs): 1997.718 | TFLOPs: 16.08 |
g0220: [2024-08-09 18:02:41,410] [INFO] [logging.py:96:log_dist] [Rank 0] step=10840, skipped=10, lr=[0.00018942525440000002, 0.00018942525440000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10840 loss: 0.9972 iter time (s): 5.778 samples/sec: 22.153
g0238:  iteration    10840/10000000 | consumed samples:      1387520 | consumed tokens:   2841640960 | elapsed time per iteration (ms): 5811.0 | learning rate: 1.894E-04 | global batch size:   128 | lm loss: 9.800703E-01 | loss scale: 262144.0 | grad norm: 0.352 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.027 | tokens per gpu per second (tgs): 1409.730 | TFLOPs: 11.34 |
g0220: [2024-08-09 18:03:30,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=10850, skipped=10, lr=[0.00018960001706666666, 0.00018960001706666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10850 loss: 0.9710 iter time (s): 4.917 samples/sec: 26.033
g0238:  iteration    10850/10000000 | consumed samples:      1388800 | consumed tokens:   2844262400 | elapsed time per iteration (ms): 4950.2 | learning rate: 1.896E-04 | global batch size:   128 | lm loss: 9.994870E-01 | loss scale: 262144.0 | grad norm: 0.273 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.858 | tokens per gpu per second (tgs): 1654.892 | TFLOPs: 13.32 |
g0237: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 18:03:40,019] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 18:03:40,020] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 18:04:21,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=10860, skipped=10, lr=[0.00018977477973333335, 0.00018977477973333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10860 loss: 0.9913 iter time (s): 5.032 samples/sec: 25.438
g0238:  iteration    10860/10000000 | consumed samples:      1390080 | consumed tokens:   2846883840 | elapsed time per iteration (ms): 5064.8 | learning rate: 1.898E-04 | global batch size:   128 | lm loss: 9.941206E-01 | loss scale: 524288.0 | grad norm: 0.337 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.272 | tokens per gpu per second (tgs): 1617.438 | TFLOPs: 13.02 |
g0220: [2024-08-09 18:05:03,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=10870, skipped=10, lr=[0.0001899495424, 0.0001899495424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10870 loss: 0.9746 iter time (s): 4.113 samples/sec: 31.122
g0238:  iteration    10870/10000000 | consumed samples:      1391360 | consumed tokens:   2849505280 | elapsed time per iteration (ms): 4145.6 | learning rate: 1.899E-04 | global batch size:   128 | lm loss: 9.920561E-01 | loss scale: 524288.0 | grad norm: 0.320 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.876 | tokens per gpu per second (tgs): 1976.058 | TFLOPs: 15.90 |
g0220: [2024-08-09 18:05:44,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=10880, skipped=10, lr=[0.00019012430506666668, 0.00019012430506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10880 loss: 1.0064 iter time (s): 4.096 samples/sec: 31.247
g0238:  iteration    10880/10000000 | consumed samples:      1392640 | consumed tokens:   2852126720 | elapsed time per iteration (ms): 4129.2 | learning rate: 1.901E-04 | global batch size:   128 | lm loss: 9.929519E-01 | loss scale: 524288.0 | grad norm: 0.371 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.999 | tokens per gpu per second (tgs): 1983.936 | TFLOPs: 15.97 |
g0220: [2024-08-09 18:06:25,280] [INFO] [logging.py:96:log_dist] [Rank 0] step=10890, skipped=10, lr=[0.00019029906773333332, 0.00019029906773333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10890 loss: 0.9784 iter time (s): 4.064 samples/sec: 31.494
g0238:  iteration    10890/10000000 | consumed samples:      1393920 | consumed tokens:   2854748160 | elapsed time per iteration (ms): 4097.1 | learning rate: 1.903E-04 | global batch size:   128 | lm loss: 9.977386E-01 | loss scale: 524288.0 | grad norm: 0.289 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.241 | tokens per gpu per second (tgs): 1999.450 | TFLOPs: 16.09 |
g0220: [2024-08-09 18:07:20,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=10900, skipped=10, lr=[0.00019047383040000001, 0.00019047383040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10900 loss: 1.0136 iter time (s): 5.468 samples/sec: 23.408
g0238:  iteration    10900/10000000 | consumed samples:      1395200 | consumed tokens:   2857369600 | elapsed time per iteration (ms): 5500.7 | learning rate: 1.905E-04 | global batch size:   128 | lm loss: 9.893904E-01 | loss scale: 524288.0 | grad norm: 0.329 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.270 | tokens per gpu per second (tgs): 1489.254 | TFLOPs: 11.98 |
g0220: [2024-08-09 18:08:13,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=10910, skipped=10, lr=[0.00019064859306666665, 0.00019064859306666665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10910 loss: 0.9858 iter time (s): 5.256 samples/sec: 24.353
g0238:  iteration    10910/10000000 | consumed samples:      1396480 | consumed tokens:   2859991040 | elapsed time per iteration (ms): 5288.8 | learning rate: 1.906E-04 | global batch size:   128 | lm loss: 9.956095E-01 | loss scale: 524288.0 | grad norm: 0.306 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.202 | tokens per gpu per second (tgs): 1548.924 | TFLOPs: 12.46 |
g0220: [2024-08-09 18:08:59,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=10920, skipped=10, lr=[0.00019082335573333335, 0.00019082335573333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10920 loss: 1.0401 iter time (s): 4.577 samples/sec: 27.963
g0238:  iteration    10920/10000000 | consumed samples:      1397760 | consumed tokens:   2862612480 | elapsed time per iteration (ms): 4609.9 | learning rate: 1.908E-04 | global batch size:   128 | lm loss: 9.860349E-01 | loss scale: 524288.0 | grad norm: 0.526 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.766 | tokens per gpu per second (tgs): 1777.030 | TFLOPs: 14.30 |
g0220: [2024-08-09 18:09:41,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=10930, skipped=10, lr=[0.00019099811839999999, 0.00019099811839999999], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10930 loss: 0.9958 iter time (s): 4.206 samples/sec: 30.432
g0238:  iteration    10930/10000000 | consumed samples:      1399040 | consumed tokens:   2865233920 | elapsed time per iteration (ms): 4239.1 | learning rate: 1.910E-04 | global batch size:   128 | lm loss: 9.809384E-01 | loss scale: 524288.0 | grad norm: 0.316 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.195 | tokens per gpu per second (tgs): 1932.502 | TFLOPs: 15.55 |
g0220: [2024-08-09 18:10:23,836] [INFO] [logging.py:96:log_dist] [Rank 0] step=10940, skipped=10, lr=[0.00019117288106666668, 0.00019117288106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10940 loss: 0.9970 iter time (s): 4.184 samples/sec: 30.591
g0238:  iteration    10940/10000000 | consumed samples:      1400320 | consumed tokens:   2867855360 | elapsed time per iteration (ms): 4217.1 | learning rate: 1.912E-04 | global batch size:   128 | lm loss: 9.837230E-01 | loss scale: 524288.0 | grad norm: 0.403 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.353 | tokens per gpu per second (tgs): 1942.567 | TFLOPs: 15.63 |
g0220: [2024-08-09 18:11:04,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=10950, skipped=10, lr=[0.00019134764373333332, 0.00019134764373333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10950 loss: 0.9807 iter time (s): 4.044 samples/sec: 31.655
g0238:  iteration    10950/10000000 | consumed samples:      1401600 | consumed tokens:   2870476800 | elapsed time per iteration (ms): 4076.4 | learning rate: 1.913E-04 | global batch size:   128 | lm loss: 9.915848E-01 | loss scale: 524288.0 | grad norm: 0.312 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.401 | tokens per gpu per second (tgs): 2009.637 | TFLOPs: 16.17 |
g0220: [2024-08-09 18:11:49,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=10960, skipped=10, lr=[0.0001915224064, 0.0001915224064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10960 loss: 0.9967 iter time (s): 4.416 samples/sec: 28.986
g0238:  iteration    10960/10000000 | consumed samples:      1402880 | consumed tokens:   2873098240 | elapsed time per iteration (ms): 4448.5 | learning rate: 1.915E-04 | global batch size:   128 | lm loss: 9.874965E-01 | loss scale: 524288.0 | grad norm: 0.302 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.774 | tokens per gpu per second (tgs): 1841.511 | TFLOPs: 14.82 |
g0220: [2024-08-09 18:12:46,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=10970, skipped=10, lr=[0.00019169716906666668, 0.00019169716906666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10970 loss: 1.0344 iter time (s): 5.716 samples/sec: 22.394
g0238:  iteration    10970/10000000 | consumed samples:      1404160 | consumed tokens:   2875719680 | elapsed time per iteration (ms): 5750.7 | learning rate: 1.917E-04 | global batch size:   128 | lm loss: 9.886965E-01 | loss scale: 524288.0 | grad norm: 0.435 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.258 | tokens per gpu per second (tgs): 1424.533 | TFLOPs: 11.46 |
g0220: [2024-08-09 18:13:47,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=10980, skipped=10, lr=[0.00019187193173333334, 0.00019187193173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10980 loss: 1.0067 iter time (s): 6.061 samples/sec: 21.118
g0238:  iteration    10980/10000000 | consumed samples:      1405440 | consumed tokens:   2878341120 | elapsed time per iteration (ms): 6101.7 | learning rate: 1.919E-04 | global batch size:   128 | lm loss: 9.913794E-01 | loss scale: 524288.0 | grad norm: 0.355 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 20.978 | tokens per gpu per second (tgs): 1342.574 | TFLOPs: 10.80 |
g0220: [2024-08-09 18:14:34,387] [INFO] [logging.py:96:log_dist] [Rank 0] step=10990, skipped=10, lr=[0.0001920466944, 0.0001920466944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 10990 loss: 0.9488 iter time (s): 4.645 samples/sec: 27.556
g0238:  iteration    10990/10000000 | consumed samples:      1406720 | consumed tokens:   2880962560 | elapsed time per iteration (ms): 4677.7 | learning rate: 1.920E-04 | global batch size:   128 | lm loss: 9.833767E-01 | loss scale: 524288.0 | grad norm: 0.309 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.364 | tokens per gpu per second (tgs): 1751.284 | TFLOPs: 14.09 |
g0220: [2024-08-09 18:15:17,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=11000, skipped=10, lr=[0.00019222145706666668, 0.00019222145706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11000 loss: 0.9760 iter time (s): 4.233 samples/sec: 30.236
g0238:  iteration    11000/10000000 | consumed samples:      1408000 | consumed tokens:   2883584000 | elapsed time per iteration (ms): 4265.9 | learning rate: 1.922E-04 | global batch size:   128 | lm loss: 9.899773E-01 | loss scale: 524288.0 | grad norm: 0.305 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.006 | tokens per gpu per second (tgs): 1920.365 | TFLOPs: 15.45 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 11000 | lm loss value: 9.876604E-01 | lm loss PPL: 2.684945E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   11000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-09 18:21:56,314] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step11000 is about to be saved!
g0220: [2024-08-09 18:21:56,320] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0220: [2024-08-09 18:21:56,320] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0220: [2024-08-09 18:21:56,320] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0238: [2024-08-09 18:21:56,321] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0238: [2024-08-09 18:21:56,321] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0238: [2024-08-09 18:21:56,321] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0225: [2024-08-09 18:21:56,322] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0225: [2024-08-09 18:21:56,322] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0236: [2024-08-09 18:21:56,322] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0236: [2024-08-09 18:21:56,322] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0236: [2024-08-09 18:21:56,322] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0225: [2024-08-09 18:21:56,322] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0234: [2024-08-09 18:21:56,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0234: [2024-08-09 18:21:56,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0234: [2024-08-09 18:21:56,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0237: [2024-08-09 18:21:56,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0237: [2024-08-09 18:21:56,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0237: [2024-08-09 18:21:56,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0235: [2024-08-09 18:21:56,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0235: [2024-08-09 18:21:56,323] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0235: [2024-08-09 18:21:56,324] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0233: [2024-08-09 18:21:56,324] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0233: [2024-08-09 18:21:56,324] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0233: [2024-08-09 18:21:56,324] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0238: [2024-08-09 18:21:56,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_23-model_00-model_states.pt...
g0225: [2024-08-09 18:21:56,360] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_05-model_00-model_states.pt...
g0237: [2024-08-09 18:21:56,360] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_20-model_00-model_states.pt...
g0236: [2024-08-09 18:21:56,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_17-model_00-model_states.pt...
g0234: [2024-08-09 18:21:56,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_11-model_00-model_states.pt...
g0235: [2024-08-09 18:21:56,361] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_14-model_00-model_states.pt...
g0233: [2024-08-09 18:21:56,362] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_08-model_00-model_states.pt...
g0220: [2024-08-09 18:21:56,375] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_01-model_00-model_states.pt...
g0225: [2024-08-09 18:21:56,494] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_05-model_00-model_states.pt.
g0235: [2024-08-09 18:21:56,495] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_14-model_00-model_states.pt.
g0238: [2024-08-09 18:21:56,498] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 18:21:56,499] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 18:21:56,501] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_24-model_00-model_states.pt.
g0236: [2024-08-09 18:21:56,522] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_17-model_00-model_states.pt.
g0225: [2024-08-09 18:21:56,533] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_06-model_00-model_states.pt...
g0235: [2024-08-09 18:21:56,535] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_15-model_00-model_states.pt...
g0238: [2024-08-09 18:21:56,554] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_25-model_00-model_states.pt...
g0236: [2024-08-09 18:21:56,562] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_18-model_00-model_states.pt...
g0237: [2024-08-09 18:21:56,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_20-model_00-model_states.pt.
g0234: [2024-08-09 18:21:56,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_11-model_00-model_states.pt.
g0233: [2024-08-09 18:21:56,566] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_08-model_00-model_states.pt.
g0237: [2024-08-09 18:21:56,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_21-model_00-model_states.pt...
g0234: [2024-08-09 18:21:56,604] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_12-model_00-model_states.pt...
g0233: [2024-08-09 18:21:56,604] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_09-model_00-model_states.pt...
g0220: [2024-08-09 18:21:56,642] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_01-model_00-model_states.pt.
g0220: [2024-08-09 18:21:56,661] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_02-model_00-model_states.pt...
g0225: [2024-08-09 18:21:56,717] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_06-model_00-model_states.pt.
g0225: [2024-08-09 18:21:56,752] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_07-model_00-model_states.pt...
g0238: [2024-08-09 18:21:56,771] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 18:21:56,772] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_07_model_states.pt...
g0233: [2024-08-09 18:21:56,914] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_09-model_00-model_states.pt.
g0233: [2024-08-09 18:21:56,948] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_10-model_00-model_states.pt...
g0236: [2024-08-09 18:21:56,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_18-model_00-model_states.pt.
g0220: [2024-08-09 18:21:56,999] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_02-model_00-model_states.pt.
g0236: [2024-08-09 18:21:57,007] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_19-model_00-model_states.pt...
g0220: [2024-08-09 18:21:57,028] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_03-model_00-model_states.pt...
g0233: [2024-08-09 18:21:57,052] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 18:21:57,054] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_02_model_states.pt...
g0236: [2024-08-09 18:21:57,123] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 18:21:57,126] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_05_model_states.pt...
g0235: [2024-08-09 18:21:57,144] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_15-model_00-model_states.pt.
g0235: [2024-08-09 18:21:57,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_16-model_00-model_states.pt...
g0237: [2024-08-09 18:21:57,193] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_21-model_00-model_states.pt.
g0234: [2024-08-09 18:21:57,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_12-model_00-model_states.pt.
g0237: [2024-08-09 18:21:57,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_22-model_00-model_states.pt...
g0220: [2024-08-09 18:21:57,232] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_03-model_00-model_states.pt.
g0225: [2024-08-09 18:21:57,241] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 18:21:57,243] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_01_model_states.pt...
g0234: [2024-08-09 18:21:57,248] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_13-model_00-model_states.pt...
g0220: [2024-08-09 18:21:57,251] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_04-model_00-model_states.pt...
g0235: [2024-08-09 18:21:57,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 18:21:57,338] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_04_model_states.pt...
g0237: [2024-08-09 18:21:57,385] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 18:21:57,387] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_06_model_states.pt...
g0234: [2024-08-09 18:21:57,391] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 18:21:57,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_03_model_states.pt...
g0220: [2024-08-09 18:21:57,463] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 18:21:57,464] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_00_model_states.pt
g0220: [2024-08-09 18:21:57,464] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_00_model_states.pt...
g0238: [2024-08-09 18:21:58,743] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 18:21:58,743] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0233: [2024-08-09 18:21:59,525] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 18:21:59,525] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0236: [2024-08-09 18:21:59,559] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 18:21:59,559] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0225: [2024-08-09 18:21:59,714] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_01_model_states.pt.
g0225: [2024-08-09 18:21:59,714] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0234: [2024-08-09 18:21:59,733] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 18:21:59,734] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0237: [2024-08-09 18:21:59,908] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 18:21:59,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0235: [2024-08-09 18:21:59,997] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 18:21:59,997] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0220: [2024-08-09 18:22:00,962] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step11000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 18:22:00,963] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11000 is ready now!
g0220:   successfully saved checkpoint at iteration   11000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 4.78, Latency(second): 4.712
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (4711.90, 4712.07)
g0220: [2024-08-09 18:22:49,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=11010, skipped=10, lr=[0.00019239621973333334, 0.00019239621973333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11010 loss: 0.9916 iter time (s): 4.791 samples/sec: 26.719
g0238:  iteration    11010/10000000 | consumed samples:      1409280 | consumed tokens:   2886205440 | elapsed time per iteration (ms): 45216.3 | learning rate: 1.924E-04 | global batch size:   128 | lm loss: 9.921285E-01 | loss scale: 524288.0 | grad norm: 0.325 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.831 | tokens per gpu per second (tgs): 181.173 | TFLOPs: 1.46 |
g0220: [2024-08-09 18:23:33,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=11020, skipped=10, lr=[0.0001925709824, 0.0001925709824], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11020 loss: 1.0111 iter time (s): 4.413 samples/sec: 29.007
g0238:  iteration    11020/10000000 | consumed samples:      1410560 | consumed tokens:   2888826880 | elapsed time per iteration (ms): 4445.8 | learning rate: 1.926E-04 | global batch size:   128 | lm loss: 9.887685E-01 | loss scale: 524288.0 | grad norm: 0.312 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.791 | tokens per gpu per second (tgs): 1842.621 | TFLOPs: 14.83 |
g0220: [2024-08-09 18:24:16,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=11030, skipped=10, lr=[0.00019274574506666667, 0.00019274574506666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11030 loss: 1.0001 iter time (s): 4.266 samples/sec: 30.002
g0238:  iteration    11030/10000000 | consumed samples:      1411840 | consumed tokens:   2891448320 | elapsed time per iteration (ms): 4299.6 | learning rate: 1.927E-04 | global batch size:   128 | lm loss: 9.958139E-01 | loss scale: 524288.0 | grad norm: 0.296 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.770 | tokens per gpu per second (tgs): 1905.305 | TFLOPs: 15.33 |
g0220: [2024-08-09 18:25:02,707] [INFO] [logging.py:96:log_dist] [Rank 0] step=11040, skipped=10, lr=[0.00019292050773333334, 0.00019292050773333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11040 loss: 0.9795 iter time (s): 4.572 samples/sec: 27.998
g0238:  iteration    11040/10000000 | consumed samples:      1413120 | consumed tokens:   2894069760 | elapsed time per iteration (ms): 4604.4 | learning rate: 1.929E-04 | global batch size:   128 | lm loss: 9.808488E-01 | loss scale: 524288.0 | grad norm: 0.347 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.800 | tokens per gpu per second (tgs): 1779.168 | TFLOPs: 14.32 |
g0220: [2024-08-09 18:25:46,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=11050, skipped=10, lr=[0.0001930952704, 0.0001930952704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11050 loss: 0.9811 iter time (s): 4.396 samples/sec: 29.116
g0238:  iteration    11050/10000000 | consumed samples:      1414400 | consumed tokens:   2896691200 | elapsed time per iteration (ms): 4428.8 | learning rate: 1.931E-04 | global batch size:   128 | lm loss: 9.896712E-01 | loss scale: 524288.0 | grad norm: 0.295 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.902 | tokens per gpu per second (tgs): 1849.726 | TFLOPs: 14.89 |
g0235: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 11053
g0235: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 11053
g0235: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 11053
g0237: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 11053
g0235: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 11053
g0238: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: Grad overflow on iteration 11053
g0234: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 11053
g0235: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: Grad overflow on iteration 11053
g0234: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 11053
g0234: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 11053
g0234: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 11053
g0234: Grad overflow on iteration 11053
g0237: Grad overflow on iteration 11053
g0238: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 11053
g0236: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 11053
g0236: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 11053
g0237: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 11053
g0237: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 11053
g0236: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 11053
g0233: Grad overflow on iteration 11053
g0234: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 11053
g0220: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: Grad overflow on iteration 11053
g0238: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: Grad overflow on iteration 11053
g0225: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 11053
g0225: Grad overflow on iteration 11053
g0233: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 11053
g0233: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 11053
g0225: Grad overflow on iteration 11053
g0220: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 11053
g0233: Grad overflow on iteration 11053
g0220: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 11053
g0225: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 11053
g0225: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 18:26:04,709] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 18:26:04,710] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 18:26:04,709] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0238: [2024-08-09 18:26:04,710] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 18:26:35,752] [INFO] [logging.py:96:log_dist] [Rank 0] step=11060, skipped=11, lr=[0.0001932700330666667, 0.0001932700330666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11060 loss: 0.9621 iter time (s): 4.843 samples/sec: 26.432
g0238:  iteration    11060/10000000 | consumed samples:      1415680 | consumed tokens:   2899312640 | elapsed time per iteration (ms): 4875.5 | learning rate: 1.933E-04 | global batch size:   128 | lm loss: 9.782687E-01 | loss scale: 262144.0 | grad norm: 0.302 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.254 | tokens per gpu per second (tgs): 1680.226 | TFLOPs: 13.52 |
g0220: [2024-08-09 18:27:24,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=11070, skipped=11, lr=[0.00019344479573333334, 0.00019344479573333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11070 loss: 0.9793 iter time (s): 4.806 samples/sec: 26.631
g0238:  iteration    11070/10000000 | consumed samples:      1416960 | consumed tokens:   2901934080 | elapsed time per iteration (ms): 4840.0 | learning rate: 1.934E-04 | global batch size:   128 | lm loss: 9.770423E-01 | loss scale: 262144.0 | grad norm: 0.304 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.446 | tokens per gpu per second (tgs): 1692.573 | TFLOPs: 13.62 |
g0220: [2024-08-09 18:28:10,274] [INFO] [logging.py:96:log_dist] [Rank 0] step=11080, skipped=11, lr=[0.00019361955840000003, 0.00019361955840000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11080 loss: 0.9862 iter time (s): 4.570 samples/sec: 28.007
g0238:  iteration    11080/10000000 | consumed samples:      1418240 | consumed tokens:   2904555520 | elapsed time per iteration (ms): 4614.5 | learning rate: 1.936E-04 | global batch size:   128 | lm loss: 9.760614E-01 | loss scale: 262144.0 | grad norm: 0.420 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.739 | tokens per gpu per second (tgs): 1775.273 | TFLOPs: 14.29 |
g0220: [2024-08-09 18:28:55,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=11090, skipped=11, lr=[0.00019379432106666667, 0.00019379432106666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11090 loss: 1.0019 iter time (s): 4.443 samples/sec: 28.812
g0238:  iteration    11090/10000000 | consumed samples:      1419520 | consumed tokens:   2907176960 | elapsed time per iteration (ms): 4493.1 | learning rate: 1.938E-04 | global batch size:   128 | lm loss: 9.833749E-01 | loss scale: 262144.0 | grad norm: 0.327 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.488 | tokens per gpu per second (tgs): 1823.239 | TFLOPs: 14.67 |
g0220: [2024-08-09 18:29:38,262] [INFO] [logging.py:96:log_dist] [Rank 0] step=11100, skipped=11, lr=[0.00019396908373333336, 0.00019396908373333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11100 loss: 1.0095 iter time (s): 4.270 samples/sec: 29.973
g0238:  iteration    11100/10000000 | consumed samples:      1420800 | consumed tokens:   2909798400 | elapsed time per iteration (ms): 4303.2 | learning rate: 1.940E-04 | global batch size:   128 | lm loss: 9.849794E-01 | loss scale: 262144.0 | grad norm: 0.323 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.745 | tokens per gpu per second (tgs): 1903.682 | TFLOPs: 15.32 |
g0220: [2024-08-09 18:30:19,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=11110, skipped=11, lr=[0.0001941438464, 0.0001941438464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11110 loss: 0.9847 iter time (s): 4.066 samples/sec: 31.484
g0238:  iteration    11110/10000000 | consumed samples:      1422080 | consumed tokens:   2912419840 | elapsed time per iteration (ms): 4097.9 | learning rate: 1.941E-04 | global batch size:   128 | lm loss: 9.892638E-01 | loss scale: 262144.0 | grad norm: 0.306 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.236 | tokens per gpu per second (tgs): 1999.083 | TFLOPs: 16.09 |
g0220: [2024-08-09 18:31:04,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=11120, skipped=11, lr=[0.0001943186090666667, 0.0001943186090666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11120 loss: 0.9728 iter time (s): 4.517 samples/sec: 28.336
g0238:  iteration    11120/10000000 | consumed samples:      1423360 | consumed tokens:   2915041280 | elapsed time per iteration (ms): 4550.0 | learning rate: 1.943E-04 | global batch size:   128 | lm loss: 9.893599E-01 | loss scale: 262144.0 | grad norm: 0.322 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.132 | tokens per gpu per second (tgs): 1800.450 | TFLOPs: 14.49 |
g0220: [2024-08-09 18:32:00,606] [INFO] [logging.py:96:log_dist] [Rank 0] step=11130, skipped=11, lr=[0.00019449337173333333, 0.00019449337173333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11130 loss: 0.9999 iter time (s): 5.554 samples/sec: 23.046
g0238:  iteration    11130/10000000 | consumed samples:      1424640 | consumed tokens:   2917662720 | elapsed time per iteration (ms): 5586.5 | learning rate: 1.945E-04 | global batch size:   128 | lm loss: 9.844515E-01 | loss scale: 262144.0 | grad norm: 0.318 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.912 | tokens per gpu per second (tgs): 1466.389 | TFLOPs: 11.80 |
g0220: [2024-08-09 18:32:46,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=11140, skipped=11, lr=[0.00019466813440000003, 0.00019466813440000003], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11140 loss: 0.9912 iter time (s): 4.589 samples/sec: 27.891
g0238:  iteration    11140/10000000 | consumed samples:      1425920 | consumed tokens:   2920284160 | elapsed time per iteration (ms): 4621.9 | learning rate: 1.947E-04 | global batch size:   128 | lm loss: 9.837826E-01 | loss scale: 262144.0 | grad norm: 0.363 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.694 | tokens per gpu per second (tgs): 1772.423 | TFLOPs: 14.26 |
g0220: [2024-08-09 18:33:30,232] [INFO] [logging.py:96:log_dist] [Rank 0] step=11150, skipped=11, lr=[0.00019484289706666667, 0.00019484289706666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11150 loss: 0.9810 iter time (s): 4.308 samples/sec: 29.711
g0238:  iteration    11150/10000000 | consumed samples:      1427200 | consumed tokens:   2922905600 | elapsed time per iteration (ms): 4340.7 | learning rate: 1.948E-04 | global batch size:   128 | lm loss: 9.768669E-01 | loss scale: 262144.0 | grad norm: 0.275 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.488 | tokens per gpu per second (tgs): 1887.262 | TFLOPs: 15.19 |
g0220: [2024-08-09 18:34:12,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=11160, skipped=11, lr=[0.00019501765973333336, 0.00019501765973333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11160 loss: 0.9758 iter time (s): 4.180 samples/sec: 30.623
g0238:  iteration    11160/10000000 | consumed samples:      1428480 | consumed tokens:   2925527040 | elapsed time per iteration (ms): 4212.3 | learning rate: 1.950E-04 | global batch size:   128 | lm loss: 9.896982E-01 | loss scale: 262144.0 | grad norm: 0.279 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.388 | tokens per gpu per second (tgs): 1944.803 | TFLOPs: 15.65 |
g0220: [2024-08-09 18:34:54,313] [INFO] [logging.py:96:log_dist] [Rank 0] step=11170, skipped=11, lr=[0.0001951924224, 0.0001951924224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11170 loss: 0.9559 iter time (s): 4.163 samples/sec: 30.746
g0238:  iteration    11170/10000000 | consumed samples:      1429760 | consumed tokens:   2928148480 | elapsed time per iteration (ms): 4195.7 | learning rate: 1.952E-04 | global batch size:   128 | lm loss: 9.777441E-01 | loss scale: 262144.0 | grad norm: 0.339 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.507 | tokens per gpu per second (tgs): 1952.462 | TFLOPs: 15.71 |
g0220: [2024-08-09 18:35:37,699] [INFO] [logging.py:96:log_dist] [Rank 0] step=11180, skipped=11, lr=[0.0001953671850666667, 0.0001953671850666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11180 loss: 0.9664 iter time (s): 4.306 samples/sec: 29.726
g0238:  iteration    11180/10000000 | consumed samples:      1431040 | consumed tokens:   2930769920 | elapsed time per iteration (ms): 4338.7 | learning rate: 1.954E-04 | global batch size:   128 | lm loss: 9.858348E-01 | loss scale: 262144.0 | grad norm: 0.311 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.502 | tokens per gpu per second (tgs): 1888.118 | TFLOPs: 15.19 |
g0220: [2024-08-09 18:36:26,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=11190, skipped=11, lr=[0.00019554194773333333, 0.00019554194773333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11190 loss: 0.9705 iter time (s): 4.830 samples/sec: 26.503
g0238:  iteration    11190/10000000 | consumed samples:      1432320 | consumed tokens:   2933391360 | elapsed time per iteration (ms): 4862.1 | learning rate: 1.955E-04 | global batch size:   128 | lm loss: 9.833879E-01 | loss scale: 262144.0 | grad norm: 0.316 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.326 | tokens per gpu per second (tgs): 1684.860 | TFLOPs: 13.56 |
g0220: [2024-08-09 18:37:24,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=11200, skipped=11, lr=[0.00019571671040000002, 0.00019571671040000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11200 loss: 0.9687 iter time (s): 5.755 samples/sec: 22.243
g0238:  iteration    11200/10000000 | consumed samples:      1433600 | consumed tokens:   2936012800 | elapsed time per iteration (ms): 5787.1 | learning rate: 1.957E-04 | global batch size:   128 | lm loss: 9.774941E-01 | loss scale: 262144.0 | grad norm: 0.294 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.118 | tokens per gpu per second (tgs): 1415.561 | TFLOPs: 11.39 |
g0220: [2024-08-09 18:38:19,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=11210, skipped=11, lr=[0.00019589147306666666, 0.00019589147306666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11210 loss: 0.9579 iter time (s): 5.482 samples/sec: 23.350
g0238:  iteration    11210/10000000 | consumed samples:      1434880 | consumed tokens:   2938634240 | elapsed time per iteration (ms): 5514.6 | learning rate: 1.959E-04 | global batch size:   128 | lm loss: 9.636602E-01 | loss scale: 262144.0 | grad norm: 0.314 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 23.211 | tokens per gpu per second (tgs): 1485.516 | TFLOPs: 11.95 |
g0220: [2024-08-09 18:39:02,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=11220, skipped=11, lr=[0.00019606623573333336, 0.00019606623573333336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11220 loss: 0.9888 iter time (s): 4.331 samples/sec: 29.555
g0238:  iteration    11220/10000000 | consumed samples:      1436160 | consumed tokens:   2941255680 | elapsed time per iteration (ms): 4363.9 | learning rate: 1.961E-04 | global batch size:   128 | lm loss: 9.802577E-01 | loss scale: 262144.0 | grad norm: 0.326 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.331 | tokens per gpu per second (tgs): 1877.209 | TFLOPs: 15.11 |
g0220: [2024-08-09 18:39:52,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=11230, skipped=11, lr=[0.0001962409984, 0.0001962409984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11230 loss: 0.9770 iter time (s): 4.905 samples/sec: 26.095
g0238:  iteration    11230/10000000 | consumed samples:      1437440 | consumed tokens:   2943877120 | elapsed time per iteration (ms): 4937.7 | learning rate: 1.962E-04 | global batch size:   128 | lm loss: 9.757430E-01 | loss scale: 262144.0 | grad norm: 0.306 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.923 | tokens per gpu per second (tgs): 1659.077 | TFLOPs: 13.35 |
g0220: [2024-08-09 18:40:36,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=11240, skipped=11, lr=[0.0001964157610666667, 0.0001964157610666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11240 loss: 1.0023 iter time (s): 4.415 samples/sec: 28.993
g0238:  iteration    11240/10000000 | consumed samples:      1438720 | consumed tokens:   2946498560 | elapsed time per iteration (ms): 4448.4 | learning rate: 1.964E-04 | global batch size:   128 | lm loss: 9.819901E-01 | loss scale: 262144.0 | grad norm: 0.408 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.774 | tokens per gpu per second (tgs): 1841.555 | TFLOPs: 14.82 |
g0220: [2024-08-09 18:41:35,890] [INFO] [logging.py:96:log_dist] [Rank 0] step=11250, skipped=11, lr=[0.00019659052373333333, 0.00019659052373333333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11250 loss: 0.9568 iter time (s): 5.873 samples/sec: 21.796
g0238:  iteration    11250/10000000 | consumed samples:      1440000 | consumed tokens:   2949120000 | elapsed time per iteration (ms): 5905.5 | learning rate: 1.966E-04 | global batch size:   128 | lm loss: 9.740810E-01 | loss scale: 262144.0 | grad norm: 0.373 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 21.675 | tokens per gpu per second (tgs): 1387.186 | TFLOPs: 11.16 |
g0220: [2024-08-09 18:43:12,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=11260, skipped=11, lr=[0.00019676528640000002, 0.00019676528640000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11260 loss: 0.9696 iter time (s): 9.636 samples/sec: 13.284
g0238:  iteration    11260/10000000 | consumed samples:      1441280 | consumed tokens:   2951741440 | elapsed time per iteration (ms): 9670.1 | learning rate: 1.968E-04 | global batch size:   128 | lm loss: 9.756851E-01 | loss scale: 262144.0 | grad norm: 0.290 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 13.237 | tokens per gpu per second (tgs): 847.148 | TFLOPs: 6.82 |
g0220: [2024-08-09 18:43:56,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=11270, skipped=11, lr=[0.00019694004906666666, 0.00019694004906666666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11270 loss: 1.0254 iter time (s): 4.408 samples/sec: 29.041
g0238:  iteration    11270/10000000 | consumed samples:      1442560 | consumed tokens:   2954362880 | elapsed time per iteration (ms): 4440.0 | learning rate: 1.969E-04 | global batch size:   128 | lm loss: 9.870100E-01 | loss scale: 262144.0 | grad norm: 0.299 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.829 | tokens per gpu per second (tgs): 1845.032 | TFLOPs: 14.85 |
g0220: [2024-08-09 18:44:42,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=11280, skipped=11, lr=[0.00019711481173333335, 0.00019711481173333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11280 loss: 0.9710 iter time (s): 4.484 samples/sec: 28.543
g0238:  iteration    11280/10000000 | consumed samples:      1443840 | consumed tokens:   2956984320 | elapsed time per iteration (ms): 4516.8 | learning rate: 1.971E-04 | global batch size:   128 | lm loss: 9.757594E-01 | loss scale: 262144.0 | grad norm: 0.325 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.339 | tokens per gpu per second (tgs): 1813.673 | TFLOPs: 14.59 |
g0220: [2024-08-09 18:45:31,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=11290, skipped=11, lr=[0.0001972895744, 0.0001972895744], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11290 loss: 0.9735 iter time (s): 4.883 samples/sec: 26.211
g0238:  iteration    11290/10000000 | consumed samples:      1445120 | consumed tokens:   2959605760 | elapsed time per iteration (ms): 4916.5 | learning rate: 1.973E-04 | global batch size:   128 | lm loss: 9.795181E-01 | loss scale: 262144.0 | grad norm: 0.297 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.035 | tokens per gpu per second (tgs): 1666.223 | TFLOPs: 13.41 |
g0220: [2024-08-09 18:46:19,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=11300, skipped=11, lr=[0.00019746433706666668, 0.00019746433706666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11300 loss: 0.9801 iter time (s): 4.739 samples/sec: 27.009
g0238:  iteration    11300/10000000 | consumed samples:      1446400 | consumed tokens:   2962227200 | elapsed time per iteration (ms): 4771.9 | learning rate: 1.975E-04 | global batch size:   128 | lm loss: 9.734251E-01 | loss scale: 262144.0 | grad norm: 0.341 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.824 | tokens per gpu per second (tgs): 1716.723 | TFLOPs: 13.81 |
g0220: [2024-08-09 18:47:10,133] [INFO] [logging.py:96:log_dist] [Rank 0] step=11310, skipped=11, lr=[0.00019763909973333332, 0.00019763909973333332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11310 loss: 0.9569 iter time (s): 5.076 samples/sec: 25.218
g0238:  iteration    11310/10000000 | consumed samples:      1447680 | consumed tokens:   2964848640 | elapsed time per iteration (ms): 5108.5 | learning rate: 1.976E-04 | global batch size:   128 | lm loss: 9.766674E-01 | loss scale: 262144.0 | grad norm: 0.345 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.056 | tokens per gpu per second (tgs): 1603.589 | TFLOPs: 12.90 |
g0220: [2024-08-09 18:47:59,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=11320, skipped=11, lr=[0.00019781386240000002, 0.00019781386240000002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11320 loss: 0.9937 iter time (s): 4.911 samples/sec: 26.065
g0238:  iteration    11320/10000000 | consumed samples:      1448960 | consumed tokens:   2967470080 | elapsed time per iteration (ms): 4944.4 | learning rate: 1.978E-04 | global batch size:   128 | lm loss: 9.873377E-01 | loss scale: 262144.0 | grad norm: 0.285 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 25.888 | tokens per gpu per second (tgs): 1656.830 | TFLOPs: 13.33 |
g0220: [2024-08-09 18:48:47,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=11330, skipped=11, lr=[0.00019798862506666668, 0.00019798862506666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11330 loss: 0.9830 iter time (s): 4.745 samples/sec: 26.975
g0238:  iteration    11330/10000000 | consumed samples:      1450240 | consumed tokens:   2970091520 | elapsed time per iteration (ms): 4778.2 | learning rate: 1.980E-04 | global batch size:   128 | lm loss: 9.839334E-01 | loss scale: 262144.0 | grad norm: 0.350 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.788 | tokens per gpu per second (tgs): 1714.448 | TFLOPs: 13.80 |
g0220: [2024-08-09 18:49:32,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=11340, skipped=11, lr=[0.00019816338773333335, 0.00019816338773333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11340 loss: 1.0254 iter time (s): 4.482 samples/sec: 28.556
g0238:  iteration    11340/10000000 | consumed samples:      1451520 | consumed tokens:   2972712960 | elapsed time per iteration (ms): 4515.2 | learning rate: 1.982E-04 | global batch size:   128 | lm loss: 9.863394E-01 | loss scale: 262144.0 | grad norm: 0.356 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.348 | tokens per gpu per second (tgs): 1814.297 | TFLOPs: 14.60 |
g0220: [2024-08-09 18:50:21,086] [INFO] [logging.py:96:log_dist] [Rank 0] step=11350, skipped=11, lr=[0.00019833815040000001, 0.00019833815040000001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11350 loss: 0.9724 iter time (s): 4.824 samples/sec: 26.533
g0238:  iteration    11350/10000000 | consumed samples:      1452800 | consumed tokens:   2975334400 | elapsed time per iteration (ms): 4857.5 | learning rate: 1.983E-04 | global batch size:   128 | lm loss: 9.794449E-01 | loss scale: 262144.0 | grad norm: 0.319 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.351 | tokens per gpu per second (tgs): 1686.481 | TFLOPs: 13.57 |
g0220: [2024-08-09 18:51:05,235] [INFO] [logging.py:96:log_dist] [Rank 0] step=11360, skipped=11, lr=[0.00019851291306666668, 0.00019851291306666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11360 loss: 0.9803 iter time (s): 4.382 samples/sec: 29.211
g0238:  iteration    11360/10000000 | consumed samples:      1454080 | consumed tokens:   2977955840 | elapsed time per iteration (ms): 4414.8 | learning rate: 1.985E-04 | global batch size:   128 | lm loss: 9.709655E-01 | loss scale: 262144.0 | grad norm: 0.291 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.993 | tokens per gpu per second (tgs): 1855.557 | TFLOPs: 14.93 |
g0220: [2024-08-09 18:51:48,575] [INFO] [logging.py:96:log_dist] [Rank 0] step=11370, skipped=11, lr=[0.00019868767573333335, 0.00019868767573333335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11370 loss: 0.9536 iter time (s): 4.301 samples/sec: 29.763
g0238:  iteration    11370/10000000 | consumed samples:      1455360 | consumed tokens:   2980577280 | elapsed time per iteration (ms): 4333.9 | learning rate: 1.987E-04 | global batch size:   128 | lm loss: 9.826707E-01 | loss scale: 262144.0 | grad norm: 0.276 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.534 | tokens per gpu per second (tgs): 1890.204 | TFLOPs: 15.21 |
g0220: [2024-08-09 18:52:28,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=11380, skipped=11, lr=[0.0001988624384, 0.0001988624384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11380 loss: 0.9871 iter time (s): 3.995 samples/sec: 32.037
g0238:  iteration    11380/10000000 | consumed samples:      1456640 | consumed tokens:   2983198720 | elapsed time per iteration (ms): 4028.0 | learning rate: 1.989E-04 | global batch size:   128 | lm loss: 9.677977E-01 | loss scale: 262144.0 | grad norm: 0.377 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.777 | tokens per gpu per second (tgs): 2033.750 | TFLOPs: 16.37 |
g0220: [2024-08-09 18:53:08,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=11390, skipped=11, lr=[0.00019903720106666668, 0.00019903720106666668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11390 loss: 0.9737 iter time (s): 3.922 samples/sec: 32.640
g0238:  iteration    11390/10000000 | consumed samples:      1457920 | consumed tokens:   2985820160 | elapsed time per iteration (ms): 3954.0 | learning rate: 1.990E-04 | global batch size:   128 | lm loss: 9.698313E-01 | loss scale: 262144.0 | grad norm: 0.436 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.372 | tokens per gpu per second (tgs): 2071.804 | TFLOPs: 16.67 |
g0220: [2024-08-09 18:53:54,418] [INFO] [logging.py:96:log_dist] [Rank 0] step=11400, skipped=11, lr=[0.00019921196373333334, 0.00019921196373333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11400 loss: 0.9485 iter time (s): 4.569 samples/sec: 28.014
g0238:  iteration    11400/10000000 | consumed samples:      1459200 | consumed tokens:   2988441600 | elapsed time per iteration (ms): 4602.4 | learning rate: 1.992E-04 | global batch size:   128 | lm loss: 9.621598E-01 | loss scale: 262144.0 | grad norm: 0.255 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.812 | tokens per gpu per second (tgs): 1779.949 | TFLOPs: 14.32 |
g0220: [2024-08-09 18:54:41,339] [INFO] [logging.py:96:log_dist] [Rank 0] step=11410, skipped=11, lr=[0.0001993867264, 0.0001993867264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11410 loss: 1.0044 iter time (s): 4.659 samples/sec: 27.475
g0238:  iteration    11410/10000000 | consumed samples:      1460480 | consumed tokens:   2991063040 | elapsed time per iteration (ms): 4692.0 | learning rate: 1.994E-04 | global batch size:   128 | lm loss: 9.806566E-01 | loss scale: 262144.0 | grad norm: 0.350 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.280 | tokens per gpu per second (tgs): 1745.935 | TFLOPs: 14.05 |
g0220: [2024-08-09 18:55:22,907] [INFO] [logging.py:96:log_dist] [Rank 0] step=11420, skipped=11, lr=[0.0001995614890666667, 0.0001995614890666667], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11420 loss: 0.9969 iter time (s): 4.124 samples/sec: 31.041
g0238:  iteration    11420/10000000 | consumed samples:      1461760 | consumed tokens:   2993684480 | elapsed time per iteration (ms): 4156.7 | learning rate: 1.996E-04 | global batch size:   128 | lm loss: 9.801044E-01 | loss scale: 262144.0 | grad norm: 0.322 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.794 | tokens per gpu per second (tgs): 1970.798 | TFLOPs: 15.86 |
g0220: [2024-08-09 18:56:03,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=11430, skipped=11, lr=[0.00019973625173333334, 0.00019973625173333334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11430 loss: 0.9654 iter time (s): 4.071 samples/sec: 31.445
g0238:  iteration    11430/10000000 | consumed samples:      1463040 | consumed tokens:   2996305920 | elapsed time per iteration (ms): 4103.3 | learning rate: 1.997E-04 | global batch size:   128 | lm loss: 9.701733E-01 | loss scale: 262144.0 | grad norm: 0.429 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.194 | tokens per gpu per second (tgs): 1996.446 | TFLOPs: 16.07 |
g0220: [2024-08-09 18:56:47,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=11440, skipped=11, lr=[0.00019991101440000004, 0.00019991101440000004], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11440 loss: 1.0009 iter time (s): 4.313 samples/sec: 29.677
g0238:  iteration    11440/10000000 | consumed samples:      1464320 | consumed tokens:   2998927360 | elapsed time per iteration (ms): 4345.8 | learning rate: 1.999E-04 | global batch size:   128 | lm loss: 9.762933E-01 | loss scale: 262144.0 | grad norm: 0.288 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.454 | tokens per gpu per second (tgs): 1885.039 | TFLOPs: 15.17 |
g0220: [2024-08-09 18:57:29,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=11450, skipped=11, lr=[0.00019999999999120157, 0.00019999999999120157], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11450 loss: 0.9910 iter time (s): 4.160 samples/sec: 30.771
g0238:  iteration    11450/10000000 | consumed samples:      1465600 | consumed tokens:   3001548800 | elapsed time per iteration (ms): 4192.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.689093E-01 | loss scale: 262144.0 | grad norm: 0.282 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.530 | tokens per gpu per second (tgs): 1953.949 | TFLOPs: 15.72 |
g0220: [2024-08-09 18:58:17,072] [INFO] [logging.py:96:log_dist] [Rank 0] step=11460, skipped=11, lr=[0.00019999999991882715, 0.00019999999991882715], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11460 loss: 1.0000 iter time (s): 4.199 samples/sec: 30.484
g0238:  iteration    11460/10000000 | consumed samples:      1466880 | consumed tokens:   3004170240 | elapsed time per iteration (ms): 4774.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.741086E-01 | loss scale: 262144.0 | grad norm: 0.316 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 26.807 | tokens per gpu per second (tgs): 1715.661 | TFLOPs: 13.81 |
g0220: [2024-08-09 18:58:56,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=11470, skipped=11, lr=[0.00019999999977340775, 0.00019999999977340775], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11470 loss: 0.9779 iter time (s): 3.921 samples/sec: 32.647
g0238:  iteration    11470/10000000 | consumed samples:      1468160 | consumed tokens:   3006791680 | elapsed time per iteration (ms): 3953.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.730104E-01 | loss scale: 262144.0 | grad norm: 0.337 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.378 | tokens per gpu per second (tgs): 2072.218 | TFLOPs: 16.68 |
g0220: [2024-08-09 18:59:37,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=11480, skipped=11, lr=[0.00019999999955494337, 0.00019999999955494337], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11480 loss: 0.9602 iter time (s): 4.043 samples/sec: 31.661
g0238:  iteration    11480/10000000 | consumed samples:      1469440 | consumed tokens:   3009413120 | elapsed time per iteration (ms): 4076.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.602035E-01 | loss scale: 262144.0 | grad norm: 0.300 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.400 | tokens per gpu per second (tgs): 2009.614 | TFLOPs: 16.17 |
g0220: [2024-08-09 19:00:23,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=11490, skipped=11, lr=[0.00019999999926343404, 0.00019999999926343404], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11490 loss: 0.9785 iter time (s): 4.614 samples/sec: 27.739
g0238:  iteration    11490/10000000 | consumed samples:      1470720 | consumed tokens:   3012034560 | elapsed time per iteration (ms): 4647.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.617026E-01 | loss scale: 262144.0 | grad norm: 0.514 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.543 | tokens per gpu per second (tgs): 1762.722 | TFLOPs: 14.18 |
g0220: [2024-08-09 19:01:05,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=11500, skipped=11, lr=[0.00019999999889887973, 0.00019999999889887973], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11500 loss: 0.9659 iter time (s): 4.099 samples/sec: 31.226
g0238:  iteration    11500/10000000 | consumed samples:      1472000 | consumed tokens:   3014656000 | elapsed time per iteration (ms): 4144.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.711695E-01 | loss scale: 262144.0 | grad norm: 0.410 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.882 | tokens per gpu per second (tgs): 1976.472 | TFLOPs: 15.91 |
g0220: [2024-08-09 19:01:56,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=11510, skipped=11, lr=[0.00019999999846128045, 0.00019999999846128045], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11510 loss: 0.9982 iter time (s): 4.544 samples/sec: 28.170
g0238:  iteration    11510/10000000 | consumed samples:      1473280 | consumed tokens:   3017277440 | elapsed time per iteration (ms): 5254.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.708927E-01 | loss scale: 262144.0 | grad norm: 0.264 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.359 | tokens per gpu per second (tgs): 1558.945 | TFLOPs: 12.55 |
g0220: [2024-08-09 19:02:44,311] [INFO] [logging.py:96:log_dist] [Rank 0] step=11520, skipped=11, lr=[0.0001999999979506362, 0.0001999999979506362], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11520 loss: 0.9452 iter time (s): 4.277 samples/sec: 29.928
g0238:  iteration    11520/10000000 | consumed samples:      1474560 | consumed tokens:   3019898880 | elapsed time per iteration (ms): 4647.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.594272E-01 | loss scale: 262144.0 | grad norm: 0.336 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.543 | tokens per gpu per second (tgs): 1762.779 | TFLOPs: 14.19 |
g0220: [2024-08-09 19:03:27,913] [INFO] [logging.py:96:log_dist] [Rank 0] step=11530, skipped=11, lr=[0.00019999999736694703, 0.00019999999736694703], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11530 loss: 0.9658 iter time (s): 4.327 samples/sec: 29.579
g0238:  iteration    11530/10000000 | consumed samples:      1475840 | consumed tokens:   3022520320 | elapsed time per iteration (ms): 4360.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.681344E-01 | loss scale: 262144.0 | grad norm: 0.287 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.357 | tokens per gpu per second (tgs): 1878.855 | TFLOPs: 15.12 |
g0220: [2024-08-09 19:04:13,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=11540, skipped=11, lr=[0.00019999999671021284, 0.00019999999671021284], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11540 loss: 0.9846 iter time (s): 4.547 samples/sec: 28.148
g0238:  iteration    11540/10000000 | consumed samples:      1477120 | consumed tokens:   3025141760 | elapsed time per iteration (ms): 4591.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.793673E-01 | loss scale: 262144.0 | grad norm: 0.414 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.876 | tokens per gpu per second (tgs): 1784.063 | TFLOPs: 14.36 |
g0220: [2024-08-09 19:04:58,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=11550, skipped=11, lr=[0.00019999999598043375, 0.00019999999598043375], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11550 loss: 0.9644 iter time (s): 4.467 samples/sec: 28.654
g0238:  iteration    11550/10000000 | consumed samples:      1478400 | consumed tokens:   3027763200 | elapsed time per iteration (ms): 4499.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.699929E-01 | loss scale: 262144.0 | grad norm: 0.362 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.446 | tokens per gpu per second (tgs): 1820.524 | TFLOPs: 14.65 |
g0220: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:05:20,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 19:05:20,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 19:05:20,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 19:05:20,285] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 19:05:20,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 19:05:20,286] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 19:05:41,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=11560, skipped=11, lr=[0.00019999999517760964, 0.00019999999517760964], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11560 loss: 0.9546 iter time (s): 4.209 samples/sec: 30.413
g0238:  iteration    11560/10000000 | consumed samples:      1479680 | consumed tokens:   3030384640 | elapsed time per iteration (ms): 4241.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.757173E-01 | loss scale: 524288.0 | grad norm: 0.312 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.181 | tokens per gpu per second (tgs): 1931.554 | TFLOPs: 15.54 |
g0235: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 11566
g0235: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 11566
g0235: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 11566
g0233: Grad overflow on iteration 11566
g0234: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 11566
g0233: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: Grad overflow on iteration 11566
g0233: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 11566
g0237: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 11566
g0233: Grad overflow on iteration 11566
g0237: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 11566
g0220: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 11566
g0235: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 11566
g0234: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: Grad overflow on iteration 11566
g0235: Grad overflow on iteration 11566
g0225: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 11566
g0236: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 11566
g0234: Grad overflow on iteration 11566
g0225: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 11566
g0220: Grad overflow on iteration 11566
g0238: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: Grad overflow on iteration 11566
g0225: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: Grad overflow on iteration 11566
g0225: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 11566
g0235: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: Grad overflow on iteration 11566
g0220: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 11566
g0234: Grad overflow on iteration 11566
g0220: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: Grad overflow on iteration 11566
g0236: Grad overflow on iteration 11566
g0238: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 11566
g0225: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: Grad overflow on iteration 11566
g0236: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: Grad overflow on iteration 11566
g0238: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 11566
g0238: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: Grad overflow on iteration 11566
g0220: [2024-08-09 19:06:09,991] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 19:06:09,990] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 19:06:09,991] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 19:06:09,991] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 19:06:09,991] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0236: [2024-08-09 19:06:09,991] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 19:06:23,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=11570, skipped=12, lr=[0.00019999999430174057, 0.00019999999430174057], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11570 loss: 0.9796 iter time (s): 4.155 samples/sec: 30.806
g0238:  iteration    11570/10000000 | consumed samples:      1480960 | consumed tokens:   3033006080 | elapsed time per iteration (ms): 4187.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.807467E-01 | loss scale: 262144.0 | grad norm: 0.349 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.565 | tokens per gpu per second (tgs): 1956.162 | TFLOPs: 15.74 |
g0220: [2024-08-09 19:07:05,784] [INFO] [logging.py:96:log_dist] [Rank 0] step=11580, skipped=12, lr=[0.00019999999335282656, 0.00019999999335282656], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11580 loss: 1.0096 iter time (s): 4.234 samples/sec: 30.232
g0238:  iteration    11580/10000000 | consumed samples:      1482240 | consumed tokens:   3035627520 | elapsed time per iteration (ms): 4266.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.833500E-01 | loss scale: 262144.0 | grad norm: 0.291 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.001 | tokens per gpu per second (tgs): 1920.064 | TFLOPs: 15.45 |
g0220: [2024-08-09 19:08:02,388] [INFO] [logging.py:96:log_dist] [Rank 0] step=11590, skipped=12, lr=[0.00019999999233086756, 0.00019999999233086756], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11590 loss: 0.9471 iter time (s): 4.933 samples/sec: 25.945
g0238:  iteration    11590/10000000 | consumed samples:      1483520 | consumed tokens:   3038248960 | elapsed time per iteration (ms): 5660.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.648973E-01 | loss scale: 262144.0 | grad norm: 0.346 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 22.613 | tokens per gpu per second (tgs): 1447.214 | TFLOPs: 11.65 |
g0220: [2024-08-09 19:08:54,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=11600, skipped=12, lr=[0.00019999999123586365, 0.00019999999123586365], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11600 loss: 0.9789 iter time (s): 4.773 samples/sec: 26.816
g0238:  iteration    11600/10000000 | consumed samples:      1484800 | consumed tokens:   3040870400 | elapsed time per iteration (ms): 5235.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.745187E-01 | loss scale: 262144.0 | grad norm: 0.324 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.450 | tokens per gpu per second (tgs): 1564.788 | TFLOPs: 12.59 |
g0220: [2024-08-09 19:09:36,190] [INFO] [logging.py:96:log_dist] [Rank 0] step=11610, skipped=12, lr=[0.00019999999006781475, 0.00019999999006781475], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11610 loss: 0.9699 iter time (s): 4.112 samples/sec: 31.128
g0238:  iteration    11610/10000000 | consumed samples:      1486080 | consumed tokens:   3043491840 | elapsed time per iteration (ms): 4145.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.856845E-01 | loss scale: 262144.0 | grad norm: 0.359 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.880 | tokens per gpu per second (tgs): 1976.350 | TFLOPs: 15.90 |
g0220: [2024-08-09 19:10:20,792] [INFO] [logging.py:96:log_dist] [Rank 0] step=11620, skipped=12, lr=[0.0001999999888267209, 0.0001999999888267209], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11620 loss: 0.9885 iter time (s): 4.427 samples/sec: 28.911
g0238:  iteration    11620/10000000 | consumed samples:      1487360 | consumed tokens:   3046113280 | elapsed time per iteration (ms): 4460.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.796274E-01 | loss scale: 262144.0 | grad norm: 0.348 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.699 | tokens per gpu per second (tgs): 1836.758 | TFLOPs: 14.78 |
g0220: [2024-08-09 19:11:04,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=11630, skipped=12, lr=[0.00019999998751258206, 0.00019999998751258206], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11630 loss: 0.9617 iter time (s): 4.349 samples/sec: 29.431
g0238:  iteration    11630/10000000 | consumed samples:      1488640 | consumed tokens:   3048734720 | elapsed time per iteration (ms): 4382.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.680861E-01 | loss scale: 262144.0 | grad norm: 0.316 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.211 | tokens per gpu per second (tgs): 1869.480 | TFLOPs: 15.04 |
g0220: [2024-08-09 19:11:48,517] [INFO] [logging.py:96:log_dist] [Rank 0] step=11640, skipped=12, lr=[0.0001999999861253983, 0.0001999999861253983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11640 loss: 0.9389 iter time (s): 4.358 samples/sec: 29.372
g0238:  iteration    11640/10000000 | consumed samples:      1489920 | consumed tokens:   3051356160 | elapsed time per iteration (ms): 4390.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.604391E-01 | loss scale: 262144.0 | grad norm: 0.283 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.154 | tokens per gpu per second (tgs): 1865.848 | TFLOPs: 15.01 |
g0220: [2024-08-09 19:12:32,457] [INFO] [logging.py:96:log_dist] [Rank 0] step=11650, skipped=12, lr=[0.00019999998466516957, 0.00019999998466516957], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11650 loss: 0.9748 iter time (s): 4.361 samples/sec: 29.348
g0238:  iteration    11650/10000000 | consumed samples:      1491200 | consumed tokens:   3053977600 | elapsed time per iteration (ms): 4394.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.744051E-01 | loss scale: 262144.0 | grad norm: 0.308 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.130 | tokens per gpu per second (tgs): 1864.338 | TFLOPs: 15.00 |
g0220: [2024-08-09 19:13:16,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=11660, skipped=12, lr=[0.00019999998313189587, 0.00019999998313189587], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11660 loss: 0.9589 iter time (s): 4.413 samples/sec: 29.006
g0238:  iteration    11660/10000000 | consumed samples:      1492480 | consumed tokens:   3056599040 | elapsed time per iteration (ms): 4445.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.766266E-01 | loss scale: 262144.0 | grad norm: 0.319 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.794 | tokens per gpu per second (tgs): 1842.792 | TFLOPs: 14.83 |
g0220: [2024-08-09 19:14:01,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=11670, skipped=12, lr=[0.00019999998152557723, 0.00019999998152557723], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11670 loss: 0.9761 iter time (s): 4.382 samples/sec: 29.213
g0238:  iteration    11670/10000000 | consumed samples:      1493760 | consumed tokens:   3059220480 | elapsed time per iteration (ms): 4415.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.699304E-01 | loss scale: 262144.0 | grad norm: 0.287 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.992 | tokens per gpu per second (tgs): 1855.480 | TFLOPs: 14.93 |
g0220: [2024-08-09 19:14:48,110] [INFO] [logging.py:96:log_dist] [Rank 0] step=11680, skipped=12, lr=[0.00019999997984621363, 0.00019999997984621363], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11680 loss: 0.9719 iter time (s): 4.672 samples/sec: 27.396
g0238:  iteration    11680/10000000 | consumed samples:      1495040 | consumed tokens:   3061841920 | elapsed time per iteration (ms): 4704.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.550860E-01 | loss scale: 262144.0 | grad norm: 0.306 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.206 | tokens per gpu per second (tgs): 1741.200 | TFLOPs: 14.01 |
g0220: [2024-08-09 19:15:32,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=11690, skipped=12, lr=[0.0001999999780938051, 0.0001999999780938051], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11690 loss: 0.9353 iter time (s): 4.448 samples/sec: 28.774
g0238:  iteration    11690/10000000 | consumed samples:      1496320 | consumed tokens:   3064463360 | elapsed time per iteration (ms): 4481.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.649673E-01 | loss scale: 262144.0 | grad norm: 0.287 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.561 | tokens per gpu per second (tgs): 1827.886 | TFLOPs: 14.71 |
g0220: [2024-08-09 19:16:16,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=11700, skipped=12, lr=[0.00019999997626835158, 0.00019999997626835158], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11700 loss: 0.9936 iter time (s): 4.348 samples/sec: 29.442
g0238:  iteration    11700/10000000 | consumed samples:      1497600 | consumed tokens:   3067084800 | elapsed time per iteration (ms): 4380.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.770306E-01 | loss scale: 262144.0 | grad norm: 0.317 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.220 | tokens per gpu per second (tgs): 1870.106 | TFLOPs: 15.05 |
g0220: [2024-08-09 19:17:00,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=11710, skipped=12, lr=[0.0001999999743698532, 0.0001999999743698532], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11710 loss: 0.8849 iter time (s): 4.325 samples/sec: 29.598
g0238:  iteration    11710/10000000 | consumed samples:      1498880 | consumed tokens:   3069706240 | elapsed time per iteration (ms): 4357.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.558350E-01 | loss scale: 262144.0 | grad norm: 0.371 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.374 | tokens per gpu per second (tgs): 1879.949 | TFLOPs: 15.13 |
g0220: [2024-08-09 19:17:42,053] [INFO] [logging.py:96:log_dist] [Rank 0] step=11720, skipped=12, lr=[0.00019999997239830976, 0.00019999997239830976], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11720 loss: 0.9554 iter time (s): 4.142 samples/sec: 30.902
g0238:  iteration    11720/10000000 | consumed samples:      1500160 | consumed tokens:   3072327680 | elapsed time per iteration (ms): 4174.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.638595E-01 | loss scale: 262144.0 | grad norm: 0.291 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.662 | tokens per gpu per second (tgs): 1962.391 | TFLOPs: 15.79 |
g0220: [2024-08-09 19:18:22,649] [INFO] [logging.py:96:log_dist] [Rank 0] step=11730, skipped=12, lr=[0.00019999997035372144, 0.00019999997035372144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11730 loss: 0.9845 iter time (s): 4.026 samples/sec: 31.790
g0238:  iteration    11730/10000000 | consumed samples:      1501440 | consumed tokens:   3074949120 | elapsed time per iteration (ms): 4059.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.744518E-01 | loss scale: 262144.0 | grad norm: 0.288 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.530 | tokens per gpu per second (tgs): 2017.951 | TFLOPs: 16.24 |
g0220: [2024-08-09 19:19:03,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=11740, skipped=12, lr=[0.00019999996823608817, 0.00019999996823608817], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11740 loss: 0.9790 iter time (s): 4.093 samples/sec: 31.277
g0238:  iteration    11740/10000000 | consumed samples:      1502720 | consumed tokens:   3077570560 | elapsed time per iteration (ms): 4124.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.575364E-01 | loss scale: 262144.0 | grad norm: 0.258 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.031 | tokens per gpu per second (tgs): 1985.982 | TFLOPs: 15.98 |
g0220: [2024-08-09 19:19:47,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=11750, skipped=12, lr=[0.00019999996604540995, 0.00019999996604540995], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11750 loss: 0.9647 iter time (s): 4.339 samples/sec: 29.501
g0238:  iteration    11750/10000000 | consumed samples:      1504000 | consumed tokens:   3080192000 | elapsed time per iteration (ms): 4372.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.629912E-01 | loss scale: 262144.0 | grad norm: 0.272 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.274 | tokens per gpu per second (tgs): 1873.525 | TFLOPs: 15.08 |
g0220: [2024-08-09 19:20:40,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=11760, skipped=12, lr=[0.00019999996378168678, 0.00019999996378168678], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11760 loss: 0.9859 iter time (s): 4.532 samples/sec: 28.244
g0238:  iteration    11760/10000000 | consumed samples:      1505280 | consumed tokens:   3082813440 | elapsed time per iteration (ms): 5314.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.689839E-01 | loss scale: 262144.0 | grad norm: 0.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 24.086 | tokens per gpu per second (tgs): 1541.533 | TFLOPs: 12.40 |
g0220: [2024-08-09 19:21:25,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=11770, skipped=12, lr=[0.00019999996144491868, 0.00019999996144491868], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11770 loss: 0.9427 iter time (s): 4.474 samples/sec: 28.607
g0238:  iteration    11770/10000000 | consumed samples:      1506560 | consumed tokens:   3085434880 | elapsed time per iteration (ms): 4507.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.618334E-01 | loss scale: 262144.0 | grad norm: 0.268 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.400 | tokens per gpu per second (tgs): 1817.628 | TFLOPs: 14.63 |
g0220: [2024-08-09 19:22:07,840] [INFO] [logging.py:96:log_dist] [Rank 0] step=11780, skipped=12, lr=[0.00019999995903510564, 0.00019999995903510564], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11780 loss: 0.9377 iter time (s): 4.168 samples/sec: 30.712
g0238:  iteration    11780/10000000 | consumed samples:      1507840 | consumed tokens:   3088056320 | elapsed time per iteration (ms): 4200.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.532498E-01 | loss scale: 262144.0 | grad norm: 0.304 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.473 | tokens per gpu per second (tgs): 1950.258 | TFLOPs: 15.69 |
g0220: [2024-08-09 19:22:51,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=11790, skipped=12, lr=[0.00019999995655224767, 0.00019999995655224767], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11790 loss: 0.9751 iter time (s): 4.296 samples/sec: 29.797
g0238:  iteration    11790/10000000 | consumed samples:      1509120 | consumed tokens:   3090677760 | elapsed time per iteration (ms): 4328.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.648355E-01 | loss scale: 262144.0 | grad norm: 0.287 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.573 | tokens per gpu per second (tgs): 1892.652 | TFLOPs: 15.23 |
g0220: [2024-08-09 19:23:32,873] [INFO] [logging.py:96:log_dist] [Rank 0] step=11800, skipped=12, lr=[0.00019999995399634475, 0.00019999995399634475], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11800 loss: 0.9375 iter time (s): 4.142 samples/sec: 30.901
g0238:  iteration    11800/10000000 | consumed samples:      1510400 | consumed tokens:   3093299200 | elapsed time per iteration (ms): 4174.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.668253E-01 | loss scale: 262144.0 | grad norm: 0.252 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.659 | tokens per gpu per second (tgs): 1962.189 | TFLOPs: 15.79 |
g0220: [2024-08-09 19:24:13,730] [INFO] [logging.py:96:log_dist] [Rank 0] step=11810, skipped=12, lr=[0.0001999999513673969, 0.0001999999513673969], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11810 loss: 0.9652 iter time (s): 4.053 samples/sec: 31.582
g0238:  iteration    11810/10000000 | consumed samples:      1511680 | consumed tokens:   3095920640 | elapsed time per iteration (ms): 4085.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.544436E-01 | loss scale: 262144.0 | grad norm: 0.286 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.329 | tokens per gpu per second (tgs): 2005.060 | TFLOPs: 16.14 |
g0220: [2024-08-09 19:24:57,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=11820, skipped=12, lr=[0.00019999994866540417, 0.00019999994866540417], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11820 loss: 0.9991 iter time (s): 4.309 samples/sec: 29.707
g0238:  iteration    11820/10000000 | consumed samples:      1512960 | consumed tokens:   3098542080 | elapsed time per iteration (ms): 4341.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.623175E-01 | loss scale: 262144.0 | grad norm: 0.288 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.485 | tokens per gpu per second (tgs): 1887.051 | TFLOPs: 15.19 |
g0220: [2024-08-09 19:25:40,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=11830, skipped=12, lr=[0.00019999994589036646, 0.00019999994589036646], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11830 loss: 0.9703 iter time (s): 4.269 samples/sec: 29.984
g0238:  iteration    11830/10000000 | consumed samples:      1514240 | consumed tokens:   3101163520 | elapsed time per iteration (ms): 4301.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.566370E-01 | loss scale: 262144.0 | grad norm: 0.264 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.756 | tokens per gpu per second (tgs): 1904.411 | TFLOPs: 15.33 |
g0220: [2024-08-09 19:26:22,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=11840, skipped=12, lr=[0.00019999994304228385, 0.00019999994304228385], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11840 loss: 0.9451 iter time (s): 4.156 samples/sec: 30.798
g0238:  iteration    11840/10000000 | consumed samples:      1515520 | consumed tokens:   3103784960 | elapsed time per iteration (ms): 4188.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.627694E-01 | loss scale: 262144.0 | grad norm: 0.298 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.560 | tokens per gpu per second (tgs): 1955.854 | TFLOPs: 15.74 |
g0220: [2024-08-09 19:27:04,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=11850, skipped=12, lr=[0.0001999999401211563, 0.0001999999401211563], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11850 loss: 0.9564 iter time (s): 4.241 samples/sec: 30.181
g0238:  iteration    11850/10000000 | consumed samples:      1516800 | consumed tokens:   3106406400 | elapsed time per iteration (ms): 4273.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.540874E-01 | loss scale: 262144.0 | grad norm: 0.305 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.951 | tokens per gpu per second (tgs): 1916.859 | TFLOPs: 15.43 |
g0220: [2024-08-09 19:27:45,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=11860, skipped=12, lr=[0.00019999993712698386, 0.00019999993712698386], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11860 loss: 0.9903 iter time (s): 4.027 samples/sec: 31.784
g0238:  iteration    11860/10000000 | consumed samples:      1518080 | consumed tokens:   3109027840 | elapsed time per iteration (ms): 4060.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.660366E-01 | loss scale: 262144.0 | grad norm: 0.321 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.526 | tokens per gpu per second (tgs): 2017.634 | TFLOPs: 16.24 |
g0220: [2024-08-09 19:28:27,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=11870, skipped=12, lr=[0.00019999993405976646, 0.00019999993405976646], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11870 loss: 0.9515 iter time (s): 4.197 samples/sec: 30.496
g0238:  iteration    11870/10000000 | consumed samples:      1519360 | consumed tokens:   3111649280 | elapsed time per iteration (ms): 4230.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.674306E-01 | loss scale: 262144.0 | grad norm: 0.396 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.258 | tokens per gpu per second (tgs): 1936.541 | TFLOPs: 15.58 |
g0220: [2024-08-09 19:29:11,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=11880, skipped=12, lr=[0.00019999993091950416, 0.00019999993091950416], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11880 loss: 0.9725 iter time (s): 4.353 samples/sec: 29.406
g0238:  iteration    11880/10000000 | consumed samples:      1520640 | consumed tokens:   3114270720 | elapsed time per iteration (ms): 4385.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.562932E-01 | loss scale: 262144.0 | grad norm: 0.267 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.186 | tokens per gpu per second (tgs): 1867.924 | TFLOPs: 15.03 |
g0220: [2024-08-09 19:29:54,624] [INFO] [logging.py:96:log_dist] [Rank 0] step=11890, skipped=12, lr=[0.00019999992770619696, 0.00019999992770619696], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11890 loss: 0.9775 iter time (s): 4.276 samples/sec: 29.934
g0238:  iteration    11890/10000000 | consumed samples:      1521920 | consumed tokens:   3116892160 | elapsed time per iteration (ms): 4308.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.485024E-01 | loss scale: 262144.0 | grad norm: 0.338 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.709 | tokens per gpu per second (tgs): 1901.394 | TFLOPs: 15.30 |
g0220: [2024-08-09 19:30:36,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=11900, skipped=12, lr=[0.0001999999244198448, 0.0001999999244198448], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11900 loss: 0.9742 iter time (s): 4.158 samples/sec: 30.785
g0238:  iteration    11900/10000000 | consumed samples:      1523200 | consumed tokens:   3119513600 | elapsed time per iteration (ms): 4191.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.762113E-01 | loss scale: 262144.0 | grad norm: 0.289 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.537 | tokens per gpu per second (tgs): 1954.366 | TFLOPs: 15.73 |
g0220: [2024-08-09 19:31:17,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=11910, skipped=12, lr=[0.0001999999210604478, 0.0001999999210604478], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11910 loss: 0.9399 iter time (s): 4.015 samples/sec: 31.880
g0238:  iteration    11910/10000000 | consumed samples:      1524480 | consumed tokens:   3122135040 | elapsed time per iteration (ms): 4047.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.573602E-01 | loss scale: 262144.0 | grad norm: 0.342 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.621 | tokens per gpu per second (tgs): 2023.741 | TFLOPs: 16.29 |
g0220: [2024-08-09 19:32:00,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=11920, skipped=12, lr=[0.00019999991762800586, 0.00019999991762800586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11920 loss: 0.9824 iter time (s): 4.290 samples/sec: 29.836
g0238:  iteration    11920/10000000 | consumed samples:      1525760 | consumed tokens:   3124756480 | elapsed time per iteration (ms): 4324.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.768347E-01 | loss scale: 262144.0 | grad norm: 0.412 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.602 | tokens per gpu per second (tgs): 1894.552 | TFLOPs: 15.25 |
g0220: [2024-08-09 19:32:46,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=11930, skipped=12, lr=[0.00019999991412251902, 0.00019999991412251902], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11930 loss: 0.9423 iter time (s): 4.600 samples/sec: 27.829
g0238:  iteration    11930/10000000 | consumed samples:      1527040 | consumed tokens:   3127377920 | elapsed time per iteration (ms): 4632.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.503568E-01 | loss scale: 262144.0 | grad norm: 0.311 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.631 | tokens per gpu per second (tgs): 1768.367 | TFLOPs: 14.23 |
g0220: [2024-08-09 19:33:31,008] [INFO] [logging.py:96:log_dist] [Rank 0] step=11940, skipped=12, lr=[0.0001999999105439873, 0.0001999999105439873], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11940 loss: 0.9396 iter time (s): 4.397 samples/sec: 29.109
g0238:  iteration    11940/10000000 | consumed samples:      1528320 | consumed tokens:   3129999360 | elapsed time per iteration (ms): 4442.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.501052E-01 | loss scale: 262144.0 | grad norm: 0.402 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.815 | tokens per gpu per second (tgs): 1844.133 | TFLOPs: 14.84 |
g0220: [2024-08-09 19:34:13,645] [INFO] [logging.py:96:log_dist] [Rank 0] step=11950, skipped=12, lr=[0.0001999999068924106, 0.0001999999068924106], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11950 loss: 0.9940 iter time (s): 4.210 samples/sec: 30.403
g0238:  iteration    11950/10000000 | consumed samples:      1529600 | consumed tokens:   3132620800 | elapsed time per iteration (ms): 4263.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.708284E-01 | loss scale: 262144.0 | grad norm: 0.296 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.019 | tokens per gpu per second (tgs): 1921.244 | TFLOPs: 15.46 |
g0220: [2024-08-09 19:34:54,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=11960, skipped=12, lr=[0.00019999990316778909, 0.00019999990316778909], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11960 loss: 0.9896 iter time (s): 4.035 samples/sec: 31.723
g0238:  iteration    11960/10000000 | consumed samples:      1530880 | consumed tokens:   3135242240 | elapsed time per iteration (ms): 4067.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.549312E-01 | loss scale: 262144.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.468 | tokens per gpu per second (tgs): 2013.965 | TFLOPs: 16.21 |
g0220: [2024-08-09 19:35:35,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=11970, skipped=12, lr=[0.00019999989937012267, 0.00019999989937012267], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11970 loss: 0.9526 iter time (s): 4.102 samples/sec: 31.207
g0238:  iteration    11970/10000000 | consumed samples:      1532160 | consumed tokens:   3137863680 | elapsed time per iteration (ms): 4133.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.596616E-01 | loss scale: 262144.0 | grad norm: 0.270 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.963 | tokens per gpu per second (tgs): 1981.655 | TFLOPs: 15.95 |
g0220: [2024-08-09 19:36:18,969] [INFO] [logging.py:96:log_dist] [Rank 0] step=11980, skipped=12, lr=[0.00019999989549941135, 0.00019999989549941135], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11980 loss: 0.9344 iter time (s): 4.298 samples/sec: 29.784
g0238:  iteration    11980/10000000 | consumed samples:      1533440 | consumed tokens:   3140485120 | elapsed time per iteration (ms): 4330.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.517744E-01 | loss scale: 262144.0 | grad norm: 0.251 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.558 | tokens per gpu per second (tgs): 1891.722 | TFLOPs: 15.22 |
g0220: [2024-08-09 19:37:00,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=11990, skipped=12, lr=[0.00019999989155565514, 0.00019999989155565514], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 11990 loss: 0.9779 iter time (s): 4.113 samples/sec: 31.122
g0238:  iteration    11990/10000000 | consumed samples:      1534720 | consumed tokens:   3143106560 | elapsed time per iteration (ms): 4146.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.692570E-01 | loss scale: 262144.0 | grad norm: 0.264 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.871 | tokens per gpu per second (tgs): 1975.762 | TFLOPs: 15.90 |
g0220: [2024-08-09 19:37:41,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=12000, skipped=12, lr=[0.00019999988753885406, 0.00019999988753885406], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12000 loss: 0.9555 iter time (s): 4.011 samples/sec: 31.912
g0238:  iteration    12000/10000000 | consumed samples:      1536000 | consumed tokens:   3145728000 | elapsed time per iteration (ms): 4134.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.593548E-01 | loss scale: 262144.0 | grad norm: 0.258 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.958 | tokens per gpu per second (tgs): 1981.323 | TFLOPs: 15.94 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 12000 | lm loss value: 9.556600E-01 | lm loss PPL: 2.600386E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   12000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-09 19:44:16,024] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step12000 is about to be saved!
g0220: [2024-08-09 19:44:16,031] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0238: [2024-08-09 19:44:16,031] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0238: [2024-08-09 19:44:16,031] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0238: [2024-08-09 19:44:16,031] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0233: [2024-08-09 19:44:16,032] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0233: [2024-08-09 19:44:16,032] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0220: [2024-08-09 19:44:16,032] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0233: [2024-08-09 19:44:16,032] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0234: [2024-08-09 19:44:16,033] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0234: [2024-08-09 19:44:16,033] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0237: [2024-08-09 19:44:16,033] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0225: [2024-08-09 19:44:16,033] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0237: [2024-08-09 19:44:16,033] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0237: [2024-08-09 19:44:16,033] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0225: [2024-08-09 19:44:16,033] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0220: [2024-08-09 19:44:16,034] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0234: [2024-08-09 19:44:16,034] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0225: [2024-08-09 19:44:16,034] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0235: [2024-08-09 19:44:16,034] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0235: [2024-08-09 19:44:16,034] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0235: [2024-08-09 19:44:16,034] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0236: [2024-08-09 19:44:16,036] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0236: [2024-08-09 19:44:16,036] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0236: [2024-08-09 19:44:16,036] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0238: [2024-08-09 19:44:16,055] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_23-model_00-model_states.pt...
g0235: [2024-08-09 19:44:16,070] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_14-model_00-model_states.pt...
g0237: [2024-08-09 19:44:16,071] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_20-model_00-model_states.pt...
g0233: [2024-08-09 19:44:16,071] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_08-model_00-model_states.pt...
g0225: [2024-08-09 19:44:16,072] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_05-model_00-model_states.pt...
g0234: [2024-08-09 19:44:16,072] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_11-model_00-model_states.pt...
g0236: [2024-08-09 19:44:16,074] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_17-model_00-model_states.pt...
g0220: [2024-08-09 19:44:16,084] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_01-model_00-model_states.pt...
g0237: [2024-08-09 19:44:16,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_20-model_00-model_states.pt.
g0237: [2024-08-09 19:44:16,252] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_21-model_00-model_states.pt...
g0234: [2024-08-09 19:44:16,255] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_11-model_00-model_states.pt.
g0238: [2024-08-09 19:44:16,264] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 19:44:16,265] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_24-model_00-model_states.pt...
g0233: [2024-08-09 19:44:16,266] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_08-model_00-model_states.pt.
g0238: [2024-08-09 19:44:16,267] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_24-model_00-model_states.pt.
g0234: [2024-08-09 19:44:16,295] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_12-model_00-model_states.pt...
g0236: [2024-08-09 19:44:16,300] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_17-model_00-model_states.pt.
g0233: [2024-08-09 19:44:16,304] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_09-model_00-model_states.pt...
g0238: [2024-08-09 19:44:16,313] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_25-model_00-model_states.pt...
g0236: [2024-08-09 19:44:16,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_18-model_00-model_states.pt...
g0235: [2024-08-09 19:44:16,381] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_14-model_00-model_states.pt.
g0237: [2024-08-09 19:44:16,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_21-model_00-model_states.pt.
g0225: [2024-08-09 19:44:16,386] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_05-model_00-model_states.pt.
g0237: [2024-08-09 19:44:16,418] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_22-model_00-model_states.pt...
g0235: [2024-08-09 19:44:16,420] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_15-model_00-model_states.pt...
g0225: [2024-08-09 19:44:16,426] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_06-model_00-model_states.pt...
g0233: [2024-08-09 19:44:16,484] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_09-model_00-model_states.pt.
g0234: [2024-08-09 19:44:16,489] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_12-model_00-model_states.pt.
g0233: [2024-08-09 19:44:16,518] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_10-model_00-model_states.pt...
g0234: [2024-08-09 19:44:16,524] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_13-model_00-model_states.pt...
g0220: [2024-08-09 19:44:16,543] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_01-model_00-model_states.pt.
g0225: [2024-08-09 19:44:16,555] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_06-model_00-model_states.pt.
g0236: [2024-08-09 19:44:16,561] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_18-model_00-model_states.pt.
g0220: [2024-08-09 19:44:16,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_02-model_00-model_states.pt...
g0235: [2024-08-09 19:44:16,567] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_15-model_00-model_states.pt.
g0225: [2024-08-09 19:44:16,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_07-model_00-model_states.pt...
g0237: [2024-08-09 19:44:16,595] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_22-model_00-model_states.pt.
g0236: [2024-08-09 19:44:16,596] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_19-model_00-model_states.pt...
g0237: [2024-08-09 19:44:16,597] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_06_model_states.pt...
g0235: [2024-08-09 19:44:16,601] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_16-model_00-model_states.pt...
g0238: [2024-08-09 19:44:16,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 19:44:16,623] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_07_model_states.pt...
g0233: [2024-08-09 19:44:16,646] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 19:44:16,647] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_02_model_states.pt...
g0220: [2024-08-09 19:44:16,716] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_02-model_00-model_states.pt.
g0234: [2024-08-09 19:44:16,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 19:44:16,736] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_03_model_states.pt...
g0220: [2024-08-09 19:44:16,739] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_03-model_00-model_states.pt...
g0235: [2024-08-09 19:44:16,783] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 19:44:16,785] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_04_model_states.pt...
g0225: [2024-08-09 19:44:16,792] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 19:44:16,794] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_01_model_states.pt...
g0220: [2024-08-09 19:44:16,896] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 19:44:16,926] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_04-model_00-model_states.pt...
g0236: [2024-08-09 19:44:17,004] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 19:44:17,006] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_05_model_states.pt...
g0220: [2024-08-09 19:44:17,115] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 19:44:17,116] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_00_model_states.pt
g0220: [2024-08-09 19:44:17,116] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_00_model_states.pt...
g0238: [2024-08-09 19:44:18,599] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 19:44:18,600] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0237: [2024-08-09 19:44:19,016] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 19:44:19,017] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0235: [2024-08-09 19:44:19,230] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 19:44:19,230] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0225: [2024-08-09 19:44:19,269] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_01_model_states.pt.
g0225: [2024-08-09 19:44:19,269] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0233: [2024-08-09 19:44:19,331] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 19:44:19,332] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0234: [2024-08-09 19:44:19,662] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 19:44:19,663] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0236: [2024-08-09 19:44:19,875] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 19:44:19,876] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0220: [2024-08-09 19:44:20,617] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step12000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 19:44:20,617] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12000 is ready now!
g0220:   successfully saved checkpoint at iteration   12000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 4.83, Latency(second): 4.658
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (4657.77, 4658.06)
g0220: [2024-08-09 19:45:04,937] [INFO] [logging.py:96:log_dist] [Rank 0] step=12010, skipped=12, lr=[0.0001999998834490081, 0.0001999998834490081], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12010 loss: 0.9194 iter time (s): 4.398 samples/sec: 29.104
g0238:  iteration    12010/10000000 | consumed samples:      1537280 | consumed tokens:   3148349440 | elapsed time per iteration (ms): 44315.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.575518E-01 | loss scale: 262144.0 | grad norm: 0.288 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.888 | tokens per gpu per second (tgs): 184.855 | TFLOPs: 1.49 |
g0220: [2024-08-09 19:45:47,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=12020, skipped=12, lr=[0.00019999987928611725, 0.00019999987928611725], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12020 loss: 0.9567 iter time (s): 4.198 samples/sec: 30.487
g0238:  iteration    12020/10000000 | consumed samples:      1538560 | consumed tokens:   3150970880 | elapsed time per iteration (ms): 4231.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.609286E-01 | loss scale: 262144.0 | grad norm: 0.250 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.252 | tokens per gpu per second (tgs): 1936.102 | TFLOPs: 15.58 |
g0220: [2024-08-09 19:46:28,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=12030, skipped=12, lr=[0.00019999987505018153, 0.00019999987505018153], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12030 loss: 0.9660 iter time (s): 4.086 samples/sec: 31.328
g0238:  iteration    12030/10000000 | consumed samples:      1539840 | consumed tokens:   3153592320 | elapsed time per iteration (ms): 4118.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.677128E-01 | loss scale: 262144.0 | grad norm: 0.303 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.081 | tokens per gpu per second (tgs): 1989.176 | TFLOPs: 16.01 |
g0220: [2024-08-09 19:47:10,843] [INFO] [logging.py:96:log_dist] [Rank 0] step=12040, skipped=12, lr=[0.00019999987074120095, 0.00019999987074120095], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12040 loss: 0.9341 iter time (s): 4.209 samples/sec: 30.412
g0238:  iteration    12040/10000000 | consumed samples:      1541120 | consumed tokens:   3156213760 | elapsed time per iteration (ms): 4241.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.571329E-01 | loss scale: 262144.0 | grad norm: 0.283 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.180 | tokens per gpu per second (tgs): 1931.537 | TFLOPs: 15.54 |
g0220: [2024-08-09 19:47:53,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=12050, skipped=12, lr=[0.00019999986635917547, 0.00019999986635917547], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12050 loss: 0.9778 iter time (s): 4.234 samples/sec: 30.233
g0238:  iteration    12050/10000000 | consumed samples:      1542400 | consumed tokens:   3158835200 | elapsed time per iteration (ms): 4266.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.559890E-01 | loss scale: 262144.0 | grad norm: 0.266 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.001 | tokens per gpu per second (tgs): 1920.084 | TFLOPs: 15.45 |
g0220: [2024-08-09 19:48:35,538] [INFO] [logging.py:96:log_dist] [Rank 0] step=12060, skipped=12, lr=[0.00019999986190410518, 0.00019999986190410518], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12060 loss: 0.9625 iter time (s): 4.171 samples/sec: 30.691
g0238:  iteration    12060/10000000 | consumed samples:      1543680 | consumed tokens:   3161456640 | elapsed time per iteration (ms): 4203.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.585750E-01 | loss scale: 262144.0 | grad norm: 0.417 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.455 | tokens per gpu per second (tgs): 1949.099 | TFLOPs: 15.68 |
g0220: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 19:49:09,522] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-09 19:49:09,523] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-09 19:49:17,736] [INFO] [logging.py:96:log_dist] [Rank 0] step=12070, skipped=12, lr=[0.00019999985737599, 0.00019999985737599], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12070 loss: 0.9912 iter time (s): 4.187 samples/sec: 30.571
g0238:  iteration    12070/10000000 | consumed samples:      1544960 | consumed tokens:   3164078080 | elapsed time per iteration (ms): 4220.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.541035E-01 | loss scale: 524288.0 | grad norm: 0.284 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.331 | tokens per gpu per second (tgs): 1941.184 | TFLOPs: 15.62 |
g0220: [2024-08-09 19:49:58,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=12080, skipped=12, lr=[0.00019999985277482997, 0.00019999985277482997], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12080 loss: 0.9548 iter time (s): 4.078 samples/sec: 31.386
g0238:  iteration    12080/10000000 | consumed samples:      1546240 | consumed tokens:   3166699520 | elapsed time per iteration (ms): 4111.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.402670E-01 | loss scale: 524288.0 | grad norm: 0.262 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.133 | tokens per gpu per second (tgs): 1992.513 | TFLOPs: 16.03 |
g0220: [2024-08-09 19:50:39,717] [INFO] [logging.py:96:log_dist] [Rank 0] step=12090, skipped=12, lr=[0.00019999984810062504, 0.00019999984810062504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12090 loss: 0.9176 iter time (s): 4.054 samples/sec: 31.574
g0238:  iteration    12090/10000000 | consumed samples:      1547520 | consumed tokens:   3169320960 | elapsed time per iteration (ms): 4086.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.526965E-01 | loss scale: 524288.0 | grad norm: 0.268 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.324 | tokens per gpu per second (tgs): 2004.744 | TFLOPs: 16.13 |
g0220: [2024-08-09 19:51:21,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=12100, skipped=12, lr=[0.00019999984335337533, 0.00019999984335337533], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12100 loss: 0.9439 iter time (s): 4.162 samples/sec: 30.752
g0238:  iteration    12100/10000000 | consumed samples:      1548800 | consumed tokens:   3171942400 | elapsed time per iteration (ms): 4195.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.387488E-01 | loss scale: 524288.0 | grad norm: 0.315 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.512 | tokens per gpu per second (tgs): 1952.787 | TFLOPs: 15.71 |
g0220: [2024-08-09 19:52:03,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=12110, skipped=12, lr=[0.00019999983853308072, 0.00019999983853308072], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12110 loss: 0.9234 iter time (s): 4.175 samples/sec: 30.659
g0238:  iteration    12110/10000000 | consumed samples:      1550080 | consumed tokens:   3174563840 | elapsed time per iteration (ms): 4207.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.584375E-01 | loss scale: 524288.0 | grad norm: 0.357 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.423 | tokens per gpu per second (tgs): 1947.082 | TFLOPs: 15.67 |
g0220: [2024-08-09 19:52:45,954] [INFO] [logging.py:96:log_dist] [Rank 0] step=12120, skipped=12, lr=[0.0001999998336397413, 0.0001999998336397413], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12120 loss: 0.9258 iter time (s): 4.189 samples/sec: 30.558
g0238:  iteration    12120/10000000 | consumed samples:      1551360 | consumed tokens:   3177185280 | elapsed time per iteration (ms): 4221.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.505837E-01 | loss scale: 524288.0 | grad norm: 0.266 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.322 | tokens per gpu per second (tgs): 1940.639 | TFLOPs: 15.62 |
g0220: [2024-08-09 19:53:28,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=12130, skipped=12, lr=[0.00019999982867335703, 0.00019999982867335703], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12130 loss: 0.9613 iter time (s): 4.242 samples/sec: 30.176
g0238:  iteration    12130/10000000 | consumed samples:      1552640 | consumed tokens:   3179806720 | elapsed time per iteration (ms): 4274.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.470679E-01 | loss scale: 524288.0 | grad norm: 0.383 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.947 | tokens per gpu per second (tgs): 1916.639 | TFLOPs: 15.42 |
g0220: [2024-08-09 19:54:09,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=12140, skipped=12, lr=[0.00019999982363392793, 0.00019999982363392793], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12140 loss: 0.9545 iter time (s): 4.028 samples/sec: 31.777
g0238:  iteration    12140/10000000 | consumed samples:      1553920 | consumed tokens:   3182428160 | elapsed time per iteration (ms): 4060.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.588009E-01 | loss scale: 524288.0 | grad norm: 0.258 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.522 | tokens per gpu per second (tgs): 2017.414 | TFLOPs: 16.23 |
g0220: [2024-08-09 19:54:49,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=12150, skipped=12, lr=[0.00019999981852145398, 0.00019999981852145398], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12150 loss: 0.9610 iter time (s): 4.036 samples/sec: 31.716
g0238:  iteration    12150/10000000 | consumed samples:      1555200 | consumed tokens:   3185049600 | elapsed time per iteration (ms): 4068.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.578537E-01 | loss scale: 524288.0 | grad norm: 0.285 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.462 | tokens per gpu per second (tgs): 2013.588 | TFLOPs: 16.20 |
g0220: [2024-08-09 19:55:29,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=12160, skipped=12, lr=[0.00019999981333593522, 0.00019999981333593522], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12160 loss: 0.9389 iter time (s): 3.964 samples/sec: 32.291
g0238:  iteration    12160/10000000 | consumed samples:      1556480 | consumed tokens:   3187671040 | elapsed time per iteration (ms): 3996.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.512292E-01 | loss scale: 524288.0 | grad norm: 0.290 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.028 | tokens per gpu per second (tgs): 2049.776 | TFLOPs: 16.49 |
g0220: [2024-08-09 19:56:10,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=12170, skipped=12, lr=[0.00019999980807737164, 0.00019999980807737164], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12170 loss: 0.9544 iter time (s): 3.981 samples/sec: 32.152
g0238:  iteration    12170/10000000 | consumed samples:      1557760 | consumed tokens:   3190292480 | elapsed time per iteration (ms): 4013.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.525552E-01 | loss scale: 524288.0 | grad norm: 0.295 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.892 | tokens per gpu per second (tgs): 2041.110 | TFLOPs: 16.43 |
g0220: [2024-08-09 19:56:52,467] [INFO] [logging.py:96:log_dist] [Rank 0] step=12180, skipped=12, lr=[0.00019999980274576322, 0.00019999980274576322], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12180 loss: 0.9960 iter time (s): 4.206 samples/sec: 30.434
g0238:  iteration    12180/10000000 | consumed samples:      1559040 | consumed tokens:   3192913920 | elapsed time per iteration (ms): 4238.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.594344E-01 | loss scale: 524288.0 | grad norm: 0.261 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.202 | tokens per gpu per second (tgs): 1932.932 | TFLOPs: 15.55 |
g0220: [2024-08-09 19:57:33,739] [INFO] [logging.py:96:log_dist] [Rank 0] step=12190, skipped=12, lr=[0.00019999979734111001, 0.00019999979734111001], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12190 loss: 0.9306 iter time (s): 4.095 samples/sec: 31.259
g0238:  iteration    12190/10000000 | consumed samples:      1560320 | consumed tokens:   3195535360 | elapsed time per iteration (ms): 4127.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.434733E-01 | loss scale: 524288.0 | grad norm: 0.238 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.013 | tokens per gpu per second (tgs): 1984.817 | TFLOPs: 15.97 |
g0220: [2024-08-09 19:58:15,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=12200, skipped=12, lr=[0.000199999791863412, 0.000199999791863412], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12200 loss: 0.9754 iter time (s): 4.115 samples/sec: 31.107
g0238:  iteration    12200/10000000 | consumed samples:      1561600 | consumed tokens:   3198156800 | elapsed time per iteration (ms): 4150.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.637692E-01 | loss scale: 524288.0 | grad norm: 0.491 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.843 | tokens per gpu per second (tgs): 1973.972 | TFLOPs: 15.88 |
g0220: [2024-08-09 19:58:56,142] [INFO] [logging.py:96:log_dist] [Rank 0] step=12210, skipped=12, lr=[0.00019999978631266913, 0.00019999978631266913], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12210 loss: 0.9522 iter time (s): 4.057 samples/sec: 31.547
g0238:  iteration    12210/10000000 | consumed samples:      1562880 | consumed tokens:   3200778240 | elapsed time per iteration (ms): 4090.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.505217E-01 | loss scale: 524288.0 | grad norm: 0.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.295 | tokens per gpu per second (tgs): 2002.890 | TFLOPs: 16.12 |
g0225: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 12217
g0225: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 12217
g0225: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 12217
g0237: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 12217
g0237: Grad overflow on iteration 12217
g0237: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: Grad overflow on iteration 12217
g0234: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: Grad overflow on iteration 12217
g0237: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 12217
g0233: Grad overflow on iteration 12217
g0220: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 12217
g0225: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 12217
g0220: Grad overflow on iteration 12217
g0233: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 12217
g0236: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 12217
g0235: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 12217
g0234: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 12217
g0220: Grad overflow on iteration 12217
g0236: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: Grad overflow on iteration 12217
g0237: Grad overflow on iteration 12217
g0233: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 12217
g0234: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 12217
g0238: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 12217
g0220: Grad overflow on iteration 12217
g0236: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 12217
g0237: Grad overflow on iteration 12217
g0234: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: Grad overflow on iteration 12217
g0235: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: Grad overflow on iteration 12217
g0236: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: Grad overflow on iteration 12217
g0236: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 12217
g0235: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: Grad overflow on iteration 12217
g0237: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: Grad overflow on iteration 12217
g0238: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 12217
g0238: [2024-08-09 19:59:28,347] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-09 19:59:28,347] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0220: [2024-08-09 19:59:36,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=12220, skipped=13, lr=[0.0001999997806888815, 0.0001999997806888815], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12220 loss: 0.9250 iter time (s): 3.975 samples/sec: 32.204
g0238:  iteration    12220/10000000 | consumed samples:      1564160 | consumed tokens:   3203399680 | elapsed time per iteration (ms): 4007.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.444921E-01 | loss scale: 262144.0 | grad norm: 0.286 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.941 | tokens per gpu per second (tgs): 2044.234 | TFLOPs: 16.45 |
g0220: [2024-08-09 20:00:16,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=12230, skipped=13, lr=[0.00019999977499204907, 0.00019999977499204907], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12230 loss: 0.9641 iter time (s): 3.952 samples/sec: 32.388
g0238:  iteration    12230/10000000 | consumed samples:      1565440 | consumed tokens:   3206021120 | elapsed time per iteration (ms): 3984.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.536995E-01 | loss scale: 262144.0 | grad norm: 0.295 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.126 | tokens per gpu per second (tgs): 2056.043 | TFLOPs: 16.55 |
g0220: [2024-08-09 20:00:56,113] [INFO] [logging.py:96:log_dist] [Rank 0] step=12240, skipped=13, lr=[0.00019999976922217185, 0.00019999976922217185], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12240 loss: 0.9788 iter time (s): 3.973 samples/sec: 32.221
g0238:  iteration    12240/10000000 | consumed samples:      1566720 | consumed tokens:   3208642560 | elapsed time per iteration (ms): 4005.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.535324E-01 | loss scale: 262144.0 | grad norm: 0.272 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.957 | tokens per gpu per second (tgs): 2045.270 | TFLOPs: 16.46 |
g0220: [2024-08-09 20:01:38,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=12250, skipped=13, lr=[0.0001999997633792498, 0.0001999997633792498], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12250 loss: 0.9600 iter time (s): 4.194 samples/sec: 30.518
g0238:  iteration    12250/10000000 | consumed samples:      1568000 | consumed tokens:   3211264000 | elapsed time per iteration (ms): 4226.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.538665E-01 | loss scale: 262144.0 | grad norm: 0.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.285 | tokens per gpu per second (tgs): 1938.243 | TFLOPs: 15.60 |
g0220: [2024-08-09 20:02:19,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=12260, skipped=13, lr=[0.000199999757463283, 0.000199999757463283], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12260 loss: 0.9114 iter time (s): 4.120 samples/sec: 31.070
g0238:  iteration    12260/10000000 | consumed samples:      1569280 | consumed tokens:   3213885440 | elapsed time per iteration (ms): 4153.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.397511E-01 | loss scale: 262144.0 | grad norm: 0.337 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.820 | tokens per gpu per second (tgs): 1972.506 | TFLOPs: 15.87 |
g0220: [2024-08-09 20:03:00,549] [INFO] [logging.py:96:log_dist] [Rank 0] step=12270, skipped=13, lr=[0.00019999975147427142, 0.00019999975147427142], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12270 loss: 0.9608 iter time (s): 4.031 samples/sec: 31.755
g0238:  iteration    12270/10000000 | consumed samples:      1570560 | consumed tokens:   3216506880 | elapsed time per iteration (ms): 4063.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.473857E-01 | loss scale: 262144.0 | grad norm: 0.295 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.497 | tokens per gpu per second (tgs): 2015.799 | TFLOPs: 16.22 |
g0220: [2024-08-09 20:03:41,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=12280, skipped=13, lr=[0.00019999974541221505, 0.00019999974541221505], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12280 loss: 0.9418 iter time (s): 4.035 samples/sec: 31.721
g0238:  iteration    12280/10000000 | consumed samples:      1571840 | consumed tokens:   3219128320 | elapsed time per iteration (ms): 4068.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.438394E-01 | loss scale: 262144.0 | grad norm: 0.336 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.465 | tokens per gpu per second (tgs): 2013.750 | TFLOPs: 16.20 |
g0220: [2024-08-09 20:04:23,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=12290, skipped=13, lr=[0.0001999997392771139, 0.0001999997392771139], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12290 loss: 0.9544 iter time (s): 4.244 samples/sec: 30.163
g0238:  iteration    12290/10000000 | consumed samples:      1573120 | consumed tokens:   3221749760 | elapsed time per iteration (ms): 4276.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.461864E-01 | loss scale: 262144.0 | grad norm: 0.279 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.933 | tokens per gpu per second (tgs): 1915.728 | TFLOPs: 15.42 |
g0220: [2024-08-09 20:05:07,780] [INFO] [logging.py:96:log_dist] [Rank 0] step=12300, skipped=13, lr=[0.00019999973306896802, 0.00019999973306896802], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12300 loss: 0.9508 iter time (s): 4.346 samples/sec: 29.455
g0238:  iteration    12300/10000000 | consumed samples:      1574400 | consumed tokens:   3224371200 | elapsed time per iteration (ms): 4379.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.587979E-01 | loss scale: 262144.0 | grad norm: 0.316 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.230 | tokens per gpu per second (tgs): 1870.734 | TFLOPs: 15.05 |
g0220: [2024-08-09 20:05:50,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=12310, skipped=13, lr=[0.00019999972678777736, 0.00019999972678777736], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12310 loss: 0.9604 iter time (s): 4.285 samples/sec: 29.873
g0238:  iteration    12310/10000000 | consumed samples:      1575680 | consumed tokens:   3226992640 | elapsed time per iteration (ms): 4317.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.482580E-01 | loss scale: 262144.0 | grad norm: 0.379 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.648 | tokens per gpu per second (tgs): 1897.479 | TFLOPs: 15.27 |
g0220: [2024-08-09 20:06:30,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=12320, skipped=13, lr=[0.00019999972043354195, 0.00019999972043354195], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12320 loss: 0.9062 iter time (s): 3.930 samples/sec: 32.569
g0238:  iteration    12320/10000000 | consumed samples:      1576960 | consumed tokens:   3229614080 | elapsed time per iteration (ms): 3963.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.379006E-01 | loss scale: 262144.0 | grad norm: 0.281 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.297 | tokens per gpu per second (tgs): 2066.983 | TFLOPs: 16.63 |
g0220: [2024-08-09 20:07:10,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=12330, skipped=13, lr=[0.00019999971400626177, 0.00019999971400626177], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12330 loss: 0.9822 iter time (s): 3.986 samples/sec: 32.113
g0238:  iteration    12330/10000000 | consumed samples:      1578240 | consumed tokens:   3232235520 | elapsed time per iteration (ms): 4019.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.524275E-01 | loss scale: 262144.0 | grad norm: 0.333 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.848 | tokens per gpu per second (tgs): 2038.254 | TFLOPs: 16.40 |
g0220: [2024-08-09 20:07:51,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=12340, skipped=13, lr=[0.00019999970750593686, 0.00019999970750593686], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12340 loss: 0.9541 iter time (s): 4.043 samples/sec: 31.657
g0238:  iteration    12340/10000000 | consumed samples:      1579520 | consumed tokens:   3234856960 | elapsed time per iteration (ms): 4075.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.420252E-01 | loss scale: 262144.0 | grad norm: 0.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.404 | tokens per gpu per second (tgs): 2009.868 | TFLOPs: 16.17 |
g0220: [2024-08-09 20:08:31,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=12350, skipped=13, lr=[0.0001999997009325672, 0.0001999997009325672], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12350 loss: 0.9326 iter time (s): 3.980 samples/sec: 32.163
g0238:  iteration    12350/10000000 | consumed samples:      1580800 | consumed tokens:   3237478400 | elapsed time per iteration (ms): 4012.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.499521E-01 | loss scale: 262144.0 | grad norm: 0.311 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.901 | tokens per gpu per second (tgs): 2041.666 | TFLOPs: 16.43 |
g0220: [2024-08-09 20:09:13,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=12360, skipped=13, lr=[0.0001999996942861528, 0.0001999996942861528], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12360 loss: 0.9424 iter time (s): 4.138 samples/sec: 30.933
g0238:  iteration    12360/10000000 | consumed samples:      1582080 | consumed tokens:   3240099840 | elapsed time per iteration (ms): 4170.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.424337E-01 | loss scale: 262144.0 | grad norm: 0.296 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.693 | tokens per gpu per second (tgs): 1964.351 | TFLOPs: 15.81 |
g0220: [2024-08-09 20:09:54,503] [INFO] [logging.py:96:log_dist] [Rank 0] step=12370, skipped=13, lr=[0.00019999968756669368, 0.00019999968756669368], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12370 loss: 0.9255 iter time (s): 4.081 samples/sec: 31.367
g0238:  iteration    12370/10000000 | consumed samples:      1583360 | consumed tokens:   3242721280 | elapsed time per iteration (ms): 4113.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.452748E-01 | loss scale: 262144.0 | grad norm: 0.301 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.115 | tokens per gpu per second (tgs): 1991.382 | TFLOPs: 16.02 |
g0220: [2024-08-09 20:10:35,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=12380, skipped=13, lr=[0.0001999996807741898, 0.0001999996807741898], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12380 loss: 0.9579 iter time (s): 4.050 samples/sec: 31.608
g0238:  iteration    12380/10000000 | consumed samples:      1584640 | consumed tokens:   3245342720 | elapsed time per iteration (ms): 4082.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.438469E-01 | loss scale: 262144.0 | grad norm: 0.243 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.351 | tokens per gpu per second (tgs): 2006.475 | TFLOPs: 16.15 |
g0220: [2024-08-09 20:11:17,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=12390, skipped=13, lr=[0.00019999967390864123, 0.00019999967390864123], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12390 loss: 0.9223 iter time (s): 4.168 samples/sec: 30.712
g0238:  iteration    12390/10000000 | consumed samples:      1585920 | consumed tokens:   3247964160 | elapsed time per iteration (ms): 4200.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.471226E-01 | loss scale: 262144.0 | grad norm: 0.323 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.472 | tokens per gpu per second (tgs): 1950.205 | TFLOPs: 15.69 |
g0220: [2024-08-09 20:11:58,136] [INFO] [logging.py:96:log_dist] [Rank 0] step=12400, skipped=13, lr=[0.00019999966697004795, 0.00019999966697004795], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12400 loss: 1.0042 iter time (s): 4.047 samples/sec: 31.628
g0238:  iteration    12400/10000000 | consumed samples:      1587200 | consumed tokens:   3250585600 | elapsed time per iteration (ms): 4079.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.469487E-01 | loss scale: 262144.0 | grad norm: 0.290 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.373 | tokens per gpu per second (tgs): 2007.893 | TFLOPs: 16.16 |
g0220: [2024-08-09 20:12:40,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=12410, skipped=13, lr=[0.00019999965995840996, 0.00019999965995840996], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12410 loss: 0.9535 iter time (s): 4.221 samples/sec: 30.326
g0238:  iteration    12410/10000000 | consumed samples:      1588480 | consumed tokens:   3253207040 | elapsed time per iteration (ms): 4253.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.359908E-01 | loss scale: 262144.0 | grad norm: 0.254 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.091 | tokens per gpu per second (tgs): 1925.830 | TFLOPs: 15.50 |
g0220: [2024-08-09 20:13:22,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=12420, skipped=13, lr=[0.00019999965287372724, 0.00019999965287372724], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12420 loss: 0.9382 iter time (s): 4.128 samples/sec: 31.011
g0238:  iteration    12420/10000000 | consumed samples:      1589760 | consumed tokens:   3255828480 | elapsed time per iteration (ms): 4160.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.357561E-01 | loss scale: 262144.0 | grad norm: 0.287 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.766 | tokens per gpu per second (tgs): 1969.040 | TFLOPs: 15.85 |
g0220: [2024-08-09 20:14:02,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=12430, skipped=13, lr=[0.00019999964571599984, 0.00019999964571599984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12430 loss: 0.9652 iter time (s): 4.034 samples/sec: 31.733
g0238:  iteration    12430/10000000 | consumed samples:      1591040 | consumed tokens:   3258449920 | elapsed time per iteration (ms): 4066.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.499955E-01 | loss scale: 262144.0 | grad norm: 0.257 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.478 | tokens per gpu per second (tgs): 2014.607 | TFLOPs: 16.21 |
g0220: [2024-08-09 20:14:42,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=12440, skipped=13, lr=[0.00019999963848522774, 0.00019999963848522774], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12440 loss: 0.9362 iter time (s): 3.959 samples/sec: 32.329
g0238:  iteration    12440/10000000 | consumed samples:      1592320 | consumed tokens:   3261071360 | elapsed time per iteration (ms): 3992.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.447090E-01 | loss scale: 262144.0 | grad norm: 0.260 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.064 | tokens per gpu per second (tgs): 2052.116 | TFLOPs: 16.51 |
g0220: [2024-08-09 20:15:23,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=12450, skipped=13, lr=[0.00019999963118141092, 0.00019999963118141092], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12450 loss: 0.9409 iter time (s): 4.059 samples/sec: 31.532
g0238:  iteration    12450/10000000 | consumed samples:      1593600 | consumed tokens:   3263692800 | elapsed time per iteration (ms): 4092.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.487526E-01 | loss scale: 262144.0 | grad norm: 0.307 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.278 | tokens per gpu per second (tgs): 2001.772 | TFLOPs: 16.11 |
g0220: [2024-08-09 20:16:04,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=12460, skipped=13, lr=[0.00019999962380454946, 0.00019999962380454946], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12460 loss: 0.9655 iter time (s): 4.044 samples/sec: 31.648
g0238:  iteration    12460/10000000 | consumed samples:      1594880 | consumed tokens:   3266314240 | elapsed time per iteration (ms): 4077.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.403095E-01 | loss scale: 262144.0 | grad norm: 0.277 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.393 | tokens per gpu per second (tgs): 2009.169 | TFLOPs: 16.17 |
g0220: [2024-08-09 20:16:47,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=12470, skipped=13, lr=[0.00019999961635464332, 0.00019999961635464332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12470 loss: 0.9509 iter time (s): 4.239 samples/sec: 30.194
g0238:  iteration    12470/10000000 | consumed samples:      1596160 | consumed tokens:   3268935680 | elapsed time per iteration (ms): 4271.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.515815E-01 | loss scale: 262144.0 | grad norm: 0.289 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.966 | tokens per gpu per second (tgs): 1917.811 | TFLOPs: 15.43 |
g0220: [2024-08-09 20:17:31,493] [INFO] [logging.py:96:log_dist] [Rank 0] step=12480, skipped=13, lr=[0.0001999996088316925, 0.0001999996088316925], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12480 loss: 0.9657 iter time (s): 4.389 samples/sec: 29.164
g0238:  iteration    12480/10000000 | consumed samples:      1597440 | consumed tokens:   3271557120 | elapsed time per iteration (ms): 4422.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.535366E-01 | loss scale: 262144.0 | grad norm: 0.298 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.946 | tokens per gpu per second (tgs): 1852.540 | TFLOPs: 14.91 |
g0220: [2024-08-09 20:18:12,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=12490, skipped=13, lr=[0.000199999601235697, 0.000199999601235697], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12490 loss: 0.9453 iter time (s): 4.082 samples/sec: 31.354
g0238:  iteration    12490/10000000 | consumed samples:      1598720 | consumed tokens:   3274178560 | elapsed time per iteration (ms): 4115.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.414741E-01 | loss scale: 262144.0 | grad norm: 0.316 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.105 | tokens per gpu per second (tgs): 1990.736 | TFLOPs: 16.02 |
g0220: [2024-08-09 20:18:53,108] [INFO] [logging.py:96:log_dist] [Rank 0] step=12500, skipped=13, lr=[0.00019999959356665687, 0.00019999959356665687], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12500 loss: 0.9664 iter time (s): 4.013 samples/sec: 31.897
g0238:  iteration    12500/10000000 | consumed samples:      1600000 | consumed tokens:   3276800000 | elapsed time per iteration (ms): 4046.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.420339E-01 | loss scale: 262144.0 | grad norm: 0.276 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.634 | tokens per gpu per second (tgs): 2024.548 | TFLOPs: 16.29 |
g0220: [2024-08-09 20:19:34,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=12510, skipped=13, lr=[0.00019999958582457203, 0.00019999958582457203], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12510 loss: 0.8820 iter time (s): 4.061 samples/sec: 31.516
g0238:  iteration    12510/10000000 | consumed samples:      1601280 | consumed tokens:   3279421440 | elapsed time per iteration (ms): 4094.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.388332E-01 | loss scale: 262144.0 | grad norm: 0.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.263 | tokens per gpu per second (tgs): 2000.819 | TFLOPs: 16.10 |
g0236: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 12515
g0236: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 12515
g0236: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 12515
g0233: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 12515
g0234: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 12515
g0236: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 12515
g0225: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 12515
g0225: Grad overflow on iteration 12515
g0225: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 12515
g0225: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: Grad overflow on iteration 12515
g0233: Grad overflow on iteration 12515
g0238: Grad overflow on iteration 12515
g0225: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 12515
g0225: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 12515
g0235: Grad overflow on iteration 12515
g0237: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 12515
g0220: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 12515
g0237: Grad overflow on iteration 12515
g0233: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 12515
g0235: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 12515
g0235: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 12515
g0234: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 12515
g0235: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 12515
g0220: Grad overflow on iteration 12515
g0225: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: Grad overflow on iteration 12515
g0238: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 12515
g0233: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: Grad overflow on iteration 12515
g0235: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: Grad overflow on iteration 12515
g0233: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: Grad overflow on iteration 12515
g0238: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 12515
g0237: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 20:19:58,396] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: Grad overflow on iteration 12515
g0220: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 12515
g0220: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 20:19:58,397] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0234: [2024-08-09 20:19:58,397] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 20:20:15,273] [INFO] [logging.py:96:log_dist] [Rank 0] step=12520, skipped=14, lr=[0.0001999995780094426, 0.0001999995780094426], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12520 loss: 0.9658 iter time (s): 4.090 samples/sec: 31.299
g0238:  iteration    12520/10000000 | consumed samples:      1602560 | consumed tokens:   3282042880 | elapsed time per iteration (ms): 4122.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.505323E-01 | loss scale: 131072.0 | grad norm: 0.297 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.052 | tokens per gpu per second (tgs): 1987.344 | TFLOPs: 15.99 |
g0220: [2024-08-09 20:20:58,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=12530, skipped=14, lr=[0.00019999957012126852, 0.00019999957012126852], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12530 loss: 0.9199 iter time (s): 4.250 samples/sec: 30.117
g0238:  iteration    12530/10000000 | consumed samples:      1603840 | consumed tokens:   3284664320 | elapsed time per iteration (ms): 4282.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.442822E-01 | loss scale: 131072.0 | grad norm: 0.263 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.887 | tokens per gpu per second (tgs): 1912.781 | TFLOPs: 15.39 |
g0220: [2024-08-09 20:21:40,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=12540, skipped=14, lr=[0.0001999995621600498, 0.0001999995621600498], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12540 loss: 0.9736 iter time (s): 4.189 samples/sec: 30.553
g0238:  iteration    12540/10000000 | consumed samples:      1605120 | consumed tokens:   3287285760 | elapsed time per iteration (ms): 4222.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.390312E-01 | loss scale: 131072.0 | grad norm: 0.415 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.314 | tokens per gpu per second (tgs): 1940.095 | TFLOPs: 15.61 |
g0220: [2024-08-09 20:22:25,361] [INFO] [logging.py:96:log_dist] [Rank 0] step=12550, skipped=14, lr=[0.00019999955412578643, 0.00019999955412578643], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12550 loss: 0.9370 iter time (s): 4.471 samples/sec: 28.631
g0238:  iteration    12550/10000000 | consumed samples:      1606400 | consumed tokens:   3289907200 | elapsed time per iteration (ms): 4503.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.435437E-01 | loss scale: 131072.0 | grad norm: 0.302 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.422 | tokens per gpu per second (tgs): 1818.983 | TFLOPs: 14.64 |
g0220: [2024-08-09 20:23:07,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=12560, skipped=14, lr=[0.00019999954601847842, 0.00019999954601847842], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12560 loss: 0.9663 iter time (s): 4.227 samples/sec: 30.279
g0238:  iteration    12560/10000000 | consumed samples:      1607680 | consumed tokens:   3292528640 | elapsed time per iteration (ms): 4260.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.509706E-01 | loss scale: 131072.0 | grad norm: 0.272 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.046 | tokens per gpu per second (tgs): 1922.923 | TFLOPs: 15.47 |
g0220: [2024-08-09 20:23:47,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=12570, skipped=14, lr=[0.00019999953783812584, 0.00019999953783812584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12570 loss: 0.9625 iter time (s): 3.928 samples/sec: 32.591
g0238:  iteration    12570/10000000 | consumed samples:      1608960 | consumed tokens:   3295150080 | elapsed time per iteration (ms): 3960.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.422663E-01 | loss scale: 131072.0 | grad norm: 0.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.319 | tokens per gpu per second (tgs): 2068.441 | TFLOPs: 16.65 |
g0220: [2024-08-09 20:24:27,717] [INFO] [logging.py:96:log_dist] [Rank 0] step=12580, skipped=14, lr=[0.00019999952958472863, 0.00019999952958472863], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12580 loss: 0.9537 iter time (s): 3.982 samples/sec: 32.148
g0238:  iteration    12580/10000000 | consumed samples:      1610240 | consumed tokens:   3297771520 | elapsed time per iteration (ms): 4015.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.445683E-01 | loss scale: 131072.0 | grad norm: 0.269 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.880 | tokens per gpu per second (tgs): 2040.344 | TFLOPs: 16.42 |
g0220: [2024-08-09 20:25:09,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=12590, skipped=14, lr=[0.0001999995212582868, 0.0001999995212582868], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12590 loss: 0.9264 iter time (s): 4.143 samples/sec: 30.897
g0238:  iteration    12590/10000000 | consumed samples:      1611520 | consumed tokens:   3300392960 | elapsed time per iteration (ms): 4175.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.384448E-01 | loss scale: 131072.0 | grad norm: 0.286 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.653 | tokens per gpu per second (tgs): 1961.818 | TFLOPs: 15.79 |
g0220: [2024-08-09 20:25:51,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=12600, skipped=14, lr=[0.00019999951285880037, 0.00019999951285880037], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12600 loss: 0.9336 iter time (s): 4.213 samples/sec: 30.382
g0238:  iteration    12600/10000000 | consumed samples:      1612800 | consumed tokens:   3303014400 | elapsed time per iteration (ms): 4245.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.402296E-01 | loss scale: 131072.0 | grad norm: 0.272 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.147 | tokens per gpu per second (tgs): 1929.426 | TFLOPs: 15.53 |
g0220: [2024-08-09 20:26:34,627] [INFO] [logging.py:96:log_dist] [Rank 0] step=12610, skipped=14, lr=[0.00019999950438626937, 0.00019999950438626937], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12610 loss: 0.9341 iter time (s): 4.236 samples/sec: 30.216
g0238:  iteration    12610/10000000 | consumed samples:      1614080 | consumed tokens:   3305635840 | elapsed time per iteration (ms): 4269.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.488477E-01 | loss scale: 131072.0 | grad norm: 0.288 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.982 | tokens per gpu per second (tgs): 1918.826 | TFLOPs: 15.44 |
g0220: [2024-08-09 20:27:16,641] [INFO] [logging.py:96:log_dist] [Rank 0] step=12620, skipped=14, lr=[0.00019999949584069377, 0.00019999949584069377], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12620 loss: 0.9282 iter time (s): 4.169 samples/sec: 30.706
g0238:  iteration    12620/10000000 | consumed samples:      1615360 | consumed tokens:   3308257280 | elapsed time per iteration (ms): 4201.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.420868E-01 | loss scale: 131072.0 | grad norm: 0.272 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.466 | tokens per gpu per second (tgs): 1949.853 | TFLOPs: 15.69 |
g0220: [2024-08-09 20:27:57,589] [INFO] [logging.py:96:log_dist] [Rank 0] step=12630, skipped=14, lr=[0.0001999994872220736, 0.0001999994872220736], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12630 loss: 0.9121 iter time (s): 4.062 samples/sec: 31.515
g0238:  iteration    12630/10000000 | consumed samples:      1616640 | consumed tokens:   3310878720 | elapsed time per iteration (ms): 4094.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.373403E-01 | loss scale: 131072.0 | grad norm: 0.321 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.258 | tokens per gpu per second (tgs): 2000.531 | TFLOPs: 16.10 |
g0220: [2024-08-09 20:28:39,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=12640, skipped=14, lr=[0.00019999947853040884, 0.00019999947853040884], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12640 loss: 0.9054 iter time (s): 4.145 samples/sec: 30.878
g0238:  iteration    12640/10000000 | consumed samples:      1617920 | consumed tokens:   3313500160 | elapsed time per iteration (ms): 4178.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.402593E-01 | loss scale: 131072.0 | grad norm: 0.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.637 | tokens per gpu per second (tgs): 1960.767 | TFLOPs: 15.78 |
g0220: [2024-08-09 20:29:19,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=12650, skipped=14, lr=[0.00019999946976569956, 0.00019999946976569956], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12650 loss: 0.9665 iter time (s): 4.017 samples/sec: 31.862
g0238:  iteration    12650/10000000 | consumed samples:      1619200 | consumed tokens:   3316121600 | elapsed time per iteration (ms): 4049.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.470395E-01 | loss scale: 131072.0 | grad norm: 0.373 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.607 | tokens per gpu per second (tgs): 2022.844 | TFLOPs: 16.28 |
g0220: [2024-08-09 20:30:01,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=12660, skipped=14, lr=[0.00019999946092794565, 0.00019999946092794565], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12660 loss: 0.9347 iter time (s): 4.148 samples/sec: 30.856
g0238:  iteration    12660/10000000 | consumed samples:      1620480 | consumed tokens:   3318743040 | elapsed time per iteration (ms): 4181.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.337828E-01 | loss scale: 131072.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.614 | tokens per gpu per second (tgs): 1959.322 | TFLOPs: 15.77 |
g0220: [2024-08-09 20:30:46,965] [INFO] [logging.py:96:log_dist] [Rank 0] step=12670, skipped=14, lr=[0.00019999945201714726, 0.00019999945201714726], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12670 loss: 0.9737 iter time (s): 4.496 samples/sec: 28.469
g0238:  iteration    12670/10000000 | consumed samples:      1621760 | consumed tokens:   3321364480 | elapsed time per iteration (ms): 4528.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.477188E-01 | loss scale: 131072.0 | grad norm: 0.255 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.265 | tokens per gpu per second (tgs): 1808.991 | TFLOPs: 14.56 |
g0220: [2024-08-09 20:31:26,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=12680, skipped=14, lr=[0.00019999944303330426, 0.00019999944303330426], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12680 loss: 0.9331 iter time (s): 3.952 samples/sec: 32.392
g0238:  iteration    12680/10000000 | consumed samples:      1623040 | consumed tokens:   3323985920 | elapsed time per iteration (ms): 3985.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.330358E-01 | loss scale: 131072.0 | grad norm: 0.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.119 | tokens per gpu per second (tgs): 2055.586 | TFLOPs: 16.54 |
g0220: [2024-08-09 20:32:07,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=12690, skipped=14, lr=[0.00019999943397641675, 0.00019999943397641675], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12690 loss: 0.9221 iter time (s): 4.084 samples/sec: 31.344
g0238:  iteration    12690/10000000 | consumed samples:      1624320 | consumed tokens:   3326607360 | elapsed time per iteration (ms): 4116.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.309763E-01 | loss scale: 131072.0 | grad norm: 0.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.098 | tokens per gpu per second (tgs): 1990.270 | TFLOPs: 16.02 |
g0220: [2024-08-09 20:32:49,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=12700, skipped=14, lr=[0.00019999942484648473, 0.00019999942484648473], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12700 loss: 0.9626 iter time (s): 4.086 samples/sec: 31.325
g0238:  iteration    12700/10000000 | consumed samples:      1625600 | consumed tokens:   3329228800 | elapsed time per iteration (ms): 4118.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.418979E-01 | loss scale: 131072.0 | grad norm: 0.351 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.076 | tokens per gpu per second (tgs): 1988.888 | TFLOPs: 16.00 |
g0220: [2024-08-09 20:33:30,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=12710, skipped=14, lr=[0.00019999941564350816, 0.00019999941564350816], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12710 loss: 0.9651 iter time (s): 4.105 samples/sec: 31.179
g0238:  iteration    12710/10000000 | consumed samples:      1626880 | consumed tokens:   3331850240 | elapsed time per iteration (ms): 4138.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.432983E-01 | loss scale: 131072.0 | grad norm: 0.637 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.928 | tokens per gpu per second (tgs): 1979.361 | TFLOPs: 15.93 |
g0220: [2024-08-09 20:34:13,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=12720, skipped=14, lr=[0.0001999994063674871, 0.0001999994063674871], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12720 loss: 0.9582 iter time (s): 4.261 samples/sec: 30.036
g0238:  iteration    12720/10000000 | consumed samples:      1628160 | consumed tokens:   3334471680 | elapsed time per iteration (ms): 4294.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.395449E-01 | loss scale: 131072.0 | grad norm: 0.269 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.807 | tokens per gpu per second (tgs): 1907.660 | TFLOPs: 15.35 |
g0220: [2024-08-09 20:34:55,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=12730, skipped=14, lr=[0.00019999939701842147, 0.00019999939701842147], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12730 loss: 0.9391 iter time (s): 4.194 samples/sec: 30.519
g0238:  iteration    12730/10000000 | consumed samples:      1629440 | consumed tokens:   3337093120 | elapsed time per iteration (ms): 4227.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.433157E-01 | loss scale: 131072.0 | grad norm: 0.300 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.281 | tokens per gpu per second (tgs): 1938.002 | TFLOPs: 15.60 |
g0220: [2024-08-09 20:35:37,705] [INFO] [logging.py:96:log_dist] [Rank 0] step=12740, skipped=14, lr=[0.0001999993875963114, 0.0001999993875963114], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12740 loss: 0.9353 iter time (s): 4.161 samples/sec: 30.761
g0238:  iteration    12740/10000000 | consumed samples:      1630720 | consumed tokens:   3339714560 | elapsed time per iteration (ms): 4193.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.381831E-01 | loss scale: 131072.0 | grad norm: 0.280 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.521 | tokens per gpu per second (tgs): 1953.321 | TFLOPs: 15.72 |
g0220: [2024-08-09 20:36:18,526] [INFO] [logging.py:96:log_dist] [Rank 0] step=12750, skipped=14, lr=[0.0001999993781011568, 0.0001999993781011568], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12750 loss: 0.9684 iter time (s): 4.050 samples/sec: 31.608
g0238:  iteration    12750/10000000 | consumed samples:      1632000 | consumed tokens:   3342336000 | elapsed time per iteration (ms): 4082.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.349969E-01 | loss scale: 131072.0 | grad norm: 0.361 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.357 | tokens per gpu per second (tgs): 2006.868 | TFLOPs: 16.15 |
g0220: [2024-08-09 20:36:58,857] [INFO] [logging.py:96:log_dist] [Rank 0] step=12760, skipped=14, lr=[0.00019999936853295773, 0.00019999936853295773], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12760 loss: 0.9611 iter time (s): 4.000 samples/sec: 31.997
g0238:  iteration    12760/10000000 | consumed samples:      1633280 | consumed tokens:   3344957440 | elapsed time per iteration (ms): 4033.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.434942E-01 | loss scale: 131072.0 | grad norm: 0.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.737 | tokens per gpu per second (tgs): 2031.185 | TFLOPs: 16.35 |
g0220: [2024-08-09 20:37:40,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=12770, skipped=14, lr=[0.00019999935889171414, 0.00019999935889171414], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12770 loss: 0.9353 iter time (s): 4.118 samples/sec: 31.079
g0238:  iteration    12770/10000000 | consumed samples:      1634560 | consumed tokens:   3347578880 | elapsed time per iteration (ms): 4151.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.390022E-01 | loss scale: 131072.0 | grad norm: 0.255 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.836 | tokens per gpu per second (tgs): 1973.474 | TFLOPs: 15.88 |
g0220: [2024-08-09 20:38:24,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=12780, skipped=14, lr=[0.00019999934917742612, 0.00019999934917742612], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12780 loss: 0.9025 iter time (s): 4.396 samples/sec: 29.119
g0238:  iteration    12780/10000000 | consumed samples:      1635840 | consumed tokens:   3350200320 | elapsed time per iteration (ms): 4445.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.289275E-01 | loss scale: 131072.0 | grad norm: 0.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.791 | tokens per gpu per second (tgs): 1842.625 | TFLOPs: 14.83 |
g0220: [2024-08-09 20:39:06,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=12790, skipped=14, lr=[0.00019999933939009363, 0.00019999933939009363], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12790 loss: 0.9393 iter time (s): 4.113 samples/sec: 31.118
g0238:  iteration    12790/10000000 | consumed samples:      1637120 | consumed tokens:   3352821760 | elapsed time per iteration (ms): 4165.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.324649E-01 | loss scale: 131072.0 | grad norm: 0.257 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.730 | tokens per gpu per second (tgs): 1966.695 | TFLOPs: 15.83 |
g0220: [2024-08-09 20:39:46,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=12800, skipped=14, lr=[0.00019999932952971666, 0.00019999932952971666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12800 loss: 0.9329 iter time (s): 4.004 samples/sec: 31.970
g0238:  iteration    12800/10000000 | consumed samples:      1638400 | consumed tokens:   3355443200 | elapsed time per iteration (ms): 4037.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.349641E-01 | loss scale: 131072.0 | grad norm: 0.363 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.705 | tokens per gpu per second (tgs): 2029.141 | TFLOPs: 16.33 |
g0220: [2024-08-09 20:40:29,714] [INFO] [logging.py:96:log_dist] [Rank 0] step=12810, skipped=14, lr=[0.00019999931959629525, 0.00019999931959629525], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12810 loss: 0.9286 iter time (s): 4.254 samples/sec: 30.093
g0238:  iteration    12810/10000000 | consumed samples:      1639680 | consumed tokens:   3358064640 | elapsed time per iteration (ms): 4286.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.388010E-01 | loss scale: 131072.0 | grad norm: 0.965 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.864 | tokens per gpu per second (tgs): 1911.290 | TFLOPs: 15.38 |
g0220: [2024-08-09 20:41:10,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=12820, skipped=14, lr=[0.0001999993095898294, 0.0001999993095898294], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12820 loss: 0.9316 iter time (s): 4.085 samples/sec: 31.338
g0238:  iteration    12820/10000000 | consumed samples:      1640960 | consumed tokens:   3360686080 | elapsed time per iteration (ms): 4116.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.343960E-01 | loss scale: 131072.0 | grad norm: 0.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.091 | tokens per gpu per second (tgs): 1989.851 | TFLOPs: 16.01 |
g0220: [2024-08-09 20:41:54,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=12830, skipped=14, lr=[0.00019999929951031913, 0.00019999929951031913], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12830 loss: 0.9136 iter time (s): 4.281 samples/sec: 29.900
g0238:  iteration    12830/10000000 | consumed samples:      1642240 | consumed tokens:   3363307520 | elapsed time per iteration (ms): 4313.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.275687E-01 | loss scale: 131072.0 | grad norm: 0.246 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.674 | tokens per gpu per second (tgs): 1899.128 | TFLOPs: 15.28 |
g0220: [2024-08-09 20:42:38,484] [INFO] [logging.py:96:log_dist] [Rank 0] step=12840, skipped=14, lr=[0.0001999992893577644, 0.0001999992893577644], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12840 loss: 0.9637 iter time (s): 4.414 samples/sec: 28.998
g0238:  iteration    12840/10000000 | consumed samples:      1643520 | consumed tokens:   3365928960 | elapsed time per iteration (ms): 4446.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.407434E-01 | loss scale: 131072.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.786 | tokens per gpu per second (tgs): 1842.336 | TFLOPs: 14.83 |
g0220: [2024-08-09 20:43:20,057] [INFO] [logging.py:96:log_dist] [Rank 0] step=12850, skipped=14, lr=[0.00019999927913216524, 0.00019999927913216524], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12850 loss: 0.9246 iter time (s): 4.125 samples/sec: 31.033
g0238:  iteration    12850/10000000 | consumed samples:      1644800 | consumed tokens:   3368550400 | elapsed time per iteration (ms): 4157.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.311872E-01 | loss scale: 131072.0 | grad norm: 0.269 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.788 | tokens per gpu per second (tgs): 1970.440 | TFLOPs: 15.86 |
g0220: [2024-08-09 20:44:01,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=12860, skipped=14, lr=[0.00019999926883352168, 0.00019999926883352168], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12860 loss: 0.9595 iter time (s): 4.148 samples/sec: 30.858
g0238:  iteration    12860/10000000 | consumed samples:      1646080 | consumed tokens:   3371171840 | elapsed time per iteration (ms): 4180.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.332678E-01 | loss scale: 131072.0 | grad norm: 0.255 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.617 | tokens per gpu per second (tgs): 1959.506 | TFLOPs: 15.77 |
g0220: [2024-08-09 20:44:42,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=12870, skipped=14, lr=[0.00019999925846183372, 0.00019999925846183372], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12870 loss: 0.9078 iter time (s): 4.030 samples/sec: 31.762
g0238:  iteration    12870/10000000 | consumed samples:      1647360 | consumed tokens:   3373793280 | elapsed time per iteration (ms): 4062.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.347649E-01 | loss scale: 131072.0 | grad norm: 0.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.507 | tokens per gpu per second (tgs): 2016.420 | TFLOPs: 16.23 |
g0220: [2024-08-09 20:45:24,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=12880, skipped=14, lr=[0.0001999992480171014, 0.0001999992480171014], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12880 loss: 0.9048 iter time (s): 4.211 samples/sec: 30.395
g0238:  iteration    12880/10000000 | consumed samples:      1648640 | consumed tokens:   3376414720 | elapsed time per iteration (ms): 4243.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.300922E-01 | loss scale: 131072.0 | grad norm: 0.363 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.162 | tokens per gpu per second (tgs): 1930.359 | TFLOPs: 15.53 |
g0220: [2024-08-09 20:46:07,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=12890, skipped=14, lr=[0.00019999923749932464, 0.00019999923749932464], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12890 loss: 0.9721 iter time (s): 4.212 samples/sec: 30.390
g0238:  iteration    12890/10000000 | consumed samples:      1649920 | consumed tokens:   3379036160 | elapsed time per iteration (ms): 4244.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.427604E-01 | loss scale: 131072.0 | grad norm: 0.265 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.159 | tokens per gpu per second (tgs): 1930.164 | TFLOPs: 15.53 |
g0220: [2024-08-09 20:46:47,979] [INFO] [logging.py:96:log_dist] [Rank 0] step=12900, skipped=14, lr=[0.00019999922690850354, 0.00019999922690850354], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12900 loss: 0.9253 iter time (s): 4.028 samples/sec: 31.775
g0238:  iteration    12900/10000000 | consumed samples:      1651200 | consumed tokens:   3381657600 | elapsed time per iteration (ms): 4060.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.417604E-01 | loss scale: 131072.0 | grad norm: 0.260 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.522 | tokens per gpu per second (tgs): 2017.421 | TFLOPs: 16.23 |
g0220: [2024-08-09 20:47:28,239] [INFO] [logging.py:96:log_dist] [Rank 0] step=12910, skipped=14, lr=[0.00019999921624463802, 0.00019999921624463802], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12910 loss: 0.9366 iter time (s): 3.994 samples/sec: 32.050
g0238:  iteration    12910/10000000 | consumed samples:      1652480 | consumed tokens:   3384279040 | elapsed time per iteration (ms): 4026.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.301944E-01 | loss scale: 131072.0 | grad norm: 0.306 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.793 | tokens per gpu per second (tgs): 2034.756 | TFLOPs: 16.37 |
g0220: [2024-08-09 20:48:07,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=12920, skipped=14, lr=[0.00019999920550772818, 0.00019999920550772818], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12920 loss: 0.9301 iter time (s): 3.859 samples/sec: 33.172
g0238:  iteration    12920/10000000 | consumed samples:      1653760 | consumed tokens:   3386900480 | elapsed time per iteration (ms): 3891.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.301639E-01 | loss scale: 131072.0 | grad norm: 0.252 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.895 | tokens per gpu per second (tgs): 2105.284 | TFLOPs: 16.94 |
g0220: [2024-08-09 20:48:48,144] [INFO] [logging.py:96:log_dist] [Rank 0] step=12930, skipped=14, lr=[0.00019999919469777396, 0.00019999919469777396], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12930 loss: 0.9345 iter time (s): 4.067 samples/sec: 31.475
g0238:  iteration    12930/10000000 | consumed samples:      1655040 | consumed tokens:   3389521920 | elapsed time per iteration (ms): 4099.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.338718E-01 | loss scale: 131072.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.220 | tokens per gpu per second (tgs): 1998.096 | TFLOPs: 16.08 |
g0220: [2024-08-09 20:49:30,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=12940, skipped=14, lr=[0.00019999918381477538, 0.00019999918381477538], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12940 loss: 0.9116 iter time (s): 4.211 samples/sec: 30.399
g0238:  iteration    12940/10000000 | consumed samples:      1656320 | consumed tokens:   3392143360 | elapsed time per iteration (ms): 4243.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.365291E-01 | loss scale: 131072.0 | grad norm: 0.236 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.162 | tokens per gpu per second (tgs): 1930.392 | TFLOPs: 15.53 |
g0220: [2024-08-09 20:50:11,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=12950, skipped=14, lr=[0.0001999991728587325, 0.0001999991728587325], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12950 loss: 0.9508 iter time (s): 4.060 samples/sec: 31.527
g0238:  iteration    12950/10000000 | consumed samples:      1657600 | consumed tokens:   3394764800 | elapsed time per iteration (ms): 4093.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.348688E-01 | loss scale: 131072.0 | grad norm: 0.307 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.273 | tokens per gpu per second (tgs): 2001.446 | TFLOPs: 16.11 |
g0220: [2024-08-09 20:50:53,118] [INFO] [logging.py:96:log_dist] [Rank 0] step=12960, skipped=14, lr=[0.00019999916182964527, 0.00019999916182964527], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12960 loss: 0.8932 iter time (s): 4.126 samples/sec: 31.020
g0238:  iteration    12960/10000000 | consumed samples:      1658880 | consumed tokens:   3397386240 | elapsed time per iteration (ms): 4160.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.290789E-01 | loss scale: 131072.0 | grad norm: 0.257 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.769 | tokens per gpu per second (tgs): 1969.213 | TFLOPs: 15.85 |
g0220: [2024-08-09 20:51:34,016] [INFO] [logging.py:96:log_dist] [Rank 0] step=12970, skipped=14, lr=[0.0001999991507275137, 0.0001999991507275137], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12970 loss: 0.9409 iter time (s): 4.057 samples/sec: 31.552
g0238:  iteration    12970/10000000 | consumed samples:      1660160 | consumed tokens:   3400007680 | elapsed time per iteration (ms): 4089.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.455272E-01 | loss scale: 131072.0 | grad norm: 0.281 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.298 | tokens per gpu per second (tgs): 2003.042 | TFLOPs: 16.12 |
g0220: [2024-08-09 20:52:14,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=12980, skipped=14, lr=[0.00019999913955233783, 0.00019999913955233783], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12980 loss: 0.9013 iter time (s): 4.052 samples/sec: 31.592
g0238:  iteration    12980/10000000 | consumed samples:      1661440 | consumed tokens:   3402629120 | elapsed time per iteration (ms): 4084.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.222502E-01 | loss scale: 131072.0 | grad norm: 0.244 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.339 | tokens per gpu per second (tgs): 2005.686 | TFLOPs: 16.14 |
g0220: [2024-08-09 20:52:55,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=12990, skipped=14, lr=[0.00019999912830411765, 0.00019999912830411765], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 12990 loss: 0.9065 iter time (s): 3.983 samples/sec: 32.135
g0238:  iteration    12990/10000000 | consumed samples:      1662720 | consumed tokens:   3405250560 | elapsed time per iteration (ms): 4015.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.244722E-01 | loss scale: 131072.0 | grad norm: 0.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.875 | tokens per gpu per second (tgs): 2040.020 | TFLOPs: 16.42 |
g0220: [2024-08-09 20:53:36,182] [INFO] [logging.py:96:log_dist] [Rank 0] step=13000, skipped=14, lr=[0.0001999991169828532, 0.0001999991169828532], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13000 loss: 0.9443 iter time (s): 4.084 samples/sec: 31.343
g0238:  iteration    13000/10000000 | consumed samples:      1664000 | consumed tokens:   3407872000 | elapsed time per iteration (ms): 4116.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.379400E-01 | loss scale: 131072.0 | grad norm: 0.291 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.095 | tokens per gpu per second (tgs): 1990.061 | TFLOPs: 16.01 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 13000 | lm loss value: 9.334614E-01 | lm loss PPL: 2.543297E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   13000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-09 21:00:14,677] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step13000 is about to be saved!
g0238: [2024-08-09 21:00:14,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0238: [2024-08-09 21:00:14,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0238: [2024-08-09 21:00:14,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0234: [2024-08-09 21:00:14,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0234: [2024-08-09 21:00:14,683] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0234: [2024-08-09 21:00:14,684] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0220: [2024-08-09 21:00:14,684] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0220: [2024-08-09 21:00:14,684] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0220: [2024-08-09 21:00:14,684] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0236: [2024-08-09 21:00:14,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0236: [2024-08-09 21:00:14,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0236: [2024-08-09 21:00:14,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0233: [2024-08-09 21:00:14,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0233: [2024-08-09 21:00:14,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0233: [2024-08-09 21:00:14,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0237: [2024-08-09 21:00:14,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0237: [2024-08-09 21:00:14,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0225: [2024-08-09 21:00:14,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0225: [2024-08-09 21:00:14,685] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0235: [2024-08-09 21:00:14,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0235: [2024-08-09 21:00:14,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0237: [2024-08-09 21:00:14,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0225: [2024-08-09 21:00:14,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0235: [2024-08-09 21:00:14,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0238: [2024-08-09 21:00:14,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_23-model_00-model_states.pt...
g0233: [2024-08-09 21:00:14,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_08-model_00-model_states.pt...
g0234: [2024-08-09 21:00:14,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_11-model_00-model_states.pt...
g0237: [2024-08-09 21:00:14,723] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_20-model_00-model_states.pt...
g0236: [2024-08-09 21:00:14,723] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_17-model_00-model_states.pt...
g0225: [2024-08-09 21:00:14,724] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_05-model_00-model_states.pt...
g0235: [2024-08-09 21:00:14,724] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_14-model_00-model_states.pt...
g0220: [2024-08-09 21:00:14,737] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_01-model_00-model_states.pt...
g0234: [2024-08-09 21:00:14,828] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_11-model_00-model_states.pt.
g0237: [2024-08-09 21:00:14,862] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_20-model_00-model_states.pt.
g0225: [2024-08-09 21:00:14,863] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_05-model_00-model_states.pt.
g0234: [2024-08-09 21:00:14,868] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_12-model_00-model_states.pt...
g0238: [2024-08-09 21:00:14,870] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_23-model_00-model_states.pt.
g0233: [2024-08-09 21:00:14,871] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_08-model_00-model_states.pt.
g0238: [2024-08-09 21:00:14,871] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_24-model_00-model_states.pt...
g0236: [2024-08-09 21:00:14,871] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_17-model_00-model_states.pt.
g0238: [2024-08-09 21:00:14,873] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_24-model_00-model_states.pt.
g0235: [2024-08-09 21:00:14,880] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_14-model_00-model_states.pt.
g0237: [2024-08-09 21:00:14,900] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_21-model_00-model_states.pt...
g0225: [2024-08-09 21:00:14,903] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_06-model_00-model_states.pt...
g0233: [2024-08-09 21:00:14,908] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_09-model_00-model_states.pt...
g0236: [2024-08-09 21:00:14,911] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_18-model_00-model_states.pt...
g0238: [2024-08-09 21:00:14,918] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_25-model_00-model_states.pt...
g0235: [2024-08-09 21:00:14,920] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_15-model_00-model_states.pt...
g0220: [2024-08-09 21:00:14,943] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_01-model_00-model_states.pt.
g0220: [2024-08-09 21:00:14,964] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_02-model_00-model_states.pt...
g0234: [2024-08-09 21:00:14,991] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_12-model_00-model_states.pt.
g0225: [2024-08-09 21:00:15,002] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_06-model_00-model_states.pt.
g0234: [2024-08-09 21:00:15,026] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_13-model_00-model_states.pt...
g0225: [2024-08-09 21:00:15,037] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_07-model_00-model_states.pt...
g0236: [2024-08-09 21:00:15,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_18-model_00-model_states.pt.
g0237: [2024-08-09 21:00:15,047] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_21-model_00-model_states.pt.
g0235: [2024-08-09 21:00:15,071] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_15-model_00-model_states.pt.
g0236: [2024-08-09 21:00:15,081] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_19-model_00-model_states.pt...
g0237: [2024-08-09 21:00:15,081] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_22-model_00-model_states.pt...
g0238: [2024-08-09 21:00:15,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_25-model_00-model_states.pt.
g0235: [2024-08-09 21:00:15,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_16-model_00-model_states.pt...
g0238: [2024-08-09 21:00:15,106] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_07_model_states.pt...
g0220: [2024-08-09 21:00:15,107] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_02-model_00-model_states.pt.
g0233: [2024-08-09 21:00:15,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_09-model_00-model_states.pt.
g0220: [2024-08-09 21:00:15,137] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_03-model_00-model_states.pt...
g0233: [2024-08-09 21:00:15,158] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_10-model_00-model_states.pt...
g0234: [2024-08-09 21:00:15,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 21:00:15,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_03_model_states.pt...
g0225: [2024-08-09 21:00:15,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 21:00:15,172] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_01_model_states.pt...
g0236: [2024-08-09 21:00:15,183] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 21:00:15,185] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_05_model_states.pt...
g0237: [2024-08-09 21:00:15,197] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 21:00:15,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_06_model_states.pt...
g0235: [2024-08-09 21:00:15,265] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 21:00:15,267] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_04_model_states.pt...
g0220: [2024-08-09 21:00:15,314] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 21:00:15,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_04-model_00-model_states.pt...
g0233: [2024-08-09 21:00:15,387] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 21:00:15,389] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_02_model_states.pt...
g0220: [2024-08-09 21:00:15,446] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 21:00:15,448] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_00_model_states.pt
g0220: [2024-08-09 21:00:15,449] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_00_model_states.pt...
g0238: [2024-08-09 21:00:17,012] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 21:00:17,012] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0234: [2024-08-09 21:00:17,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 21:00:17,510] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0225: [2024-08-09 21:00:17,525] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_01_model_states.pt.
g0225: [2024-08-09 21:00:17,526] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0237: [2024-08-09 21:00:17,545] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 21:00:17,545] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0235: [2024-08-09 21:00:17,670] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 21:00:17,670] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0236: [2024-08-09 21:00:17,673] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 21:00:17,674] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0233: [2024-08-09 21:00:17,857] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 21:00:17,858] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0220: [2024-08-09 21:00:18,966] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step13000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 21:00:18,966] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13000 is ready now!
g0220:   successfully saved checkpoint at iteration   13000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 5.18, Latency(second): 4.348
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (4348.14, 4348.31)
g0220: [2024-08-09 21:01:01,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=13010, skipped=14, lr=[0.00019999910558854444, 0.00019999910558854444], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13010 loss: 0.9400 iter time (s): 4.207 samples/sec: 30.425
g0238:  iteration    13010/10000000 | consumed samples:      1665280 | consumed tokens:   3410493440 | elapsed time per iteration (ms): 44519.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.320452E-01 | loss scale: 131072.0 | grad norm: 0.332 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.875 | tokens per gpu per second (tgs): 184.009 | TFLOPs: 1.48 |
g0234: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 21:01:32,536] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 21:01:32,538] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 21:01:32,538] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 21:01:32,538] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 21:01:32,537] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 21:01:32,538] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 21:01:32,538] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 21:01:32,538] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 21:01:46,724] [INFO] [logging.py:96:log_dist] [Rank 0] step=13020, skipped=14, lr=[0.0001999990941211914, 0.0001999990941211914], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13020 loss: 0.9433 iter time (s): 4.502 samples/sec: 28.433
g0238:  iteration    13020/10000000 | consumed samples:      1666560 | consumed tokens:   3413114880 | elapsed time per iteration (ms): 4534.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.313900E-01 | loss scale: 262144.0 | grad norm: 0.292 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.227 | tokens per gpu per second (tgs): 1806.505 | TFLOPs: 14.54 |
g0220: [2024-08-09 21:02:31,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=13030, skipped=14, lr=[0.0001999990825807941, 0.0001999990825807941], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13030 loss: 0.9311 iter time (s): 4.399 samples/sec: 29.100
g0238:  iteration    13030/10000000 | consumed samples:      1667840 | consumed tokens:   3415736320 | elapsed time per iteration (ms): 4431.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.284075E-01 | loss scale: 262144.0 | grad norm: 0.261 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.887 | tokens per gpu per second (tgs): 1848.777 | TFLOPs: 14.88 |
g0220: [2024-08-09 21:03:13,926] [INFO] [logging.py:96:log_dist] [Rank 0] step=13040, skipped=14, lr=[0.0001999990709673525, 0.0001999990709673525], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13040 loss: 0.9733 iter time (s): 4.256 samples/sec: 30.075
g0238:  iteration    13040/10000000 | consumed samples:      1669120 | consumed tokens:   3418357760 | elapsed time per iteration (ms): 4289.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.436838E-01 | loss scale: 262144.0 | grad norm: 0.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.844 | tokens per gpu per second (tgs): 1909.984 | TFLOPs: 15.37 |
g0220: [2024-08-09 21:03:55,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=13050, skipped=14, lr=[0.00019999905928086672, 0.00019999905928086672], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13050 loss: 0.9503 iter time (s): 4.173 samples/sec: 30.671
g0238:  iteration    13050/10000000 | consumed samples:      1670400 | consumed tokens:   3420979200 | elapsed time per iteration (ms): 4206.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.313528E-01 | loss scale: 262144.0 | grad norm: 0.277 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.433 | tokens per gpu per second (tgs): 1947.713 | TFLOPs: 15.67 |
g0220: [2024-08-09 21:04:37,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=13060, skipped=14, lr=[0.00019999904752133666, 0.00019999904752133666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13060 loss: 0.9001 iter time (s): 4.103 samples/sec: 31.198
g0238:  iteration    13060/10000000 | consumed samples:      1671680 | consumed tokens:   3423600640 | elapsed time per iteration (ms): 4136.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.270752E-01 | loss scale: 262144.0 | grad norm: 0.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.945 | tokens per gpu per second (tgs): 1980.466 | TFLOPs: 15.94 |
g0220: [2024-08-09 21:05:21,929] [INFO] [logging.py:96:log_dist] [Rank 0] step=13070, skipped=14, lr=[0.00019999903568876235, 0.00019999903568876235], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13070 loss: 0.9394 iter time (s): 4.425 samples/sec: 28.928
g0238:  iteration    13070/10000000 | consumed samples:      1672960 | consumed tokens:   3426222080 | elapsed time per iteration (ms): 4458.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.332584E-01 | loss scale: 262144.0 | grad norm: 0.270 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.713 | tokens per gpu per second (tgs): 1837.603 | TFLOPs: 14.79 |
g0220: [2024-08-09 21:06:06,152] [INFO] [logging.py:96:log_dist] [Rank 0] step=13080, skipped=14, lr=[0.00019999902378314382, 0.00019999902378314382], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13080 loss: 0.9486 iter time (s): 4.390 samples/sec: 29.160
g0238:  iteration    13080/10000000 | consumed samples:      1674240 | consumed tokens:   3428843520 | elapsed time per iteration (ms): 4422.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.296425E-01 | loss scale: 262144.0 | grad norm: 0.253 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.945 | tokens per gpu per second (tgs): 1852.450 | TFLOPs: 14.91 |
g0220: [2024-08-09 21:06:53,364] [INFO] [logging.py:96:log_dist] [Rank 0] step=13090, skipped=14, lr=[0.00019999901180448108, 0.00019999901180448108], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13090 loss: 0.9223 iter time (s): 4.688 samples/sec: 27.304
g0238:  iteration    13090/10000000 | consumed samples:      1675520 | consumed tokens:   3431464960 | elapsed time per iteration (ms): 4721.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.237499E-01 | loss scale: 262144.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.111 | tokens per gpu per second (tgs): 1735.133 | TFLOPs: 13.96 |
g0220: [2024-08-09 21:07:40,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=13100, skipped=14, lr=[0.00019999899975277414, 0.00019999899975277414], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13100 loss: 0.9243 iter time (s): 4.690 samples/sec: 27.291
g0238:  iteration    13100/10000000 | consumed samples:      1676800 | consumed tokens:   3434086400 | elapsed time per iteration (ms): 4722.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.259236E-01 | loss scale: 262144.0 | grad norm: 0.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 27.103 | tokens per gpu per second (tgs): 1734.610 | TFLOPs: 13.96 |
g0220: [2024-08-09 21:08:22,552] [INFO] [logging.py:96:log_dist] [Rank 0] step=13110, skipped=14, lr=[0.000199998987628023, 0.000199998987628023], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13110 loss: 0.9209 iter time (s): 4.163 samples/sec: 30.750
g0238:  iteration    13110/10000000 | consumed samples:      1678080 | consumed tokens:   3436707840 | elapsed time per iteration (ms): 4196.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.385182E-01 | loss scale: 262144.0 | grad norm: 0.275 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.503 | tokens per gpu per second (tgs): 1952.223 | TFLOPs: 15.71 |
g0220: [2024-08-09 21:09:05,315] [INFO] [logging.py:96:log_dist] [Rank 0] step=13120, skipped=14, lr=[0.00019999897543022764, 0.00019999897543022764], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13120 loss: 0.9408 iter time (s): 4.244 samples/sec: 30.164
g0238:  iteration    13120/10000000 | consumed samples:      1679360 | consumed tokens:   3439329280 | elapsed time per iteration (ms): 4276.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.269654E-01 | loss scale: 262144.0 | grad norm: 0.290 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.934 | tokens per gpu per second (tgs): 1915.773 | TFLOPs: 15.42 |
g0220: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 13124
g0220: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 13124
g0220: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 13124
g0225: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 13124
g0225: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 13124
g0225: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 13124
g0225: Grad overflow on iteration 13124
g0236: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 13124
g0236: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 13124
g0225: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: Grad overflow on iteration 13124
g0236: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 13124
g0235: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 13124
g0236: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 13124
g0235: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: Grad overflow on iteration 13124
g0233: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 13124
g0235: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: Grad overflow on iteration 13124
g0220: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 13124
g0235: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 13124
g0220: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 13124
g0233: Grad overflow on iteration 13124
g0220: Grad overflow on iteration 13124
g0237: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 13124
g0238: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 13124
g0234: Grad overflow on iteration 13124
g0238: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 13124
g0237: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 13124
g0234: Grad overflow on iteration 13124
g0238: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 13124
g0233: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: Grad overflow on iteration 13124
g0234: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 13124
g0234: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 21:09:26,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 13124
g0237: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 13124
g0238: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 21:09:26,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 21:09:26,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 21:09:26,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 21:09:26,581] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 21:09:26,582] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0220: [2024-08-09 21:09:48,027] [INFO] [logging.py:96:log_dist] [Rank 0] step=13130, skipped=15, lr=[0.00019999896315938813, 0.00019999896315938813], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13130 loss: 0.9430 iter time (s): 4.238 samples/sec: 30.199
g0238:  iteration    13130/10000000 | consumed samples:      1680640 | consumed tokens:   3441950720 | elapsed time per iteration (ms): 4271.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.276389E-01 | loss scale: 131072.0 | grad norm: 0.277 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.968 | tokens per gpu per second (tgs): 1917.945 | TFLOPs: 15.43 |
g0220: [2024-08-09 21:10:31,668] [INFO] [logging.py:96:log_dist] [Rank 0] step=13140, skipped=15, lr=[0.00019999895081550446, 0.00019999895081550446], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13140 loss: 0.9158 iter time (s): 4.330 samples/sec: 29.559
g0238:  iteration    13140/10000000 | consumed samples:      1681920 | consumed tokens:   3444572160 | elapsed time per iteration (ms): 4364.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.230171E-01 | loss scale: 131072.0 | grad norm: 0.258 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.326 | tokens per gpu per second (tgs): 1876.859 | TFLOPs: 15.10 |
g0220: [2024-08-09 21:11:14,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=13150, skipped=15, lr=[0.00019999893839857662, 0.00019999893839857662], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13150 loss: 0.9570 iter time (s): 4.213 samples/sec: 30.385
g0238:  iteration    13150/10000000 | consumed samples:      1683200 | consumed tokens:   3447193600 | elapsed time per iteration (ms): 4245.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.373343E-01 | loss scale: 131072.0 | grad norm: 0.285 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.153 | tokens per gpu per second (tgs): 1929.801 | TFLOPs: 15.53 |
g0220: [2024-08-09 21:11:53,476] [INFO] [logging.py:96:log_dist] [Rank 0] step=13160, skipped=15, lr=[0.00019999892590860462, 0.00019999892590860462], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13160 loss: 0.9365 iter time (s): 3.902 samples/sec: 32.802
g0238:  iteration    13160/10000000 | consumed samples:      1684480 | consumed tokens:   3449815040 | elapsed time per iteration (ms): 3935.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.214705E-01 | loss scale: 131072.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.528 | tokens per gpu per second (tgs): 2081.794 | TFLOPs: 16.75 |
g0220: [2024-08-09 21:12:34,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=13170, skipped=15, lr=[0.00019999891334558848, 0.00019999891334558848], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13170 loss: 0.9578 iter time (s): 4.119 samples/sec: 31.073
g0238:  iteration    13170/10000000 | consumed samples:      1685760 | consumed tokens:   3452436480 | elapsed time per iteration (ms): 4152.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.250442E-01 | loss scale: 131072.0 | grad norm: 0.281 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.828 | tokens per gpu per second (tgs): 1973.016 | TFLOPs: 15.88 |
g0220: [2024-08-09 21:13:16,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=13180, skipped=15, lr=[0.00019999890070952823, 0.00019999890070952823], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13180 loss: 0.9608 iter time (s): 4.132 samples/sec: 30.976
g0238:  iteration    13180/10000000 | consumed samples:      1687040 | consumed tokens:   3455057920 | elapsed time per iteration (ms): 4164.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.304878E-01 | loss scale: 131072.0 | grad norm: 0.254 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.734 | tokens per gpu per second (tgs): 1967.002 | TFLOPs: 15.83 |
g0220: [2024-08-09 21:14:00,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=13190, skipped=15, lr=[0.00019999888800042385, 0.00019999888800042385], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13190 loss: 0.9714 iter time (s): 4.341 samples/sec: 29.486
g0238:  iteration    13190/10000000 | consumed samples:      1688320 | consumed tokens:   3457679360 | elapsed time per iteration (ms): 4373.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.290092E-01 | loss scale: 131072.0 | grad norm: 0.285 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.266 | tokens per gpu per second (tgs): 1873.055 | TFLOPs: 15.07 |
g0220: [2024-08-09 21:14:43,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=13200, skipped=15, lr=[0.00019999887521827535, 0.00019999887521827535], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13200 loss: 0.9568 iter time (s): 4.258 samples/sec: 30.062
g0238:  iteration    13200/10000000 | consumed samples:      1689600 | consumed tokens:   3460300800 | elapsed time per iteration (ms): 4291.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.306696E-01 | loss scale: 131072.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.830 | tokens per gpu per second (tgs): 1909.089 | TFLOPs: 15.36 |
g0220: [2024-08-09 21:15:23,418] [INFO] [logging.py:96:log_dist] [Rank 0] step=13210, skipped=15, lr=[0.00019999886236308272, 0.00019999886236308272], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13210 loss: 0.9291 iter time (s): 3.980 samples/sec: 32.161
g0238:  iteration    13210/10000000 | consumed samples:      1690880 | consumed tokens:   3462922240 | elapsed time per iteration (ms): 4012.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.314714E-01 | loss scale: 131072.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.898 | tokens per gpu per second (tgs): 2041.463 | TFLOPs: 16.43 |
g0220: [2024-08-09 21:16:05,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=13220, skipped=15, lr=[0.00019999884943484606, 0.00019999884943484606], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13220 loss: 0.9024 iter time (s): 4.212 samples/sec: 30.389
g0238:  iteration    13220/10000000 | consumed samples:      1692160 | consumed tokens:   3465543680 | elapsed time per iteration (ms): 4245.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.221784E-01 | loss scale: 131072.0 | grad norm: 0.254 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.153 | tokens per gpu per second (tgs): 1929.783 | TFLOPs: 15.53 |
g0220: [2024-08-09 21:16:47,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=13230, skipped=15, lr=[0.00019999883643356526, 0.00019999883643356526], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13230 loss: 0.9182 iter time (s): 4.144 samples/sec: 30.887
g0238:  iteration    13230/10000000 | consumed samples:      1693440 | consumed tokens:   3468165120 | elapsed time per iteration (ms): 4176.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.355558E-01 | loss scale: 131072.0 | grad norm: 0.261 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.647 | tokens per gpu per second (tgs): 1961.381 | TFLOPs: 15.78 |
g0220: [2024-08-09 21:17:30,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=13240, skipped=15, lr=[0.00019999882335924044, 0.00019999882335924044], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13240 loss: 0.9316 iter time (s): 4.230 samples/sec: 30.263
g0238:  iteration    13240/10000000 | consumed samples:      1694720 | consumed tokens:   3470786560 | elapsed time per iteration (ms): 4262.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.250603E-01 | loss scale: 131072.0 | grad norm: 0.307 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.031 | tokens per gpu per second (tgs): 1921.954 | TFLOPs: 15.47 |
g0220: [2024-08-09 21:18:10,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=13250, skipped=15, lr=[0.00019999881021187153, 0.00019999881021187153], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13250 loss: 0.9479 iter time (s): 3.983 samples/sec: 32.140
g0238:  iteration    13250/10000000 | consumed samples:      1696000 | consumed tokens:   3473408000 | elapsed time per iteration (ms): 4015.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.340757E-01 | loss scale: 131072.0 | grad norm: 0.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.876 | tokens per gpu per second (tgs): 2040.088 | TFLOPs: 16.42 |
g0220: [2024-08-09 21:18:51,919] [INFO] [logging.py:96:log_dist] [Rank 0] step=13260, skipped=15, lr=[0.00019999879699145857, 0.00019999879699145857], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13260 loss: 0.9084 iter time (s): 4.117 samples/sec: 31.089
g0238:  iteration    13260/10000000 | consumed samples:      1697280 | consumed tokens:   3476029440 | elapsed time per iteration (ms): 4150.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.304907E-01 | loss scale: 131072.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.840 | tokens per gpu per second (tgs): 1973.762 | TFLOPs: 15.88 |
g0220: [2024-08-09 21:19:31,741] [INFO] [logging.py:96:log_dist] [Rank 0] step=13270, skipped=15, lr=[0.00019999878369800157, 0.00019999878369800157], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13270 loss: 0.9188 iter time (s): 3.950 samples/sec: 32.406
g0238:  iteration    13270/10000000 | consumed samples:      1698560 | consumed tokens:   3478650880 | elapsed time per iteration (ms): 3982.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.289497E-01 | loss scale: 131072.0 | grad norm: 0.238 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.143 | tokens per gpu per second (tgs): 2057.151 | TFLOPs: 16.55 |
g0220: [2024-08-09 21:20:13,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=13280, skipped=15, lr=[0.00019999877033150052, 0.00019999877033150052], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13280 loss: 0.8902 iter time (s): 4.161 samples/sec: 30.763
g0238:  iteration    13280/10000000 | consumed samples:      1699840 | consumed tokens:   3481272320 | elapsed time per iteration (ms): 4193.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.249306E-01 | loss scale: 131072.0 | grad norm: 0.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.522 | tokens per gpu per second (tgs): 1953.409 | TFLOPs: 15.72 |
g0220: [2024-08-09 21:20:56,911] [INFO] [logging.py:96:log_dist] [Rank 0] step=13290, skipped=15, lr=[0.00019999875689195547, 0.00019999875689195547], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13290 loss: 0.9154 iter time (s): 4.290 samples/sec: 29.834
g0238:  iteration    13290/10000000 | consumed samples:      1701120 | consumed tokens:   3483893760 | elapsed time per iteration (ms): 4323.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.157243E-01 | loss scale: 131072.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.605 | tokens per gpu per second (tgs): 1894.747 | TFLOPs: 15.25 |
g0220: [2024-08-09 21:21:40,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=13300, skipped=15, lr=[0.0001999987433793664, 0.0001999987433793664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13300 loss: 0.9112 iter time (s): 4.319 samples/sec: 29.638
g0238:  iteration    13300/10000000 | consumed samples:      1702400 | consumed tokens:   3486515200 | elapsed time per iteration (ms): 4351.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.250374E-01 | loss scale: 131072.0 | grad norm: 0.260 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.414 | tokens per gpu per second (tgs): 1882.506 | TFLOPs: 15.15 |
g0220: [2024-08-09 21:22:20,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=13310, skipped=15, lr=[0.00019999872979373332, 0.00019999872979373332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13310 loss: 0.9176 iter time (s): 3.934 samples/sec: 32.538
g0238:  iteration    13310/10000000 | consumed samples:      1703680 | consumed tokens:   3489136640 | elapsed time per iteration (ms): 3966.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.243208E-01 | loss scale: 131072.0 | grad norm: 0.263 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.272 | tokens per gpu per second (tgs): 2065.385 | TFLOPs: 16.62 |
g0220: [2024-08-09 21:23:01,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=13320, skipped=15, lr=[0.00019999871613505626, 0.00019999871613505626], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13320 loss: 0.9406 iter time (s): 4.083 samples/sec: 31.347
g0238:  iteration    13320/10000000 | consumed samples:      1704960 | consumed tokens:   3491758080 | elapsed time per iteration (ms): 4117.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.234064E-01 | loss scale: 131072.0 | grad norm: 0.275 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.089 | tokens per gpu per second (tgs): 1989.716 | TFLOPs: 16.01 |
g0220: [2024-08-09 21:23:42,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=13330, skipped=15, lr=[0.00019999870240333523, 0.00019999870240333523], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13330 loss: 0.9099 iter time (s): 4.086 samples/sec: 31.326
g0238:  iteration    13330/10000000 | consumed samples:      1706240 | consumed tokens:   3494379520 | elapsed time per iteration (ms): 4118.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.273490E-01 | loss scale: 131072.0 | grad norm: 0.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.077 | tokens per gpu per second (tgs): 1988.926 | TFLOPs: 16.01 |
g0220: [2024-08-09 21:24:24,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=13340, skipped=15, lr=[0.00019999868859857023, 0.00019999868859857023], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13340 loss: 0.9222 iter time (s): 4.153 samples/sec: 30.820
g0238:  iteration    13340/10000000 | consumed samples:      1707520 | consumed tokens:   3497000960 | elapsed time per iteration (ms): 4185.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.178569E-01 | loss scale: 131072.0 | grad norm: 0.295 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.581 | tokens per gpu per second (tgs): 1957.190 | TFLOPs: 15.75 |
g0220: [2024-08-09 21:25:05,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=13350, skipped=15, lr=[0.00019999867472076128, 0.00019999867472076128], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13350 loss: 0.9282 iter time (s): 4.079 samples/sec: 31.380
g0238:  iteration    13350/10000000 | consumed samples:      1708800 | consumed tokens:   3499622400 | elapsed time per iteration (ms): 4111.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.242262E-01 | loss scale: 131072.0 | grad norm: 0.320 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.130 | tokens per gpu per second (tgs): 1992.329 | TFLOPs: 16.03 |
g0220: [2024-08-09 21:25:45,070] [INFO] [logging.py:96:log_dist] [Rank 0] step=13360, skipped=15, lr=[0.00019999866076990836, 0.00019999866076990836], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13360 loss: 0.9182 iter time (s): 3.931 samples/sec: 32.558
g0238:  iteration    13360/10000000 | consumed samples:      1710080 | consumed tokens:   3502243840 | elapsed time per iteration (ms): 3964.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.361548E-01 | loss scale: 131072.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.289 | tokens per gpu per second (tgs): 2066.483 | TFLOPs: 16.63 |
g0220: [2024-08-09 21:26:27,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=13370, skipped=15, lr=[0.00019999864674601151, 0.00019999864674601151], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13370 loss: 0.9282 iter time (s): 4.222 samples/sec: 30.317
g0238:  iteration    13370/10000000 | consumed samples:      1711360 | consumed tokens:   3504865280 | elapsed time per iteration (ms): 4254.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.091899E-01 | loss scale: 131072.0 | grad norm: 0.501 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.084 | tokens per gpu per second (tgs): 1925.345 | TFLOPs: 15.49 |
g0220: [2024-08-09 21:27:12,611] [INFO] [logging.py:96:log_dist] [Rank 0] step=13380, skipped=15, lr=[0.00019999863264907072, 0.00019999863264907072], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13380 loss: 0.8830 iter time (s): 4.467 samples/sec: 28.657
g0238:  iteration    13380/10000000 | consumed samples:      1712640 | consumed tokens:   3507486720 | elapsed time per iteration (ms): 4499.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.178437E-01 | loss scale: 131072.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.449 | tokens per gpu per second (tgs): 1820.720 | TFLOPs: 14.65 |
g0220: [2024-08-09 21:27:53,828] [INFO] [logging.py:96:log_dist] [Rank 0] step=13390, skipped=15, lr=[0.00019999861847908604, 0.00019999861847908604], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13390 loss: 0.9294 iter time (s): 4.089 samples/sec: 31.303
g0238:  iteration    13390/10000000 | consumed samples:      1713920 | consumed tokens:   3510108160 | elapsed time per iteration (ms): 4121.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.086114E-01 | loss scale: 131072.0 | grad norm: 0.256 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.055 | tokens per gpu per second (tgs): 1987.538 | TFLOPs: 15.99 |
g0220: [2024-08-09 21:28:33,886] [INFO] [logging.py:96:log_dist] [Rank 0] step=13400, skipped=15, lr=[0.00019999860423605743, 0.00019999860423605743], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13400 loss: 0.8882 iter time (s): 3.973 samples/sec: 32.219
g0238:  iteration    13400/10000000 | consumed samples:      1715200 | consumed tokens:   3512729600 | elapsed time per iteration (ms): 4005.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.211978E-01 | loss scale: 131072.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.954 | tokens per gpu per second (tgs): 2045.056 | TFLOPs: 16.46 |
g0220: [2024-08-09 21:29:16,589] [INFO] [logging.py:96:log_dist] [Rank 0] step=13410, skipped=15, lr=[0.00019999858991998496, 0.00019999858991998496], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13410 loss: 0.9280 iter time (s): 4.238 samples/sec: 30.205
g0238:  iteration    13410/10000000 | consumed samples:      1716480 | consumed tokens:   3515351040 | elapsed time per iteration (ms): 4270.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.177507E-01 | loss scale: 131072.0 | grad norm: 0.272 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.975 | tokens per gpu per second (tgs): 1918.394 | TFLOPs: 15.44 |
g0220: [2024-08-09 21:29:57,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=13420, skipped=15, lr=[0.0001999985755308686, 0.0001999985755308686], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13420 loss: 0.9509 iter time (s): 4.086 samples/sec: 31.328
g0238:  iteration    13420/10000000 | consumed samples:      1717760 | consumed tokens:   3517972480 | elapsed time per iteration (ms): 4119.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.131065E-01 | loss scale: 131072.0 | grad norm: 0.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.069 | tokens per gpu per second (tgs): 1988.390 | TFLOPs: 16.00 |
g0220: [2024-08-09 21:30:38,786] [INFO] [logging.py:96:log_dist] [Rank 0] step=13430, skipped=15, lr=[0.00019999856106870834, 0.00019999856106870834], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13430 loss: 0.9143 iter time (s): 4.067 samples/sec: 31.476
g0238:  iteration    13430/10000000 | consumed samples:      1719040 | consumed tokens:   3520593920 | elapsed time per iteration (ms): 4099.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.199717E-01 | loss scale: 131072.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.221 | tokens per gpu per second (tgs): 1998.158 | TFLOPs: 16.08 |
g0220: [2024-08-09 21:31:20,345] [INFO] [logging.py:96:log_dist] [Rank 0] step=13440, skipped=15, lr=[0.00019999854653350425, 0.00019999854653350425], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13440 loss: 0.9087 iter time (s): 4.123 samples/sec: 31.045
g0238:  iteration    13440/10000000 | consumed samples:      1720320 | consumed tokens:   3523215360 | elapsed time per iteration (ms): 4156.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.248595E-01 | loss scale: 131072.0 | grad norm: 0.401 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.799 | tokens per gpu per second (tgs): 1971.114 | TFLOPs: 15.86 |
g0220: [2024-08-09 21:32:02,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=13450, skipped=15, lr=[0.0001999985319252563, 0.0001999985319252563], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13450 loss: 0.9306 iter time (s): 4.177 samples/sec: 30.640
g0238:  iteration    13450/10000000 | consumed samples:      1721600 | consumed tokens:   3525836800 | elapsed time per iteration (ms): 4210.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.114777E-01 | loss scale: 131072.0 | grad norm: 0.282 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.404 | tokens per gpu per second (tgs): 1945.842 | TFLOPs: 15.66 |
g0220: [2024-08-09 21:32:42,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=13460, skipped=15, lr=[0.00019999851724396452, 0.00019999851724396452], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13460 loss: 0.9157 iter time (s): 3.998 samples/sec: 32.014
g0238:  iteration    13460/10000000 | consumed samples:      1722880 | consumed tokens:   3528458240 | elapsed time per iteration (ms): 4031.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.224008E-01 | loss scale: 131072.0 | grad norm: 0.639 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.750 | tokens per gpu per second (tgs): 2031.974 | TFLOPs: 16.35 |
g0220: [2024-08-09 21:33:23,466] [INFO] [logging.py:96:log_dist] [Rank 0] step=13470, skipped=15, lr=[0.00019999850248962888, 0.00019999850248962888], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13470 loss: 0.9142 iter time (s): 4.038 samples/sec: 31.700
g0238:  iteration    13470/10000000 | consumed samples:      1724160 | consumed tokens:   3531079680 | elapsed time per iteration (ms): 4070.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.161661E-01 | loss scale: 131072.0 | grad norm: 0.259 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.447 | tokens per gpu per second (tgs): 2012.582 | TFLOPs: 16.20 |
g0220: [2024-08-09 21:34:05,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=13480, skipped=15, lr=[0.00019999848766224945, 0.00019999848766224945], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13480 loss: 0.9091 iter time (s): 4.182 samples/sec: 30.611
g0238:  iteration    13480/10000000 | consumed samples:      1725440 | consumed tokens:   3533701120 | elapsed time per iteration (ms): 4214.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.156590E-01 | loss scale: 131072.0 | grad norm: 0.264 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.374 | tokens per gpu per second (tgs): 1943.957 | TFLOPs: 15.64 |
g0220: [2024-08-09 21:34:46,263] [INFO] [logging.py:96:log_dist] [Rank 0] step=13490, skipped=15, lr=[0.00019999847276182622, 0.00019999847276182622], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13490 loss: 0.9042 iter time (s): 4.033 samples/sec: 31.738
g0238:  iteration    13490/10000000 | consumed samples:      1726720 | consumed tokens:   3536322560 | elapsed time per iteration (ms): 4065.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.191775E-01 | loss scale: 131072.0 | grad norm: 0.251 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.484 | tokens per gpu per second (tgs): 2014.962 | TFLOPs: 16.21 |
g0220: [2024-08-09 21:35:28,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=13500, skipped=15, lr=[0.00019999845778835917, 0.00019999845778835917], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13500 loss: 0.8936 iter time (s): 4.238 samples/sec: 30.205
g0238:  iteration    13500/10000000 | consumed samples:      1728000 | consumed tokens:   3538944000 | elapsed time per iteration (ms): 4270.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.145887E-01 | loss scale: 131072.0 | grad norm: 0.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.971 | tokens per gpu per second (tgs): 1918.152 | TFLOPs: 15.44 |
g0220: [2024-08-09 21:36:11,047] [INFO] [logging.py:96:log_dist] [Rank 0] step=13510, skipped=15, lr=[0.00019999844274184837, 0.00019999844274184837], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13510 loss: 0.9420 iter time (s): 4.175 samples/sec: 30.660
g0238:  iteration    13510/10000000 | consumed samples:      1729280 | consumed tokens:   3541565440 | elapsed time per iteration (ms): 4207.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.173675E-01 | loss scale: 131072.0 | grad norm: 0.250 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.422 | tokens per gpu per second (tgs): 1946.995 | TFLOPs: 15.67 |
g0220: [2024-08-09 21:36:52,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=13520, skipped=15, lr=[0.00019999842762229378, 0.00019999842762229378], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13520 loss: 0.9415 iter time (s): 4.098 samples/sec: 31.236
g0238:  iteration    13520/10000000 | consumed samples:      1730560 | consumed tokens:   3544186880 | elapsed time per iteration (ms): 4130.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.261353E-01 | loss scale: 131072.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.987 | tokens per gpu per second (tgs): 1983.148 | TFLOPs: 15.96 |
g0220: [2024-08-09 21:37:34,419] [INFO] [logging.py:96:log_dist] [Rank 0] step=13530, skipped=15, lr=[0.00019999841242969546, 0.00019999841242969546], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13530 loss: 0.9166 iter time (s): 4.174 samples/sec: 30.669
g0238:  iteration    13530/10000000 | consumed samples:      1731840 | consumed tokens:   3546808320 | elapsed time per iteration (ms): 4206.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.184048E-01 | loss scale: 131072.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.428 | tokens per gpu per second (tgs): 1947.423 | TFLOPs: 15.67 |
g0220: [2024-08-09 21:38:16,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=13540, skipped=15, lr=[0.00019999839716405336, 0.00019999839716405336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13540 loss: 0.8906 iter time (s): 4.131 samples/sec: 30.982
g0238:  iteration    13540/10000000 | consumed samples:      1733120 | consumed tokens:   3549429760 | elapsed time per iteration (ms): 4165.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.083966E-01 | loss scale: 131072.0 | grad norm: 0.263 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.731 | tokens per gpu per second (tgs): 1966.768 | TFLOPs: 15.83 |
g0220: [2024-08-09 21:39:00,132] [INFO] [logging.py:96:log_dist] [Rank 0] step=13550, skipped=15, lr=[0.00019999838182536755, 0.00019999838182536755], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13550 loss: 0.9044 iter time (s): 4.373 samples/sec: 29.270
g0238:  iteration    13550/10000000 | consumed samples:      1734400 | consumed tokens:   3552051200 | elapsed time per iteration (ms): 4405.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.194875E-01 | loss scale: 131072.0 | grad norm: 0.257 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.052 | tokens per gpu per second (tgs): 1859.338 | TFLOPs: 14.96 |
g0220: [2024-08-09 21:39:41,368] [INFO] [logging.py:96:log_dist] [Rank 0] step=13560, skipped=15, lr=[0.00019999836641363802, 0.00019999836641363802], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13560 loss: 0.9069 iter time (s): 4.090 samples/sec: 31.293
g0238:  iteration    13560/10000000 | consumed samples:      1735680 | consumed tokens:   3554672640 | elapsed time per iteration (ms): 4123.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.204690E-01 | loss scale: 131072.0 | grad norm: 0.243 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.041 | tokens per gpu per second (tgs): 1986.604 | TFLOPs: 15.99 |
g0220: [2024-08-09 21:40:21,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=13570, skipped=15, lr=[0.00019999835092886478, 0.00019999835092886478], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13570 loss: 0.9320 iter time (s): 3.953 samples/sec: 32.384
g0238:  iteration    13570/10000000 | consumed samples:      1736960 | consumed tokens:   3557294080 | elapsed time per iteration (ms): 3984.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.219719E-01 | loss scale: 131072.0 | grad norm: 0.273 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.121 | tokens per gpu per second (tgs): 2055.756 | TFLOPs: 16.54 |
g0220: [2024-08-09 21:41:01,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=13580, skipped=15, lr=[0.00019999833537104783, 0.00019999833537104783], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13580 loss: 0.9496 iter time (s): 4.005 samples/sec: 31.962
g0238:  iteration    13580/10000000 | consumed samples:      1738240 | consumed tokens:   3559915520 | elapsed time per iteration (ms): 4037.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.114985E-01 | loss scale: 131072.0 | grad norm: 0.298 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.704 | tokens per gpu per second (tgs): 2029.086 | TFLOPs: 16.33 |
g0220: [2024-08-09 21:41:40,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=13590, skipped=15, lr=[0.00019999831974018719, 0.00019999831974018719], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13590 loss: 0.9296 iter time (s): 3.858 samples/sec: 33.177
g0238:  iteration    13590/10000000 | consumed samples:      1739520 | consumed tokens:   3562536960 | elapsed time per iteration (ms): 3891.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.101292E-01 | loss scale: 131072.0 | grad norm: 0.236 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.896 | tokens per gpu per second (tgs): 2105.347 | TFLOPs: 16.94 |
g0220: [2024-08-09 21:42:20,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=13600, skipped=15, lr=[0.00019999830403628286, 0.00019999830403628286], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13600 loss: 0.9135 iter time (s): 3.994 samples/sec: 32.048
g0238:  iteration    13600/10000000 | consumed samples:      1740800 | consumed tokens:   3565158400 | elapsed time per iteration (ms): 4026.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.206258E-01 | loss scale: 131072.0 | grad norm: 0.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.787 | tokens per gpu per second (tgs): 2034.360 | TFLOPs: 16.37 |
g0220: [2024-08-09 21:43:00,687] [INFO] [logging.py:96:log_dist] [Rank 0] step=13610, skipped=15, lr=[0.0001999982882593349, 0.0001999982882593349], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13610 loss: 0.9596 iter time (s): 3.954 samples/sec: 32.373
g0238:  iteration    13610/10000000 | consumed samples:      1742080 | consumed tokens:   3567779840 | elapsed time per iteration (ms): 3991.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.223486E-01 | loss scale: 131072.0 | grad norm: 0.272 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.066 | tokens per gpu per second (tgs): 2052.252 | TFLOPs: 16.51 |
g0220: [2024-08-09 21:43:43,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=13620, skipped=15, lr=[0.00019999827240934327, 0.00019999827240934327], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13620 loss: 0.8917 iter time (s): 4.258 samples/sec: 30.063
g0238:  iteration    13620/10000000 | consumed samples:      1743360 | consumed tokens:   3570401280 | elapsed time per iteration (ms): 4310.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.110446E-01 | loss scale: 131072.0 | grad norm: 0.279 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.694 | tokens per gpu per second (tgs): 1900.396 | TFLOPs: 15.29 |
g0220: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 21:44:07,846] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 21:44:07,847] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 21:44:24,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=13630, skipped=15, lr=[0.000199998256486308, 0.000199998256486308], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13630 loss: 0.9204 iter time (s): 4.039 samples/sec: 31.693
g0238:  iteration    13630/10000000 | consumed samples:      1744640 | consumed tokens:   3573022720 | elapsed time per iteration (ms): 4078.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.201401E-01 | loss scale: 262144.0 | grad norm: 0.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.387 | tokens per gpu per second (tgs): 2008.784 | TFLOPs: 16.17 |
g0220: [2024-08-09 21:45:04,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=13640, skipped=15, lr=[0.0001999982404902291, 0.0001999982404902291], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13640 loss: 0.9262 iter time (s): 3.962 samples/sec: 32.309
g0238:  iteration    13640/10000000 | consumed samples:      1745920 | consumed tokens:   3575644160 | elapsed time per iteration (ms): 3995.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.207627E-01 | loss scale: 262144.0 | grad norm: 0.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.039 | tokens per gpu per second (tgs): 2050.508 | TFLOPs: 16.50 |
g0220: [2024-08-09 21:45:45,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=13650, skipped=15, lr=[0.00019999822442110662, 0.00019999822442110662], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13650 loss: 0.9548 iter time (s): 4.031 samples/sec: 31.750
g0238:  iteration    13650/10000000 | consumed samples:      1747200 | consumed tokens:   3578265600 | elapsed time per iteration (ms): 4064.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.096153E-01 | loss scale: 262144.0 | grad norm: 0.289 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.492 | tokens per gpu per second (tgs): 2015.460 | TFLOPs: 16.22 |
g0220: [2024-08-09 21:46:25,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=13660, skipped=15, lr=[0.0001999982082789405, 0.0001999982082789405], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13660 loss: 0.8978 iter time (s): 4.019 samples/sec: 31.849
g0238:  iteration    13660/10000000 | consumed samples:      1748480 | consumed tokens:   3580887040 | elapsed time per iteration (ms): 4052.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.117282E-01 | loss scale: 262144.0 | grad norm: 0.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.588 | tokens per gpu per second (tgs): 2021.642 | TFLOPs: 16.27 |
g0220: [2024-08-09 21:47:06,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=13670, skipped=15, lr=[0.0001999981920637308, 0.0001999981920637308], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13670 loss: 0.8997 iter time (s): 4.058 samples/sec: 31.544
g0238:  iteration    13670/10000000 | consumed samples:      1749760 | consumed tokens:   3583508480 | elapsed time per iteration (ms): 4091.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.139258E-01 | loss scale: 262144.0 | grad norm: 0.236 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.285 | tokens per gpu per second (tgs): 2002.260 | TFLOPs: 16.11 |
g0220: [2024-08-09 21:47:46,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=13680, skipped=15, lr=[0.00019999817577547757, 0.00019999817577547757], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13680 loss: 0.9191 iter time (s): 3.916 samples/sec: 32.688
g0238:  iteration    13680/10000000 | consumed samples:      1751040 | consumed tokens:   3586129920 | elapsed time per iteration (ms): 3949.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.176427E-01 | loss scale: 262144.0 | grad norm: 0.453 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.413 | tokens per gpu per second (tgs): 2074.428 | TFLOPs: 16.69 |
g0220: [2024-08-09 21:48:26,074] [INFO] [logging.py:96:log_dist] [Rank 0] step=13690, skipped=15, lr=[0.0001999981594141807, 0.0001999981594141807], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13690 loss: 0.9241 iter time (s): 3.964 samples/sec: 32.289
g0238:  iteration    13690/10000000 | consumed samples:      1752320 | consumed tokens:   3588751360 | elapsed time per iteration (ms): 3997.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.108220E-01 | loss scale: 262144.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.020 | tokens per gpu per second (tgs): 2049.255 | TFLOPs: 16.49 |
g0220: [2024-08-09 21:49:06,487] [INFO] [logging.py:96:log_dist] [Rank 0] step=13700, skipped=15, lr=[0.00019999814297984032, 0.00019999814297984032], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13700 loss: 0.9100 iter time (s): 4.007 samples/sec: 31.946
g0238:  iteration    13700/10000000 | consumed samples:      1753600 | consumed tokens:   3591372800 | elapsed time per iteration (ms): 4041.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.103635E-01 | loss scale: 262144.0 | grad norm: 0.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.675 | tokens per gpu per second (tgs): 2027.176 | TFLOPs: 16.31 |
g0220: [2024-08-09 21:49:46,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=13710, skipped=15, lr=[0.0001999981264724564, 0.0001999981264724564], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13710 loss: 0.9209 iter time (s): 3.994 samples/sec: 32.050
g0238:  iteration    13710/10000000 | consumed samples:      1754880 | consumed tokens:   3593994240 | elapsed time per iteration (ms): 4027.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.047394E-01 | loss scale: 262144.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.780 | tokens per gpu per second (tgs): 2033.935 | TFLOPs: 16.37 |
g0220: [2024-08-09 21:50:26,912] [INFO] [logging.py:96:log_dist] [Rank 0] step=13720, skipped=15, lr=[0.00019999810989202897, 0.00019999810989202897], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13720 loss: 0.9247 iter time (s): 3.981 samples/sec: 32.149
g0238:  iteration    13720/10000000 | consumed samples:      1756160 | consumed tokens:   3596615680 | elapsed time per iteration (ms): 4014.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.048536E-01 | loss scale: 262144.0 | grad norm: 0.260 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.882 | tokens per gpu per second (tgs): 2040.426 | TFLOPs: 16.42 |
g0220: [2024-08-09 21:51:08,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=13730, skipped=15, lr=[0.000199998093238558, 0.000199998093238558], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13730 loss: 0.8913 iter time (s): 4.167 samples/sec: 30.715
g0238:  iteration    13730/10000000 | consumed samples:      1757440 | consumed tokens:   3599237120 | elapsed time per iteration (ms): 4200.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.118432E-01 | loss scale: 262144.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.472 | tokens per gpu per second (tgs): 1950.224 | TFLOPs: 15.69 |
g0220: [2024-08-09 21:51:49,599] [INFO] [logging.py:96:log_dist] [Rank 0] step=13740, skipped=15, lr=[0.0001999980765120435, 0.0001999980765120435], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13740 loss: 0.9165 iter time (s): 4.035 samples/sec: 31.719
g0238:  iteration    13740/10000000 | consumed samples:      1758720 | consumed tokens:   3601858560 | elapsed time per iteration (ms): 4068.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.159317E-01 | loss scale: 262144.0 | grad norm: 0.265 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.464 | tokens per gpu per second (tgs): 2013.722 | TFLOPs: 16.20 |
g0220: [2024-08-09 21:52:29,207] [INFO] [logging.py:96:log_dist] [Rank 0] step=13750, skipped=15, lr=[0.00019999805971248556, 0.00019999805971248556], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13750 loss: 0.8709 iter time (s): 3.928 samples/sec: 32.586
g0238:  iteration    13750/10000000 | consumed samples:      1760000 | consumed tokens:   3604480000 | elapsed time per iteration (ms): 3960.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.067356E-01 | loss scale: 262144.0 | grad norm: 0.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.317 | tokens per gpu per second (tgs): 2068.294 | TFLOPs: 16.64 |
g0220: [2024-08-09 21:53:10,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=13760, skipped=15, lr=[0.00019999804283988415, 0.00019999804283988415], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13760 loss: 0.8992 iter time (s): 4.133 samples/sec: 30.973
g0238:  iteration    13760/10000000 | consumed samples:      1761280 | consumed tokens:   3607101440 | elapsed time per iteration (ms): 4165.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.122473E-01 | loss scale: 262144.0 | grad norm: 0.250 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.729 | tokens per gpu per second (tgs): 1966.645 | TFLOPs: 15.83 |
g0220: [2024-08-09 21:53:51,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=13770, skipped=15, lr=[0.00019999802589423926, 0.00019999802589423926], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13770 loss: 0.8736 iter time (s): 4.076 samples/sec: 31.407
g0238:  iteration    13770/10000000 | consumed samples:      1762560 | consumed tokens:   3609722880 | elapsed time per iteration (ms): 4108.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.156086E-01 | loss scale: 262144.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.153 | tokens per gpu per second (tgs): 1993.818 | TFLOPs: 16.04 |
g0220: [2024-08-09 21:54:32,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=13780, skipped=15, lr=[0.0001999980088755509, 0.0001999980088755509], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13780 loss: 0.9149 iter time (s): 4.024 samples/sec: 31.806
g0238:  iteration    13780/10000000 | consumed samples:      1763840 | consumed tokens:   3612344320 | elapsed time per iteration (ms): 4057.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.090978E-01 | loss scale: 262144.0 | grad norm: 0.274 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.546 | tokens per gpu per second (tgs): 2018.937 | TFLOPs: 16.25 |
g0220: [2024-08-09 21:55:13,803] [INFO] [logging.py:96:log_dist] [Rank 0] step=13790, skipped=15, lr=[0.00019999799178381912, 0.00019999799178381912], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13790 loss: 0.9508 iter time (s): 4.094 samples/sec: 31.262
g0238:  iteration    13790/10000000 | consumed samples:      1765120 | consumed tokens:   3614965760 | elapsed time per iteration (ms): 4128.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.083209E-01 | loss scale: 262144.0 | grad norm: 0.264 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.008 | tokens per gpu per second (tgs): 1984.504 | TFLOPs: 15.97 |
g0220: [2024-08-09 21:55:54,577] [INFO] [logging.py:96:log_dist] [Rank 0] step=13800, skipped=15, lr=[0.00019999797461904393, 0.00019999797461904393], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13800 loss: 0.8734 iter time (s): 4.044 samples/sec: 31.652
g0238:  iteration    13800/10000000 | consumed samples:      1766400 | consumed tokens:   3617587200 | elapsed time per iteration (ms): 4077.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.021522E-01 | loss scale: 262144.0 | grad norm: 0.231 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.394 | tokens per gpu per second (tgs): 2009.238 | TFLOPs: 16.17 |
g0220: [2024-08-09 21:56:36,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=13810, skipped=15, lr=[0.00019999795738122534, 0.00019999795738122534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13810 loss: 0.9030 iter time (s): 4.122 samples/sec: 31.054
g0238:  iteration    13810/10000000 | consumed samples:      1767680 | consumed tokens:   3620208640 | elapsed time per iteration (ms): 4155.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.079784E-01 | loss scale: 262144.0 | grad norm: 0.261 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.806 | tokens per gpu per second (tgs): 1971.579 | TFLOPs: 15.87 |
g0220: [2024-08-09 21:57:15,450] [INFO] [logging.py:96:log_dist] [Rank 0] step=13820, skipped=15, lr=[0.00019999794007036334, 0.00019999794007036334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13820 loss: 0.9022 iter time (s): 3.899 samples/sec: 32.826
g0238:  iteration    13820/10000000 | consumed samples:      1768960 | consumed tokens:   3622830080 | elapsed time per iteration (ms): 3932.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.081578E-01 | loss scale: 262144.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.552 | tokens per gpu per second (tgs): 2083.322 | TFLOPs: 16.76 |
g0220: [2024-08-09 21:57:55,756] [INFO] [logging.py:96:log_dist] [Rank 0] step=13830, skipped=15, lr=[0.00019999792268645794, 0.00019999792268645794], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13830 loss: 0.9350 iter time (s): 3.997 samples/sec: 32.022
g0238:  iteration    13830/10000000 | consumed samples:      1770240 | consumed tokens:   3625451520 | elapsed time per iteration (ms): 4030.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.133490E-01 | loss scale: 262144.0 | grad norm: 0.244 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.756 | tokens per gpu per second (tgs): 2032.413 | TFLOPs: 16.36 |
g0220: [2024-08-09 21:58:35,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=13840, skipped=15, lr=[0.00019999790522950918, 0.00019999790522950918], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13840 loss: 0.8928 iter time (s): 3.985 samples/sec: 32.117
g0238:  iteration    13840/10000000 | consumed samples:      1771520 | consumed tokens:   3628072960 | elapsed time per iteration (ms): 4018.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.194385E-01 | loss scale: 262144.0 | grad norm: 0.384 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.854 | tokens per gpu per second (tgs): 2038.674 | TFLOPs: 16.41 |
g0220: [2024-08-09 21:59:15,690] [INFO] [logging.py:96:log_dist] [Rank 0] step=13850, skipped=15, lr=[0.00019999788769951707, 0.00019999788769951707], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13850 loss: 0.9141 iter time (s): 3.942 samples/sec: 32.470
g0238:  iteration    13850/10000000 | consumed samples:      1772800 | consumed tokens:   3630694400 | elapsed time per iteration (ms): 3975.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.210472E-01 | loss scale: 262144.0 | grad norm: 0.256 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.201 | tokens per gpu per second (tgs): 2060.848 | TFLOPs: 16.58 |
g0220: [2024-08-09 21:59:56,883] [INFO] [logging.py:96:log_dist] [Rank 0] step=13860, skipped=15, lr=[0.00019999787009648162, 0.00019999787009648162], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13860 loss: 0.9146 iter time (s): 4.086 samples/sec: 31.326
g0238:  iteration    13860/10000000 | consumed samples:      1774080 | consumed tokens:   3633315840 | elapsed time per iteration (ms): 4119.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.170256E-01 | loss scale: 262144.0 | grad norm: 0.258 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.074 | tokens per gpu per second (tgs): 1988.714 | TFLOPs: 16.00 |
g0220: [2024-08-09 22:00:37,653] [INFO] [logging.py:96:log_dist] [Rank 0] step=13870, skipped=15, lr=[0.00019999785242040283, 0.00019999785242040283], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13870 loss: 0.9311 iter time (s): 4.044 samples/sec: 31.649
g0238:  iteration    13870/10000000 | consumed samples:      1775360 | consumed tokens:   3635937280 | elapsed time per iteration (ms): 4077.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.081695E-01 | loss scale: 262144.0 | grad norm: 0.236 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.394 | tokens per gpu per second (tgs): 2009.239 | TFLOPs: 16.17 |
g0220: [2024-08-09 22:01:18,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=13880, skipped=15, lr=[0.00019999783467128076, 0.00019999783467128076], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13880 loss: 0.9229 iter time (s): 4.085 samples/sec: 31.337
g0238:  iteration    13880/10000000 | consumed samples:      1776640 | consumed tokens:   3638558720 | elapsed time per iteration (ms): 4117.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.045271E-01 | loss scale: 262144.0 | grad norm: 0.263 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.087 | tokens per gpu per second (tgs): 1989.542 | TFLOPs: 16.01 |
g0220: [2024-08-09 22:01:59,157] [INFO] [logging.py:96:log_dist] [Rank 0] step=13890, skipped=15, lr=[0.00019999781684911533, 0.00019999781684911533], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13890 loss: 0.9157 iter time (s): 4.000 samples/sec: 32.001
g0238:  iteration    13890/10000000 | consumed samples:      1777920 | consumed tokens:   3641180160 | elapsed time per iteration (ms): 4032.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.132544E-01 | loss scale: 262144.0 | grad norm: 0.277 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.741 | tokens per gpu per second (tgs): 2031.451 | TFLOPs: 16.35 |
g0220: [2024-08-09 22:02:39,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=13900, skipped=15, lr=[0.00019999779895390663, 0.00019999779895390663], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13900 loss: 0.9326 iter time (s): 3.991 samples/sec: 32.075
g0238:  iteration    13900/10000000 | consumed samples:      1779200 | consumed tokens:   3643801600 | elapsed time per iteration (ms): 4023.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.117591E-01 | loss scale: 262144.0 | grad norm: 0.243 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.814 | tokens per gpu per second (tgs): 2036.092 | TFLOPs: 16.38 |
g0220: [2024-08-09 22:03:19,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=13910, skipped=15, lr=[0.0001999977809856547, 0.0001999977809856547], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13910 loss: 0.8999 iter time (s): 3.969 samples/sec: 32.251
g0238:  iteration    13910/10000000 | consumed samples:      1780480 | consumed tokens:   3646423040 | elapsed time per iteration (ms): 4002.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.034847E-01 | loss scale: 262144.0 | grad norm: 0.267 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.982 | tokens per gpu per second (tgs): 2046.828 | TFLOPs: 16.47 |
g0220: [2024-08-09 22:04:00,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=13920, skipped=15, lr=[0.00019999776294435945, 0.00019999776294435945], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13920 loss: 0.8903 iter time (s): 4.124 samples/sec: 31.037
g0238:  iteration    13920/10000000 | consumed samples:      1781760 | consumed tokens:   3649044480 | elapsed time per iteration (ms): 4157.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.050423E-01 | loss scale: 262144.0 | grad norm: 0.259 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.787 | tokens per gpu per second (tgs): 1970.338 | TFLOPs: 15.86 |
g0220: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 13924
g0220: Grad overflow on iteration 13924
g0237: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 13924
g0225: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 13924
g0236: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 13924
g0220: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 13924
g0220: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 13924
g0237: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: Grad overflow on iteration 13924
g0220: Grad overflow on iteration 13924
g0236: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: Grad overflow on iteration 13924
g0225: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: Grad overflow on iteration 13924
g0225: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: Grad overflow on iteration 13924
g0236: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 13924
g0237: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 13924
g0233: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 13924
g0236: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: Grad overflow on iteration 13924
g0233: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: Grad overflow on iteration 13924
g0235: Grad overflow on iteration 13924
g0234: Grad overflow on iteration 13924
g0235: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 13924
g0235: Grad overflow on iteration 13924
g0220: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 13924
g0235: Grad overflow on iteration 13924
g0238: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 22:04:21,376] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: Grad overflow on iteration 13924
g0238: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 22:04:21,377] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0238: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 13924
g0225: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 13924
g0237: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 13924
g0233: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 13924
g0225: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 13924
g0235: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 13924
g0234: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 13924
g0234: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 13924
g0220: [2024-08-09 22:04:21,377] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 22:04:41,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=13930, skipped=16, lr=[0.00019999774483002097, 0.00019999774483002097], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13930 loss: 0.9284 iter time (s): 4.022 samples/sec: 31.828
g0238:  iteration    13930/10000000 | consumed samples:      1783040 | consumed tokens:   3651665920 | elapsed time per iteration (ms): 4054.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.280279E-01 | loss scale: 131072.0 | grad norm: 0.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.569 | tokens per gpu per second (tgs): 2020.430 | TFLOPs: 16.26 |
g0220: [2024-08-09 22:05:22,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=13940, skipped=16, lr=[0.00019999772664263928, 0.00019999772664263928], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13940 loss: 0.9303 iter time (s): 4.094 samples/sec: 31.266
g0238:  iteration    13940/10000000 | consumed samples:      1784320 | consumed tokens:   3654287360 | elapsed time per iteration (ms): 4126.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.329750E-01 | loss scale: 131072.0 | grad norm: 0.275 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.020 | tokens per gpu per second (tgs): 1985.299 | TFLOPs: 15.98 |
g0220: [2024-08-09 22:06:03,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=13950, skipped=16, lr=[0.00019999770838221437, 0.00019999770838221437], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13950 loss: 0.8913 iter time (s): 4.006 samples/sec: 31.949
g0238:  iteration    13950/10000000 | consumed samples:      1785600 | consumed tokens:   3656908800 | elapsed time per iteration (ms): 4038.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.023308E-01 | loss scale: 131072.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.693 | tokens per gpu per second (tgs): 2028.337 | TFLOPs: 16.32 |
g0220: [2024-08-09 22:06:43,952] [INFO] [logging.py:96:log_dist] [Rank 0] step=13960, skipped=16, lr=[0.00019999769004874624, 0.00019999769004874624], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13960 loss: 0.9184 iter time (s): 4.043 samples/sec: 31.657
g0238:  iteration    13960/10000000 | consumed samples:      1786880 | consumed tokens:   3659530240 | elapsed time per iteration (ms): 4076.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.139563E-01 | loss scale: 131072.0 | grad norm: 0.261 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.400 | tokens per gpu per second (tgs): 2009.604 | TFLOPs: 16.17 |
g0220: [2024-08-09 22:07:24,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=13970, skipped=16, lr=[0.00019999767164223493, 0.00019999767164223493], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13970 loss: 0.8708 iter time (s): 3.996 samples/sec: 32.034
g0238:  iteration    13970/10000000 | consumed samples:      1788160 | consumed tokens:   3662151680 | elapsed time per iteration (ms): 4028.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.007857E-01 | loss scale: 131072.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.776 | tokens per gpu per second (tgs): 2033.675 | TFLOPs: 16.37 |
g0220: [2024-08-09 22:08:06,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=13980, skipped=16, lr=[0.00019999765316268043, 0.00019999765316268043], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13980 loss: 0.9074 iter time (s): 4.186 samples/sec: 30.576
g0238:  iteration    13980/10000000 | consumed samples:      1789440 | consumed tokens:   3664773120 | elapsed time per iteration (ms): 4219.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.150957E-01 | loss scale: 131072.0 | grad norm: 0.265 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.333 | tokens per gpu per second (tgs): 1941.313 | TFLOPs: 15.62 |
g0220: [2024-08-09 22:08:48,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=13990, skipped=16, lr=[0.00019999763461008276, 0.00019999763461008276], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 13990 loss: 0.9247 iter time (s): 4.143 samples/sec: 30.892
g0238:  iteration    13990/10000000 | consumed samples:      1790720 | consumed tokens:   3667394560 | elapsed time per iteration (ms): 4176.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.110370E-01 | loss scale: 131072.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.651 | tokens per gpu per second (tgs): 1961.642 | TFLOPs: 15.79 |
g0220: [2024-08-09 22:09:28,379] [INFO] [logging.py:96:log_dist] [Rank 0] step=14000, skipped=16, lr=[0.00019999761598444196, 0.00019999761598444196], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14000 loss: 0.9281 iter time (s): 3.986 samples/sec: 32.113
g0238:  iteration    14000/10000000 | consumed samples:      1792000 | consumed tokens:   3670016000 | elapsed time per iteration (ms): 4018.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.088269E-01 | loss scale: 131072.0 | grad norm: 0.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.852 | tokens per gpu per second (tgs): 2038.552 | TFLOPs: 16.40 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 14000 | lm loss value: 9.107794E-01 | lm loss PPL: 2.486260E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   14000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-09 22:15:52,599] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step14000 is about to be saved!
g0238: [2024-08-09 22:15:52,605] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0238: [2024-08-09 22:15:52,606] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0220: [2024-08-09 22:15:52,605] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0220: [2024-08-09 22:15:52,606] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0220: [2024-08-09 22:15:52,605] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0238: [2024-08-09 22:15:52,606] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0234: [2024-08-09 22:15:52,606] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0234: [2024-08-09 22:15:52,607] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0234: [2024-08-09 22:15:52,607] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0225: [2024-08-09 22:15:52,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0225: [2024-08-09 22:15:52,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0225: [2024-08-09 22:15:52,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0235: [2024-08-09 22:15:52,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0235: [2024-08-09 22:15:52,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0235: [2024-08-09 22:15:52,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0237: [2024-08-09 22:15:52,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0233: [2024-08-09 22:15:52,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0237: [2024-08-09 22:15:52,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0237: [2024-08-09 22:15:52,609] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0233: [2024-08-09 22:15:52,608] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0233: [2024-08-09 22:15:52,609] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0236: [2024-08-09 22:15:52,609] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0236: [2024-08-09 22:15:52,609] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0236: [2024-08-09 22:15:52,609] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0238: [2024-08-09 22:15:52,630] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_23-model_00-model_states.pt...
g0234: [2024-08-09 22:15:52,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_11-model_00-model_states.pt...
g0235: [2024-08-09 22:15:52,645] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_14-model_00-model_states.pt...
g0237: [2024-08-09 22:15:52,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_20-model_00-model_states.pt...
g0225: [2024-08-09 22:15:52,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_05-model_00-model_states.pt...
g0233: [2024-08-09 22:15:52,647] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_08-model_00-model_states.pt...
g0236: [2024-08-09 22:15:52,647] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_17-model_00-model_states.pt...
g0220: [2024-08-09 22:15:52,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_01-model_00-model_states.pt...
g0237: [2024-08-09 22:15:52,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_20-model_00-model_states.pt.
g0233: [2024-08-09 22:15:52,781] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_08-model_00-model_states.pt.
g0237: [2024-08-09 22:15:52,792] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_21-model_00-model_states.pt...
g0233: [2024-08-09 22:15:52,819] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_09-model_00-model_states.pt...
g0238: [2024-08-09 22:15:52,834] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 22:15:52,835] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 22:15:52,837] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_24-model_00-model_states.pt.
g0235: [2024-08-09 22:15:52,853] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_14-model_00-model_states.pt.
g0234: [2024-08-09 22:15:52,864] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_11-model_00-model_states.pt.
g0238: [2024-08-09 22:15:52,882] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_25-model_00-model_states.pt...
g0235: [2024-08-09 22:15:52,891] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_15-model_00-model_states.pt...
g0225: [2024-08-09 22:15:52,893] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_05-model_00-model_states.pt.
g0220: [2024-08-09 22:15:52,902] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_01-model_00-model_states.pt.
g0234: [2024-08-09 22:15:52,903] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_12-model_00-model_states.pt...
g0220: [2024-08-09 22:15:52,924] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_02-model_00-model_states.pt...
g0225: [2024-08-09 22:15:52,933] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_06-model_00-model_states.pt...
g0237: [2024-08-09 22:15:52,947] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_21-model_00-model_states.pt.
g0233: [2024-08-09 22:15:52,978] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_09-model_00-model_states.pt.
g0237: [2024-08-09 22:15:52,981] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_22-model_00-model_states.pt...
g0235: [2024-08-09 22:15:53,005] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_15-model_00-model_states.pt.
g0233: [2024-08-09 22:15:53,013] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_10-model_00-model_states.pt...
g0235: [2024-08-09 22:15:53,039] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_16-model_00-model_states.pt...
g0220: [2024-08-09 22:15:53,058] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_02-model_00-model_states.pt.
g0234: [2024-08-09 22:15:53,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_12-model_00-model_states.pt.
g0220: [2024-08-09 22:15:53,089] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_03-model_00-model_states.pt...
g0238: [2024-08-09 22:15:53,103] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_25-model_00-model_states.pt.
g0225: [2024-08-09 22:15:53,103] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_06-model_00-model_states.pt.
g0238: [2024-08-09 22:15:53,104] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_07_model_states.pt...
g0234: [2024-08-09 22:15:53,112] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_13-model_00-model_states.pt...
g0225: [2024-08-09 22:15:53,138] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_07-model_00-model_states.pt...
g0237: [2024-08-09 22:15:53,148] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 22:15:53,150] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_06_model_states.pt...
g0233: [2024-08-09 22:15:53,185] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 22:15:53,186] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_02_model_states.pt...
g0234: [2024-08-09 22:15:53,201] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 22:15:53,203] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_03_model_states.pt...
g0225: [2024-08-09 22:15:53,240] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 22:15:53,242] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_01_model_states.pt...
g0220: [2024-08-09 22:15:53,289] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 22:15:53,312] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_04-model_00-model_states.pt...
g0220: [2024-08-09 22:15:53,450] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 22:15:53,451] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_00_model_states.pt
g0220: [2024-08-09 22:15:53,451] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_00_model_states.pt...
g0236: [2024-08-09 22:15:53,799] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_17-model_00-model_states.pt.
g0236: [2024-08-09 22:15:53,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_18-model_00-model_states.pt...
g0235: [2024-08-09 22:15:53,881] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 22:15:53,883] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_04_model_states.pt...
g0236: [2024-08-09 22:15:54,128] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_18-model_00-model_states.pt.
g0236: [2024-08-09 22:15:54,163] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_19-model_00-model_states.pt...
g0236: [2024-08-09 22:15:54,294] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 22:15:54,296] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_05_model_states.pt...
g0238: [2024-08-09 22:15:54,968] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 22:15:54,968] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0233: [2024-08-09 22:15:55,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 22:15:55,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0234: [2024-08-09 22:15:55,650] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 22:15:55,650] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0237: [2024-08-09 22:15:55,794] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 22:15:55,795] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0225: [2024-08-09 22:15:55,898] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_01_model_states.pt.
g0225: [2024-08-09 22:15:55,898] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0235: [2024-08-09 22:15:56,298] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 22:15:56,298] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0236: [2024-08-09 22:15:56,775] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 22:15:56,776] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0220: [2024-08-09 22:15:58,846] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step14000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 22:15:58,847] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14000 is ready now!
g0220:   successfully saved checkpoint at iteration   14000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 3.59, Latency(second): 6.267
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (6267.22, 6267.39)
g0220: [2024-08-09 22:16:40,391] [INFO] [logging.py:96:log_dist] [Rank 0] step=14010, skipped=16, lr=[0.00019999759728575803, 0.00019999759728575803], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14010 loss: 0.9186 iter time (s): 4.125 samples/sec: 31.029
g0238:  iteration    14010/10000000 | consumed samples:      1793280 | consumed tokens:   3672637440 | elapsed time per iteration (ms): 43201.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.081463E-01 | loss scale: 131072.0 | grad norm: 0.276 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.963 | tokens per gpu per second (tgs): 189.623 | TFLOPs: 1.53 |
g0220: [2024-08-09 22:17:22,692] [INFO] [logging.py:96:log_dist] [Rank 0] step=14020, skipped=16, lr=[0.00019999757851403097, 0.00019999757851403097], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14020 loss: 0.9267 iter time (s): 4.197 samples/sec: 30.497
g0238:  iteration    14020/10000000 | consumed samples:      1794560 | consumed tokens:   3675258880 | elapsed time per iteration (ms): 4229.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.224947E-01 | loss scale: 131072.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.261 | tokens per gpu per second (tgs): 1936.700 | TFLOPs: 15.58 |
g0220: [2024-08-09 22:18:03,306] [INFO] [logging.py:96:log_dist] [Rank 0] step=14030, skipped=16, lr=[0.0001999975596692608, 0.0001999975596692608], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14030 loss: 0.8984 iter time (s): 4.029 samples/sec: 31.774
g0238:  iteration    14030/10000000 | consumed samples:      1795840 | consumed tokens:   3677880320 | elapsed time per iteration (ms): 4061.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.126183E-01 | loss scale: 131072.0 | grad norm: 0.238 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.516 | tokens per gpu per second (tgs): 2017.020 | TFLOPs: 16.23 |
g0220: [2024-08-09 22:18:45,468] [INFO] [logging.py:96:log_dist] [Rank 0] step=14040, skipped=16, lr=[0.00019999754075144757, 0.00019999754075144757], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14040 loss: 0.9068 iter time (s): 4.183 samples/sec: 30.597
g0238:  iteration    14040/10000000 | consumed samples:      1797120 | consumed tokens:   3680501760 | elapsed time per iteration (ms): 4216.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.036580E-01 | loss scale: 131072.0 | grad norm: 0.256 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.360 | tokens per gpu per second (tgs): 1943.013 | TFLOPs: 15.64 |
g0220: [2024-08-09 22:19:28,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=14050, skipped=16, lr=[0.00019999752176059124, 0.00019999752176059124], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14050 loss: 0.9329 iter time (s): 4.263 samples/sec: 30.028
g0238:  iteration    14050/10000000 | consumed samples:      1798400 | consumed tokens:   3683123200 | elapsed time per iteration (ms): 4295.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.126764E-01 | loss scale: 131072.0 | grad norm: 0.252 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.799 | tokens per gpu per second (tgs): 1907.106 | TFLOPs: 15.35 |
g0220: [2024-08-09 22:20:09,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=14060, skipped=16, lr=[0.00019999750269669183, 0.00019999750269669183], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14060 loss: 0.9306 iter time (s): 4.073 samples/sec: 31.425
g0238:  iteration    14060/10000000 | consumed samples:      1799680 | consumed tokens:   3685744640 | elapsed time per iteration (ms): 4106.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.088520E-01 | loss scale: 131072.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.174 | tokens per gpu per second (tgs): 1995.132 | TFLOPs: 16.06 |
g0220: [2024-08-09 22:20:49,256] [INFO] [logging.py:96:log_dist] [Rank 0] step=14070, skipped=16, lr=[0.0001999974835597494, 0.0001999974835597494], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14070 loss: 0.8897 iter time (s): 3.944 samples/sec: 32.451
g0238:  iteration    14070/10000000 | consumed samples:      1800960 | consumed tokens:   3688366080 | elapsed time per iteration (ms): 3977.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.052034E-01 | loss scale: 131072.0 | grad norm: 0.255 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.183 | tokens per gpu per second (tgs): 2059.736 | TFLOPs: 16.58 |
g0220: [2024-08-09 22:21:30,449] [INFO] [logging.py:96:log_dist] [Rank 0] step=14080, skipped=16, lr=[0.00019999746434976392, 0.00019999746434976392], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14080 loss: 0.9536 iter time (s): 4.087 samples/sec: 31.321
g0238:  iteration    14080/10000000 | consumed samples:      1802240 | consumed tokens:   3690987520 | elapsed time per iteration (ms): 4119.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.124419E-01 | loss scale: 131072.0 | grad norm: 0.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.073 | tokens per gpu per second (tgs): 1988.651 | TFLOPs: 16.00 |
g0220: [2024-08-09 22:22:11,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=14090, skipped=16, lr=[0.00019999744506673544, 0.00019999744506673544], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14090 loss: 0.9111 iter time (s): 4.052 samples/sec: 31.592
g0238:  iteration    14090/10000000 | consumed samples:      1803520 | consumed tokens:   3693608960 | elapsed time per iteration (ms): 4084.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.106802E-01 | loss scale: 131072.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.338 | tokens per gpu per second (tgs): 2005.648 | TFLOPs: 16.14 |
g0220: [2024-08-09 22:22:52,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=14100, skipped=16, lr=[0.00019999742571066392, 0.00019999742571066392], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14100 loss: 0.9080 iter time (s): 4.089 samples/sec: 31.306
g0238:  iteration    14100/10000000 | consumed samples:      1804800 | consumed tokens:   3696230400 | elapsed time per iteration (ms): 4121.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.090973E-01 | loss scale: 131072.0 | grad norm: 0.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.057 | tokens per gpu per second (tgs): 1987.664 | TFLOPs: 16.00 |
g0220: [2024-08-09 22:23:32,768] [INFO] [logging.py:96:log_dist] [Rank 0] step=14110, skipped=16, lr=[0.00019999740628154946, 0.00019999740628154946], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14110 loss: 0.9193 iter time (s): 3.994 samples/sec: 32.052
g0238:  iteration    14110/10000000 | consumed samples:      1806080 | consumed tokens:   3698851840 | elapsed time per iteration (ms): 4025.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.093143E-01 | loss scale: 131072.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.794 | tokens per gpu per second (tgs): 2034.806 | TFLOPs: 16.37 |
g0220: [2024-08-09 22:24:13,501] [INFO] [logging.py:96:log_dist] [Rank 0] step=14120, skipped=16, lr=[0.000199997386779392, 0.000199997386779392], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14120 loss: 0.9338 iter time (s): 4.040 samples/sec: 31.680
g0238:  iteration    14120/10000000 | consumed samples:      1807360 | consumed tokens:   3701473280 | elapsed time per iteration (ms): 4073.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.139855E-01 | loss scale: 131072.0 | grad norm: 0.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.424 | tokens per gpu per second (tgs): 2011.129 | TFLOPs: 16.18 |
g0220: [2024-08-09 22:24:54,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=14130, skipped=16, lr=[0.0001999973672041916, 0.0001999973672041916], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14130 loss: 0.9059 iter time (s): 4.034 samples/sec: 31.730
g0238:  iteration    14130/10000000 | consumed samples:      1808640 | consumed tokens:   3704094720 | elapsed time per iteration (ms): 4067.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.134047E-01 | loss scale: 131072.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.472 | tokens per gpu per second (tgs): 2014.178 | TFLOPs: 16.21 |
g0220: [2024-08-09 22:25:36,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=14140, skipped=16, lr=[0.00019999734755594826, 0.00019999734755594826], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14140 loss: 0.8867 iter time (s): 4.175 samples/sec: 30.659
g0238:  iteration    14140/10000000 | consumed samples:      1809920 | consumed tokens:   3706716160 | elapsed time per iteration (ms): 4207.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.094557E-01 | loss scale: 131072.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.422 | tokens per gpu per second (tgs): 1946.987 | TFLOPs: 15.67 |
g0220: [2024-08-09 22:26:17,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=14150, skipped=16, lr=[0.000199997327834662, 0.000199997327834662], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14150 loss: 0.8595 iter time (s): 4.086 samples/sec: 31.327
g0238:  iteration    14150/10000000 | consumed samples:      1811200 | consumed tokens:   3709337600 | elapsed time per iteration (ms): 4118.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.038879E-01 | loss scale: 131072.0 | grad norm: 0.244 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.078 | tokens per gpu per second (tgs): 1989.011 | TFLOPs: 16.01 |
g0220: [2024-08-09 22:26:59,271] [INFO] [logging.py:96:log_dist] [Rank 0] step=14160, skipped=16, lr=[0.00019999730804033281, 0.00019999730804033281], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14160 loss: 0.8930 iter time (s): 4.151 samples/sec: 30.835
g0238:  iteration    14160/10000000 | consumed samples:      1812480 | consumed tokens:   3711959040 | elapsed time per iteration (ms): 4183.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.166301E-01 | loss scale: 131072.0 | grad norm: 0.238 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.597 | tokens per gpu per second (tgs): 1958.203 | TFLOPs: 15.76 |
g0220: [2024-08-09 22:27:39,565] [INFO] [logging.py:96:log_dist] [Rank 0] step=14170, skipped=16, lr=[0.00019999728817296072, 0.00019999728817296072], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14170 loss: 0.9013 iter time (s): 3.997 samples/sec: 32.027
g0238:  iteration    14170/10000000 | consumed samples:      1813760 | consumed tokens:   3714580480 | elapsed time per iteration (ms): 4029.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.120962E-01 | loss scale: 131072.0 | grad norm: 0.268 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.767 | tokens per gpu per second (tgs): 2033.074 | TFLOPs: 16.36 |
g0220: [2024-08-09 22:28:21,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=14180, skipped=16, lr=[0.00019999726823254576, 0.00019999726823254576], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14180 loss: 0.9056 iter time (s): 4.167 samples/sec: 30.718
g0238:  iteration    14180/10000000 | consumed samples:      1815040 | consumed tokens:   3717201920 | elapsed time per iteration (ms): 4199.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.088116E-01 | loss scale: 131072.0 | grad norm: 0.231 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.481 | tokens per gpu per second (tgs): 1950.797 | TFLOPs: 15.70 |
g0220: [2024-08-09 22:29:02,678] [INFO] [logging.py:96:log_dist] [Rank 0] step=14190, skipped=16, lr=[0.00019999724821908793, 0.00019999724821908793], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14190 loss: 0.9148 iter time (s): 4.079 samples/sec: 31.379
g0238:  iteration    14190/10000000 | consumed samples:      1816320 | consumed tokens:   3719823360 | elapsed time per iteration (ms): 4112.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.982787E-01 | loss scale: 131072.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.128 | tokens per gpu per second (tgs): 1992.179 | TFLOPs: 16.03 |
g0220: [2024-08-09 22:29:43,234] [INFO] [logging.py:96:log_dist] [Rank 0] step=14200, skipped=16, lr=[0.00019999722813258725, 0.00019999722813258725], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14200 loss: 0.8928 iter time (s): 4.023 samples/sec: 31.820
g0238:  iteration    14200/10000000 | consumed samples:      1817600 | consumed tokens:   3722444800 | elapsed time per iteration (ms): 4055.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.989545E-01 | loss scale: 131072.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.561 | tokens per gpu per second (tgs): 2019.933 | TFLOPs: 16.25 |
g0220: [2024-08-09 22:30:22,025] [INFO] [logging.py:96:log_dist] [Rank 0] step=14210, skipped=16, lr=[0.0001999972079730437, 0.0001999972079730437], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14210 loss: 0.8845 iter time (s): 3.844 samples/sec: 33.297
g0238:  iteration    14210/10000000 | consumed samples:      1818880 | consumed tokens:   3725066240 | elapsed time per iteration (ms): 3879.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.045400E-01 | loss scale: 131072.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.997 | tokens per gpu per second (tgs): 2111.807 | TFLOPs: 16.99 |
g0220: [2024-08-09 22:31:02,684] [INFO] [logging.py:96:log_dist] [Rank 0] step=14220, skipped=16, lr=[0.0001999971877404574, 0.0001999971877404574], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14220 loss: 0.8521 iter time (s): 4.033 samples/sec: 31.739
g0238:  iteration    14220/10000000 | consumed samples:      1820160 | consumed tokens:   3727687680 | elapsed time per iteration (ms): 4065.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.885621E-01 | loss scale: 131072.0 | grad norm: 0.250 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.482 | tokens per gpu per second (tgs): 2014.817 | TFLOPs: 16.21 |
g0220: [2024-08-09 22:31:44,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=14230, skipped=16, lr=[0.00019999716743482823, 0.00019999716743482823], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14230 loss: 0.9183 iter time (s): 4.128 samples/sec: 31.004
g0238:  iteration    14230/10000000 | consumed samples:      1821440 | consumed tokens:   3730309120 | elapsed time per iteration (ms): 4162.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.023796E-01 | loss scale: 131072.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.754 | tokens per gpu per second (tgs): 1968.238 | TFLOPs: 15.84 |
g0220: [2024-08-09 22:32:24,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=14240, skipped=16, lr=[0.0001999971470561563, 0.0001999971470561563], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14240 loss: 0.9009 iter time (s): 4.029 samples/sec: 31.767
g0238:  iteration    14240/10000000 | consumed samples:      1822720 | consumed tokens:   3732930560 | elapsed time per iteration (ms): 4062.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.050861E-01 | loss scale: 131072.0 | grad norm: 0.262 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.509 | tokens per gpu per second (tgs): 2016.550 | TFLOPs: 16.23 |
g0220: [2024-08-09 22:33:06,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=14250, skipped=16, lr=[0.00019999712660444157, 0.00019999712660444157], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14250 loss: 0.9261 iter time (s): 4.124 samples/sec: 31.037
g0238:  iteration    14250/10000000 | consumed samples:      1824000 | consumed tokens:   3735552000 | elapsed time per iteration (ms): 4157.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.024885E-01 | loss scale: 131072.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.792 | tokens per gpu per second (tgs): 1970.670 | TFLOPs: 15.86 |
g0220: [2024-08-09 22:33:45,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=14260, skipped=16, lr=[0.00019999710607968414, 0.00019999710607968414], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14260 loss: 0.8726 iter time (s): 3.866 samples/sec: 33.112
g0238:  iteration    14260/10000000 | consumed samples:      1825280 | consumed tokens:   3738173440 | elapsed time per iteration (ms): 3898.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.929375E-01 | loss scale: 131072.0 | grad norm: 0.265 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.834 | tokens per gpu per second (tgs): 2101.347 | TFLOPs: 16.91 |
g0220: [2024-08-09 22:34:26,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=14270, skipped=16, lr=[0.00019999708548188393, 0.00019999708548188393], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14270 loss: 0.8632 iter time (s): 4.051 samples/sec: 31.597
g0238:  iteration    14270/10000000 | consumed samples:      1826560 | consumed tokens:   3740794880 | elapsed time per iteration (ms): 4083.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.982894E-01 | loss scale: 131072.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.344 | tokens per gpu per second (tgs): 2006.022 | TFLOPs: 16.14 |
g0220: [2024-08-09 22:35:08,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=14280, skipped=16, lr=[0.000199997064811041, 0.000199997064811041], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14280 loss: 0.8886 iter time (s): 4.144 samples/sec: 30.888
g0238:  iteration    14280/10000000 | consumed samples:      1827840 | consumed tokens:   3743416320 | elapsed time per iteration (ms): 4176.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.954646E-01 | loss scale: 131072.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.645 | tokens per gpu per second (tgs): 1961.258 | TFLOPs: 15.78 |
g0220: [2024-08-09 22:35:48,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=14290, skipped=16, lr=[0.00019999704406715535, 0.00019999704406715535], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14290 loss: 0.9234 iter time (s): 4.028 samples/sec: 31.775
g0238:  iteration    14290/10000000 | consumed samples:      1829120 | consumed tokens:   3746037760 | elapsed time per iteration (ms): 4060.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.057963E-01 | loss scale: 131072.0 | grad norm: 0.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.520 | tokens per gpu per second (tgs): 2017.298 | TFLOPs: 16.23 |
g0220: [2024-08-09 22:36:28,788] [INFO] [logging.py:96:log_dist] [Rank 0] step=14300, skipped=16, lr=[0.00019999702325022704, 0.00019999702325022704], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14300 loss: 0.9570 iter time (s): 3.976 samples/sec: 32.191
g0238:  iteration    14300/10000000 | consumed samples:      1830400 | consumed tokens:   3748659200 | elapsed time per iteration (ms): 4008.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.150516E-01 | loss scale: 131072.0 | grad norm: 0.309 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.929 | tokens per gpu per second (tgs): 2043.457 | TFLOPs: 16.44 |
g0220: [2024-08-09 22:37:09,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=14310, skipped=16, lr=[0.00019999700236025602, 0.00019999700236025602], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14310 loss: 0.8884 iter time (s): 4.065 samples/sec: 31.487
g0238:  iteration    14310/10000000 | consumed samples:      1831680 | consumed tokens:   3751280640 | elapsed time per iteration (ms): 4097.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.008893E-01 | loss scale: 131072.0 | grad norm: 0.304 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.236 | tokens per gpu per second (tgs): 1999.101 | TFLOPs: 16.09 |
g0220: [2024-08-09 22:37:50,631] [INFO] [logging.py:96:log_dist] [Rank 0] step=14320, skipped=16, lr=[0.00019999698139724234, 0.00019999698139724234], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14320 loss: 0.9018 iter time (s): 4.054 samples/sec: 31.575
g0238:  iteration    14320/10000000 | consumed samples:      1832960 | consumed tokens:   3753902080 | elapsed time per iteration (ms): 4086.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.073843E-01 | loss scale: 131072.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.325 | tokens per gpu per second (tgs): 2004.778 | TFLOPs: 16.13 |
g0220: [2024-08-09 22:38:32,541] [INFO] [logging.py:96:log_dist] [Rank 0] step=14330, skipped=16, lr=[0.000199996960361186, 0.000199996960361186], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14330 loss: 0.9122 iter time (s): 4.158 samples/sec: 30.781
g0238:  iteration    14330/10000000 | consumed samples:      1834240 | consumed tokens:   3756523520 | elapsed time per iteration (ms): 4191.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.164534E-01 | loss scale: 131072.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.540 | tokens per gpu per second (tgs): 1954.542 | TFLOPs: 15.73 |
g0220: [2024-08-09 22:39:13,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=14340, skipped=16, lr=[0.00019999693925208705, 0.00019999693925208705], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14340 loss: 0.8953 iter time (s): 4.093 samples/sec: 31.274
g0238:  iteration    14340/10000000 | consumed samples:      1835520 | consumed tokens:   3759144960 | elapsed time per iteration (ms): 4125.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.010325E-01 | loss scale: 131072.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.028 | tokens per gpu per second (tgs): 1985.792 | TFLOPs: 15.98 |
g0220: [2024-08-09 22:39:56,473] [INFO] [logging.py:96:log_dist] [Rank 0] step=14350, skipped=16, lr=[0.0001999969180699455, 0.0001999969180699455], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14350 loss: 0.8807 iter time (s): 4.235 samples/sec: 30.225
g0238:  iteration    14350/10000000 | consumed samples:      1836800 | consumed tokens:   3761766400 | elapsed time per iteration (ms): 4267.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.989872E-01 | loss scale: 131072.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.993 | tokens per gpu per second (tgs): 1919.535 | TFLOPs: 15.45 |
g0220: [2024-08-09 22:40:37,583] [INFO] [logging.py:96:log_dist] [Rank 0] step=14360, skipped=16, lr=[0.00019999689681476134, 0.00019999689681476134], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14360 loss: 0.8992 iter time (s): 4.078 samples/sec: 31.387
g0238:  iteration    14360/10000000 | consumed samples:      1838080 | consumed tokens:   3764387840 | elapsed time per iteration (ms): 4110.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.974098E-01 | loss scale: 131072.0 | grad norm: 0.259 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.137 | tokens per gpu per second (tgs): 1992.738 | TFLOPs: 16.04 |
g0220: [2024-08-09 22:41:19,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=14370, skipped=16, lr=[0.0001999968754865346, 0.0001999968754865346], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14370 loss: 0.9039 iter time (s): 4.122 samples/sec: 31.051
g0238:  iteration    14370/10000000 | consumed samples:      1839360 | consumed tokens:   3767009280 | elapsed time per iteration (ms): 4155.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.131074E-01 | loss scale: 131072.0 | grad norm: 0.264 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.801 | tokens per gpu per second (tgs): 1971.277 | TFLOPs: 15.86 |
g0220: [2024-08-09 22:42:00,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=14380, skipped=16, lr=[0.0001999968540852653, 0.0001999968540852653], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14380 loss: 0.9005 iter time (s): 4.105 samples/sec: 31.181
g0238:  iteration    14380/10000000 | consumed samples:      1840640 | consumed tokens:   3769630720 | elapsed time per iteration (ms): 4139.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.009033E-01 | loss scale: 131072.0 | grad norm: 0.240 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.922 | tokens per gpu per second (tgs): 1979.029 | TFLOPs: 15.93 |
g0220: [2024-08-09 22:42:39,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=14390, skipped=16, lr=[0.00019999683261095344, 0.00019999683261095344], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14390 loss: 0.9155 iter time (s): 3.872 samples/sec: 33.062
g0238:  iteration    14390/10000000 | consumed samples:      1841920 | consumed tokens:   3772252160 | elapsed time per iteration (ms): 3904.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.977404E-01 | loss scale: 131072.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.780 | tokens per gpu per second (tgs): 2097.936 | TFLOPs: 16.88 |
g0220: [2024-08-09 22:43:20,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=14400, skipped=16, lr=[0.00019999681106359904, 0.00019999681106359904], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14400 loss: 0.9155 iter time (s): 4.033 samples/sec: 31.738
g0238:  iteration    14400/10000000 | consumed samples:      1843200 | consumed tokens:   3774873600 | elapsed time per iteration (ms): 4065.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.059445E-01 | loss scale: 131072.0 | grad norm: 0.273 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.485 | tokens per gpu per second (tgs): 2015.064 | TFLOPs: 16.22 |
g0220: [2024-08-09 22:44:00,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=14410, skipped=16, lr=[0.00019999678944320216, 0.00019999678944320216], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14410 loss: 0.8750 iter time (s): 3.999 samples/sec: 32.012
g0238:  iteration    14410/10000000 | consumed samples:      1844480 | consumed tokens:   3777495040 | elapsed time per iteration (ms): 4031.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.006985E-01 | loss scale: 131072.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.754 | tokens per gpu per second (tgs): 2032.262 | TFLOPs: 16.35 |
g0220: [2024-08-09 22:44:41,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=14420, skipped=16, lr=[0.00019999676774976275, 0.00019999676774976275], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14420 loss: 0.8905 iter time (s): 4.038 samples/sec: 31.702
g0238:  iteration    14420/10000000 | consumed samples:      1845760 | consumed tokens:   3780116480 | elapsed time per iteration (ms): 4070.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.930415E-01 | loss scale: 131072.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.447 | tokens per gpu per second (tgs): 2012.596 | TFLOPs: 16.20 |
g0234: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-09 22:45:07,245] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-09 22:45:23,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=14430, skipped=16, lr=[0.00019999674598328087, 0.00019999674598328087], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14430 loss: 0.9089 iter time (s): 4.222 samples/sec: 30.315
g0238:  iteration    14430/10000000 | consumed samples:      1847040 | consumed tokens:   3782737920 | elapsed time per iteration (ms): 4255.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.003716E-01 | loss scale: 262144.0 | grad norm: 0.268 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.082 | tokens per gpu per second (tgs): 1925.231 | TFLOPs: 15.49 |
g0220: [2024-08-09 22:46:05,428] [INFO] [logging.py:96:log_dist] [Rank 0] step=14440, skipped=16, lr=[0.00019999672414375654, 0.00019999672414375654], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14440 loss: 0.9130 iter time (s): 4.129 samples/sec: 30.997
g0238:  iteration    14440/10000000 | consumed samples:      1848320 | consumed tokens:   3785359360 | elapsed time per iteration (ms): 4162.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.004550E-01 | loss scale: 262144.0 | grad norm: 0.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.749 | tokens per gpu per second (tgs): 1967.962 | TFLOPs: 15.84 |
g0220: [2024-08-09 22:46:44,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=14450, skipped=16, lr=[0.00019999670223118973, 0.00019999670223118973], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14450 loss: 0.8853 iter time (s): 3.910 samples/sec: 32.735
g0238:  iteration    14450/10000000 | consumed samples:      1849600 | consumed tokens:   3787980800 | elapsed time per iteration (ms): 3943.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.023091E-01 | loss scale: 262144.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.461 | tokens per gpu per second (tgs): 2077.513 | TFLOPs: 16.72 |
g0220: [2024-08-09 22:47:26,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=14460, skipped=16, lr=[0.00019999668024558054, 0.00019999668024558054], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14460 loss: 0.8950 iter time (s): 4.102 samples/sec: 31.208
g0238:  iteration    14460/10000000 | consumed samples:      1850880 | consumed tokens:   3790602240 | elapsed time per iteration (ms): 4135.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.901690E-01 | loss scale: 262144.0 | grad norm: 0.280 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.955 | tokens per gpu per second (tgs): 1981.102 | TFLOPs: 15.94 |
g0220: [2024-08-09 22:48:09,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=14470, skipped=16, lr=[0.00019999665818692887, 0.00019999665818692887], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14470 loss: 0.8985 iter time (s): 4.252 samples/sec: 30.106
g0238:  iteration    14470/10000000 | consumed samples:      1852160 | consumed tokens:   3793223680 | elapsed time per iteration (ms): 4293.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.996611E-01 | loss scale: 262144.0 | grad norm: 0.278 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.813 | tokens per gpu per second (tgs): 1908.053 | TFLOPs: 15.35 |
g0220: [2024-08-09 22:48:50,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=14480, skipped=16, lr=[0.00019999663605523485, 0.00019999663605523485], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14480 loss: 0.9211 iter time (s): 4.092 samples/sec: 31.280
g0238:  iteration    14480/10000000 | consumed samples:      1853440 | consumed tokens:   3795845120 | elapsed time per iteration (ms): 4127.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.960693E-01 | loss scale: 262144.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.010 | tokens per gpu per second (tgs): 1984.627 | TFLOPs: 15.97 |
g0220: [2024-08-09 22:49:32,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=14490, skipped=16, lr=[0.00019999661385049843, 0.00019999661385049843], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14490 loss: 0.9109 iter time (s): 4.214 samples/sec: 30.372
g0238:  iteration    14490/10000000 | consumed samples:      1854720 | consumed tokens:   3798466560 | elapsed time per iteration (ms): 4251.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.995462E-01 | loss scale: 262144.0 | grad norm: 0.277 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.108 | tokens per gpu per second (tgs): 1926.933 | TFLOPs: 15.51 |
g0220: [2024-08-09 22:50:13,153] [INFO] [logging.py:96:log_dist] [Rank 0] step=14500, skipped=16, lr=[0.00019999659157271966, 0.00019999659157271966], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14500 loss: 0.9488 iter time (s): 3.989 samples/sec: 32.088
g0238:  iteration    14500/10000000 | consumed samples:      1856000 | consumed tokens:   3801088000 | elapsed time per iteration (ms): 4021.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.019762E-01 | loss scale: 262144.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.828 | tokens per gpu per second (tgs): 2036.987 | TFLOPs: 16.39 |
g0220: [2024-08-09 22:50:55,317] [INFO] [logging.py:96:log_dist] [Rank 0] step=14510, skipped=16, lr=[0.0001999965692218985, 0.0001999965692218985], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14510 loss: 0.9033 iter time (s): 4.184 samples/sec: 30.595
g0238:  iteration    14510/10000000 | consumed samples:      1857280 | consumed tokens:   3803709440 | elapsed time per iteration (ms): 4216.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.995428E-01 | loss scale: 262144.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.356 | tokens per gpu per second (tgs): 1942.806 | TFLOPs: 15.63 |
g0220: [2024-08-09 22:51:36,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=14520, skipped=16, lr=[0.00019999654679803507, 0.00019999654679803507], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14520 loss: 0.9264 iter time (s): 4.087 samples/sec: 31.322
g0238:  iteration    14520/10000000 | consumed samples:      1858560 | consumed tokens:   3806330880 | elapsed time per iteration (ms): 4120.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.001160E-01 | loss scale: 262144.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.065 | tokens per gpu per second (tgs): 1988.154 | TFLOPs: 16.00 |
g0220: [2024-08-09 22:52:17,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=14530, skipped=16, lr=[0.00019999652430112931, 0.00019999652430112931], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14530 loss: 0.9026 iter time (s): 4.072 samples/sec: 31.438
g0238:  iteration    14530/10000000 | consumed samples:      1859840 | consumed tokens:   3808952320 | elapsed time per iteration (ms): 4104.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.053146E-01 | loss scale: 262144.0 | grad norm: 0.256 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.189 | tokens per gpu per second (tgs): 1996.087 | TFLOPs: 16.06 |
g0220: [2024-08-09 22:52:58,655] [INFO] [logging.py:96:log_dist] [Rank 0] step=14540, skipped=16, lr=[0.00019999650173118126, 0.00019999650173118126], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14540 loss: 0.8846 iter time (s): 4.077 samples/sec: 31.399
g0238:  iteration    14540/10000000 | consumed samples:      1861120 | consumed tokens:   3811573760 | elapsed time per iteration (ms): 4109.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.012601E-01 | loss scale: 262144.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.149 | tokens per gpu per second (tgs): 1993.548 | TFLOPs: 16.04 |
g0220: [2024-08-09 22:53:39,349] [INFO] [logging.py:96:log_dist] [Rank 0] step=14550, skipped=16, lr=[0.0001999964790881909, 0.0001999964790881909], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14550 loss: 0.8918 iter time (s): 4.036 samples/sec: 31.713
g0238:  iteration    14550/10000000 | consumed samples:      1862400 | consumed tokens:   3814195200 | elapsed time per iteration (ms): 4069.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.990738E-01 | loss scale: 262144.0 | grad norm: 0.255 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.456 | tokens per gpu per second (tgs): 2013.159 | TFLOPs: 16.20 |
g0220: [2024-08-09 22:54:19,993] [INFO] [logging.py:96:log_dist] [Rank 0] step=14560, skipped=16, lr=[0.00019999645637215833, 0.00019999645637215833], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14560 loss: 0.8990 iter time (s): 4.032 samples/sec: 31.749
g0238:  iteration    14560/10000000 | consumed samples:      1863680 | consumed tokens:   3816816640 | elapsed time per iteration (ms): 4064.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.905807E-01 | loss scale: 262144.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.493 | tokens per gpu per second (tgs): 2015.583 | TFLOPs: 16.22 |
g0220: [2024-08-09 22:55:01,720] [INFO] [logging.py:96:log_dist] [Rank 0] step=14570, skipped=16, lr=[0.00019999643358308347, 0.00019999643358308347], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14570 loss: 0.8842 iter time (s): 4.139 samples/sec: 30.922
g0238:  iteration    14570/10000000 | consumed samples:      1864960 | consumed tokens:   3819438080 | elapsed time per iteration (ms): 4172.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.046737E-01 | loss scale: 262144.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.674 | tokens per gpu per second (tgs): 1963.150 | TFLOPs: 15.80 |
g0220: [2024-08-09 22:55:42,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=14580, skipped=16, lr=[0.00019999641072096643, 0.00019999641072096643], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14580 loss: 0.8972 iter time (s): 3.992 samples/sec: 32.064
g0238:  iteration    14580/10000000 | consumed samples:      1866240 | consumed tokens:   3822059520 | elapsed time per iteration (ms): 4027.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.100754E-01 | loss scale: 262144.0 | grad norm: 0.288 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.778 | tokens per gpu per second (tgs): 2033.792 | TFLOPs: 16.37 |
g0220: [2024-08-09 22:56:22,656] [INFO] [logging.py:96:log_dist] [Rank 0] step=14590, skipped=16, lr=[0.00019999638778580713, 0.00019999638778580713], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14590 loss: 0.8831 iter time (s): 4.032 samples/sec: 31.742
g0238:  iteration    14590/10000000 | consumed samples:      1867520 | consumed tokens:   3824680960 | elapsed time per iteration (ms): 4070.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.893461E-01 | loss scale: 262144.0 | grad norm: 0.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.443 | tokens per gpu per second (tgs): 2012.332 | TFLOPs: 16.19 |
g0220: [2024-08-09 22:57:04,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=14600, skipped=16, lr=[0.0001999963647776057, 0.0001999963647776057], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14600 loss: 0.8725 iter time (s): 4.101 samples/sec: 31.209
g0238:  iteration    14600/10000000 | consumed samples:      1868800 | consumed tokens:   3827302400 | elapsed time per iteration (ms): 4134.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.903909E-01 | loss scale: 262144.0 | grad norm: 0.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.963 | tokens per gpu per second (tgs): 1981.617 | TFLOPs: 15.95 |
g0220: [2024-08-09 22:57:44,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=14610, skipped=16, lr=[0.00019999634169636207, 0.00019999634169636207], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14610 loss: 0.9115 iter time (s): 3.982 samples/sec: 32.147
g0238:  iteration    14610/10000000 | consumed samples:      1870080 | consumed tokens:   3829923840 | elapsed time per iteration (ms): 4014.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.999326E-01 | loss scale: 262144.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.886 | tokens per gpu per second (tgs): 2040.680 | TFLOPs: 16.42 |
g0220: [2024-08-09 22:58:24,213] [INFO] [logging.py:96:log_dist] [Rank 0] step=14620, skipped=16, lr=[0.00019999631854207626, 0.00019999631854207626], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14620 loss: 0.9029 iter time (s): 3.969 samples/sec: 32.248
g0238:  iteration    14620/10000000 | consumed samples:      1871360 | consumed tokens:   3832545280 | elapsed time per iteration (ms): 4001.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.945520E-01 | loss scale: 262144.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.985 | tokens per gpu per second (tgs): 2047.035 | TFLOPs: 16.47 |
g0220: [2024-08-09 22:59:05,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=14630, skipped=16, lr=[0.00019999629531474835, 0.00019999629531474835], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14630 loss: 0.8793 iter time (s): 4.117 samples/sec: 31.088
g0238:  iteration    14630/10000000 | consumed samples:      1872640 | consumed tokens:   3835166720 | elapsed time per iteration (ms): 4150.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.922732E-01 | loss scale: 262144.0 | grad norm: 0.258 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.843 | tokens per gpu per second (tgs): 1973.948 | TFLOPs: 15.88 |
g0220: [2024-08-09 22:59:46,821] [INFO] [logging.py:96:log_dist] [Rank 0] step=14640, skipped=16, lr=[0.0001999962720143783, 0.0001999962720143783], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14640 loss: 0.9339 iter time (s): 4.078 samples/sec: 31.387
g0238:  iteration    14640/10000000 | consumed samples:      1873920 | consumed tokens:   3837788160 | elapsed time per iteration (ms): 4110.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.937432E-01 | loss scale: 262144.0 | grad norm: 0.256 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.138 | tokens per gpu per second (tgs): 1992.824 | TFLOPs: 16.04 |
g0220: [2024-08-09 23:00:26,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=14650, skipped=16, lr=[0.0001999962486409662, 0.0001999962486409662], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14650 loss: 0.9278 iter time (s): 3.916 samples/sec: 32.685
g0238:  iteration    14650/10000000 | consumed samples:      1875200 | consumed tokens:   3840409600 | elapsed time per iteration (ms): 3949.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.030751E-01 | loss scale: 262144.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.410 | tokens per gpu per second (tgs): 2074.226 | TFLOPs: 16.69 |
g0220: [2024-08-09 23:01:08,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=14660, skipped=16, lr=[0.00019999622519451196, 0.00019999622519451196], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14660 loss: 0.8985 iter time (s): 4.166 samples/sec: 30.724
g0238:  iteration    14660/10000000 | consumed samples:      1876480 | consumed tokens:   3843031040 | elapsed time per iteration (ms): 4199.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.952015E-01 | loss scale: 262144.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.480 | tokens per gpu per second (tgs): 1950.749 | TFLOPs: 15.70 |
g0220: [2024-08-09 23:01:49,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=14670, skipped=16, lr=[0.00019999620167501566, 0.00019999620167501566], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14670 loss: 0.9053 iter time (s): 4.114 samples/sec: 31.110
g0238:  iteration    14670/10000000 | consumed samples:      1877760 | consumed tokens:   3845652480 | elapsed time per iteration (ms): 4147.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.956433E-01 | loss scale: 262144.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.862 | tokens per gpu per second (tgs): 1975.150 | TFLOPs: 15.89 |
g0220: [2024-08-09 23:02:30,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=14680, skipped=16, lr=[0.00019999617808247734, 0.00019999617808247734], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14680 loss: 0.8968 iter time (s): 4.054 samples/sec: 31.575
g0238:  iteration    14680/10000000 | consumed samples:      1879040 | consumed tokens:   3848273920 | elapsed time per iteration (ms): 4086.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.933258E-01 | loss scale: 262144.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.322 | tokens per gpu per second (tgs): 2004.611 | TFLOPs: 16.13 |
g0220: [2024-08-09 23:03:12,209] [INFO] [logging.py:96:log_dist] [Rank 0] step=14690, skipped=16, lr=[0.00019999615441689697, 0.00019999615441689697], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14690 loss: 0.8851 iter time (s): 4.123 samples/sec: 31.048
g0238:  iteration    14690/10000000 | consumed samples:      1880320 | consumed tokens:   3850895360 | elapsed time per iteration (ms): 4155.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.995951E-01 | loss scale: 262144.0 | grad norm: 0.231 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.800 | tokens per gpu per second (tgs): 1971.213 | TFLOPs: 15.86 |
g0220: [2024-08-09 23:03:52,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=14700, skipped=16, lr=[0.00019999613067827463, 0.00019999613067827463], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14700 loss: 0.8695 iter time (s): 3.965 samples/sec: 32.279
g0238:  iteration    14700/10000000 | consumed samples:      1881600 | consumed tokens:   3853516800 | elapsed time per iteration (ms): 3997.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.906065E-01 | loss scale: 262144.0 | grad norm: 0.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.017 | tokens per gpu per second (tgs): 2049.058 | TFLOPs: 16.49 |
g0220: [2024-08-09 23:04:32,233] [INFO] [logging.py:96:log_dist] [Rank 0] step=14710, skipped=16, lr=[0.00019999610686661026, 0.00019999610686661026], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14710 loss: 0.8950 iter time (s): 3.972 samples/sec: 32.229
g0238:  iteration    14710/10000000 | consumed samples:      1882880 | consumed tokens:   3856138240 | elapsed time per iteration (ms): 4004.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.982334E-01 | loss scale: 262144.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.965 | tokens per gpu per second (tgs): 2045.789 | TFLOPs: 16.46 |
g0220: [2024-08-09 23:05:13,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=14720, skipped=16, lr=[0.00019999608298190393, 0.00019999608298190393], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14720 loss: 0.9295 iter time (s): 4.111 samples/sec: 31.133
g0238:  iteration    14720/10000000 | consumed samples:      1884160 | consumed tokens:   3858759680 | elapsed time per iteration (ms): 4145.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.926156E-01 | loss scale: 262144.0 | grad norm: 0.306 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.877 | tokens per gpu per second (tgs): 1976.129 | TFLOPs: 15.90 |
g0220: [2024-08-09 23:05:56,329] [INFO] [logging.py:96:log_dist] [Rank 0] step=14730, skipped=16, lr=[0.0001999960590241556, 0.0001999960590241556], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14730 loss: 0.9062 iter time (s): 4.231 samples/sec: 30.254
g0238:  iteration    14730/10000000 | consumed samples:      1885440 | consumed tokens:   3861381120 | elapsed time per iteration (ms): 4264.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.999007E-01 | loss scale: 262144.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.018 | tokens per gpu per second (tgs): 1921.158 | TFLOPs: 15.46 |
g0220: [2024-08-09 23:06:37,196] [INFO] [logging.py:96:log_dist] [Rank 0] step=14740, skipped=16, lr=[0.00019999603499336538, 0.00019999603499336538], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14740 loss: 0.8903 iter time (s): 4.054 samples/sec: 31.574
g0238:  iteration    14740/10000000 | consumed samples:      1886720 | consumed tokens:   3864002560 | elapsed time per iteration (ms): 4086.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.911771E-01 | loss scale: 262144.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.321 | tokens per gpu per second (tgs): 2004.555 | TFLOPs: 16.13 |
g0220: [2024-08-09 23:07:17,623] [INFO] [logging.py:96:log_dist] [Rank 0] step=14750, skipped=16, lr=[0.00019999601088953323, 0.00019999601088953323], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14750 loss: 0.8788 iter time (s): 4.010 samples/sec: 31.920
g0238:  iteration    14750/10000000 | consumed samples:      1888000 | consumed tokens:   3866624000 | elapsed time per iteration (ms): 4042.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.870248E-01 | loss scale: 262144.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.662 | tokens per gpu per second (tgs): 2026.375 | TFLOPs: 16.31 |
g0220: [2024-08-09 23:07:59,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=14760, skipped=16, lr=[0.0001999959867126592, 0.0001999959867126592], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14760 loss: 0.8757 iter time (s): 4.129 samples/sec: 30.997
g0238:  iteration    14760/10000000 | consumed samples:      1889280 | consumed tokens:   3869245440 | elapsed time per iteration (ms): 4162.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.983006E-01 | loss scale: 262144.0 | grad norm: 0.262 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.752 | tokens per gpu per second (tgs): 1968.102 | TFLOPs: 15.84 |
g0220: [2024-08-09 23:08:39,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=14770, skipped=16, lr=[0.00019999596246274328, 0.00019999596246274328], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14770 loss: 0.9068 iter time (s): 4.030 samples/sec: 31.762
g0238:  iteration    14770/10000000 | consumed samples:      1890560 | consumed tokens:   3871866880 | elapsed time per iteration (ms): 4062.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.877851E-01 | loss scale: 262144.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.510 | tokens per gpu per second (tgs): 2016.611 | TFLOPs: 16.23 |
g0220: [2024-08-09 23:09:20,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=14780, skipped=16, lr=[0.00019999593813978549, 0.00019999593813978549], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14780 loss: 0.9084 iter time (s): 4.052 samples/sec: 31.586
g0238:  iteration    14780/10000000 | consumed samples:      1891840 | consumed tokens:   3874488320 | elapsed time per iteration (ms): 4085.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.922905E-01 | loss scale: 262144.0 | grad norm: 0.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.329 | tokens per gpu per second (tgs): 2005.059 | TFLOPs: 16.14 |
g0220: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14780
g0220: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14780
g0220: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14780
g0220: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: Grad overflow on iteration 14780
g0236: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 14780
g0238: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 14780
g0238: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 14780
g0238: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 23:09:25,000] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0236: Grad overflow on iteration 14780
g0238: Grad overflow on iteration 14780
g0236: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 14780
g0236: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 14780
g0235: Grad overflow on iteration 14780
g0237: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 14780
g0233: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 14780
g0235: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14780
g0233: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14780
g0233: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14780
g0225: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 14780
g0225: Grad overflow on iteration 14780
g0233: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 14780
g0234: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 14780
g0238: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 14780
g0237: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 14780
g0235: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 14780
g0234: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 14780
g0235: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 14780
g0233: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14780
g0236: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 14780
g0235: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 14780
g0225: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 14780
g0234: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 14780
g0238: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14780
g0220: [2024-08-09 23:09:25,000] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-09 23:10:02,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=14790, skipped=17, lr=[0.00019999591374378586, 0.00019999591374378586], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14790 loss: 0.9383 iter time (s): 4.145 samples/sec: 30.879
g0238:  iteration    14790/10000000 | consumed samples:      1893120 | consumed tokens:   3877109760 | elapsed time per iteration (ms): 4178.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.037278E-01 | loss scale: 131072.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.636 | tokens per gpu per second (tgs): 1960.688 | TFLOPs: 15.78 |
g0235: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 14798
g0225: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 14798
g0225: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: Grad overflow on iteration 14798
g0234: Grad overflow on iteration 14798
g0233: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: Grad overflow on iteration 14798
g0233: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14798
g0233: Grad overflow on iteration 14798
g0238: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 14798
g0236: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 14798
g0236: Grad overflow on iteration 14798
g0234: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 14798
g0233: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: Grad overflow on iteration 14798
g0235: Grad overflow on iteration 14798
g0235: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: Grad overflow on iteration 14798
g0237: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 14798
g0234: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 14798
g0225: Grad overflow on iteration 14798
g0233: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: Grad overflow on iteration 14798
g0235: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: Grad overflow on iteration 14798
g0236: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14798
g0220: Grad overflow on iteration 14798
g0225: Grad overflow on iteration 14798
g0238: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 14798
g0237: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: Grad overflow on iteration 14798
g0236: Grad overflow on iteration 14798
g0237: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14798
g0237: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 14798
g0220: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 14798
g0237: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-09 23:10:38,964] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 14798
g0236: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 14798
g0236: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 23:10:38,964] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: Grad overflow on iteration 14798
g0220: Grad overflow on iteration 14798
g0220: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-09 23:10:38,963] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-09 23:10:38,964] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-09 23:10:38,964] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
g0238: [2024-08-09 23:10:38,964] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-09 23:10:43,137] [INFO] [logging.py:96:log_dist] [Rank 0] step=14800, skipped=18, lr=[0.0001999958892747444, 0.0001999958892747444], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14800 loss: 0.9203 iter time (s): 4.030 samples/sec: 31.761
g0238:  iteration    14800/10000000 | consumed samples:      1894400 | consumed tokens:   3879731200 | elapsed time per iteration (ms): 4063.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.998577E-01 | loss scale: 65536.0 | grad norm: 0.342 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.504 | tokens per gpu per second (tgs): 2016.248 | TFLOPs: 16.23 |
g0220: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14803
g0220: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14803
g0225: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 14803
g0237: Grad overflow on iteration 14803
g0225: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: Grad overflow on iteration 14803
g0237: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 14803
g0235: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 14803
g0237: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 14803
g0225: Grad overflow on iteration 14803
g0233: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14803
g0235: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 14803
g0235: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 14803
g0238: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 14803
g0238: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 14803
g0238: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 14803
g0238: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: Grad overflow on iteration 14803
g0220: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: Grad overflow on iteration 14803
g0236: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: Grad overflow on iteration 14803
g0225: Grad overflow on iteration 14803
g0236: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 14803
g0236: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: Grad overflow on iteration 14803
g0236: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: Grad overflow on iteration 14803
g0237: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 14803
g0236: Grad overflow on iteration 14803
g0233: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14803
g0225: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 14803
g0234: Grad overflow on iteration 14803
g0233: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: Grad overflow on iteration 14803
g0238: Grad overflow on iteration 14803
g0234: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14803
g0220: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14803
g0220: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-09 23:10:59,569] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14803
g0233: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-09 23:10:59,570] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-09 23:10:59,570] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0225: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 14808
g0225: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 14808
g0225: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0225: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 14808
g0225: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 14808
g0225: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0225: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0225: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0234: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 14808
g0235: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14808
g0237: Grad overflow on iteration 14808
g0220: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0238: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14808
g0220: Grad overflow on iteration 14808
g0236: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 14808
g0234: Grad overflow on iteration 14808
g0236: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0234: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0220: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0234: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14808
g0234: Grad overflow on iteration 14808
g0233: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0233: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 14808
g0237: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 14808
g0237: Grad overflow on iteration 14808
g0238: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0220: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0238: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0238: Grad overflow on iteration 14808
g0237: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 14808
g0238: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0234: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0234: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 14808
g0235: Grad overflow on iteration 14808
g0236: Grad overflow on iteration 14808
g0236: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0236: Grad overflow on iteration 14808
g0234: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0235: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0235: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0233: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0238: Grad overflow on iteration 14808
g0236: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0233: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 14808
g0233: Grad overflow on iteration 14808
g0238: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0236: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0233: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0237: Grad overflow on iteration 14808
g0238: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 14808
g0233: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0238: Grad overflow on iteration 14808
g0235: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0235: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 14808
g0235: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 14808
g0235: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0235: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0237: [2024-08-09 23:11:19,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0236: [2024-08-09 23:11:19,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0220: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14808
g0220: [2024-08-09 23:11:19,976] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768.0, reducing to 16384.0
g0220: [2024-08-09 23:11:19,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0220: [2024-08-09 23:11:19,975] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 14808
g0220: [2024-08-09 23:11:19,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0238: [2024-08-09 23:11:19,976] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 32768.0 to 16384.0
g0220: [2024-08-09 23:11:23,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=14810, skipped=20, lr=[0.00019999586473266114, 0.00019999586473266114], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14810 loss: 0.9046 iter time (s): 4.044 samples/sec: 31.654
g0238:  iteration    14810/10000000 | consumed samples:      1895680 | consumed tokens:   3882352640 | elapsed time per iteration (ms): 4077.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.946457E-01 | loss scale: 16384.0 | grad norm: 0.339 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.395 | tokens per gpu per second (tgs): 2009.302 | TFLOPs: 16.17 |
g0220: [2024-08-09 23:12:04,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=14820, skipped=20, lr=[0.0001999958401175361, 0.0001999958401175361], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14820 loss: 0.8723 iter time (s): 4.003 samples/sec: 31.972
g0238:  iteration    14820/10000000 | consumed samples:      1896960 | consumed tokens:   3884974080 | elapsed time per iteration (ms): 4037.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.982720E-01 | loss scale: 16384.0 | grad norm: 0.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.706 | tokens per gpu per second (tgs): 2029.213 | TFLOPs: 16.33 |
g0220: [2024-08-09 23:12:44,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=14830, skipped=20, lr=[0.00019999581542936928, 0.00019999581542936928], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14830 loss: 0.8848 iter time (s): 4.034 samples/sec: 31.733
g0238:  iteration    14830/10000000 | consumed samples:      1898240 | consumed tokens:   3887595520 | elapsed time per iteration (ms): 4067.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.887263E-01 | loss scale: 16384.0 | grad norm: 0.243 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.471 | tokens per gpu per second (tgs): 2014.139 | TFLOPs: 16.21 |
g0220: [2024-08-09 23:13:24,328] [INFO] [logging.py:96:log_dist] [Rank 0] step=14840, skipped=20, lr=[0.0001999957906681607, 0.0001999957906681607], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14840 loss: 0.9018 iter time (s): 3.904 samples/sec: 32.786
g0238:  iteration    14840/10000000 | consumed samples:      1899520 | consumed tokens:   3890216960 | elapsed time per iteration (ms): 3937.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.041231E-01 | loss scale: 16384.0 | grad norm: 0.260 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.507 | tokens per gpu per second (tgs): 2080.462 | TFLOPs: 16.74 |
g0220: [2024-08-09 23:14:05,957] [INFO] [logging.py:96:log_dist] [Rank 0] step=14850, skipped=20, lr=[0.00019999576583391043, 0.00019999576583391043], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14850 loss: 0.8879 iter time (s): 4.130 samples/sec: 30.989
g0238:  iteration    14850/10000000 | consumed samples:      1900800 | consumed tokens:   3892838400 | elapsed time per iteration (ms): 4162.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.932417E-01 | loss scale: 16384.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.748 | tokens per gpu per second (tgs): 1967.877 | TFLOPs: 15.84 |
g0220: [2024-08-09 23:14:51,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=14860, skipped=20, lr=[0.00019999574092661838, 0.00019999574092661838], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14860 loss: 0.8908 iter time (s): 4.501 samples/sec: 28.437
g0238:  iteration    14860/10000000 | consumed samples:      1902080 | consumed tokens:   3895459840 | elapsed time per iteration (ms): 4534.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.954775E-01 | loss scale: 16384.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.228 | tokens per gpu per second (tgs): 1806.588 | TFLOPs: 14.54 |
g0220: [2024-08-09 23:15:33,006] [INFO] [logging.py:96:log_dist] [Rank 0] step=14870, skipped=20, lr=[0.0001999957159462847, 0.0001999957159462847], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14870 loss: 0.8862 iter time (s): 4.138 samples/sec: 30.934
g0238:  iteration    14870/10000000 | consumed samples:      1903360 | consumed tokens:   3898081280 | elapsed time per iteration (ms): 4170.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 9.002291E-01 | loss scale: 16384.0 | grad norm: 0.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.692 | tokens per gpu per second (tgs): 1964.310 | TFLOPs: 15.81 |
g0220: [2024-08-09 23:16:13,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=14880, skipped=20, lr=[0.00019999569089290933, 0.00019999569089290933], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14880 loss: 0.9032 iter time (s): 4.057 samples/sec: 31.550
g0238:  iteration    14880/10000000 | consumed samples:      1904640 | consumed tokens:   3900702720 | elapsed time per iteration (ms): 4089.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.843938E-01 | loss scale: 16384.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.299 | tokens per gpu per second (tgs): 2003.167 | TFLOPs: 16.12 |
g0220: [2024-08-09 23:16:55,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=14890, skipped=20, lr=[0.00019999566576649227, 0.00019999566576649227], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14890 loss: 0.8630 iter time (s): 4.132 samples/sec: 30.980
g0238:  iteration    14890/10000000 | consumed samples:      1905920 | consumed tokens:   3903324160 | elapsed time per iteration (ms): 4164.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.872962E-01 | loss scale: 16384.0 | grad norm: 0.279 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.739 | tokens per gpu per second (tgs): 1967.297 | TFLOPs: 15.83 |
g0220: [2024-08-09 23:17:36,892] [INFO] [logging.py:96:log_dist] [Rank 0] step=14900, skipped=20, lr=[0.00019999564056703362, 0.00019999564056703362], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14900 loss: 0.8959 iter time (s): 4.102 samples/sec: 31.202
g0238:  iteration    14900/10000000 | consumed samples:      1907200 | consumed tokens:   3905945600 | elapsed time per iteration (ms): 4135.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.877305E-01 | loss scale: 16384.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.955 | tokens per gpu per second (tgs): 1981.094 | TFLOPs: 15.94 |
g0220: [2024-08-09 23:18:16,860] [INFO] [logging.py:96:log_dist] [Rank 0] step=14910, skipped=20, lr=[0.00019999561529453335, 0.00019999561529453335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14910 loss: 0.8916 iter time (s): 3.950 samples/sec: 32.404
g0238:  iteration    14910/10000000 | consumed samples:      1908480 | consumed tokens:   3908567040 | elapsed time per iteration (ms): 3996.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.896471E-01 | loss scale: 16384.0 | grad norm: 0.292 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.026 | tokens per gpu per second (tgs): 2049.683 | TFLOPs: 16.49 |
g0220: [2024-08-09 23:18:58,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=14920, skipped=20, lr=[0.00019999558994899147, 0.00019999558994899147], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14920 loss: 0.8877 iter time (s): 4.131 samples/sec: 30.983
g0238:  iteration    14920/10000000 | consumed samples:      1909760 | consumed tokens:   3911188480 | elapsed time per iteration (ms): 4164.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.860317E-01 | loss scale: 16384.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.737 | tokens per gpu per second (tgs): 1967.184 | TFLOPs: 15.83 |
g0220: [2024-08-09 23:19:38,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=14930, skipped=20, lr=[0.00019999556453040803, 0.00019999556453040803], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14930 loss: 0.9105 iter time (s): 3.946 samples/sec: 32.439
g0238:  iteration    14930/10000000 | consumed samples:      1911040 | consumed tokens:   3913809920 | elapsed time per iteration (ms): 3978.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.903996E-01 | loss scale: 16384.0 | grad norm: 0.263 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.173 | tokens per gpu per second (tgs): 2059.044 | TFLOPs: 16.57 |
g0220: [2024-08-09 23:20:18,433] [INFO] [logging.py:96:log_dist] [Rank 0] step=14940, skipped=20, lr=[0.00019999553903878305, 0.00019999553903878305], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14940 loss: 0.8857 iter time (s): 3.982 samples/sec: 32.146
g0238:  iteration    14940/10000000 | consumed samples:      1912320 | consumed tokens:   3916431360 | elapsed time per iteration (ms): 4014.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.922493E-01 | loss scale: 16384.0 | grad norm: 0.262 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.886 | tokens per gpu per second (tgs): 2040.685 | TFLOPs: 16.42 |
g0220: [2024-08-09 23:20:58,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=14950, skipped=20, lr=[0.00019999551347411648, 0.00019999551347411648], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14950 loss: 0.9080 iter time (s): 4.003 samples/sec: 31.973
g0238:  iteration    14950/10000000 | consumed samples:      1913600 | consumed tokens:   3919052800 | elapsed time per iteration (ms): 4036.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.938657E-01 | loss scale: 16384.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.714 | tokens per gpu per second (tgs): 2029.718 | TFLOPs: 16.33 |
g0220: [2024-08-09 23:21:40,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=14960, skipped=20, lr=[0.00019999548783640843, 0.00019999548783640843], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14960 loss: 0.9257 iter time (s): 4.121 samples/sec: 31.063
g0238:  iteration    14960/10000000 | consumed samples:      1914880 | consumed tokens:   3921674240 | elapsed time per iteration (ms): 4153.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.960045E-01 | loss scale: 16384.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.821 | tokens per gpu per second (tgs): 1972.538 | TFLOPs: 15.87 |
g0220: [2024-08-09 23:22:20,252] [INFO] [logging.py:96:log_dist] [Rank 0] step=14970, skipped=20, lr=[0.00019999546212565888, 0.00019999546212565888], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14970 loss: 0.8977 iter time (s): 3.960 samples/sec: 32.325
g0238:  iteration    14970/10000000 | consumed samples:      1916160 | consumed tokens:   3924295680 | elapsed time per iteration (ms): 3992.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.929159E-01 | loss scale: 16384.0 | grad norm: 0.301 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.058 | tokens per gpu per second (tgs): 2051.734 | TFLOPs: 16.51 |
g0220: [2024-08-09 23:23:01,525] [INFO] [logging.py:96:log_dist] [Rank 0] step=14980, skipped=20, lr=[0.00019999543634186784, 0.00019999543634186784], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14980 loss: 0.9096 iter time (s): 4.095 samples/sec: 31.258
g0238:  iteration    14980/10000000 | consumed samples:      1917440 | consumed tokens:   3926917120 | elapsed time per iteration (ms): 4127.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.912850E-01 | loss scale: 16384.0 | grad norm: 0.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.012 | tokens per gpu per second (tgs): 1984.777 | TFLOPs: 15.97 |
g0220: [2024-08-09 23:23:42,834] [INFO] [logging.py:96:log_dist] [Rank 0] step=14990, skipped=20, lr=[0.00019999541048503538, 0.00019999541048503538], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 14990 loss: 0.8772 iter time (s): 4.098 samples/sec: 31.231
g0238:  iteration    14990/10000000 | consumed samples:      1918720 | consumed tokens:   3929538560 | elapsed time per iteration (ms): 4130.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.847575E-01 | loss scale: 16384.0 | grad norm: 0.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.987 | tokens per gpu per second (tgs): 1983.139 | TFLOPs: 15.96 |
g0220: [2024-08-09 23:24:22,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=15000, skipped=20, lr=[0.00019999538455516144, 0.00019999538455516144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15000 loss: 0.9272 iter time (s): 3.927 samples/sec: 32.598
g0238:  iteration    15000/10000000 | consumed samples:      1920000 | consumed tokens:   3932160000 | elapsed time per iteration (ms): 3959.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.933471E-01 | loss scale: 16384.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.330 | tokens per gpu per second (tgs): 2069.117 | TFLOPs: 16.65 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 15000 | lm loss value: 8.896030E-01 | lm loss PPL: 2.434163E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   15000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-09 23:30:41,132] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step15000 is about to be saved!
g0238: [2024-08-09 23:30:41,138] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0238: [2024-08-09 23:30:41,138] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0238: [2024-08-09 23:30:41,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0220: [2024-08-09 23:30:41,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0236: [2024-08-09 23:30:41,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0236: [2024-08-09 23:30:41,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0220: [2024-08-09 23:30:41,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0220: [2024-08-09 23:30:41,139] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0236: [2024-08-09 23:30:41,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0237: [2024-08-09 23:30:41,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0237: [2024-08-09 23:30:41,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0237: [2024-08-09 23:30:41,140] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0225: [2024-08-09 23:30:41,141] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0225: [2024-08-09 23:30:41,141] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0225: [2024-08-09 23:30:41,141] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0234: [2024-08-09 23:30:41,141] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0234: [2024-08-09 23:30:41,141] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0234: [2024-08-09 23:30:41,141] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0235: [2024-08-09 23:30:41,142] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0235: [2024-08-09 23:30:41,142] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0235: [2024-08-09 23:30:41,142] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0233: [2024-08-09 23:30:41,142] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0233: [2024-08-09 23:30:41,142] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0233: [2024-08-09 23:30:41,142] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0238: [2024-08-09 23:30:41,166] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_23-model_00-model_states.pt...
g0237: [2024-08-09 23:30:41,177] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_20-model_00-model_states.pt...
g0236: [2024-08-09 23:30:41,177] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_17-model_00-model_states.pt...
g0235: [2024-08-09 23:30:41,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_14-model_00-model_states.pt...
g0234: [2024-08-09 23:30:41,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_11-model_00-model_states.pt...
g0225: [2024-08-09 23:30:41,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_05-model_00-model_states.pt...
g0233: [2024-08-09 23:30:41,180] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_08-model_00-model_states.pt...
g0220: [2024-08-09 23:30:41,192] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_01-model_00-model_states.pt...
g0238: [2024-08-09 23:30:41,283] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_23-model_00-model_states.pt.
g0238: [2024-08-09 23:30:41,284] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_24-model_00-model_states.pt...
g0238: [2024-08-09 23:30:41,286] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_24-model_00-model_states.pt.
g0233: [2024-08-09 23:30:41,286] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_08-model_00-model_states.pt.
g0236: [2024-08-09 23:30:41,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_17-model_00-model_states.pt.
g0235: [2024-08-09 23:30:41,307] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_14-model_00-model_states.pt.
g0234: [2024-08-09 23:30:41,323] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_11-model_00-model_states.pt.
g0233: [2024-08-09 23:30:41,324] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_09-model_00-model_states.pt...
g0225: [2024-08-09 23:30:41,333] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_05-model_00-model_states.pt.
g0236: [2024-08-09 23:30:41,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_18-model_00-model_states.pt...
g0238: [2024-08-09 23:30:41,339] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_25-model_00-model_states.pt...
g0235: [2024-08-09 23:30:41,345] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_15-model_00-model_states.pt...
g0237: [2024-08-09 23:30:41,355] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_20-model_00-model_states.pt.
g0234: [2024-08-09 23:30:41,362] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_12-model_00-model_states.pt...
g0225: [2024-08-09 23:30:41,373] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_06-model_00-model_states.pt...
g0237: [2024-08-09 23:30:41,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_21-model_00-model_states.pt...
g0220: [2024-08-09 23:30:41,439] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_01-model_00-model_states.pt.
g0235: [2024-08-09 23:30:41,466] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_15-model_00-model_states.pt.
g0233: [2024-08-09 23:30:41,467] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_09-model_00-model_states.pt.
g0220: [2024-08-09 23:30:41,468] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_02-model_00-model_states.pt...
g0236: [2024-08-09 23:30:41,475] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_18-model_00-model_states.pt.
g0234: [2024-08-09 23:30:41,480] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_12-model_00-model_states.pt.
g0225: [2024-08-09 23:30:41,486] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_06-model_00-model_states.pt.
g0235: [2024-08-09 23:30:41,500] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_16-model_00-model_states.pt...
g0233: [2024-08-09 23:30:41,501] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_10-model_00-model_states.pt...
g0236: [2024-08-09 23:30:41,511] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_19-model_00-model_states.pt...
g0234: [2024-08-09 23:30:41,515] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_13-model_00-model_states.pt...
g0225: [2024-08-09 23:30:41,522] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_07-model_00-model_states.pt...
g0237: [2024-08-09 23:30:41,522] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_21-model_00-model_states.pt.
g0238: [2024-08-09 23:30:41,530] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_25-model_00-model_states.pt.
g0238: [2024-08-09 23:30:41,531] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_07_model_states.pt...
g0237: [2024-08-09 23:30:41,556] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_22-model_00-model_states.pt...
g0233: [2024-08-09 23:30:41,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_10-model_00-model_states.pt.
g0233: [2024-08-09 23:30:41,630] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_02_model_states.pt...
g0235: [2024-08-09 23:30:41,631] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_16-model_00-model_states.pt.
g0235: [2024-08-09 23:30:41,633] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_04_model_states.pt...
g0220: [2024-08-09 23:30:41,642] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_02-model_00-model_states.pt.
g0237: [2024-08-09 23:30:41,650] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_22-model_00-model_states.pt.
g0237: [2024-08-09 23:30:41,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_06_model_states.pt...
g0225: [2024-08-09 23:30:41,656] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_07-model_00-model_states.pt.
g0225: [2024-08-09 23:30:41,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_01_model_states.pt...
g0234: [2024-08-09 23:30:41,662] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_13-model_00-model_states.pt.
g0234: [2024-08-09 23:30:41,665] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_03_model_states.pt...
g0236: [2024-08-09 23:30:41,667] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_19-model_00-model_states.pt.
g0236: [2024-08-09 23:30:41,669] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_05_model_states.pt...
g0220: [2024-08-09 23:30:41,673] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_03-model_00-model_states.pt...
g0220: [2024-08-09 23:30:41,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_03-model_00-model_states.pt.
g0220: [2024-08-09 23:30:41,836] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_04-model_00-model_states.pt...
g0220: [2024-08-09 23:30:41,973] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/layer_04-model_00-model_states.pt.
g0220: [2024-08-09 23:30:41,974] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_00_model_states.pt
g0220: [2024-08-09 23:30:41,974] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_00_model_states.pt...
g0238: [2024-08-09 23:30:43,455] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_07_model_states.pt.
g0238: [2024-08-09 23:30:43,456] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0237: [2024-08-09 23:30:44,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_06_model_states.pt.
g0237: [2024-08-09 23:30:44,047] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0234: [2024-08-09 23:30:44,048] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_03_model_states.pt.
g0234: [2024-08-09 23:30:44,048] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0233: [2024-08-09 23:30:44,050] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_02_model_states.pt.
g0233: [2024-08-09 23:30:44,050] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0235: [2024-08-09 23:30:44,063] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_04_model_states.pt.
g0235: [2024-08-09 23:30:44,064] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0236: [2024-08-09 23:30:44,348] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_05_model_states.pt.
g0236: [2024-08-09 23:30:44,348] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0225: [2024-08-09 23:30:44,420] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_01_model_states.pt.
g0225: [2024-08-09 23:30:44,420] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0220: [2024-08-09 23:30:45,564] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step15000/mp_rank_00_model_states.pt.
g0220: [2024-08-09 23:30:45,565] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15000 is ready now!
g0220:   successfully saved checkpoint at iteration   15000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 5.01, Latency(second): 4.492
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (4491.87, 4492.07)
g0220: [2024-08-09 23:31:26,128] [INFO] [logging.py:96:log_dist] [Rank 0] step=15010, skipped=20, lr=[0.0001999953585522461, 0.0001999953585522461], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15010 loss: 0.8750 iter time (s): 4.023 samples/sec: 31.818
g0238:  iteration    15010/10000000 | consumed samples:      1921280 | consumed tokens:   3934781440 | elapsed time per iteration (ms): 42370.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.825661E-01 | loss scale: 16384.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 3.021 | tokens per gpu per second (tgs): 193.343 | TFLOPs: 1.56 |
g0220: [2024-08-09 23:32:07,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=15020, skipped=20, lr=[0.00019999533247628935, 0.00019999533247628935], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15020 loss: 0.8930 iter time (s): 4.117 samples/sec: 31.090
g0238:  iteration    15020/10000000 | consumed samples:      1922560 | consumed tokens:   3937402880 | elapsed time per iteration (ms): 4150.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.859569E-01 | loss scale: 16384.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.841 | tokens per gpu per second (tgs): 1973.849 | TFLOPs: 15.88 |
g0220: [2024-08-09 23:32:47,877] [INFO] [logging.py:96:log_dist] [Rank 0] step=15030, skipped=20, lr=[0.0001999953063272912, 0.0001999953063272912], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15030 loss: 0.8888 iter time (s): 3.991 samples/sec: 32.069
g0238:  iteration    15030/10000000 | consumed samples:      1923840 | consumed tokens:   3940024320 | elapsed time per iteration (ms): 4024.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.941714E-01 | loss scale: 16384.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.805 | tokens per gpu per second (tgs): 2035.542 | TFLOPs: 16.38 |
g0220: [2024-08-09 23:33:29,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=15040, skipped=20, lr=[0.00019999528010525175, 0.00019999528010525175], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15040 loss: 0.9086 iter time (s): 4.092 samples/sec: 31.282
g0238:  iteration    15040/10000000 | consumed samples:      1925120 | consumed tokens:   3942645760 | elapsed time per iteration (ms): 4124.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.968272E-01 | loss scale: 16384.0 | grad norm: 0.428 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.037 | tokens per gpu per second (tgs): 1986.341 | TFLOPs: 15.98 |
g0220: [2024-08-09 23:34:10,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=15050, skipped=20, lr=[0.00019999525381017092, 0.00019999525381017092], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15050 loss: 0.9214 iter time (s): 4.119 samples/sec: 31.073
g0238:  iteration    15050/10000000 | consumed samples:      1926400 | consumed tokens:   3945267200 | elapsed time per iteration (ms): 4152.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.921098E-01 | loss scale: 16384.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.828 | tokens per gpu per second (tgs): 1973.005 | TFLOPs: 15.88 |
g0220: [2024-08-09 23:34:51,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=15060, skipped=20, lr=[0.00019999522744204877, 0.00019999522744204877], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15060 loss: 0.8747 iter time (s): 4.085 samples/sec: 31.333
g0238:  iteration    15060/10000000 | consumed samples:      1927680 | consumed tokens:   3947888640 | elapsed time per iteration (ms): 4117.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.824953E-01 | loss scale: 16384.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.084 | tokens per gpu per second (tgs): 1989.371 | TFLOPs: 16.01 |
g0220: [2024-08-09 23:35:31,277] [INFO] [logging.py:96:log_dist] [Rank 0] step=15070, skipped=20, lr=[0.00019999520100088535, 0.00019999520100088535], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15070 loss: 0.9059 iter time (s): 3.913 samples/sec: 32.710
g0238:  iteration    15070/10000000 | consumed samples:      1928960 | consumed tokens:   3950510080 | elapsed time per iteration (ms): 3945.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.873337E-01 | loss scale: 16384.0 | grad norm: 0.231 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.438 | tokens per gpu per second (tgs): 2076.063 | TFLOPs: 16.71 |
g0220: [2024-08-09 23:36:13,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=15080, skipped=20, lr=[0.00019999517448668065, 0.00019999517448668065], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15080 loss: 0.8856 iter time (s): 4.169 samples/sec: 30.699
g0238:  iteration    15080/10000000 | consumed samples:      1930240 | consumed tokens:   3953131520 | elapsed time per iteration (ms): 4203.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.800714E-01 | loss scale: 16384.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.453 | tokens per gpu per second (tgs): 1948.984 | TFLOPs: 15.68 |
g0220: [2024-08-09 23:36:54,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=15090, skipped=20, lr=[0.00019999514789943468, 0.00019999514789943468], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15090 loss: 0.9147 iter time (s): 4.074 samples/sec: 31.417
g0238:  iteration    15090/10000000 | consumed samples:      1931520 | consumed tokens:   3955752960 | elapsed time per iteration (ms): 4107.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.908472E-01 | loss scale: 16384.0 | grad norm: 0.423 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.166 | tokens per gpu per second (tgs): 1994.643 | TFLOPs: 16.05 |
g0220: [2024-08-09 23:37:35,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=15100, skipped=20, lr=[0.00019999512123914748, 0.00019999512123914748], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15100 loss: 0.9104 iter time (s): 4.046 samples/sec: 31.634
g0238:  iteration    15100/10000000 | consumed samples:      1932800 | consumed tokens:   3958374400 | elapsed time per iteration (ms): 4079.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.917180E-01 | loss scale: 16384.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.380 | tokens per gpu per second (tgs): 2008.329 | TFLOPs: 16.16 |
g0220: [2024-08-09 23:38:15,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=15110, skipped=20, lr=[0.00019999509450581906, 0.00019999509450581906], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15110 loss: 0.8744 iter time (s): 3.984 samples/sec: 32.132
g0238:  iteration    15110/10000000 | consumed samples:      1934080 | consumed tokens:   3960995840 | elapsed time per iteration (ms): 4017.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.929615E-01 | loss scale: 16384.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.861 | tokens per gpu per second (tgs): 2039.075 | TFLOPs: 16.41 |
g0220: [2024-08-09 23:38:55,426] [INFO] [logging.py:96:log_dist] [Rank 0] step=15120, skipped=20, lr=[0.00019999506769944944, 0.00019999506769944944], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15120 loss: 0.9129 iter time (s): 3.975 samples/sec: 32.198
g0238:  iteration    15120/10000000 | consumed samples:      1935360 | consumed tokens:   3963617280 | elapsed time per iteration (ms): 4008.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.786864E-01 | loss scale: 16384.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.936 | tokens per gpu per second (tgs): 2043.910 | TFLOPs: 16.45 |
g0220: [2024-08-09 23:39:37,197] [INFO] [logging.py:96:log_dist] [Rank 0] step=15130, skipped=20, lr=[0.00019999504082003866, 0.00019999504082003866], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15130 loss: 0.9088 iter time (s): 4.144 samples/sec: 30.885
g0238:  iteration    15130/10000000 | consumed samples:      1936640 | consumed tokens:   3966238720 | elapsed time per iteration (ms): 4177.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.736831E-01 | loss scale: 16384.0 | grad norm: 0.345 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.643 | tokens per gpu per second (tgs): 1961.149 | TFLOPs: 15.78 |
g0220: [2024-08-09 23:40:18,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=15140, skipped=20, lr=[0.00019999501386758672, 0.00019999501386758672], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15140 loss: 0.8730 iter time (s): 4.058 samples/sec: 31.540
g0238:  iteration    15140/10000000 | consumed samples:      1937920 | consumed tokens:   3968860160 | elapsed time per iteration (ms): 4091.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.821220E-01 | loss scale: 16384.0 | grad norm: 0.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.283 | tokens per gpu per second (tgs): 2002.083 | TFLOPs: 16.11 |
g0220: [2024-08-09 23:40:59,845] [INFO] [logging.py:96:log_dist] [Rank 0] step=15150, skipped=20, lr=[0.00019999498684209363, 0.00019999498684209363], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15150 loss: 0.8750 iter time (s): 4.140 samples/sec: 30.919
g0238:  iteration    15150/10000000 | consumed samples:      1939200 | consumed tokens:   3971481600 | elapsed time per iteration (ms): 4173.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.887984E-01 | loss scale: 16384.0 | grad norm: 0.283 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.673 | tokens per gpu per second (tgs): 1963.077 | TFLOPs: 15.80 |
g0220: [2024-08-09 23:41:40,859] [INFO] [logging.py:96:log_dist] [Rank 0] step=15160, skipped=20, lr=[0.00019999495974355947, 0.00019999495974355947], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15160 loss: 0.8794 iter time (s): 4.069 samples/sec: 31.459
g0238:  iteration    15160/10000000 | consumed samples:      1940480 | consumed tokens:   3974103040 | elapsed time per iteration (ms): 4101.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.794194E-01 | loss scale: 16384.0 | grad norm: 0.462 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.209 | tokens per gpu per second (tgs): 1997.385 | TFLOPs: 16.07 |
g0220: [2024-08-09 23:42:20,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=15170, skipped=20, lr=[0.00019999493257198418, 0.00019999493257198418], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15170 loss: 0.9063 iter time (s): 3.969 samples/sec: 32.248
g0238:  iteration    15170/10000000 | consumed samples:      1941760 | consumed tokens:   3976724480 | elapsed time per iteration (ms): 4001.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.932011E-01 | loss scale: 16384.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.988 | tokens per gpu per second (tgs): 2047.245 | TFLOPs: 16.47 |
g0220: [2024-08-09 23:43:01,034] [INFO] [logging.py:96:log_dist] [Rank 0] step=15180, skipped=20, lr=[0.0001999949053273678, 0.0001999949053273678], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15180 loss: 0.8831 iter time (s): 3.983 samples/sec: 32.140
g0238:  iteration    15180/10000000 | consumed samples:      1943040 | consumed tokens:   3979345920 | elapsed time per iteration (ms): 4016.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.847022E-01 | loss scale: 16384.0 | grad norm: 0.251 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.873 | tokens per gpu per second (tgs): 2039.858 | TFLOPs: 16.42 |
g0220: [2024-08-09 23:43:41,579] [INFO] [logging.py:96:log_dist] [Rank 0] step=15190, skipped=20, lr=[0.00019999487800971042, 0.00019999487800971042], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15190 loss: 0.8615 iter time (s): 4.022 samples/sec: 31.825
g0238:  iteration    15190/10000000 | consumed samples:      1944320 | consumed tokens:   3981967360 | elapsed time per iteration (ms): 4054.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.873879E-01 | loss scale: 16384.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.570 | tokens per gpu per second (tgs): 2020.471 | TFLOPs: 16.26 |
g0220: [2024-08-09 23:44:22,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=15200, skipped=20, lr=[0.000199994850619012, 0.000199994850619012], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15200 loss: 0.9116 iter time (s): 4.093 samples/sec: 31.274
g0238:  iteration    15200/10000000 | consumed samples:      1945600 | consumed tokens:   3984588800 | elapsed time per iteration (ms): 4125.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.841356E-01 | loss scale: 16384.0 | grad norm: 0.317 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.026 | tokens per gpu per second (tgs): 1985.645 | TFLOPs: 15.98 |
g0220: [2024-08-09 23:45:04,183] [INFO] [logging.py:96:log_dist] [Rank 0] step=15210, skipped=20, lr=[0.00019999482315527257, 0.00019999482315527257], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15210 loss: 0.8867 iter time (s): 4.102 samples/sec: 31.203
g0238:  iteration    15210/10000000 | consumed samples:      1946880 | consumed tokens:   3987210240 | elapsed time per iteration (ms): 4134.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.951699E-01 | loss scale: 16384.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.957 | tokens per gpu per second (tgs): 1981.261 | TFLOPs: 15.94 |
g0220: [2024-08-09 23:45:44,536] [INFO] [logging.py:96:log_dist] [Rank 0] step=15220, skipped=20, lr=[0.00019999479561849217, 0.00019999479561849217], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15220 loss: 0.8762 iter time (s): 4.002 samples/sec: 31.981
g0238:  iteration    15220/10000000 | consumed samples:      1948160 | consumed tokens:   3989831680 | elapsed time per iteration (ms): 4035.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.827216E-01 | loss scale: 16384.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.720 | tokens per gpu per second (tgs): 2030.075 | TFLOPs: 16.34 |
g0220: [2024-08-09 23:46:24,186] [INFO] [logging.py:96:log_dist] [Rank 0] step=15230, skipped=20, lr=[0.00019999476800867076, 0.00019999476800867076], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15230 loss: 0.8876 iter time (s): 3.932 samples/sec: 32.551
g0238:  iteration    15230/10000000 | consumed samples:      1949440 | consumed tokens:   3992453120 | elapsed time per iteration (ms): 3965.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.843545E-01 | loss scale: 16384.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.283 | tokens per gpu per second (tgs): 2066.090 | TFLOPs: 16.63 |
g0220: [2024-08-09 23:47:04,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=15240, skipped=20, lr=[0.00019999474032580847, 0.00019999474032580847], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15240 loss: 0.8912 iter time (s): 4.048 samples/sec: 31.622
g0238:  iteration    15240/10000000 | consumed samples:      1950720 | consumed tokens:   3995074560 | elapsed time per iteration (ms): 4081.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.863808E-01 | loss scale: 16384.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.365 | tokens per gpu per second (tgs): 2007.345 | TFLOPs: 16.15 |
g0220: [2024-08-09 23:47:45,914] [INFO] [logging.py:96:log_dist] [Rank 0] step=15250, skipped=20, lr=[0.0001999947125699052, 0.0001999947125699052], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15250 loss: 0.8918 iter time (s): 4.059 samples/sec: 31.533
g0238:  iteration    15250/10000000 | consumed samples:      1952000 | consumed tokens:   3997696000 | elapsed time per iteration (ms): 4091.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.791762E-01 | loss scale: 16384.0 | grad norm: 0.231 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.283 | tokens per gpu per second (tgs): 2002.083 | TFLOPs: 16.11 |
g0220: [2024-08-09 23:48:27,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=15260, skipped=20, lr=[0.00019999468474096103, 0.00019999468474096103], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15260 loss: 0.8813 iter time (s): 4.128 samples/sec: 31.011
g0238:  iteration    15260/10000000 | consumed samples:      1953280 | consumed tokens:   4000317440 | elapsed time per iteration (ms): 4161.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.915411E-01 | loss scale: 16384.0 | grad norm: 0.261 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.762 | tokens per gpu per second (tgs): 1968.747 | TFLOPs: 15.84 |
g0220: [2024-08-09 23:49:09,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=15270, skipped=20, lr=[0.000199994656838976, 0.000199994656838976], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15270 loss: 0.8728 iter time (s): 4.136 samples/sec: 30.947
g0238:  iteration    15270/10000000 | consumed samples:      1954560 | consumed tokens:   4002938880 | elapsed time per iteration (ms): 4172.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.854221E-01 | loss scale: 16384.0 | grad norm: 0.251 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.674 | tokens per gpu per second (tgs): 1963.159 | TFLOPs: 15.80 |
g0220: [2024-08-09 23:49:49,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=15280, skipped=20, lr=[0.00019999462886395011, 0.00019999462886395011], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15280 loss: 0.8693 iter time (s): 3.977 samples/sec: 32.188
g0238:  iteration    15280/10000000 | consumed samples:      1955840 | consumed tokens:   4005560320 | elapsed time per iteration (ms): 4009.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.791488E-01 | loss scale: 16384.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.927 | tokens per gpu per second (tgs): 2043.346 | TFLOPs: 16.44 |
g0220: [2024-08-09 23:50:29,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=15290, skipped=20, lr=[0.0001999946008158834, 0.0001999946008158834], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15290 loss: 0.8596 iter time (s): 3.976 samples/sec: 32.189
g0238:  iteration    15290/10000000 | consumed samples:      1957120 | consumed tokens:   4008181760 | elapsed time per iteration (ms): 4009.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.853037E-01 | loss scale: 16384.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.926 | tokens per gpu per second (tgs): 2043.279 | TFLOPs: 16.44 |
g0220: [2024-08-09 23:51:09,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=15300, skipped=20, lr=[0.00019999457269477585, 0.00019999457269477585], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15300 loss: 0.8815 iter time (s): 3.987 samples/sec: 32.102
g0238:  iteration    15300/10000000 | consumed samples:      1958400 | consumed tokens:   4010803200 | elapsed time per iteration (ms): 4020.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.818208E-01 | loss scale: 16384.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.838 | tokens per gpu per second (tgs): 2037.619 | TFLOPs: 16.40 |
g0220: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0220: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0225: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0225: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0225: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0235: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0235: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0235: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0238: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0234: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0238: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0238: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0234: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0238: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0237: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0237: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0237: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0236: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0233: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0237: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0236: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0236: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0233: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0236: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-09 23:51:50,687] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0234: [2024-08-09 23:51:50,688] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0236: [2024-08-09 23:51:50,688] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0233: [2024-08-09 23:51:50,688] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0235: [2024-08-09 23:51:50,688] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0225: [2024-08-09 23:51:50,688] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0220: [2024-08-09 23:51:50,688] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0238: [2024-08-09 23:51:50,688] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0237: [2024-08-09 23:51:50,688] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 16384.0 to 32768.0
g0220: [2024-08-09 23:51:50,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=15310, skipped=20, lr=[0.00019999454450062753, 0.00019999454450062753], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15310 loss: 0.8751 iter time (s): 4.072 samples/sec: 31.433
g0238:  iteration    15310/10000000 | consumed samples:      1959680 | consumed tokens:   4013424640 | elapsed time per iteration (ms): 4105.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.828638E-01 | loss scale: 32768.0 | grad norm: 0.104 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.180 | tokens per gpu per second (tgs): 1995.499 | TFLOPs: 16.06 |
g0220: [2024-08-09 23:52:32,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=15320, skipped=20, lr=[0.0001999945162334384, 0.0001999945162334384], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15320 loss: 0.8940 iter time (s): 4.179 samples/sec: 30.628
g0238:  iteration    15320/10000000 | consumed samples:      1960960 | consumed tokens:   4016046080 | elapsed time per iteration (ms): 4212.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.809643E-01 | loss scale: 32768.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.389 | tokens per gpu per second (tgs): 1944.866 | TFLOPs: 15.65 |
g0220: [2024-08-09 23:53:15,096] [INFO] [logging.py:96:log_dist] [Rank 0] step=15330, skipped=20, lr=[0.00019999448789320855, 0.00019999448789320855], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15330 loss: 0.9095 iter time (s): 4.187 samples/sec: 30.570
g0238:  iteration    15330/10000000 | consumed samples:      1962240 | consumed tokens:   4018667520 | elapsed time per iteration (ms): 4228.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.785817E-01 | loss scale: 32768.0 | grad norm: 0.304 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.269 | tokens per gpu per second (tgs): 1937.237 | TFLOPs: 15.59 |
g0220: [2024-08-09 23:53:55,290] [INFO] [logging.py:96:log_dist] [Rank 0] step=15340, skipped=20, lr=[0.00019999445947993795, 0.00019999445947993795], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15340 loss: 0.8650 iter time (s): 3.987 samples/sec: 32.106
g0238:  iteration    15340/10000000 | consumed samples:      1963520 | consumed tokens:   4021288960 | elapsed time per iteration (ms): 4032.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.779432E-01 | loss scale: 32768.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.743 | tokens per gpu per second (tgs): 2031.522 | TFLOPs: 16.35 |
g0220: [2024-08-09 23:54:36,170] [INFO] [logging.py:96:log_dist] [Rank 0] step=15350, skipped=20, lr=[0.00019999443099362666, 0.00019999443099362666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15350 loss: 0.8734 iter time (s): 4.035 samples/sec: 31.726
g0238:  iteration    15350/10000000 | consumed samples:      1964800 | consumed tokens:   4023910400 | elapsed time per iteration (ms): 4074.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.747744E-01 | loss scale: 32768.0 | grad norm: 0.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.417 | tokens per gpu per second (tgs): 2010.667 | TFLOPs: 16.18 |
g0220: [2024-08-09 23:55:17,849] [INFO] [logging.py:96:log_dist] [Rank 0] step=15360, skipped=20, lr=[0.00019999440243427468, 0.00019999440243427468], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15360 loss: 0.8910 iter time (s): 4.134 samples/sec: 30.960
g0238:  iteration    15360/10000000 | consumed samples:      1966080 | consumed tokens:   4026531840 | elapsed time per iteration (ms): 4167.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.724753E-01 | loss scale: 32768.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.711 | tokens per gpu per second (tgs): 1965.484 | TFLOPs: 15.82 |
g0220: [2024-08-09 23:55:59,427] [INFO] [logging.py:96:log_dist] [Rank 0] step=15370, skipped=20, lr=[0.00019999437380188205, 0.00019999437380188205], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15370 loss: 0.8876 iter time (s): 4.124 samples/sec: 31.035
g0238:  iteration    15370/10000000 | consumed samples:      1967360 | consumed tokens:   4029153280 | elapsed time per iteration (ms): 4157.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.850731E-01 | loss scale: 32768.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.787 | tokens per gpu per second (tgs): 1970.341 | TFLOPs: 15.86 |
g0220: [2024-08-09 23:56:41,941] [INFO] [logging.py:96:log_dist] [Rank 0] step=15380, skipped=20, lr=[0.00019999434509644876, 0.00019999434509644876], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15380 loss: 0.8555 iter time (s): 4.217 samples/sec: 30.351
g0238:  iteration    15380/10000000 | consumed samples:      1968640 | consumed tokens:   4031774720 | elapsed time per iteration (ms): 4251.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.752453E-01 | loss scale: 32768.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.108 | tokens per gpu per second (tgs): 1926.904 | TFLOPs: 15.51 |
g0220: [2024-08-09 23:57:22,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=15390, skipped=20, lr=[0.00019999431631797485, 0.00019999431631797485], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15390 loss: 0.8970 iter time (s): 4.067 samples/sec: 31.475
g0238:  iteration    15390/10000000 | consumed samples:      1969920 | consumed tokens:   4034396160 | elapsed time per iteration (ms): 4102.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.874000E-01 | loss scale: 32768.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.197 | tokens per gpu per second (tgs): 1996.636 | TFLOPs: 16.07 |
g0220: [2024-08-09 23:58:05,261] [INFO] [logging.py:96:log_dist] [Rank 0] step=15400, skipped=20, lr=[0.00019999428746646035, 0.00019999428746646035], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15400 loss: 0.8564 iter time (s): 4.196 samples/sec: 30.505
g0238:  iteration    15400/10000000 | consumed samples:      1971200 | consumed tokens:   4037017600 | elapsed time per iteration (ms): 4229.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.778275E-01 | loss scale: 32768.0 | grad norm: 0.265 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.266 | tokens per gpu per second (tgs): 1937.037 | TFLOPs: 15.59 |
g0220: [2024-08-09 23:58:45,402] [INFO] [logging.py:96:log_dist] [Rank 0] step=15410, skipped=20, lr=[0.00019999425854190527, 0.00019999425854190527], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15410 loss: 0.9036 iter time (s): 3.981 samples/sec: 32.153
g0238:  iteration    15410/10000000 | consumed samples:      1972480 | consumed tokens:   4039639040 | elapsed time per iteration (ms): 4014.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.867268E-01 | loss scale: 32768.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.888 | tokens per gpu per second (tgs): 2040.849 | TFLOPs: 16.42 |
g0220: [2024-08-09 23:59:26,377] [INFO] [logging.py:96:log_dist] [Rank 0] step=15420, skipped=20, lr=[0.00019999422954430966, 0.00019999422954430966], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15420 loss: 0.8981 iter time (s): 4.065 samples/sec: 31.490
g0238:  iteration    15420/10000000 | consumed samples:      1973760 | consumed tokens:   4042260480 | elapsed time per iteration (ms): 4097.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.786059E-01 | loss scale: 32768.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.239 | tokens per gpu per second (tgs): 1999.270 | TFLOPs: 16.09 |
g0220: [2024-08-10 00:00:07,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=15430, skipped=20, lr=[0.0001999942004736735, 0.0001999942004736735], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15430 loss: 0.8745 iter time (s): 4.056 samples/sec: 31.560
g0238:  iteration    15430/10000000 | consumed samples:      1975040 | consumed tokens:   4044881920 | elapsed time per iteration (ms): 4088.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.765877E-01 | loss scale: 32768.0 | grad norm: 0.245 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.310 | tokens per gpu per second (tgs): 2003.863 | TFLOPs: 16.13 |
g0220: [2024-08-10 00:00:46,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=15440, skipped=20, lr=[0.00019999417132999684, 0.00019999417132999684], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15440 loss: 0.8643 iter time (s): 3.905 samples/sec: 32.782
g0238:  iteration    15440/10000000 | consumed samples:      1976320 | consumed tokens:   4047503360 | elapsed time per iteration (ms): 3937.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.856156E-01 | loss scale: 32768.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.512 | tokens per gpu per second (tgs): 2080.791 | TFLOPs: 16.74 |
g0220: [2024-08-10 00:01:28,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=15450, skipped=20, lr=[0.0001999941421132797, 0.0001999941421132797], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15450 loss: 0.8834 iter time (s): 4.111 samples/sec: 31.137
g0238:  iteration    15450/10000000 | consumed samples:      1977600 | consumed tokens:   4050124800 | elapsed time per iteration (ms): 4144.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.777852E-01 | loss scale: 32768.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.888 | tokens per gpu per second (tgs): 1976.810 | TFLOPs: 15.91 |
g0220: [2024-08-10 00:02:09,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=15460, skipped=20, lr=[0.0001999941128235221, 0.0001999941128235221], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15460 loss: 0.8778 iter time (s): 4.141 samples/sec: 30.908
g0238:  iteration    15460/10000000 | consumed samples:      1978880 | consumed tokens:   4052746240 | elapsed time per iteration (ms): 4173.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.825276E-01 | loss scale: 32768.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.667 | tokens per gpu per second (tgs): 1962.660 | TFLOPs: 15.79 |
g0220: [2024-08-10 00:02:52,026] [INFO] [logging.py:96:log_dist] [Rank 0] step=15470, skipped=20, lr=[0.00019999408346072406, 0.00019999408346072406], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15470 loss: 0.9232 iter time (s): 4.188 samples/sec: 30.562
g0238:  iteration    15470/10000000 | consumed samples:      1980160 | consumed tokens:   4055367680 | elapsed time per iteration (ms): 4221.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.958319E-01 | loss scale: 32768.0 | grad norm: 0.324 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.318 | tokens per gpu per second (tgs): 1940.350 | TFLOPs: 15.61 |
g0220: [2024-08-10 00:03:33,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=15480, skipped=20, lr=[0.00019999405402488555, 0.00019999405402488555], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15480 loss: 0.8708 iter time (s): 4.135 samples/sec: 30.952
g0238:  iteration    15480/10000000 | consumed samples:      1981440 | consumed tokens:   4057989120 | elapsed time per iteration (ms): 4168.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.803150E-01 | loss scale: 32768.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.708 | tokens per gpu per second (tgs): 1965.342 | TFLOPs: 15.82 |
g0220: [2024-08-10 00:04:14,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=15490, skipped=20, lr=[0.00019999402451600668, 0.00019999402451600668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15490 loss: 0.8975 iter time (s): 4.015 samples/sec: 31.880
g0238:  iteration    15490/10000000 | consumed samples:      1982720 | consumed tokens:   4060610560 | elapsed time per iteration (ms): 4047.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.827774E-01 | loss scale: 32768.0 | grad norm: 0.269 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.622 | tokens per gpu per second (tgs): 2023.784 | TFLOPs: 16.29 |
g0220: [2024-08-10 00:04:55,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=15500, skipped=20, lr=[0.00019999399493408746, 0.00019999399493408746], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15500 loss: 0.8906 iter time (s): 4.096 samples/sec: 31.251
g0238:  iteration    15500/10000000 | consumed samples:      1984000 | consumed tokens:   4063232000 | elapsed time per iteration (ms): 4128.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.765224E-01 | loss scale: 32768.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.002 | tokens per gpu per second (tgs): 1984.144 | TFLOPs: 15.97 |
g0220: [2024-08-10 00:05:35,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=15510, skipped=20, lr=[0.0001999939652791279, 0.0001999939652791279], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15510 loss: 0.8943 iter time (s): 4.007 samples/sec: 31.943
g0238:  iteration    15510/10000000 | consumed samples:      1985280 | consumed tokens:   4065853440 | elapsed time per iteration (ms): 4040.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.878445E-01 | loss scale: 32768.0 | grad norm: 0.246 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.681 | tokens per gpu per second (tgs): 2027.584 | TFLOPs: 16.32 |
g0220: [2024-08-10 00:06:17,540] [INFO] [logging.py:96:log_dist] [Rank 0] step=15520, skipped=20, lr=[0.00019999393555112798, 0.00019999393555112798], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15520 loss: 0.8865 iter time (s): 4.133 samples/sec: 30.967
g0238:  iteration    15520/10000000 | consumed samples:      1986560 | consumed tokens:   4068474880 | elapsed time per iteration (ms): 4166.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.737113E-01 | loss scale: 32768.0 | grad norm: 0.375 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.724 | tokens per gpu per second (tgs): 1966.334 | TFLOPs: 15.82 |
g0220: [2024-08-10 00:06:59,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=15530, skipped=20, lr=[0.00019999390575008776, 0.00019999390575008776], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15530 loss: 0.8773 iter time (s): 4.200 samples/sec: 30.478
g0238:  iteration    15530/10000000 | consumed samples:      1987840 | consumed tokens:   4071096320 | elapsed time per iteration (ms): 4232.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.874797E-01 | loss scale: 32768.0 | grad norm: 0.347 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.242 | tokens per gpu per second (tgs): 1935.512 | TFLOPs: 15.58 |
g0220: [2024-08-10 00:07:40,852] [INFO] [logging.py:96:log_dist] [Rank 0] step=15540, skipped=20, lr=[0.00019999387587600726, 0.00019999387587600726], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15540 loss: 0.8493 iter time (s): 4.066 samples/sec: 31.480
g0238:  iteration    15540/10000000 | consumed samples:      1989120 | consumed tokens:   4073717760 | elapsed time per iteration (ms): 4098.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.778584E-01 | loss scale: 32768.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.230 | tokens per gpu per second (tgs): 1998.714 | TFLOPs: 16.08 |
g0220: [2024-08-10 00:08:21,145] [INFO] [logging.py:96:log_dist] [Rank 0] step=15550, skipped=20, lr=[0.0001999938459288865, 0.0001999938459288865], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15550 loss: 0.8831 iter time (s): 3.996 samples/sec: 32.029
g0238:  iteration    15550/10000000 | consumed samples:      1990400 | consumed tokens:   4076339200 | elapsed time per iteration (ms): 4029.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.838803E-01 | loss scale: 32768.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.767 | tokens per gpu per second (tgs): 2033.070 | TFLOPs: 16.36 |
g0220: [2024-08-10 00:09:01,213] [INFO] [logging.py:96:log_dist] [Rank 0] step=15560, skipped=20, lr=[0.00019999381590872552, 0.00019999381590872552], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15560 loss: 0.8885 iter time (s): 3.974 samples/sec: 32.209
g0238:  iteration    15560/10000000 | consumed samples:      1991680 | consumed tokens:   4078960640 | elapsed time per iteration (ms): 4006.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.782125E-01 | loss scale: 32768.0 | grad norm: 0.272 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.946 | tokens per gpu per second (tgs): 2044.537 | TFLOPs: 16.45 |
g0220: [2024-08-10 00:09:42,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=15570, skipped=20, lr=[0.0001999937858155243, 0.0001999937858155243], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15570 loss: 0.8940 iter time (s): 4.084 samples/sec: 31.341
g0238:  iteration    15570/10000000 | consumed samples:      1992960 | consumed tokens:   4081582080 | elapsed time per iteration (ms): 4116.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.838196E-01 | loss scale: 32768.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.092 | tokens per gpu per second (tgs): 1989.912 | TFLOPs: 16.01 |
g0220: [2024-08-10 00:10:23,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=15580, skipped=20, lr=[0.0001999937556492829, 0.0001999937556492829], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15580 loss: 0.8797 iter time (s): 4.034 samples/sec: 31.732
g0238:  iteration    15580/10000000 | consumed samples:      1994240 | consumed tokens:   4084203520 | elapsed time per iteration (ms): 4066.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.794213E-01 | loss scale: 32768.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.479 | tokens per gpu per second (tgs): 2014.682 | TFLOPs: 16.21 |
g0220: [2024-08-10 00:11:03,274] [INFO] [logging.py:96:log_dist] [Rank 0] step=15590, skipped=20, lr=[0.00019999372541000136, 0.00019999372541000136], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15590 loss: 0.8474 iter time (s): 3.990 samples/sec: 32.080
g0238:  iteration    15590/10000000 | consumed samples:      1995520 | consumed tokens:   4086824960 | elapsed time per iteration (ms): 4023.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.716326E-01 | loss scale: 32768.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.816 | tokens per gpu per second (tgs): 2036.221 | TFLOPs: 16.39 |
g0220: [2024-08-10 00:11:44,104] [INFO] [logging.py:96:log_dist] [Rank 0] step=15600, skipped=20, lr=[0.00019999369509767965, 0.00019999369509767965], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15600 loss: 0.8911 iter time (s): 4.051 samples/sec: 31.600
g0238:  iteration    15600/10000000 | consumed samples:      1996800 | consumed tokens:   4089446400 | elapsed time per iteration (ms): 4082.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.786525E-01 | loss scale: 32768.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.350 | tokens per gpu per second (tgs): 2006.397 | TFLOPs: 16.15 |
g0220: [2024-08-10 00:12:25,335] [INFO] [logging.py:96:log_dist] [Rank 0] step=15610, skipped=20, lr=[0.00019999366471231786, 0.00019999366471231786], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15610 loss: 0.8910 iter time (s): 4.089 samples/sec: 31.301
g0238:  iteration    15610/10000000 | consumed samples:      1998080 | consumed tokens:   4092067840 | elapsed time per iteration (ms): 4123.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.887855E-01 | loss scale: 32768.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.045 | tokens per gpu per second (tgs): 1986.848 | TFLOPs: 15.99 |
g0220: [2024-08-10 00:13:08,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=15620, skipped=20, lr=[0.00019999363425391594, 0.00019999363425391594], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15620 loss: 0.8709 iter time (s): 4.273 samples/sec: 29.953
g0238:  iteration    15620/10000000 | consumed samples:      1999360 | consumed tokens:   4094689280 | elapsed time per iteration (ms): 4306.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.828520E-01 | loss scale: 32768.0 | grad norm: 0.252 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.725 | tokens per gpu per second (tgs): 1902.381 | TFLOPs: 15.31 |
g0220: [2024-08-10 00:13:48,940] [INFO] [logging.py:96:log_dist] [Rank 0] step=15630, skipped=20, lr=[0.00019999360372247396, 0.00019999360372247396], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15630 loss: 0.8904 iter time (s): 4.022 samples/sec: 31.825
g0238:  iteration    15630/10000000 | consumed samples:      2000640 | consumed tokens:   4097310720 | elapsed time per iteration (ms): 4054.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.767268E-01 | loss scale: 32768.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.568 | tokens per gpu per second (tgs): 2020.342 | TFLOPs: 16.26 |
g0220: [2024-08-10 00:14:28,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=15640, skipped=20, lr=[0.00019999357311799193, 0.00019999357311799193], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15640 loss: 0.8571 iter time (s): 3.948 samples/sec: 32.421
g0238:  iteration    15640/10000000 | consumed samples:      2001920 | consumed tokens:   4099932160 | elapsed time per iteration (ms): 3980.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.809622E-01 | loss scale: 32768.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.154 | tokens per gpu per second (tgs): 2057.846 | TFLOPs: 16.56 |
g0220: [2024-08-10 00:15:08,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=15650, skipped=20, lr=[0.00019999354244046986, 0.00019999354244046986], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15650 loss: 0.8728 iter time (s): 3.901 samples/sec: 32.812
g0238:  iteration    15650/10000000 | consumed samples:      2003200 | consumed tokens:   4102553600 | elapsed time per iteration (ms): 3933.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.851418E-01 | loss scale: 32768.0 | grad norm: 0.270 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.542 | tokens per gpu per second (tgs): 2082.683 | TFLOPs: 16.76 |
g0220: [2024-08-10 00:15:50,101] [INFO] [logging.py:96:log_dist] [Rank 0] step=15660, skipped=20, lr=[0.00019999351168990782, 0.00019999351168990782], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15660 loss: 0.8742 iter time (s): 4.169 samples/sec: 30.703
g0238:  iteration    15660/10000000 | consumed samples:      2004480 | consumed tokens:   4105175040 | elapsed time per iteration (ms): 4201.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.846706E-01 | loss scale: 32768.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.467 | tokens per gpu per second (tgs): 1949.861 | TFLOPs: 15.69 |
g0220: [2024-08-10 00:16:31,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=15670, skipped=20, lr=[0.00019999348086630577, 0.00019999348086630577], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15670 loss: 0.8939 iter time (s): 4.078 samples/sec: 31.389
g0238:  iteration    15670/10000000 | consumed samples:      2005760 | consumed tokens:   4107796480 | elapsed time per iteration (ms): 4110.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.772056E-01 | loss scale: 32768.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.140 | tokens per gpu per second (tgs): 1992.934 | TFLOPs: 16.04 |
g0220: [2024-08-10 00:17:13,542] [INFO] [logging.py:96:log_dist] [Rank 0] step=15680, skipped=20, lr=[0.00019999344996966378, 0.00019999344996966378], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15680 loss: 0.8762 iter time (s): 4.201 samples/sec: 30.470
g0238:  iteration    15680/10000000 | consumed samples:      2007040 | consumed tokens:   4110417920 | elapsed time per iteration (ms): 4233.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.807144E-01 | loss scale: 32768.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.235 | tokens per gpu per second (tgs): 1935.043 | TFLOPs: 15.57 |
g0220: [2024-08-10 00:17:52,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=15690, skipped=20, lr=[0.00019999341899998185, 0.00019999341899998185], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15690 loss: 0.8406 iter time (s): 3.901 samples/sec: 32.813
g0238:  iteration    15690/10000000 | consumed samples:      2008320 | consumed tokens:   4113039360 | elapsed time per iteration (ms): 3933.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.670062E-01 | loss scale: 32768.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.543 | tokens per gpu per second (tgs): 2082.743 | TFLOPs: 16.76 |
g0220: [2024-08-10 00:18:33,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=15700, skipped=20, lr=[0.00019999338795726002, 0.00019999338795726002], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15700 loss: 0.8555 iter time (s): 4.046 samples/sec: 31.639
g0238:  iteration    15700/10000000 | consumed samples:      2009600 | consumed tokens:   4115660800 | elapsed time per iteration (ms): 4078.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.773812E-01 | loss scale: 32768.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.383 | tokens per gpu per second (tgs): 2008.544 | TFLOPs: 16.16 |
g0220: [2024-08-10 00:19:14,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=15710, skipped=20, lr=[0.0001999933568414983, 0.0001999933568414983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15710 loss: 0.8619 iter time (s): 4.098 samples/sec: 31.234
g0238:  iteration    15710/10000000 | consumed samples:      2010880 | consumed tokens:   4118282240 | elapsed time per iteration (ms): 4131.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.822977E-01 | loss scale: 32768.0 | grad norm: 0.276 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.984 | tokens per gpu per second (tgs): 1982.971 | TFLOPs: 15.96 |
g0220: [2024-08-10 00:19:56,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=15720, skipped=20, lr=[0.00019999332565269674, 0.00019999332565269674], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15720 loss: 0.9069 iter time (s): 4.078 samples/sec: 31.389
g0238:  iteration    15720/10000000 | consumed samples:      2012160 | consumed tokens:   4120903680 | elapsed time per iteration (ms): 4110.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.823117E-01 | loss scale: 32768.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.140 | tokens per gpu per second (tgs): 1992.947 | TFLOPs: 16.04 |
g0220: [2024-08-10 00:20:36,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=15730, skipped=20, lr=[0.00019999329439085534, 0.00019999329439085534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15730 loss: 0.9000 iter time (s): 3.966 samples/sec: 32.275
g0238:  iteration    15730/10000000 | consumed samples:      2013440 | consumed tokens:   4123525120 | elapsed time per iteration (ms): 3998.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.747149E-01 | loss scale: 32768.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.015 | tokens per gpu per second (tgs): 2048.947 | TFLOPs: 16.49 |
g0220: [2024-08-10 00:21:16,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=15740, skipped=20, lr=[0.00019999326305597412, 0.00019999326305597412], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15740 loss: 0.9196 iter time (s): 4.028 samples/sec: 31.781
g0238:  iteration    15740/10000000 | consumed samples:      2014720 | consumed tokens:   4126146560 | elapsed time per iteration (ms): 4060.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.903401E-01 | loss scale: 32768.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.523 | tokens per gpu per second (tgs): 2017.477 | TFLOPs: 16.23 |
g0220: [2024-08-10 00:21:54,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=15750, skipped=20, lr=[0.00019999323164805312, 0.00019999323164805312], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15750 loss: 0.8859 iter time (s): 3.795 samples/sec: 33.731
g0238:  iteration    15750/10000000 | consumed samples:      2016000 | consumed tokens:   4128768000 | elapsed time per iteration (ms): 3827.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.695267E-01 | loss scale: 32768.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.438 | tokens per gpu per second (tgs): 2140.055 | TFLOPs: 17.22 |
g0220: [2024-08-10 00:22:36,067] [INFO] [logging.py:96:log_dist] [Rank 0] step=15760, skipped=20, lr=[0.00019999320016709235, 0.00019999320016709235], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15760 loss: 0.8994 iter time (s): 4.079 samples/sec: 31.381
g0238:  iteration    15760/10000000 | consumed samples:      2017280 | consumed tokens:   4131389440 | elapsed time per iteration (ms): 4112.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.776031E-01 | loss scale: 32768.0 | grad norm: 0.311 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.126 | tokens per gpu per second (tgs): 1992.034 | TFLOPs: 16.03 |
g0220: [2024-08-10 00:23:16,977] [INFO] [logging.py:96:log_dist] [Rank 0] step=15770, skipped=20, lr=[0.00019999316861309184, 0.00019999316861309184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15770 loss: 0.8574 iter time (s): 4.058 samples/sec: 31.544
g0238:  iteration    15770/10000000 | consumed samples:      2018560 | consumed tokens:   4134010880 | elapsed time per iteration (ms): 4090.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.857712E-01 | loss scale: 32768.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.289 | tokens per gpu per second (tgs): 2002.481 | TFLOPs: 16.11 |
g0220: [2024-08-10 00:23:57,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=15780, skipped=20, lr=[0.00019999313698605163, 0.00019999313698605163], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15780 loss: 0.8937 iter time (s): 4.033 samples/sec: 31.737
g0238:  iteration    15780/10000000 | consumed samples:      2019840 | consumed tokens:   4136632320 | elapsed time per iteration (ms): 4065.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.862513E-01 | loss scale: 32768.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.482 | tokens per gpu per second (tgs): 2014.848 | TFLOPs: 16.21 |
g0220: [2024-08-10 00:24:38,571] [INFO] [logging.py:96:log_dist] [Rank 0] step=15790, skipped=20, lr=[0.0001999931052859717, 0.0001999931052859717], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15790 loss: 0.8806 iter time (s): 4.061 samples/sec: 31.523
g0238:  iteration    15790/10000000 | consumed samples:      2021120 | consumed tokens:   4139253760 | elapsed time per iteration (ms): 4093.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.735106E-01 | loss scale: 32768.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.269 | tokens per gpu per second (tgs): 2001.205 | TFLOPs: 16.10 |
g0220: [2024-08-10 00:25:19,085] [INFO] [logging.py:96:log_dist] [Rank 0] step=15800, skipped=20, lr=[0.00019999307351285214, 0.00019999307351285214], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15800 loss: 0.8712 iter time (s): 4.015 samples/sec: 31.882
g0238:  iteration    15800/10000000 | consumed samples:      2022400 | consumed tokens:   4141875200 | elapsed time per iteration (ms): 4051.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.840451E-01 | loss scale: 32768.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.594 | tokens per gpu per second (tgs): 2022.006 | TFLOPs: 16.27 |
g0225: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 00:25:59,116] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 00:25:59,117] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 00:25:59,118] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 00:25:59,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=15810, skipped=20, lr=[0.00019999304166669294, 0.00019999304166669294], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15810 loss: 0.8665 iter time (s): 3.971 samples/sec: 32.231
g0238:  iteration    15810/10000000 | consumed samples:      2023680 | consumed tokens:   4144496640 | elapsed time per iteration (ms): 4003.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.719969E-01 | loss scale: 65536.0 | grad norm: 0.096 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.970 | tokens per gpu per second (tgs): 2046.049 | TFLOPs: 16.46 |
g0220: [2024-08-10 00:26:40,923] [INFO] [logging.py:96:log_dist] [Rank 0] step=15820, skipped=20, lr=[0.00019999300974749408, 0.00019999300974749408], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15820 loss: 0.8428 iter time (s): 4.147 samples/sec: 30.862
g0238:  iteration    15820/10000000 | consumed samples:      2024960 | consumed tokens:   4147118080 | elapsed time per iteration (ms): 4180.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.644567E-01 | loss scale: 65536.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.622 | tokens per gpu per second (tgs): 1959.830 | TFLOPs: 15.77 |
g0220: [2024-08-10 00:27:20,960] [INFO] [logging.py:96:log_dist] [Rank 0] step=15830, skipped=20, lr=[0.00019999297775525566, 0.00019999297775525566], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15830 loss: 0.8997 iter time (s): 3.971 samples/sec: 32.234
g0238:  iteration    15830/10000000 | consumed samples:      2026240 | consumed tokens:   4149739520 | elapsed time per iteration (ms): 4003.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.727034E-01 | loss scale: 65536.0 | grad norm: 0.296 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.971 | tokens per gpu per second (tgs): 2046.133 | TFLOPs: 16.47 |
g0220: [2024-08-10 00:28:00,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=15840, skipped=20, lr=[0.00019999294568997768, 0.00019999294568997768], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15840 loss: 0.8863 iter time (s): 3.916 samples/sec: 32.686
g0238:  iteration    15840/10000000 | consumed samples:      2027520 | consumed tokens:   4152360960 | elapsed time per iteration (ms): 3948.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.773854E-01 | loss scale: 65536.0 | grad norm: 0.270 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.415 | tokens per gpu per second (tgs): 2074.546 | TFLOPs: 16.69 |
g0220: [2024-08-10 00:28:41,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=15850, skipped=20, lr=[0.00019999291355166012, 0.00019999291355166012], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15850 loss: 0.8768 iter time (s): 4.114 samples/sec: 31.111
g0238:  iteration    15850/10000000 | consumed samples:      2028800 | consumed tokens:   4154982400 | elapsed time per iteration (ms): 4146.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.796572E-01 | loss scale: 65536.0 | grad norm: 0.263 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.868 | tokens per gpu per second (tgs): 1975.525 | TFLOPs: 15.90 |
g0220: [2024-08-10 00:29:22,574] [INFO] [logging.py:96:log_dist] [Rank 0] step=15860, skipped=20, lr=[0.00019999288134030308, 0.00019999288134030308], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15860 loss: 0.8638 iter time (s): 4.033 samples/sec: 31.738
g0238:  iteration    15860/10000000 | consumed samples:      2030080 | consumed tokens:   4157603840 | elapsed time per iteration (ms): 4065.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.815929E-01 | loss scale: 65536.0 | grad norm: 0.244 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.482 | tokens per gpu per second (tgs): 2014.821 | TFLOPs: 16.21 |
g0220: [2024-08-10 00:30:04,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=15870, skipped=20, lr=[0.00019999284905590655, 0.00019999284905590655], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15870 loss: 0.8839 iter time (s): 4.185 samples/sec: 30.588
g0238:  iteration    15870/10000000 | consumed samples:      2031360 | consumed tokens:   4160225280 | elapsed time per iteration (ms): 4217.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.819312E-01 | loss scale: 65536.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.353 | tokens per gpu per second (tgs): 1942.567 | TFLOPs: 15.63 |
g0220: [2024-08-10 00:30:45,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=15880, skipped=20, lr=[0.00019999281669847054, 0.00019999281669847054], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15880 loss: 0.8421 iter time (s): 4.030 samples/sec: 31.763
g0238:  iteration    15880/10000000 | consumed samples:      2032640 | consumed tokens:   4162846720 | elapsed time per iteration (ms): 4063.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.812591E-01 | loss scale: 65536.0 | grad norm: 0.242 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.501 | tokens per gpu per second (tgs): 2016.068 | TFLOPs: 16.22 |
g0220: [2024-08-10 00:31:25,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=15890, skipped=20, lr=[0.00019999278426799508, 0.00019999278426799508], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15890 loss: 0.8733 iter time (s): 4.004 samples/sec: 31.971
g0238:  iteration    15890/10000000 | consumed samples:      2033920 | consumed tokens:   4165468160 | elapsed time per iteration (ms): 4036.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.692215E-01 | loss scale: 65536.0 | grad norm: 0.240 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.711 | tokens per gpu per second (tgs): 2029.501 | TFLOPs: 16.33 |
g0220: [2024-08-10 00:32:05,847] [INFO] [logging.py:96:log_dist] [Rank 0] step=15900, skipped=20, lr=[0.0001999927517644802, 0.0001999927517644802], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15900 loss: 0.8759 iter time (s): 3.977 samples/sec: 32.186
g0238:  iteration    15900/10000000 | consumed samples:      2035200 | consumed tokens:   4168089600 | elapsed time per iteration (ms): 4010.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.721120E-01 | loss scale: 65536.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.918 | tokens per gpu per second (tgs): 2042.740 | TFLOPs: 16.44 |
g0220: [2024-08-10 00:32:47,996] [INFO] [logging.py:96:log_dist] [Rank 0] step=15910, skipped=20, lr=[0.00019999271918792592, 0.00019999271918792592], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15910 loss: 0.8507 iter time (s): 4.182 samples/sec: 30.608
g0238:  iteration    15910/10000000 | consumed samples:      2036480 | consumed tokens:   4170711040 | elapsed time per iteration (ms): 4214.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.714314E-01 | loss scale: 65536.0 | grad norm: 0.308 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.369 | tokens per gpu per second (tgs): 1943.614 | TFLOPs: 15.64 |
g0220: [2024-08-10 00:33:28,903] [INFO] [logging.py:96:log_dist] [Rank 0] step=15920, skipped=20, lr=[0.00019999268653833228, 0.00019999268653833228], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15920 loss: 0.8888 iter time (s): 4.057 samples/sec: 31.548
g0238:  iteration    15920/10000000 | consumed samples:      2037760 | consumed tokens:   4173332480 | elapsed time per iteration (ms): 4091.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.747887E-01 | loss scale: 65536.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.287 | tokens per gpu per second (tgs): 2002.369 | TFLOPs: 16.11 |
g0220: [2024-08-10 00:34:11,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=15930, skipped=20, lr=[0.0001999926538156993, 0.0001999926538156993], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15930 loss: 0.8862 iter time (s): 4.182 samples/sec: 30.605
g0238:  iteration    15930/10000000 | consumed samples:      2039040 | consumed tokens:   4175953920 | elapsed time per iteration (ms): 4215.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.758510E-01 | loss scale: 65536.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.366 | tokens per gpu per second (tgs): 1943.434 | TFLOPs: 15.64 |
g0220: [2024-08-10 00:34:52,116] [INFO] [logging.py:96:log_dist] [Rank 0] step=15940, skipped=20, lr=[0.000199992621020027, 0.000199992621020027], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15940 loss: 0.8556 iter time (s): 4.072 samples/sec: 31.434
g0238:  iteration    15940/10000000 | consumed samples:      2040320 | consumed tokens:   4178575360 | elapsed time per iteration (ms): 4105.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.750975E-01 | loss scale: 65536.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.177 | tokens per gpu per second (tgs): 1995.324 | TFLOPs: 16.06 |
g0220: [2024-08-10 00:35:32,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=15950, skipped=20, lr=[0.0001999925881513154, 0.0001999925881513154], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15950 loss: 0.9204 iter time (s): 4.031 samples/sec: 31.755
g0238:  iteration    15950/10000000 | consumed samples:      2041600 | consumed tokens:   4181196800 | elapsed time per iteration (ms): 4064.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.916316E-01 | loss scale: 65536.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.494 | tokens per gpu per second (tgs): 2015.587 | TFLOPs: 16.22 |
g0220: [2024-08-10 00:36:12,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=15960, skipped=20, lr=[0.00019999255520956457, 0.00019999255520956457], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15960 loss: 0.8589 iter time (s): 3.970 samples/sec: 32.242
g0238:  iteration    15960/10000000 | consumed samples:      2042880 | consumed tokens:   4183818240 | elapsed time per iteration (ms): 4003.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.735564E-01 | loss scale: 65536.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.972 | tokens per gpu per second (tgs): 2046.203 | TFLOPs: 16.47 |
g0220: [2024-08-10 00:36:54,161] [INFO] [logging.py:96:log_dist] [Rank 0] step=15970, skipped=20, lr=[0.00019999252219477445, 0.00019999252219477445], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15970 loss: 0.8664 iter time (s): 4.104 samples/sec: 31.189
g0238:  iteration    15970/10000000 | consumed samples:      2044160 | consumed tokens:   4186439680 | elapsed time per iteration (ms): 4136.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.734076E-01 | loss scale: 65536.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.943 | tokens per gpu per second (tgs): 1980.326 | TFLOPs: 15.94 |
g0220: [2024-08-10 00:37:34,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=15980, skipped=20, lr=[0.00019999248910694512, 0.00019999248910694512], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15980 loss: 0.8622 iter time (s): 4.030 samples/sec: 31.762
g0238:  iteration    15980/10000000 | consumed samples:      2045440 | consumed tokens:   4189061120 | elapsed time per iteration (ms): 4063.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.759911E-01 | loss scale: 65536.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.502 | tokens per gpu per second (tgs): 2016.151 | TFLOPs: 16.22 |
g0220: [2024-08-10 00:38:15,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=15990, skipped=20, lr=[0.00019999245594607662, 0.00019999245594607662], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 15990 loss: 0.8800 iter time (s): 4.012 samples/sec: 31.907
g0238:  iteration    15990/10000000 | consumed samples:      2046720 | consumed tokens:   4191682560 | elapsed time per iteration (ms): 4044.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.670957E-01 | loss scale: 65536.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.646 | tokens per gpu per second (tgs): 2025.346 | TFLOPs: 16.30 |
g0220: [2024-08-10 00:38:55,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=16000, skipped=20, lr=[0.00019999242271216894, 0.00019999242271216894], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16000 loss: 0.8598 iter time (s): 3.987 samples/sec: 32.108
g0238:  iteration    16000/10000000 | consumed samples:      2048000 | consumed tokens:   4194304000 | elapsed time per iteration (ms): 4019.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.741577E-01 | loss scale: 65536.0 | grad norm: 0.263 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.845 | tokens per gpu per second (tgs): 2038.073 | TFLOPs: 16.40 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 16000 | lm loss value: 8.740543E-01 | lm loss PPL: 2.396608E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   16000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-10 00:45:18,544] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step16000 is about to be saved!
g0238: [2024-08-10 00:45:18,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0238: [2024-08-10 00:45:18,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0238: [2024-08-10 00:45:18,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0220: [2024-08-10 00:45:18,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0220: [2024-08-10 00:45:18,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0220: [2024-08-10 00:45:18,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0236: [2024-08-10 00:45:18,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0236: [2024-08-10 00:45:18,551] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0236: [2024-08-10 00:45:18,552] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0233: [2024-08-10 00:45:18,552] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0233: [2024-08-10 00:45:18,552] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0233: [2024-08-10 00:45:18,552] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0235: [2024-08-10 00:45:18,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0235: [2024-08-10 00:45:18,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0237: [2024-08-10 00:45:18,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0237: [2024-08-10 00:45:18,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0234: [2024-08-10 00:45:18,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0234: [2024-08-10 00:45:18,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0237: [2024-08-10 00:45:18,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0235: [2024-08-10 00:45:18,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0234: [2024-08-10 00:45:18,553] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0225: [2024-08-10 00:45:18,554] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0225: [2024-08-10 00:45:18,554] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0225: [2024-08-10 00:45:18,554] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0238: [2024-08-10 00:45:18,577] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_23-model_00-model_states.pt...
g0235: [2024-08-10 00:45:18,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_14-model_00-model_states.pt...
g0236: [2024-08-10 00:45:18,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_17-model_00-model_states.pt...
g0237: [2024-08-10 00:45:18,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_20-model_00-model_states.pt...
g0233: [2024-08-10 00:45:18,590] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_08-model_00-model_states.pt...
g0234: [2024-08-10 00:45:18,592] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_11-model_00-model_states.pt...
g0225: [2024-08-10 00:45:18,593] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_05-model_00-model_states.pt...
g0220: [2024-08-10 00:45:18,603] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_01-model_00-model_states.pt...
g0238: [2024-08-10 00:45:18,700] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_23-model_00-model_states.pt.
g0238: [2024-08-10 00:45:18,701] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_24-model_00-model_states.pt...
g0238: [2024-08-10 00:45:18,703] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_24-model_00-model_states.pt.
g0234: [2024-08-10 00:45:18,723] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_11-model_00-model_states.pt.
g0238: [2024-08-10 00:45:18,749] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_25-model_00-model_states.pt...
g0225: [2024-08-10 00:45:18,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_05-model_00-model_states.pt.
g0234: [2024-08-10 00:45:18,763] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_12-model_00-model_states.pt...
g0233: [2024-08-10 00:45:18,769] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_08-model_00-model_states.pt.
g0236: [2024-08-10 00:45:18,780] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_17-model_00-model_states.pt.
g0220: [2024-08-10 00:45:18,790] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_01-model_00-model_states.pt.
g0225: [2024-08-10 00:45:18,794] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_06-model_00-model_states.pt...
g0237: [2024-08-10 00:45:18,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_20-model_00-model_states.pt.
g0233: [2024-08-10 00:45:18,807] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_09-model_00-model_states.pt...
g0220: [2024-08-10 00:45:18,808] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_02-model_00-model_states.pt...
g0236: [2024-08-10 00:45:18,821] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_18-model_00-model_states.pt...
g0235: [2024-08-10 00:45:18,830] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_14-model_00-model_states.pt.
g0237: [2024-08-10 00:45:18,835] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_21-model_00-model_states.pt...
g0234: [2024-08-10 00:45:18,866] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_12-model_00-model_states.pt.
g0235: [2024-08-10 00:45:18,868] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_15-model_00-model_states.pt...
g0234: [2024-08-10 00:45:18,901] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_13-model_00-model_states.pt...
g0220: [2024-08-10 00:45:18,923] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_02-model_00-model_states.pt.
g0233: [2024-08-10 00:45:18,929] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 00:45:18,937] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_06-model_00-model_states.pt.
g0220: [2024-08-10 00:45:18,943] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_03-model_00-model_states.pt...
g0233: [2024-08-10 00:45:18,965] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_10-model_00-model_states.pt...
g0225: [2024-08-10 00:45:18,973] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_07-model_00-model_states.pt...
g0236: [2024-08-10 00:45:18,987] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_18-model_00-model_states.pt.
g0237: [2024-08-10 00:45:18,990] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_21-model_00-model_states.pt.
g0235: [2024-08-10 00:45:18,997] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_15-model_00-model_states.pt.
g0234: [2024-08-10 00:45:19,018] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_13-model_00-model_states.pt.
g0234: [2024-08-10 00:45:19,020] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_03_model_states.pt...
g0236: [2024-08-10 00:45:19,022] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_19-model_00-model_states.pt...
g0237: [2024-08-10 00:45:19,024] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_22-model_00-model_states.pt...
g0235: [2024-08-10 00:45:19,030] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_16-model_00-model_states.pt...
g0238: [2024-08-10 00:45:19,061] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_25-model_00-model_states.pt.
g0238: [2024-08-10 00:45:19,062] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_07_model_states.pt...
g0225: [2024-08-10 00:45:19,074] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_07-model_00-model_states.pt.
g0225: [2024-08-10 00:45:19,076] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_01_model_states.pt...
g0220: [2024-08-10 00:45:19,090] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_03-model_00-model_states.pt.
g0220: [2024-08-10 00:45:19,111] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_04-model_00-model_states.pt...
g0233: [2024-08-10 00:45:19,118] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_10-model_00-model_states.pt.
g0233: [2024-08-10 00:45:19,119] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_02_model_states.pt...
g0236: [2024-08-10 00:45:19,145] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_19-model_00-model_states.pt.
g0236: [2024-08-10 00:45:19,147] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_05_model_states.pt...
g0237: [2024-08-10 00:45:19,147] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_22-model_00-model_states.pt.
g0237: [2024-08-10 00:45:19,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_06_model_states.pt...
g0235: [2024-08-10 00:45:19,152] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_16-model_00-model_states.pt.
g0235: [2024-08-10 00:45:19,154] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_04_model_states.pt...
g0220: [2024-08-10 00:45:19,205] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/layer_04-model_00-model_states.pt.
g0220: [2024-08-10 00:45:19,206] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_00_model_states.pt
g0220: [2024-08-10 00:45:19,207] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_00_model_states.pt...
g0234: [2024-08-10 00:45:21,356] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_03_model_states.pt.
g0234: [2024-08-10 00:45:21,356] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0225: [2024-08-10 00:45:21,411] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_01_model_states.pt.
g0225: [2024-08-10 00:45:21,412] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0237: [2024-08-10 00:45:21,470] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_06_model_states.pt.
g0237: [2024-08-10 00:45:21,471] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0235: [2024-08-10 00:45:21,550] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_04_model_states.pt.
g0235: [2024-08-10 00:45:21,550] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0233: [2024-08-10 00:45:21,576] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_02_model_states.pt.
g0233: [2024-08-10 00:45:21,576] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0236: [2024-08-10 00:45:21,752] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_05_model_states.pt.
g0236: [2024-08-10 00:45:21,753] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0220: [2024-08-10 00:45:23,052] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_00_model_states.pt.
g0220: [2024-08-10 00:45:23,053] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0238: [2024-08-10 00:45:27,343] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step16000/mp_rank_07_model_states.pt.
g0238: [2024-08-10 00:45:27,343] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16000 is ready now!
g0220:   successfully saved checkpoint at iteration   16000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 2.55, Latency(second): 8.816
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (8815.62, 8815.84)
g0220: [2024-08-10 00:46:10,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=16010, skipped=20, lr=[0.00019999238940522212, 0.00019999238940522212], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16010 loss: 0.8797 iter time (s): 4.265 samples/sec: 30.012
g0238:  iteration    16010/10000000 | consumed samples:      2049280 | consumed tokens:   4196925440 | elapsed time per iteration (ms): 43485.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.758800E-01 | loss scale: 65536.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.944 | tokens per gpu per second (tgs): 188.386 | TFLOPs: 1.52 |
g0220: [2024-08-10 00:46:51,268] [INFO] [logging.py:96:log_dist] [Rank 0] step=16020, skipped=20, lr=[0.0001999923560252362, 0.0001999923560252362], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16020 loss: 0.8923 iter time (s): 4.066 samples/sec: 31.483
g0238:  iteration    16020/10000000 | consumed samples:      2050560 | consumed tokens:   4199546880 | elapsed time per iteration (ms): 4098.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.745883E-01 | loss scale: 65536.0 | grad norm: 0.243 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.234 | tokens per gpu per second (tgs): 1998.955 | TFLOPs: 16.09 |
g0220: [2024-08-10 00:47:32,201] [INFO] [logging.py:96:log_dist] [Rank 0] step=16030, skipped=20, lr=[0.00019999232257221115, 0.00019999232257221115], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16030 loss: 0.8842 iter time (s): 4.060 samples/sec: 31.527
g0238:  iteration    16030/10000000 | consumed samples:      2051840 | consumed tokens:   4202168320 | elapsed time per iteration (ms): 4093.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.738082E-01 | loss scale: 65536.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.271 | tokens per gpu per second (tgs): 2001.353 | TFLOPs: 16.11 |
g0220: [2024-08-10 00:48:13,560] [INFO] [logging.py:96:log_dist] [Rank 0] step=16040, skipped=20, lr=[0.0001999922890461471, 0.0001999922890461471], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16040 loss: 0.8638 iter time (s): 4.103 samples/sec: 31.200
g0238:  iteration    16040/10000000 | consumed samples:      2053120 | consumed tokens:   4204789760 | elapsed time per iteration (ms): 4135.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.671977E-01 | loss scale: 65536.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.949 | tokens per gpu per second (tgs): 1980.758 | TFLOPs: 15.94 |
g0220: [2024-08-10 00:48:54,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=16050, skipped=20, lr=[0.00019999225544704393, 0.00019999225544704393], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16050 loss: 0.8580 iter time (s): 4.066 samples/sec: 31.481
g0238:  iteration    16050/10000000 | consumed samples:      2054400 | consumed tokens:   4207411200 | elapsed time per iteration (ms): 4098.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.687216E-01 | loss scale: 65536.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.231 | tokens per gpu per second (tgs): 1998.775 | TFLOPs: 16.08 |
g0220: [2024-08-10 00:49:34,872] [INFO] [logging.py:96:log_dist] [Rank 0] step=16060, skipped=20, lr=[0.0001999922217749018, 0.0001999922217749018], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16060 loss: 0.8814 iter time (s): 3.999 samples/sec: 32.004
g0238:  iteration    16060/10000000 | consumed samples:      2055680 | consumed tokens:   4210032640 | elapsed time per iteration (ms): 4032.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.625092E-01 | loss scale: 65536.0 | grad norm: 0.413 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.740 | tokens per gpu per second (tgs): 2031.361 | TFLOPs: 16.35 |
g0220: [2024-08-10 00:50:13,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=16070, skipped=20, lr=[0.0001999921880297207, 0.0001999921880297207], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16070 loss: 0.9129 iter time (s): 3.877 samples/sec: 33.016
g0238:  iteration    16070/10000000 | consumed samples:      2056960 | consumed tokens:   4212654080 | elapsed time per iteration (ms): 3909.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.774131E-01 | loss scale: 65536.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.740 | tokens per gpu per second (tgs): 2095.349 | TFLOPs: 16.86 |
g0220: [2024-08-10 00:50:55,208] [INFO] [logging.py:96:log_dist] [Rank 0] step=16080, skipped=20, lr=[0.0001999921542115006, 0.0001999921542115006], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16080 loss: 0.9163 iter time (s): 4.091 samples/sec: 31.289
g0238:  iteration    16080/10000000 | consumed samples:      2058240 | consumed tokens:   4215275520 | elapsed time per iteration (ms): 4123.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.743357E-01 | loss scale: 65536.0 | grad norm: 0.284 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.039 | tokens per gpu per second (tgs): 1986.495 | TFLOPs: 15.99 |
g0220: [2024-08-10 00:51:38,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=16090, skipped=20, lr=[0.00019999212032024158, 0.00019999212032024158], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16090 loss: 0.9020 iter time (s): 4.271 samples/sec: 29.971
g0238:  iteration    16090/10000000 | consumed samples:      2059520 | consumed tokens:   4217896960 | elapsed time per iteration (ms): 4303.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.850643E-01 | loss scale: 65536.0 | grad norm: 0.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.744 | tokens per gpu per second (tgs): 1903.585 | TFLOPs: 15.32 |
g0220: [2024-08-10 00:52:19,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=16100, skipped=20, lr=[0.00019999208635594366, 0.00019999208635594366], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16100 loss: 0.8666 iter time (s): 4.126 samples/sec: 31.021
g0238:  iteration    16100/10000000 | consumed samples:      2060800 | consumed tokens:   4220518400 | elapsed time per iteration (ms): 4159.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.694313E-01 | loss scale: 65536.0 | grad norm: 0.267 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.776 | tokens per gpu per second (tgs): 1969.689 | TFLOPs: 15.85 |
g0220: [2024-08-10 00:53:00,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=16110, skipped=20, lr=[0.00019999205231860685, 0.00019999205231860685], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16110 loss: 0.8735 iter time (s): 4.016 samples/sec: 31.871
g0238:  iteration    16110/10000000 | consumed samples:      2062080 | consumed tokens:   4223139840 | elapsed time per iteration (ms): 4049.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.800028E-01 | loss scale: 65536.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.612 | tokens per gpu per second (tgs): 2023.164 | TFLOPs: 16.28 |
g0220: [2024-08-10 00:53:41,787] [INFO] [logging.py:96:log_dist] [Rank 0] step=16120, skipped=20, lr=[0.0001999920182082312, 0.0001999920182082312], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16120 loss: 0.8741 iter time (s): 4.112 samples/sec: 31.128
g0238:  iteration    16120/10000000 | consumed samples:      2063360 | consumed tokens:   4225761280 | elapsed time per iteration (ms): 4147.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.727277E-01 | loss scale: 65536.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.865 | tokens per gpu per second (tgs): 1975.360 | TFLOPs: 15.90 |
g0220: [2024-08-10 00:54:22,206] [INFO] [logging.py:96:log_dist] [Rank 0] step=16130, skipped=20, lr=[0.00019999198402481668, 0.00019999198402481668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16130 loss: 0.8872 iter time (s): 4.006 samples/sec: 31.949
g0238:  iteration    16130/10000000 | consumed samples:      2064640 | consumed tokens:   4228382720 | elapsed time per iteration (ms): 4041.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.692560E-01 | loss scale: 65536.0 | grad norm: 0.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.674 | tokens per gpu per second (tgs): 2027.129 | TFLOPs: 16.31 |
g0220: [2024-08-10 00:55:02,830] [INFO] [logging.py:96:log_dist] [Rank 0] step=16140, skipped=20, lr=[0.00019999194976836338, 0.00019999194976836338], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16140 loss: 0.8961 iter time (s): 4.029 samples/sec: 31.768
g0238:  iteration    16140/10000000 | consumed samples:      2065920 | consumed tokens:   4231004160 | elapsed time per iteration (ms): 4062.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.710011E-01 | loss scale: 65536.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.508 | tokens per gpu per second (tgs): 2016.515 | TFLOPs: 16.23 |
g0220: [2024-08-10 00:55:43,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=16150, skipped=20, lr=[0.00019999191543887133, 0.00019999191543887133], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16150 loss: 0.8462 iter time (s): 4.021 samples/sec: 31.836
g0238:  iteration    16150/10000000 | consumed samples:      2067200 | consumed tokens:   4233625600 | elapsed time per iteration (ms): 4058.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.633004E-01 | loss scale: 65536.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.542 | tokens per gpu per second (tgs): 2018.701 | TFLOPs: 16.24 |
g0220: [2024-08-10 00:56:23,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=16160, skipped=20, lr=[0.0001999918810363405, 0.0001999918810363405], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16160 loss: 0.8832 iter time (s): 3.930 samples/sec: 32.571
g0238:  iteration    16160/10000000 | consumed samples:      2068480 | consumed tokens:   4236247040 | elapsed time per iteration (ms): 3962.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.762840E-01 | loss scale: 65536.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.299 | tokens per gpu per second (tgs): 2067.155 | TFLOPs: 16.63 |
g0220: [2024-08-10 00:57:04,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=16170, skipped=20, lr=[0.00019999184656077095, 0.00019999184656077095], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16170 loss: 0.8533 iter time (s): 4.093 samples/sec: 31.273
g0238:  iteration    16170/10000000 | consumed samples:      2069760 | consumed tokens:   4238868480 | elapsed time per iteration (ms): 4126.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.610299E-01 | loss scale: 65536.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.018 | tokens per gpu per second (tgs): 1985.138 | TFLOPs: 15.97 |
g0220: [2024-08-10 00:57:44,363] [INFO] [logging.py:96:log_dist] [Rank 0] step=16180, skipped=20, lr=[0.0001999918120121627, 0.0001999918120121627], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16180 loss: 0.8819 iter time (s): 3.972 samples/sec: 32.222
g0238:  iteration    16180/10000000 | consumed samples:      2071040 | consumed tokens:   4241489920 | elapsed time per iteration (ms): 4005.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.692566E-01 | loss scale: 65536.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.957 | tokens per gpu per second (tgs): 2045.233 | TFLOPs: 16.46 |
g0220: [2024-08-10 00:58:25,043] [INFO] [logging.py:96:log_dist] [Rank 0] step=16190, skipped=20, lr=[0.0001999917773905158, 0.0001999917773905158], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16190 loss: 0.8716 iter time (s): 4.028 samples/sec: 31.774
g0238:  iteration    16190/10000000 | consumed samples:      2072320 | consumed tokens:   4244111360 | elapsed time per iteration (ms): 4068.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.755006E-01 | loss scale: 65536.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.463 | tokens per gpu per second (tgs): 2013.647 | TFLOPs: 16.20 |
g0220: [2024-08-10 00:59:06,412] [INFO] [logging.py:96:log_dist] [Rank 0] step=16200, skipped=20, lr=[0.00019999174269583021, 0.00019999174269583021], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16200 loss: 0.8770 iter time (s): 4.096 samples/sec: 31.250
g0238:  iteration    16200/10000000 | consumed samples:      2073600 | consumed tokens:   4246732800 | elapsed time per iteration (ms): 4136.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.715185E-01 | loss scale: 65536.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.943 | tokens per gpu per second (tgs): 1980.378 | TFLOPs: 15.94 |
g0220: [2024-08-10 00:59:47,664] [INFO] [logging.py:96:log_dist] [Rank 0] step=16210, skipped=20, lr=[0.00019999170792810603, 0.00019999170792810603], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16210 loss: 0.8883 iter time (s): 4.091 samples/sec: 31.288
g0238:  iteration    16210/10000000 | consumed samples:      2074880 | consumed tokens:   4249354240 | elapsed time per iteration (ms): 4125.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.786203E-01 | loss scale: 65536.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.028 | tokens per gpu per second (tgs): 1985.808 | TFLOPs: 15.98 |
g0220: [2024-08-10 01:00:29,810] [INFO] [logging.py:96:log_dist] [Rank 0] step=16220, skipped=20, lr=[0.00019999167308734327, 0.00019999167308734327], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16220 loss: 0.8474 iter time (s): 4.182 samples/sec: 30.607
g0238:  iteration    16220/10000000 | consumed samples:      2076160 | consumed tokens:   4251975680 | elapsed time per iteration (ms): 4214.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.765914E-01 | loss scale: 65536.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.371 | tokens per gpu per second (tgs): 1943.775 | TFLOPs: 15.64 |
g0220: [2024-08-10 01:01:12,017] [INFO] [logging.py:96:log_dist] [Rank 0] step=16230, skipped=20, lr=[0.00019999163817354193, 0.00019999163817354193], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16230 loss: 0.8667 iter time (s): 4.188 samples/sec: 30.565
g0238:  iteration    16230/10000000 | consumed samples:      2077440 | consumed tokens:   4254597120 | elapsed time per iteration (ms): 4220.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.772933E-01 | loss scale: 65536.0 | grad norm: 0.283 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.327 | tokens per gpu per second (tgs): 1940.946 | TFLOPs: 15.62 |
g0220: [2024-08-10 01:01:53,590] [INFO] [logging.py:96:log_dist] [Rank 0] step=16240, skipped=20, lr=[0.00019999160318670206, 0.00019999160318670206], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16240 loss: 0.9131 iter time (s): 4.124 samples/sec: 31.036
g0238:  iteration    16240/10000000 | consumed samples:      2078720 | consumed tokens:   4257218560 | elapsed time per iteration (ms): 4157.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.722230E-01 | loss scale: 65536.0 | grad norm: 0.277 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.788 | tokens per gpu per second (tgs): 1970.453 | TFLOPs: 15.86 |
g0220: [2024-08-10 01:02:34,824] [INFO] [logging.py:96:log_dist] [Rank 0] step=16250, skipped=20, lr=[0.0001999915681268237, 0.0001999915681268237], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16250 loss: 0.8510 iter time (s): 4.089 samples/sec: 31.300
g0238:  iteration    16250/10000000 | consumed samples:      2080000 | consumed tokens:   4259840000 | elapsed time per iteration (ms): 4123.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.757510E-01 | loss scale: 65536.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.043 | tokens per gpu per second (tgs): 1986.763 | TFLOPs: 15.99 |
g0220: [2024-08-10 01:03:15,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=16260, skipped=20, lr=[0.00019999153299390683, 0.00019999153299390683], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16260 loss: 0.8863 iter time (s): 3.991 samples/sec: 32.074
g0238:  iteration    16260/10000000 | consumed samples:      2081280 | consumed tokens:   4262461440 | elapsed time per iteration (ms): 4024.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.821844E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.809 | tokens per gpu per second (tgs): 2035.776 | TFLOPs: 16.38 |
g0220: [2024-08-10 01:03:56,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=16270, skipped=20, lr=[0.0001999914977879515, 0.0001999914977879515], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16270 loss: 0.8703 iter time (s): 4.111 samples/sec: 31.136
g0238:  iteration    16270/10000000 | consumed samples:      2082560 | consumed tokens:   4265082880 | elapsed time per iteration (ms): 4144.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.630989E-01 | loss scale: 65536.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.888 | tokens per gpu per second (tgs): 1976.852 | TFLOPs: 15.91 |
g0220: [2024-08-10 01:04:37,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=16280, skipped=20, lr=[0.00019999146250895773, 0.00019999146250895773], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16280 loss: 0.8715 iter time (s): 4.050 samples/sec: 31.604
g0238:  iteration    16280/10000000 | consumed samples:      2083840 | consumed tokens:   4267704320 | elapsed time per iteration (ms): 4082.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.694249E-01 | loss scale: 65536.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.352 | tokens per gpu per second (tgs): 2006.525 | TFLOPs: 16.15 |
g0220: [2024-08-10 01:05:18,005] [INFO] [logging.py:96:log_dist] [Rank 0] step=16290, skipped=20, lr=[0.00019999142715692558, 0.00019999142715692558], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16290 loss: 0.8970 iter time (s): 4.035 samples/sec: 31.726
g0238:  iteration    16290/10000000 | consumed samples:      2085120 | consumed tokens:   4270325760 | elapsed time per iteration (ms): 4067.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.668798E-01 | loss scale: 65536.0 | grad norm: 0.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.468 | tokens per gpu per second (tgs): 2013.969 | TFLOPs: 16.21 |
g0220: [2024-08-10 01:06:00,108] [INFO] [logging.py:96:log_dist] [Rank 0] step=16300, skipped=20, lr=[0.00019999139173185505, 0.00019999139173185505], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16300 loss: 0.8644 iter time (s): 4.176 samples/sec: 30.648
g0238:  iteration    16300/10000000 | consumed samples:      2086400 | consumed tokens:   4272947200 | elapsed time per iteration (ms): 4210.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.586321E-01 | loss scale: 65536.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.403 | tokens per gpu per second (tgs): 1945.803 | TFLOPs: 15.66 |
g0237: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0238: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0238: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 01:06:40,983] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 01:06:40,982] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 01:06:40,983] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 01:06:40,983] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 01:06:40,983] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0238: [2024-08-10 01:06:40,983] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 01:06:40,983] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 01:06:40,983] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 01:06:40,989] [INFO] [logging.py:96:log_dist] [Rank 0] step=16310, skipped=20, lr=[0.00019999135623374617, 0.00019999135623374617], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16310 loss: 0.8921 iter time (s): 4.056 samples/sec: 31.562
g0238:  iteration    16310/10000000 | consumed samples:      2087680 | consumed tokens:   4275568640 | elapsed time per iteration (ms): 4088.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.727177E-01 | loss scale: 131072.0 | grad norm: 0.108 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.311 | tokens per gpu per second (tgs): 2003.883 | TFLOPs: 16.13 |
g0220: [2024-08-10 01:07:22,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=16320, skipped=20, lr=[0.000199991320662599, 0.000199991320662599], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16320 loss: 0.8593 iter time (s): 4.120 samples/sec: 31.071
g0238:  iteration    16320/10000000 | consumed samples:      2088960 | consumed tokens:   4278190080 | elapsed time per iteration (ms): 4152.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.797845E-01 | loss scale: 131072.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.824 | tokens per gpu per second (tgs): 1972.752 | TFLOPs: 15.88 |
g0220: [2024-08-10 01:08:02,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=16330, skipped=20, lr=[0.0001999912850184135, 0.0001999912850184135], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16330 loss: 0.8637 iter time (s): 3.998 samples/sec: 32.015
g0238:  iteration    16330/10000000 | consumed samples:      2090240 | consumed tokens:   4280811520 | elapsed time per iteration (ms): 4030.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.862781E-01 | loss scale: 131072.0 | grad norm: 0.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.755 | tokens per gpu per second (tgs): 2032.346 | TFLOPs: 16.35 |
g0220: [2024-08-10 01:08:43,241] [INFO] [logging.py:96:log_dist] [Rank 0] step=16340, skipped=20, lr=[0.00019999124930118975, 0.00019999124930118975], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16340 loss: 0.8724 iter time (s): 4.009 samples/sec: 31.925
g0238:  iteration    16340/10000000 | consumed samples:      2091520 | consumed tokens:   4283432960 | elapsed time per iteration (ms): 4041.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.688798E-01 | loss scale: 131072.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.669 | tokens per gpu per second (tgs): 2026.840 | TFLOPs: 16.31 |
g0220: [2024-08-10 01:09:23,865] [INFO] [logging.py:96:log_dist] [Rank 0] step=16350, skipped=20, lr=[0.00019999121351092775, 0.00019999121351092775], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16350 loss: 0.8619 iter time (s): 4.029 samples/sec: 31.769
g0238:  iteration    16350/10000000 | consumed samples:      2092800 | consumed tokens:   4286054400 | elapsed time per iteration (ms): 4062.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.680145E-01 | loss scale: 131072.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.508 | tokens per gpu per second (tgs): 2016.507 | TFLOPs: 16.23 |
g0220: [2024-08-10 01:10:05,662] [INFO] [logging.py:96:log_dist] [Rank 0] step=16360, skipped=20, lr=[0.00019999117764762751, 0.00019999117764762751], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16360 loss: 0.8712 iter time (s): 4.146 samples/sec: 30.870
g0238:  iteration    16360/10000000 | consumed samples:      2094080 | consumed tokens:   4288675840 | elapsed time per iteration (ms): 4179.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.641144E-01 | loss scale: 131072.0 | grad norm: 0.278 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.625 | tokens per gpu per second (tgs): 1960.000 | TFLOPs: 15.77 |
g0220: [2024-08-10 01:10:47,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=16370, skipped=20, lr=[0.00019999114171128914, 0.00019999114171128914], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16370 loss: 0.8868 iter time (s): 4.158 samples/sec: 30.782
g0238:  iteration    16370/10000000 | consumed samples:      2095360 | consumed tokens:   4291297280 | elapsed time per iteration (ms): 4191.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.673281E-01 | loss scale: 131072.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.541 | tokens per gpu per second (tgs): 1954.653 | TFLOPs: 15.73 |
g0225: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 16376
g0225: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 16376
g0225: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 16376
g0220: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 16376
g0220: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 16376
g0225: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 16376
g0220: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 16376
g0220: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 16376
g0236: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 16376
g0236: Grad overflow on iteration 16376
g0225: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: Grad overflow on iteration 16376
g0235: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 16376
g0238: Grad overflow on iteration 16376
g0236: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: Grad overflow on iteration 16376
g0235: Grad overflow on iteration 16376
g0237: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 16376
g0236: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 16376
g0237: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: Grad overflow on iteration 16376
g0235: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 16376
g0234: Grad overflow on iteration 16376
g0237: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: Grad overflow on iteration 16376
g0225: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: Grad overflow on iteration 16376
g0234: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 16376
g0233: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: Grad overflow on iteration 16376
g0234: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: Grad overflow on iteration 16376
g0233: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 16376
g0238: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: Grad overflow on iteration 16376
g0235: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 16376
g0238: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 16376
g0234: Grad overflow on iteration 16376
g0234: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 16376
g0233: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 16376
g0220: [2024-08-10 01:11:17,664] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 01:11:17,664] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
g0220: [2024-08-10 01:11:30,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=16380, skipped=21, lr=[0.0001999911057019126, 0.0001999911057019126], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16380 loss: 0.8978 iter time (s): 4.222 samples/sec: 30.315
g0238:  iteration    16380/10000000 | consumed samples:      2096640 | consumed tokens:   4293918720 | elapsed time per iteration (ms): 4255.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.795157E-01 | loss scale: 65536.0 | grad norm: 0.335 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.079 | tokens per gpu per second (tgs): 1925.027 | TFLOPs: 15.49 |
g0236: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 16381
g0236: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 16381
g0237: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 16381
g0236: Grad overflow on iteration 16381
g0237: Grad overflow on iteration 16381
g0236: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 16381
g0237: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 16381
g0236: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 16381
g0236: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 16381
g0233: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 16381
g0234: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 16381
g0225: Grad overflow on iteration 16381
g0234: Grad overflow on iteration 16381
g0236: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 16381
g0225: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 16381
g0235: Grad overflow on iteration 16381
g0220: Grad overflow on iteration 16381
g0238: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: Grad overflow on iteration 16381
g0237: Grad overflow on iteration 16381
g0235: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 16381
g0237: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 16381
g0233: Grad overflow on iteration 16381
g0233: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: Grad overflow on iteration 16381
g0237: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: Grad overflow on iteration 16381
g0225: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 16381
g0233: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: Grad overflow on iteration 16381
g0237: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 01:11:38,372] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0235: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: Grad overflow on iteration 16381
g0234: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 16381
g0234: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 16381
g0235: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: Grad overflow on iteration 16381
g0234: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: Grad overflow on iteration 16381
g0238: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 16381
g0238: [2024-08-10 01:11:38,372] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 01:11:38,373] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 01:12:11,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=16390, skipped=22, lr=[0.00019999106961949791, 0.00019999106961949791], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16390 loss: 0.8682 iter time (s): 4.152 samples/sec: 30.826
g0238:  iteration    16390/10000000 | consumed samples:      2097920 | consumed tokens:   4296540160 | elapsed time per iteration (ms): 4185.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.833690E-01 | loss scale: 32768.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.585 | tokens per gpu per second (tgs): 1957.432 | TFLOPs: 15.75 |
g0220: [2024-08-10 01:12:52,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=16400, skipped=22, lr=[0.00019999103346404512, 0.00019999103346404512], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16400 loss: 0.8327 iter time (s): 4.025 samples/sec: 31.804
g0238:  iteration    16400/10000000 | consumed samples:      2099200 | consumed tokens:   4299161600 | elapsed time per iteration (ms): 4057.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.737468E-01 | loss scale: 32768.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.548 | tokens per gpu per second (tgs): 2019.091 | TFLOPs: 16.25 |
g0220: [2024-08-10 01:13:33,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=16410, skipped=22, lr=[0.0001999909972355543, 0.0001999909972355543], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16410 loss: 0.8603 iter time (s): 4.099 samples/sec: 31.224
g0238:  iteration    16410/10000000 | consumed samples:      2100480 | consumed tokens:   4301783040 | elapsed time per iteration (ms): 4132.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.736118E-01 | loss scale: 32768.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.974 | tokens per gpu per second (tgs): 1982.333 | TFLOPs: 15.95 |
g0220: [2024-08-10 01:14:16,300] [INFO] [logging.py:96:log_dist] [Rank 0] step=16420, skipped=22, lr=[0.0001999909609340254, 0.0001999909609340254], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16420 loss: 0.8976 iter time (s): 4.210 samples/sec: 30.405
g0238:  iteration    16420/10000000 | consumed samples:      2101760 | consumed tokens:   4304404480 | elapsed time per iteration (ms): 4242.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.712867E-01 | loss scale: 32768.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.171 | tokens per gpu per second (tgs): 1930.971 | TFLOPs: 15.54 |
g0220: [2024-08-10 01:14:57,855] [INFO] [logging.py:96:log_dist] [Rank 0] step=16430, skipped=22, lr=[0.0001999909245594585, 0.0001999909245594585], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16430 loss: 0.8871 iter time (s): 4.123 samples/sec: 31.047
g0238:  iteration    16430/10000000 | consumed samples:      2103040 | consumed tokens:   4307025920 | elapsed time per iteration (ms): 4155.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.672910E-01 | loss scale: 32768.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.804 | tokens per gpu per second (tgs): 1971.435 | TFLOPs: 15.86 |
g0220: [2024-08-10 01:15:38,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=16440, skipped=22, lr=[0.0001999908881118536, 0.0001999908881118536], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16440 loss: 0.8254 iter time (s): 4.033 samples/sec: 31.739
g0238:  iteration    16440/10000000 | consumed samples:      2104320 | consumed tokens:   4309647360 | elapsed time per iteration (ms): 4065.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.610874E-01 | loss scale: 32768.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.484 | tokens per gpu per second (tgs): 2014.984 | TFLOPs: 16.21 |
g0220: [2024-08-10 01:16:20,266] [INFO] [logging.py:96:log_dist] [Rank 0] step=16450, skipped=22, lr=[0.00019999085159121073, 0.00019999085159121073], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16450 loss: 0.8631 iter time (s): 4.143 samples/sec: 30.895
g0238:  iteration    16450/10000000 | consumed samples:      2105600 | consumed tokens:   4312268800 | elapsed time per iteration (ms): 4175.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.635311E-01 | loss scale: 32768.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.654 | tokens per gpu per second (tgs): 1961.887 | TFLOPs: 15.79 |
g0220: [2024-08-10 01:17:01,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=16460, skipped=22, lr=[0.00019999081499752993, 0.00019999081499752993], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16460 loss: 0.8601 iter time (s): 4.120 samples/sec: 31.067
g0238:  iteration    16460/10000000 | consumed samples:      2106880 | consumed tokens:   4314890240 | elapsed time per iteration (ms): 4153.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.654458E-01 | loss scale: 32768.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.819 | tokens per gpu per second (tgs): 1972.436 | TFLOPs: 15.87 |
g0220: [2024-08-10 01:17:43,978] [INFO] [logging.py:96:log_dist] [Rank 0] step=16470, skipped=22, lr=[0.00019999077833081125, 0.00019999077833081125], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16470 loss: 0.8569 iter time (s): 4.185 samples/sec: 30.586
g0238:  iteration    16470/10000000 | consumed samples:      2108160 | consumed tokens:   4317511680 | elapsed time per iteration (ms): 4218.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.614145E-01 | loss scale: 32768.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.346 | tokens per gpu per second (tgs): 1942.171 | TFLOPs: 15.63 |
g0220: [2024-08-10 01:18:25,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=16480, skipped=22, lr=[0.0001999907415910547, 0.0001999907415910547], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16480 loss: 0.8701 iter time (s): 4.127 samples/sec: 31.018
g0238:  iteration    16480/10000000 | consumed samples:      2109440 | consumed tokens:   4320133120 | elapsed time per iteration (ms): 4159.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.688757E-01 | loss scale: 32768.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.770 | tokens per gpu per second (tgs): 1969.305 | TFLOPs: 15.85 |
g0220: [2024-08-10 01:19:06,990] [INFO] [logging.py:96:log_dist] [Rank 0] step=16490, skipped=22, lr=[0.00019999070477826026, 0.00019999070477826026], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16490 loss: 0.8995 iter time (s): 4.109 samples/sec: 31.154
g0238:  iteration    16490/10000000 | consumed samples:      2110720 | consumed tokens:   4322754560 | elapsed time per iteration (ms): 4141.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.655137E-01 | loss scale: 32768.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.907 | tokens per gpu per second (tgs): 1978.038 | TFLOPs: 15.92 |
g0220: [2024-08-10 01:19:47,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=16500, skipped=22, lr=[0.00019999066789242804, 0.00019999066789242804], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16500 loss: 0.8561 iter time (s): 3.972 samples/sec: 32.229
g0238:  iteration    16500/10000000 | consumed samples:      2112000 | consumed tokens:   4325376000 | elapsed time per iteration (ms): 4004.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.637635E-01 | loss scale: 32768.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.962 | tokens per gpu per second (tgs): 2045.556 | TFLOPs: 16.46 |
g0220: [2024-08-10 01:20:28,783] [INFO] [logging.py:96:log_dist] [Rank 0] step=16510, skipped=22, lr=[0.000199990630933558, 0.000199990630933558], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16510 loss: 0.8657 iter time (s): 4.142 samples/sec: 30.902
g0238:  iteration    16510/10000000 | consumed samples:      2113280 | consumed tokens:   4327997440 | elapsed time per iteration (ms): 4174.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.735260E-01 | loss scale: 32768.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.664 | tokens per gpu per second (tgs): 1962.468 | TFLOPs: 15.79 |
g0220: [2024-08-10 01:21:10,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=16520, skipped=22, lr=[0.0001999905939016502, 0.0001999905939016502], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16520 loss: 0.8645 iter time (s): 4.102 samples/sec: 31.208
g0238:  iteration    16520/10000000 | consumed samples:      2114560 | consumed tokens:   4330618880 | elapsed time per iteration (ms): 4134.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.611868E-01 | loss scale: 32768.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.962 | tokens per gpu per second (tgs): 1981.584 | TFLOPs: 15.95 |
g0220: [2024-08-10 01:21:51,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=16530, skipped=22, lr=[0.00019999055679670468, 0.00019999055679670468], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16530 loss: 0.8904 iter time (s): 4.139 samples/sec: 30.925
g0238:  iteration    16530/10000000 | consumed samples:      2115840 | consumed tokens:   4333240320 | elapsed time per iteration (ms): 4171.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.634812E-01 | loss scale: 32768.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.682 | tokens per gpu per second (tgs): 1963.680 | TFLOPs: 15.80 |
g0220: [2024-08-10 01:22:32,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=16540, skipped=22, lr=[0.00019999051961872144, 0.00019999051961872144], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16540 loss: 0.8567 iter time (s): 4.026 samples/sec: 31.797
g0238:  iteration    16540/10000000 | consumed samples:      2117120 | consumed tokens:   4335861760 | elapsed time per iteration (ms): 4058.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.660590E-01 | loss scale: 32768.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.542 | tokens per gpu per second (tgs): 2018.718 | TFLOPs: 16.24 |
g0220: [2024-08-10 01:23:12,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=16550, skipped=22, lr=[0.00019999048236770054, 0.00019999048236770054], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16550 loss: 0.8605 iter time (s): 3.983 samples/sec: 32.138
g0238:  iteration    16550/10000000 | consumed samples:      2118400 | consumed tokens:   4338483200 | elapsed time per iteration (ms): 4015.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.651979E-01 | loss scale: 32768.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.875 | tokens per gpu per second (tgs): 2039.979 | TFLOPs: 16.42 |
g0220: [2024-08-10 01:23:54,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=16560, skipped=22, lr=[0.00019999044504364198, 0.00019999044504364198], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16560 loss: 0.8450 iter time (s): 4.147 samples/sec: 30.869
g0238:  iteration    16560/10000000 | consumed samples:      2119680 | consumed tokens:   4341104640 | elapsed time per iteration (ms): 4180.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.708737E-01 | loss scale: 32768.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.618 | tokens per gpu per second (tgs): 1959.549 | TFLOPs: 15.77 |
g0220: [2024-08-10 01:24:36,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=16570, skipped=22, lr=[0.0001999904076465458, 0.0001999904076465458], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16570 loss: 0.8392 iter time (s): 4.135 samples/sec: 30.956
g0238:  iteration    16570/10000000 | consumed samples:      2120960 | consumed tokens:   4343726080 | elapsed time per iteration (ms): 4167.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.606709E-01 | loss scale: 32768.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.714 | tokens per gpu per second (tgs): 1965.668 | TFLOPs: 15.82 |
g0220: [2024-08-10 01:25:17,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=16580, skipped=22, lr=[0.00019999037017641203, 0.00019999037017641203], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16580 loss: 0.8978 iter time (s): 4.109 samples/sec: 31.151
g0238:  iteration    16580/10000000 | consumed samples:      2122240 | consumed tokens:   4346347520 | elapsed time per iteration (ms): 4142.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.733846E-01 | loss scale: 32768.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.901 | tokens per gpu per second (tgs): 1977.683 | TFLOPs: 15.91 |
g0220: [2024-08-10 01:25:56,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=16590, skipped=22, lr=[0.00019999033263324068, 0.00019999033263324068], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16590 loss: 0.8524 iter time (s): 3.875 samples/sec: 33.035
g0238:  iteration    16590/10000000 | consumed samples:      2123520 | consumed tokens:   4348968960 | elapsed time per iteration (ms): 3907.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.618941E-01 | loss scale: 32768.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.756 | tokens per gpu per second (tgs): 2096.355 | TFLOPs: 16.87 |
g0220: [2024-08-10 01:26:38,559] [INFO] [logging.py:96:log_dist] [Rank 0] step=16600, skipped=22, lr=[0.00019999029501703184, 0.00019999029501703184], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16600 loss: 0.8732 iter time (s): 4.167 samples/sec: 30.715
g0238:  iteration    16600/10000000 | consumed samples:      2124800 | consumed tokens:   4351590400 | elapsed time per iteration (ms): 4199.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.717301E-01 | loss scale: 32768.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.478 | tokens per gpu per second (tgs): 1950.565 | TFLOPs: 15.70 |
g0220: [2024-08-10 01:27:19,797] [INFO] [logging.py:96:log_dist] [Rank 0] step=16610, skipped=22, lr=[0.00019999025732778545, 0.00019999025732778545], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16610 loss: 0.8739 iter time (s): 4.091 samples/sec: 31.288
g0238:  iteration    16610/10000000 | consumed samples:      2126080 | consumed tokens:   4354211840 | elapsed time per iteration (ms): 4123.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.689915E-01 | loss scale: 32768.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.039 | tokens per gpu per second (tgs): 1986.520 | TFLOPs: 15.99 |
g0220: [2024-08-10 01:28:00,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=16620, skipped=22, lr=[0.0001999902195655016, 0.0001999902195655016], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16620 loss: 0.8883 iter time (s): 4.052 samples/sec: 31.592
g0238:  iteration    16620/10000000 | consumed samples:      2127360 | consumed tokens:   4356833280 | elapsed time per iteration (ms): 4084.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.666504E-01 | loss scale: 32768.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.341 | tokens per gpu per second (tgs): 2005.795 | TFLOPs: 16.14 |
g0220: [2024-08-10 01:28:42,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=16630, skipped=22, lr=[0.0001999901817301803, 0.0001999901817301803], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16630 loss: 0.8424 iter time (s): 4.189 samples/sec: 30.558
g0238:  iteration    16630/10000000 | consumed samples:      2128640 | consumed tokens:   4359454720 | elapsed time per iteration (ms): 4235.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.673212E-01 | loss scale: 32768.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.220 | tokens per gpu per second (tgs): 1934.076 | TFLOPs: 15.56 |
g0220: [2024-08-10 01:29:25,709] [INFO] [logging.py:96:log_dist] [Rank 0] step=16640, skipped=22, lr=[0.0001999901438218216, 0.0001999901438218216], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16640 loss: 0.8497 iter time (s): 4.239 samples/sec: 30.195
g0238:  iteration    16640/10000000 | consumed samples:      2129920 | consumed tokens:   4362076160 | elapsed time per iteration (ms): 4271.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.588652E-01 | loss scale: 32768.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.967 | tokens per gpu per second (tgs): 1917.857 | TFLOPs: 15.43 |
g0220: [2024-08-10 01:30:07,323] [INFO] [logging.py:96:log_dist] [Rank 0] step=16650, skipped=22, lr=[0.0001999901058404255, 0.0001999901058404255], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16650 loss: 0.8416 iter time (s): 4.129 samples/sec: 31.002
g0238:  iteration    16650/10000000 | consumed samples:      2131200 | consumed tokens:   4364697600 | elapsed time per iteration (ms): 4161.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.601229E-01 | loss scale: 32768.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.759 | tokens per gpu per second (tgs): 1968.562 | TFLOPs: 15.84 |
g0220: [2024-08-10 01:30:48,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=16660, skipped=22, lr=[0.00019999006778599203, 0.00019999006778599203], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16660 loss: 0.8736 iter time (s): 4.105 samples/sec: 31.182
g0238:  iteration    16660/10000000 | consumed samples:      2132480 | consumed tokens:   4367319040 | elapsed time per iteration (ms): 4138.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.543513E-01 | loss scale: 32768.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.930 | tokens per gpu per second (tgs): 1979.531 | TFLOPs: 15.93 |
g0220: [2024-08-10 01:31:29,675] [INFO] [logging.py:96:log_dist] [Rank 0] step=16670, skipped=22, lr=[0.00019999002965852126, 0.00019999002965852126], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16670 loss: 0.8489 iter time (s): 4.063 samples/sec: 31.502
g0238:  iteration    16670/10000000 | consumed samples:      2133760 | consumed tokens:   4369940480 | elapsed time per iteration (ms): 4096.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.641220E-01 | loss scale: 32768.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.244 | tokens per gpu per second (tgs): 1999.607 | TFLOPs: 16.09 |
g0220: [2024-08-10 01:32:10,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=16680, skipped=22, lr=[0.00019998999145801315, 0.00019998999145801315], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16680 loss: 0.8594 iter time (s): 4.028 samples/sec: 31.781
g0238:  iteration    16680/10000000 | consumed samples:      2135040 | consumed tokens:   4372561920 | elapsed time per iteration (ms): 4060.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.715689E-01 | loss scale: 32768.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.527 | tokens per gpu per second (tgs): 2017.701 | TFLOPs: 16.24 |
g0220: [2024-08-10 01:32:52,968] [INFO] [logging.py:96:log_dist] [Rank 0] step=16690, skipped=22, lr=[0.0001999899531844678, 0.0001999899531844678], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16690 loss: 0.8365 iter time (s): 4.237 samples/sec: 30.212
g0238:  iteration    16690/10000000 | consumed samples:      2136320 | consumed tokens:   4375183360 | elapsed time per iteration (ms): 4269.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.624465E-01 | loss scale: 32768.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.982 | tokens per gpu per second (tgs): 1918.879 | TFLOPs: 15.44 |
g0220: [2024-08-10 01:33:33,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=16700, skipped=22, lr=[0.00019998991483788521, 0.00019998991483788521], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16700 loss: 0.8473 iter time (s): 4.047 samples/sec: 31.629
g0238:  iteration    16700/10000000 | consumed samples:      2137600 | consumed tokens:   4377804800 | elapsed time per iteration (ms): 4079.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.670415E-01 | loss scale: 32768.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.378 | tokens per gpu per second (tgs): 2008.166 | TFLOPs: 16.16 |
g0220: [2024-08-10 01:34:15,211] [INFO] [logging.py:96:log_dist] [Rank 0] step=16710, skipped=22, lr=[0.00019998987641826538, 0.00019998987641826538], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16710 loss: 0.8850 iter time (s): 4.113 samples/sec: 31.124
g0238:  iteration    16710/10000000 | consumed samples:      2138880 | consumed tokens:   4380426240 | elapsed time per iteration (ms): 4144.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.602682E-01 | loss scale: 32768.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.881 | tokens per gpu per second (tgs): 1976.404 | TFLOPs: 15.90 |
g0220: [2024-08-10 01:34:56,181] [INFO] [logging.py:96:log_dist] [Rank 0] step=16720, skipped=22, lr=[0.00019998983792560837, 0.00019998983792560837], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16720 loss: 0.9170 iter time (s): 4.065 samples/sec: 31.491
g0238:  iteration    16720/10000000 | consumed samples:      2140160 | consumed tokens:   4383047680 | elapsed time per iteration (ms): 4097.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.665405E-01 | loss scale: 32768.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.242 | tokens per gpu per second (tgs): 1999.460 | TFLOPs: 16.09 |
g0220: [2024-08-10 01:35:37,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=16730, skipped=22, lr=[0.00019998979935991422, 0.00019998979935991422], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16730 loss: 0.8765 iter time (s): 4.085 samples/sec: 31.333
g0238:  iteration    16730/10000000 | consumed samples:      2141440 | consumed tokens:   4385669120 | elapsed time per iteration (ms): 4117.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.627215E-01 | loss scale: 32768.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.086 | tokens per gpu per second (tgs): 1989.517 | TFLOPs: 16.01 |
g0220: [2024-08-10 01:36:17,425] [INFO] [logging.py:96:log_dist] [Rank 0] step=16740, skipped=22, lr=[0.00019998976072118296, 0.00019998976072118296], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16740 loss: 0.8908 iter time (s): 3.974 samples/sec: 32.206
g0238:  iteration    16740/10000000 | consumed samples:      2142720 | consumed tokens:   4388290560 | elapsed time per iteration (ms): 4006.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.804644E-01 | loss scale: 32768.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.946 | tokens per gpu per second (tgs): 2044.538 | TFLOPs: 16.45 |
g0220: [2024-08-10 01:36:59,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=16750, skipped=22, lr=[0.00019998972200941458, 0.00019998972200941458], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16750 loss: 0.8589 iter time (s): 4.160 samples/sec: 30.767
g0238:  iteration    16750/10000000 | consumed samples:      2144000 | consumed tokens:   4390912000 | elapsed time per iteration (ms): 4193.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.614013E-01 | loss scale: 32768.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.524 | tokens per gpu per second (tgs): 1953.556 | TFLOPs: 15.72 |
g0220: [2024-08-10 01:37:41,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=16760, skipped=22, lr=[0.00019998968322460913, 0.00019998968322460913], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16760 loss: 0.8672 iter time (s): 4.176 samples/sec: 30.651
g0238:  iteration    16760/10000000 | consumed samples:      2145280 | consumed tokens:   4393533440 | elapsed time per iteration (ms): 4208.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.689846E-01 | loss scale: 32768.0 | grad norm: 0.376 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.415 | tokens per gpu per second (tgs): 1946.544 | TFLOPs: 15.66 |
g0220: [2024-08-10 01:38:23,512] [INFO] [logging.py:96:log_dist] [Rank 0] step=16770, skipped=22, lr=[0.00019998964436676668, 0.00019998964436676668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16770 loss: 0.8783 iter time (s): 4.174 samples/sec: 30.668
g0238:  iteration    16770/10000000 | consumed samples:      2146560 | consumed tokens:   4396154880 | elapsed time per iteration (ms): 4206.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.678174E-01 | loss scale: 32768.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.428 | tokens per gpu per second (tgs): 1947.374 | TFLOPs: 15.67 |
g0220: [2024-08-10 01:39:05,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=16780, skipped=22, lr=[0.0001999896054358872, 0.0001999896054358872], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16780 loss: 0.8308 iter time (s): 4.183 samples/sec: 30.599
g0238:  iteration    16780/10000000 | consumed samples:      2147840 | consumed tokens:   4398776320 | elapsed time per iteration (ms): 4215.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.640382E-01 | loss scale: 32768.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.365 | tokens per gpu per second (tgs): 1943.334 | TFLOPs: 15.64 |
g0220: [2024-08-10 01:39:46,381] [INFO] [logging.py:96:log_dist] [Rank 0] step=16790, skipped=22, lr=[0.00019998956643197077, 0.00019998956643197077], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16790 loss: 0.8512 iter time (s): 4.039 samples/sec: 31.689
g0238:  iteration    16790/10000000 | consumed samples:      2149120 | consumed tokens:   4401397760 | elapsed time per iteration (ms): 4071.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.526748E-01 | loss scale: 32768.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.438 | tokens per gpu per second (tgs): 2012.035 | TFLOPs: 16.19 |
g0220: [2024-08-10 01:40:27,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=16800, skipped=22, lr=[0.00019998952735501737, 0.00019998952735501737], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16800 loss: 0.8281 iter time (s): 4.088 samples/sec: 31.313
g0238:  iteration    16800/10000000 | consumed samples:      2150400 | consumed tokens:   4404019200 | elapsed time per iteration (ms): 4120.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.595886E-01 | loss scale: 32768.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.066 | tokens per gpu per second (tgs): 1988.202 | TFLOPs: 16.00 |
g0220: [2024-08-10 01:41:08,657] [INFO] [logging.py:96:log_dist] [Rank 0] step=16810, skipped=22, lr=[0.00019998948820502708, 0.00019998948820502708], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16810 loss: 0.8740 iter time (s): 4.075 samples/sec: 31.413
g0238:  iteration    16810/10000000 | consumed samples:      2151680 | consumed tokens:   4406640640 | elapsed time per iteration (ms): 4107.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.785967E-01 | loss scale: 32768.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.165 | tokens per gpu per second (tgs): 1994.528 | TFLOPs: 16.05 |
g0220: [2024-08-10 01:41:50,582] [INFO] [logging.py:96:log_dist] [Rank 0] step=16820, skipped=22, lr=[0.00019998944898199988, 0.00019998944898199988], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16820 loss: 0.8753 iter time (s): 4.159 samples/sec: 30.775
g0238:  iteration    16820/10000000 | consumed samples:      2152960 | consumed tokens:   4409262080 | elapsed time per iteration (ms): 4192.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.542743E-01 | loss scale: 32768.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.531 | tokens per gpu per second (tgs): 1953.993 | TFLOPs: 15.72 |
g0220: [2024-08-10 01:42:30,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=16830, skipped=22, lr=[0.00019998940968593584, 0.00019998940968593584], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16830 loss: 0.8692 iter time (s): 3.992 samples/sec: 32.062
g0238:  iteration    16830/10000000 | consumed samples:      2154240 | consumed tokens:   4411883520 | elapsed time per iteration (ms): 4025.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.709801E-01 | loss scale: 32768.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.799 | tokens per gpu per second (tgs): 2035.146 | TFLOPs: 16.38 |
g0220: [2024-08-10 01:43:09,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=16840, skipped=22, lr=[0.00019998937031683498, 0.00019998937031683498], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16840 loss: 0.8342 iter time (s): 3.818 samples/sec: 33.526
g0238:  iteration    16840/10000000 | consumed samples:      2155520 | consumed tokens:   4414504960 | elapsed time per iteration (ms): 3850.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.665724E-01 | loss scale: 32768.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.242 | tokens per gpu per second (tgs): 2127.481 | TFLOPs: 17.12 |
g0220: [2024-08-10 01:43:51,615] [INFO] [logging.py:96:log_dist] [Rank 0] step=16850, skipped=22, lr=[0.00019998933087469732, 0.00019998933087469732], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16850 loss: 0.8527 iter time (s): 4.195 samples/sec: 30.515
g0238:  iteration    16850/10000000 | consumed samples:      2156800 | consumed tokens:   4417126400 | elapsed time per iteration (ms): 4227.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.680863E-01 | loss scale: 32768.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.278 | tokens per gpu per second (tgs): 1937.818 | TFLOPs: 15.59 |
g0220: [2024-08-10 01:44:32,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=16860, skipped=22, lr=[0.0001999892913595229, 0.0001999892913595229], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16860 loss: 0.8861 iter time (s): 4.056 samples/sec: 31.554
g0238:  iteration    16860/10000000 | consumed samples:      2158080 | consumed tokens:   4419747840 | elapsed time per iteration (ms): 4089.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.609232E-01 | loss scale: 32768.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.300 | tokens per gpu per second (tgs): 2003.225 | TFLOPs: 16.12 |
g0220: [2024-08-10 01:45:14,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=16870, skipped=22, lr=[0.00019998925177131172, 0.00019998925177131172], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16870 loss: 0.8609 iter time (s): 4.144 samples/sec: 30.892
g0238:  iteration    16870/10000000 | consumed samples:      2159360 | consumed tokens:   4422369280 | elapsed time per iteration (ms): 4176.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.641726E-01 | loss scale: 32768.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.652 | tokens per gpu per second (tgs): 1961.697 | TFLOPs: 15.79 |
g0220: [2024-08-10 01:45:56,901] [INFO] [logging.py:96:log_dist] [Rank 0] step=16880, skipped=22, lr=[0.00019998921211006386, 0.00019998921211006386], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16880 loss: 0.8475 iter time (s): 4.231 samples/sec: 30.255
g0238:  iteration    16880/10000000 | consumed samples:      2160640 | consumed tokens:   4424990720 | elapsed time per iteration (ms): 4263.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.623193E-01 | loss scale: 32768.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.025 | tokens per gpu per second (tgs): 1921.582 | TFLOPs: 15.46 |
g0220: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 01:46:09,153] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 01:46:09,154] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 01:46:09,154] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 01:46:09,154] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 01:46:38,601] [INFO] [logging.py:96:log_dist] [Rank 0] step=16890, skipped=22, lr=[0.00019998917237577934, 0.00019998917237577934], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16890 loss: 0.8725 iter time (s): 4.138 samples/sec: 30.935
g0238:  iteration    16890/10000000 | consumed samples:      2161920 | consumed tokens:   4427612160 | elapsed time per iteration (ms): 4170.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.650916E-01 | loss scale: 65536.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.695 | tokens per gpu per second (tgs): 1964.472 | TFLOPs: 15.81 |
g0220: [2024-08-10 01:47:19,192] [INFO] [logging.py:96:log_dist] [Rank 0] step=16900, skipped=22, lr=[0.0001999891325684582, 0.0001999891325684582], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16900 loss: 0.8380 iter time (s): 4.027 samples/sec: 31.788
g0238:  iteration    16900/10000000 | consumed samples:      2163200 | consumed tokens:   4430233600 | elapsed time per iteration (ms): 4059.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.643728E-01 | loss scale: 65536.0 | grad norm: 0.294 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.535 | tokens per gpu per second (tgs): 2018.227 | TFLOPs: 16.24 |
g0220: [2024-08-10 01:47:59,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=16910, skipped=22, lr=[0.00019998909268810039, 0.00019998909268810039], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16910 loss: 0.8654 iter time (s): 3.988 samples/sec: 32.100
g0238:  iteration    16910/10000000 | consumed samples:      2164480 | consumed tokens:   4432855040 | elapsed time per iteration (ms): 4019.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.577835E-01 | loss scale: 65536.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.842 | tokens per gpu per second (tgs): 2037.885 | TFLOPs: 16.40 |
g0220: [2024-08-10 01:48:40,168] [INFO] [logging.py:96:log_dist] [Rank 0] step=16920, skipped=22, lr=[0.00019998905273470603, 0.00019998905273470603], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16920 loss: 0.8359 iter time (s): 4.046 samples/sec: 31.640
g0238:  iteration    16920/10000000 | consumed samples:      2165760 | consumed tokens:   4435476480 | elapsed time per iteration (ms): 4077.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.547456E-01 | loss scale: 65536.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.390 | tokens per gpu per second (tgs): 2008.929 | TFLOPs: 16.17 |
g0220: [2024-08-10 01:49:21,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=16930, skipped=22, lr=[0.00019998901270827508, 0.00019998901270827508], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16930 loss: 0.8533 iter time (s): 4.093 samples/sec: 31.270
g0238:  iteration    16930/10000000 | consumed samples:      2167040 | consumed tokens:   4438097920 | elapsed time per iteration (ms): 4126.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.588576E-01 | loss scale: 65536.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.020 | tokens per gpu per second (tgs): 1985.290 | TFLOPs: 15.98 |
g0220: [2024-08-10 01:50:02,188] [INFO] [logging.py:96:log_dist] [Rank 0] step=16940, skipped=22, lr=[0.00019998897260880768, 0.00019998897260880768], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16940 loss: 0.8694 iter time (s): 4.043 samples/sec: 31.660
g0238:  iteration    16940/10000000 | consumed samples:      2168320 | consumed tokens:   4440719360 | elapsed time per iteration (ms): 4075.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.586878E-01 | loss scale: 65536.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.406 | tokens per gpu per second (tgs): 2009.981 | TFLOPs: 16.17 |
g0220: [2024-08-10 01:50:46,124] [INFO] [logging.py:96:log_dist] [Rank 0] step=16950, skipped=22, lr=[0.00019998893243630374, 0.00019998893243630374], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16950 loss: 0.8681 iter time (s): 4.361 samples/sec: 29.354
g0238:  iteration    16950/10000000 | consumed samples:      2169600 | consumed tokens:   4443340800 | elapsed time per iteration (ms): 4393.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.552047E-01 | loss scale: 65536.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.133 | tokens per gpu per second (tgs): 1864.512 | TFLOPs: 15.00 |
g0220: [2024-08-10 01:51:28,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=16960, skipped=22, lr=[0.00019998889219076336, 0.00019998889219076336], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16960 loss: 0.8589 iter time (s): 4.159 samples/sec: 30.777
g0238:  iteration    16960/10000000 | consumed samples:      2170880 | consumed tokens:   4445962240 | elapsed time per iteration (ms): 4191.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.641591E-01 | loss scale: 65536.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.537 | tokens per gpu per second (tgs): 1954.395 | TFLOPs: 15.73 |
g0220: [2024-08-10 01:52:09,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=16970, skipped=22, lr=[0.00019998885187218657, 0.00019998885187218657], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16970 loss: 0.8641 iter time (s): 4.127 samples/sec: 31.013
g0238:  iteration    16970/10000000 | consumed samples:      2172160 | consumed tokens:   4448583680 | elapsed time per iteration (ms): 4160.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.667378E-01 | loss scale: 65536.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.765 | tokens per gpu per second (tgs): 1968.965 | TFLOPs: 15.84 |
g0220: [2024-08-10 01:52:50,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=16980, skipped=22, lr=[0.00019998881148057337, 0.00019998881148057337], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16980 loss: 0.8546 iter time (s): 4.097 samples/sec: 31.246
g0238:  iteration    16980/10000000 | consumed samples:      2173440 | consumed tokens:   4451205120 | elapsed time per iteration (ms): 4129.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.642860E-01 | loss scale: 65536.0 | grad norm: 0.259 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.995 | tokens per gpu per second (tgs): 1983.698 | TFLOPs: 15.96 |
g0220: [2024-08-10 01:53:32,770] [INFO] [logging.py:96:log_dist] [Rank 0] step=16990, skipped=22, lr=[0.00019998877101592383, 0.00019998877101592383], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 16990 loss: 0.8379 iter time (s): 4.150 samples/sec: 30.844
g0238:  iteration    16990/10000000 | consumed samples:      2174720 | consumed tokens:   4453826560 | elapsed time per iteration (ms): 4182.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.520309E-01 | loss scale: 65536.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.603 | tokens per gpu per second (tgs): 1958.565 | TFLOPs: 15.76 |
g0220: [2024-08-10 01:54:14,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=17000, skipped=22, lr=[0.0001999887304782379, 0.0001999887304782379], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17000 loss: 0.8862 iter time (s): 4.110 samples/sec: 31.141
g0238:  iteration    17000/10000000 | consumed samples:      2176000 | consumed tokens:   4456448000 | elapsed time per iteration (ms): 4144.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.636098E-01 | loss scale: 65536.0 | grad norm: 0.238 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.886 | tokens per gpu per second (tgs): 1976.680 | TFLOPs: 15.91 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 17000 | lm loss value: 8.693544E-01 | lm loss PPL: 2.385370E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   17000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-10 02:00:46,049] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step17000 is about to be saved!
g0220: [2024-08-10 02:00:46,056] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0238: [2024-08-10 02:00:46,056] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0220: [2024-08-10 02:00:46,056] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0220: [2024-08-10 02:00:46,056] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0238: [2024-08-10 02:00:46,057] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0238: [2024-08-10 02:00:46,057] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0225: [2024-08-10 02:00:46,057] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0225: [2024-08-10 02:00:46,057] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0225: [2024-08-10 02:00:46,057] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0233: [2024-08-10 02:00:46,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0233: [2024-08-10 02:00:46,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0233: [2024-08-10 02:00:46,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0234: [2024-08-10 02:00:46,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0234: [2024-08-10 02:00:46,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0234: [2024-08-10 02:00:46,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0236: [2024-08-10 02:00:46,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0236: [2024-08-10 02:00:46,058] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0236: [2024-08-10 02:00:46,059] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0237: [2024-08-10 02:00:46,059] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0237: [2024-08-10 02:00:46,059] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0237: [2024-08-10 02:00:46,059] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0235: [2024-08-10 02:00:46,063] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0235: [2024-08-10 02:00:46,063] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0235: [2024-08-10 02:00:46,063] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0238: [2024-08-10 02:00:46,084] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_23-model_00-model_states.pt...
g0233: [2024-08-10 02:00:46,095] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_08-model_00-model_states.pt...
g0225: [2024-08-10 02:00:46,095] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_05-model_00-model_states.pt...
g0234: [2024-08-10 02:00:46,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_11-model_00-model_states.pt...
g0236: [2024-08-10 02:00:46,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_17-model_00-model_states.pt...
g0237: [2024-08-10 02:00:46,096] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_20-model_00-model_states.pt...
g0235: [2024-08-10 02:00:46,100] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_14-model_00-model_states.pt...
g0220: [2024-08-10 02:00:46,110] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_01-model_00-model_states.pt...
g0235: [2024-08-10 02:00:46,247] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_14-model_00-model_states.pt.
g0237: [2024-08-10 02:00:46,248] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_20-model_00-model_states.pt.
g0238: [2024-08-10 02:00:46,269] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_23-model_00-model_states.pt.
g0238: [2024-08-10 02:00:46,271] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_24-model_00-model_states.pt...
g0225: [2024-08-10 02:00:46,284] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_05-model_00-model_states.pt.
g0235: [2024-08-10 02:00:46,286] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_15-model_00-model_states.pt...
g0237: [2024-08-10 02:00:46,287] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_21-model_00-model_states.pt...
g0238: [2024-08-10 02:00:46,293] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_24-model_00-model_states.pt.
g0220: [2024-08-10 02:00:46,294] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_01-model_00-model_states.pt.
g0234: [2024-08-10 02:00:46,295] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_11-model_00-model_states.pt.
g0233: [2024-08-10 02:00:46,321] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_08-model_00-model_states.pt.
g0220: [2024-08-10 02:00:46,324] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_02-model_00-model_states.pt...
g0225: [2024-08-10 02:00:46,324] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_06-model_00-model_states.pt...
g0234: [2024-08-10 02:00:46,335] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_12-model_00-model_states.pt...
g0236: [2024-08-10 02:00:46,342] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_17-model_00-model_states.pt.
g0238: [2024-08-10 02:00:46,346] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_25-model_00-model_states.pt...
g0233: [2024-08-10 02:00:46,359] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_09-model_00-model_states.pt...
g0236: [2024-08-10 02:00:46,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_18-model_00-model_states.pt...
g0235: [2024-08-10 02:00:46,405] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_15-model_00-model_states.pt.
g0234: [2024-08-10 02:00:46,426] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_12-model_00-model_states.pt.
g0235: [2024-08-10 02:00:46,439] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_16-model_00-model_states.pt...
g0234: [2024-08-10 02:00:46,461] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_13-model_00-model_states.pt...
g0237: [2024-08-10 02:00:46,471] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_21-model_00-model_states.pt.
g0225: [2024-08-10 02:00:46,489] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_06-model_00-model_states.pt.
g0236: [2024-08-10 02:00:46,505] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_18-model_00-model_states.pt.
g0237: [2024-08-10 02:00:46,506] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_22-model_00-model_states.pt...
g0238: [2024-08-10 02:00:46,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_25-model_00-model_states.pt.
g0238: [2024-08-10 02:00:46,510] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_07_model_states.pt...
g0233: [2024-08-10 02:00:46,517] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 02:00:46,524] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_07-model_00-model_states.pt...
g0236: [2024-08-10 02:00:46,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_19-model_00-model_states.pt...
g0233: [2024-08-10 02:00:46,548] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_10-model_00-model_states.pt...
g0235: [2024-08-10 02:00:46,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_16-model_00-model_states.pt.
g0235: [2024-08-10 02:00:46,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_04_model_states.pt...
g0234: [2024-08-10 02:00:46,597] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_13-model_00-model_states.pt.
g0234: [2024-08-10 02:00:46,599] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_03_model_states.pt...
g0225: [2024-08-10 02:00:46,630] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_07-model_00-model_states.pt.
g0225: [2024-08-10 02:00:46,632] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_01_model_states.pt...
g0233: [2024-08-10 02:00:46,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_10-model_00-model_states.pt.
g0233: [2024-08-10 02:00:46,666] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_02_model_states.pt...
g0237: [2024-08-10 02:00:46,667] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_22-model_00-model_states.pt.
g0237: [2024-08-10 02:00:46,669] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_06_model_states.pt...
g0236: [2024-08-10 02:00:46,681] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_19-model_00-model_states.pt.
g0236: [2024-08-10 02:00:46,683] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_05_model_states.pt...
g0220: [2024-08-10 02:00:46,710] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_02-model_00-model_states.pt.
g0220: [2024-08-10 02:00:46,742] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_03-model_00-model_states.pt...
g0220: [2024-08-10 02:00:46,875] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_03-model_00-model_states.pt.
g0220: [2024-08-10 02:00:46,910] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_04-model_00-model_states.pt...
g0220: [2024-08-10 02:00:47,004] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/layer_04-model_00-model_states.pt.
g0220: [2024-08-10 02:00:47,006] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_00_model_states.pt
g0220: [2024-08-10 02:00:47,006] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_00_model_states.pt...
g0238: [2024-08-10 02:00:48,431] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_07_model_states.pt.
g0238: [2024-08-10 02:00:48,432] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0235: [2024-08-10 02:00:48,939] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_04_model_states.pt.
g0235: [2024-08-10 02:00:48,939] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0234: [2024-08-10 02:00:48,955] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_03_model_states.pt.
g0234: [2024-08-10 02:00:48,955] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0225: [2024-08-10 02:00:49,000] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_01_model_states.pt.
g0225: [2024-08-10 02:00:49,001] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0236: [2024-08-10 02:00:49,012] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_05_model_states.pt.
g0236: [2024-08-10 02:00:49,012] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0237: [2024-08-10 02:00:49,194] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_06_model_states.pt.
g0237: [2024-08-10 02:00:49,195] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0233: [2024-08-10 02:00:49,264] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_02_model_states.pt.
g0233: [2024-08-10 02:00:49,265] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0220: [2024-08-10 02:00:56,849] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step17000/mp_rank_00_model_states.pt.
g0220: [2024-08-10 02:00:56,849] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17000 is ready now!
g0220:   successfully saved checkpoint at iteration   17000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 2.08, Latency(second): 10.814
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (10812.50, 10814.19)
g0220: [2024-08-10 02:01:38,858] [INFO] [logging.py:96:log_dist] [Rank 0] step=17010, skipped=22, lr=[0.00019998868986751572, 0.00019998868986751572], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17010 loss: 0.8492 iter time (s): 4.169 samples/sec: 30.699
g0238:  iteration    17010/10000000 | consumed samples:      2177280 | consumed tokens:   4459069440 | elapsed time per iteration (ms): 44464.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.659362E-01 | loss scale: 65536.0 | grad norm: 0.915 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.879 | tokens per gpu per second (tgs): 184.237 | TFLOPs: 1.48 |
g0220: [2024-08-10 02:02:19,644] [INFO] [logging.py:96:log_dist] [Rank 0] step=17020, skipped=22, lr=[0.00019998864918375724, 0.00019998864918375724], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17020 loss: 0.8768 iter time (s): 4.046 samples/sec: 31.638
g0238:  iteration    17020/10000000 | consumed samples:      2178560 | consumed tokens:   4461690880 | elapsed time per iteration (ms): 4078.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.686861E-01 | loss scale: 65536.0 | grad norm: 0.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.383 | tokens per gpu per second (tgs): 2008.531 | TFLOPs: 16.16 |
g0220: [2024-08-10 02:03:00,769] [INFO] [logging.py:96:log_dist] [Rank 0] step=17030, skipped=22, lr=[0.00019998860842696255, 0.00019998860842696255], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17030 loss: 0.8777 iter time (s): 4.080 samples/sec: 31.373
g0238:  iteration    17030/10000000 | consumed samples:      2179840 | consumed tokens:   4464312320 | elapsed time per iteration (ms): 4112.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.641410E-01 | loss scale: 65536.0 | grad norm: 0.247 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.121 | tokens per gpu per second (tgs): 1991.767 | TFLOPs: 16.03 |
g0220: [2024-08-10 02:03:42,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=17040, skipped=22, lr=[0.00019998856759713163, 0.00019998856759713163], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17040 loss: 0.8697 iter time (s): 4.116 samples/sec: 31.098
g0238:  iteration    17040/10000000 | consumed samples:      2181120 | consumed tokens:   4466933760 | elapsed time per iteration (ms): 4163.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.661287E-01 | loss scale: 65536.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.747 | tokens per gpu per second (tgs): 1967.779 | TFLOPs: 15.84 |
g0220: [2024-08-10 02:04:23,841] [INFO] [logging.py:96:log_dist] [Rank 0] step=17050, skipped=22, lr=[0.00019998852669426455, 0.00019998852669426455], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17050 loss: 0.8619 iter time (s): 4.109 samples/sec: 31.151
g0238:  iteration    17050/10000000 | consumed samples:      2182400 | consumed tokens:   4469555200 | elapsed time per iteration (ms): 4143.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.593154E-01 | loss scale: 65536.0 | grad norm: 0.270 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.891 | tokens per gpu per second (tgs): 1976.997 | TFLOPs: 15.91 |
g0220: [2024-08-10 02:05:05,400] [INFO] [logging.py:96:log_dist] [Rank 0] step=17060, skipped=22, lr=[0.0001999884857183613, 0.0001999884857183613], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17060 loss: 0.8285 iter time (s): 4.122 samples/sec: 31.054
g0238:  iteration    17060/10000000 | consumed samples:      2183680 | consumed tokens:   4472176640 | elapsed time per iteration (ms): 4155.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.589648E-01 | loss scale: 65536.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.800 | tokens per gpu per second (tgs): 1971.200 | TFLOPs: 15.86 |
g0220: [2024-08-10 02:05:44,498] [INFO] [logging.py:96:log_dist] [Rank 0] step=17070, skipped=22, lr=[0.00019998844466942195, 0.00019998844466942195], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17070 loss: 0.8857 iter time (s): 3.876 samples/sec: 33.021
g0238:  iteration    17070/10000000 | consumed samples:      2184960 | consumed tokens:   4474798080 | elapsed time per iteration (ms): 3909.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.602846E-01 | loss scale: 65536.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.739 | tokens per gpu per second (tgs): 2095.264 | TFLOPs: 16.86 |
g0220: [2024-08-10 02:06:26,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=17080, skipped=22, lr=[0.00019998840354744653, 0.00019998840354744653], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17080 loss: 0.8553 iter time (s): 4.185 samples/sec: 30.582
g0238:  iteration    17080/10000000 | consumed samples:      2186240 | consumed tokens:   4477419520 | elapsed time per iteration (ms): 4218.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.554073E-01 | loss scale: 65536.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.346 | tokens per gpu per second (tgs): 1942.139 | TFLOPs: 15.63 |
g0220: [2024-08-10 02:07:08,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=17090, skipped=22, lr=[0.00019998836235243504, 0.00019998836235243504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17090 loss: 0.8758 iter time (s): 4.179 samples/sec: 30.626
g0238:  iteration    17090/10000000 | consumed samples:      2187520 | consumed tokens:   4480040960 | elapsed time per iteration (ms): 4212.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.568129E-01 | loss scale: 65536.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.389 | tokens per gpu per second (tgs): 1944.900 | TFLOPs: 15.65 |
g0220: [2024-08-10 02:07:51,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=17100, skipped=22, lr=[0.00019998832108438752, 0.00019998832108438752], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17100 loss: 0.8588 iter time (s): 4.275 samples/sec: 29.940
g0238:  iteration    17100/10000000 | consumed samples:      2188800 | consumed tokens:   4482662400 | elapsed time per iteration (ms): 4307.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.576541E-01 | loss scale: 65536.0 | grad norm: 0.351 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.714 | tokens per gpu per second (tgs): 1901.686 | TFLOPs: 15.30 |
g0220: [2024-08-10 02:08:34,225] [INFO] [logging.py:96:log_dist] [Rank 0] step=17110, skipped=22, lr=[0.00019998827974330405, 0.00019998827974330405], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17110 loss: 0.8656 iter time (s): 4.202 samples/sec: 30.463
g0238:  iteration    17110/10000000 | consumed samples:      2190080 | consumed tokens:   4485283840 | elapsed time per iteration (ms): 4234.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.588301E-01 | loss scale: 65536.0 | grad norm: 0.303 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.227 | tokens per gpu per second (tgs): 1934.517 | TFLOPs: 15.57 |
g0220: [2024-08-10 02:09:15,398] [INFO] [logging.py:96:log_dist] [Rank 0] step=17120, skipped=22, lr=[0.0001999882383291846, 0.0001999882383291846], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17120 loss: 0.8125 iter time (s): 4.085 samples/sec: 31.337
g0238:  iteration    17120/10000000 | consumed samples:      2191360 | consumed tokens:   4487905280 | elapsed time per iteration (ms): 4117.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.563712E-01 | loss scale: 65536.0 | grad norm: 0.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.088 | tokens per gpu per second (tgs): 1989.621 | TFLOPs: 16.01 |
g0220: [2024-08-10 02:09:56,276] [INFO] [logging.py:96:log_dist] [Rank 0] step=17130, skipped=22, lr=[0.00019998819684202926, 0.00019998819684202926], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17130 loss: 0.8582 iter time (s): 4.055 samples/sec: 31.568
g0238:  iteration    17130/10000000 | consumed samples:      2192640 | consumed tokens:   4490526720 | elapsed time per iteration (ms): 4087.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.550408E-01 | loss scale: 65536.0 | grad norm: 0.267 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.313 | tokens per gpu per second (tgs): 2004.027 | TFLOPs: 16.13 |
g0220: [2024-08-10 02:10:39,344] [INFO] [logging.py:96:log_dist] [Rank 0] step=17140, skipped=22, lr=[0.000199988155281838, 0.000199988155281838], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17140 loss: 0.8617 iter time (s): 4.274 samples/sec: 29.950
g0238:  iteration    17140/10000000 | consumed samples:      2193920 | consumed tokens:   4493148160 | elapsed time per iteration (ms): 4306.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.601691E-01 | loss scale: 65536.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.721 | tokens per gpu per second (tgs): 1902.120 | TFLOPs: 15.31 |
g0220: [2024-08-10 02:11:19,214] [INFO] [logging.py:96:log_dist] [Rank 0] step=17150, skipped=22, lr=[0.00019998811364861085, 0.00019998811364861085], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17150 loss: 0.8638 iter time (s): 3.953 samples/sec: 32.378
g0238:  iteration    17150/10000000 | consumed samples:      2195200 | consumed tokens:   4495769600 | elapsed time per iteration (ms): 3987.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.640674E-01 | loss scale: 65536.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.104 | tokens per gpu per second (tgs): 2054.632 | TFLOPs: 16.53 |
g0220: [2024-08-10 02:11:59,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=17160, skipped=22, lr=[0.0001999880719423479, 0.0001999880719423479], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17160 loss: 0.8514 iter time (s): 3.962 samples/sec: 32.311
g0238:  iteration    17160/10000000 | consumed samples:      2196480 | consumed tokens:   4498391040 | elapsed time per iteration (ms): 3995.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.490379E-01 | loss scale: 65536.0 | grad norm: 0.263 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.039 | tokens per gpu per second (tgs): 2050.477 | TFLOPs: 16.50 |
g0220: [2024-08-10 02:12:38,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=17170, skipped=22, lr=[0.00019998803016304918, 0.00019998803016304918], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17170 loss: 0.8246 iter time (s): 3.900 samples/sec: 32.823
g0238:  iteration    17170/10000000 | consumed samples:      2197760 | consumed tokens:   4501012480 | elapsed time per iteration (ms): 3933.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.575779E-01 | loss scale: 65536.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.543 | tokens per gpu per second (tgs): 2082.738 | TFLOPs: 16.76 |
g0220: [2024-08-10 02:13:18,899] [INFO] [logging.py:96:log_dist] [Rank 0] step=17180, skipped=22, lr=[0.00019998798831071467, 0.00019998798831071467], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17180 loss: 0.8820 iter time (s): 4.007 samples/sec: 31.942
g0238:  iteration    17180/10000000 | consumed samples:      2199040 | consumed tokens:   4503633920 | elapsed time per iteration (ms): 4040.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.596754E-01 | loss scale: 65536.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.683 | tokens per gpu per second (tgs): 2027.696 | TFLOPs: 16.32 |
g0220: [2024-08-10 02:13:58,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=17190, skipped=22, lr=[0.00019998794638534443, 0.00019998794638534443], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17190 loss: 0.8699 iter time (s): 3.974 samples/sec: 32.213
g0238:  iteration    17190/10000000 | consumed samples:      2200320 | consumed tokens:   4506255360 | elapsed time per iteration (ms): 4006.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.605986E-01 | loss scale: 65536.0 | grad norm: 0.271 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.951 | tokens per gpu per second (tgs): 2044.880 | TFLOPs: 16.46 |
g0220: [2024-08-10 02:14:39,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=17200, skipped=22, lr=[0.00019998790438693852, 0.00019998790438693852], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17200 loss: 0.8507 iter time (s): 3.978 samples/sec: 32.180
g0238:  iteration    17200/10000000 | consumed samples:      2201600 | consumed tokens:   4508876800 | elapsed time per iteration (ms): 4010.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.616849E-01 | loss scale: 65536.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.919 | tokens per gpu per second (tgs): 2042.824 | TFLOPs: 16.44 |
g0220: [2024-08-10 02:15:20,746] [INFO] [logging.py:96:log_dist] [Rank 0] step=17210, skipped=22, lr=[0.00019998786231549692, 0.00019998786231549692], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17210 loss: 0.8456 iter time (s): 4.136 samples/sec: 30.951
g0238:  iteration    17210/10000000 | consumed samples:      2202880 | consumed tokens:   4511498240 | elapsed time per iteration (ms): 4168.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.494139E-01 | loss scale: 65536.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.707 | tokens per gpu per second (tgs): 1965.272 | TFLOPs: 15.81 |
g0220: [2024-08-10 02:16:02,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=17220, skipped=22, lr=[0.0001999878201710197, 0.0001999878201710197], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17220 loss: 0.8559 iter time (s): 4.105 samples/sec: 31.183
g0238:  iteration    17220/10000000 | consumed samples:      2204160 | consumed tokens:   4514119680 | elapsed time per iteration (ms): 4138.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.644528E-01 | loss scale: 65536.0 | grad norm: 0.235 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.930 | tokens per gpu per second (tgs): 1979.520 | TFLOPs: 15.93 |
g0220: [2024-08-10 02:16:42,305] [INFO] [logging.py:96:log_dist] [Rank 0] step=17230, skipped=22, lr=[0.00019998777795350684, 0.00019998777795350684], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17230 loss: 0.8963 iter time (s): 3.985 samples/sec: 32.122
g0238:  iteration    17230/10000000 | consumed samples:      2205440 | consumed tokens:   4516741120 | elapsed time per iteration (ms): 4017.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.567400E-01 | loss scale: 65536.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.861 | tokens per gpu per second (tgs): 2039.134 | TFLOPs: 16.41 |
g0220: [2024-08-10 02:17:23,163] [INFO] [logging.py:96:log_dist] [Rank 0] step=17240, skipped=22, lr=[0.00019998773566295842, 0.00019998773566295842], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17240 loss: 0.8495 iter time (s): 4.053 samples/sec: 31.581
g0238:  iteration    17240/10000000 | consumed samples:      2206720 | consumed tokens:   4519362560 | elapsed time per iteration (ms): 4085.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.493795E-01 | loss scale: 65536.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.328 | tokens per gpu per second (tgs): 2005.017 | TFLOPs: 16.13 |
g0220: [2024-08-10 02:18:04,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=17250, skipped=22, lr=[0.00019998769329937447, 0.00019998769329937447], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17250 loss: 0.8319 iter time (s): 4.108 samples/sec: 31.159
g0238:  iteration    17250/10000000 | consumed samples:      2208000 | consumed tokens:   4521984000 | elapsed time per iteration (ms): 4140.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.530435E-01 | loss scale: 65536.0 | grad norm: 0.257 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.913 | tokens per gpu per second (tgs): 1978.404 | TFLOPs: 15.92 |
g0220: [2024-08-10 02:18:44,874] [INFO] [logging.py:96:log_dist] [Rank 0] step=17260, skipped=22, lr=[0.00019998765086275504, 0.00019998765086275504], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17260 loss: 0.8989 iter time (s): 3.997 samples/sec: 32.022
g0238:  iteration    17260/10000000 | consumed samples:      2209280 | consumed tokens:   4524605440 | elapsed time per iteration (ms): 4030.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.703736E-01 | loss scale: 65536.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.758 | tokens per gpu per second (tgs): 2032.536 | TFLOPs: 16.36 |
g0220: [2024-08-10 02:19:26,648] [INFO] [logging.py:96:log_dist] [Rank 0] step=17270, skipped=22, lr=[0.00019998760835310012, 0.00019998760835310012], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17270 loss: 0.8540 iter time (s): 4.144 samples/sec: 30.887
g0238:  iteration    17270/10000000 | consumed samples:      2210560 | consumed tokens:   4527226880 | elapsed time per iteration (ms): 4177.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.626060E-01 | loss scale: 65536.0 | grad norm: 0.413 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.641 | tokens per gpu per second (tgs): 1960.996 | TFLOPs: 15.78 |
g0220: [2024-08-10 02:20:08,246] [INFO] [logging.py:96:log_dist] [Rank 0] step=17280, skipped=22, lr=[0.00019998756577040976, 0.00019998756577040976], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17280 loss: 0.8230 iter time (s): 4.127 samples/sec: 31.012
g0238:  iteration    17280/10000000 | consumed samples:      2211840 | consumed tokens:   4529848320 | elapsed time per iteration (ms): 4159.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.480044E-01 | loss scale: 65536.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.771 | tokens per gpu per second (tgs): 1969.341 | TFLOPs: 15.85 |
g0220: [2024-08-10 02:20:50,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=17290, skipped=22, lr=[0.000199987523114684, 0.000199987523114684], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17290 loss: 0.8736 iter time (s): 4.152 samples/sec: 30.829
g0238:  iteration    17290/10000000 | consumed samples:      2213120 | consumed tokens:   4532469760 | elapsed time per iteration (ms): 4184.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.555448E-01 | loss scale: 65536.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.588 | tokens per gpu per second (tgs): 1957.660 | TFLOPs: 15.75 |
g0220: [2024-08-10 02:21:31,227] [INFO] [logging.py:96:log_dist] [Rank 0] step=17300, skipped=22, lr=[0.00019998748038592288, 0.00019998748038592288], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17300 loss: 0.8584 iter time (s): 4.081 samples/sec: 31.366
g0238:  iteration    17300/10000000 | consumed samples:      2214400 | consumed tokens:   4535091200 | elapsed time per iteration (ms): 4113.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.599791E-01 | loss scale: 65536.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.117 | tokens per gpu per second (tgs): 1991.517 | TFLOPs: 16.03 |
g0220: [2024-08-10 02:22:12,281] [INFO] [logging.py:96:log_dist] [Rank 0] step=17310, skipped=22, lr=[0.0001999874375841264, 0.0001999874375841264], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17310 loss: 0.8510 iter time (s): 4.072 samples/sec: 31.438
g0238:  iteration    17310/10000000 | consumed samples:      2215680 | consumed tokens:   4537712640 | elapsed time per iteration (ms): 4105.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.597962E-01 | loss scale: 65536.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.179 | tokens per gpu per second (tgs): 1995.441 | TFLOPs: 16.06 |
g0220: [2024-08-10 02:22:52,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=17320, skipped=22, lr=[0.00019998739470929462, 0.00019998739470929462], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17320 loss: 0.8394 iter time (s): 4.006 samples/sec: 31.950
g0238:  iteration    17320/10000000 | consumed samples:      2216960 | consumed tokens:   4540334080 | elapsed time per iteration (ms): 4039.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.514858E-01 | loss scale: 65536.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.689 | tokens per gpu per second (tgs): 2028.067 | TFLOPs: 16.32 |
g0220: [2024-08-10 02:23:32,763] [INFO] [logging.py:96:log_dist] [Rank 0] step=17330, skipped=22, lr=[0.00019998735176142757, 0.00019998735176142757], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17330 loss: 0.8437 iter time (s): 3.976 samples/sec: 32.191
g0238:  iteration    17330/10000000 | consumed samples:      2218240 | consumed tokens:   4542955520 | elapsed time per iteration (ms): 4008.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.528360E-01 | loss scale: 65536.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.929 | tokens per gpu per second (tgs): 2043.469 | TFLOPs: 16.44 |
g0220: [2024-08-10 02:24:13,995] [INFO] [logging.py:96:log_dist] [Rank 0] step=17340, skipped=22, lr=[0.00019998730874052525, 0.00019998730874052525], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17340 loss: 0.8678 iter time (s): 4.090 samples/sec: 31.292
g0238:  iteration    17340/10000000 | consumed samples:      2219520 | consumed tokens:   4545576960 | elapsed time per iteration (ms): 4123.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.546681E-01 | loss scale: 65536.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.044 | tokens per gpu per second (tgs): 1986.801 | TFLOPs: 15.99 |
g0220: [2024-08-10 02:24:54,065] [INFO] [logging.py:96:log_dist] [Rank 0] step=17350, skipped=22, lr=[0.00019998726564658776, 0.00019998726564658776], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17350 loss: 0.8808 iter time (s): 3.975 samples/sec: 32.204
g0238:  iteration    17350/10000000 | consumed samples:      2220800 | consumed tokens:   4548198400 | elapsed time per iteration (ms): 4006.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.618575E-01 | loss scale: 65536.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.945 | tokens per gpu per second (tgs): 2044.448 | TFLOPs: 16.45 |
g0220: [2024-08-10 02:25:36,103] [INFO] [logging.py:96:log_dist] [Rank 0] step=17360, skipped=22, lr=[0.00019998722247961507, 0.00019998722247961507], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17360 loss: 0.8877 iter time (s): 4.171 samples/sec: 30.686
g0238:  iteration    17360/10000000 | consumed samples:      2222080 | consumed tokens:   4550819840 | elapsed time per iteration (ms): 4203.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.544909E-01 | loss scale: 65536.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.449 | tokens per gpu per second (tgs): 1948.737 | TFLOPs: 15.68 |
g0220: [2024-08-10 02:26:17,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=17370, skipped=22, lr=[0.00019998717923960725, 0.00019998717923960725], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17370 loss: 0.8632 iter time (s): 4.140 samples/sec: 30.917
g0238:  iteration    17370/10000000 | consumed samples:      2223360 | consumed tokens:   4553441280 | elapsed time per iteration (ms): 4173.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.572956E-01 | loss scale: 65536.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.673 | tokens per gpu per second (tgs): 1963.083 | TFLOPs: 15.80 |
g0220: [2024-08-10 02:26:57,796] [INFO] [logging.py:96:log_dist] [Rank 0] step=17380, skipped=22, lr=[0.0001999871359265643, 0.0001999871359265643], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17380 loss: 0.8664 iter time (s): 3.963 samples/sec: 32.297
g0238:  iteration    17380/10000000 | consumed samples:      2224640 | consumed tokens:   4556062720 | elapsed time per iteration (ms): 3996.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.594173E-01 | loss scale: 65536.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.030 | tokens per gpu per second (tgs): 2049.930 | TFLOPs: 16.50 |
g0225: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0238: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0238: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0238: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 02:27:09,893] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 02:27:09,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0238: [2024-08-10 02:27:09,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 02:27:09,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 02:27:09,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 02:27:09,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 02:27:09,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 02:27:09,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 02:27:09,894] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 02:27:38,493] [INFO] [logging.py:96:log_dist] [Rank 0] step=17390, skipped=22, lr=[0.00019998709254048628, 0.00019998709254048628], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17390 loss: 0.8707 iter time (s): 4.037 samples/sec: 31.707
g0238:  iteration    17390/10000000 | consumed samples:      2225920 | consumed tokens:   4558684160 | elapsed time per iteration (ms): 4069.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.582975E-01 | loss scale: 131072.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.452 | tokens per gpu per second (tgs): 2012.926 | TFLOPs: 16.20 |
g0220: [2024-08-10 02:28:21,523] [INFO] [logging.py:96:log_dist] [Rank 0] step=17400, skipped=22, lr=[0.00019998704908137324, 0.00019998704908137324], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17400 loss: 0.8475 iter time (s): 4.271 samples/sec: 29.973
g0238:  iteration    17400/10000000 | consumed samples:      2227200 | consumed tokens:   4561305600 | elapsed time per iteration (ms): 4303.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.632460E-01 | loss scale: 131072.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.746 | tokens per gpu per second (tgs): 1903.766 | TFLOPs: 15.32 |
g0220: [2024-08-10 02:29:03,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=17410, skipped=22, lr=[0.00019998700554922516, 0.00019998700554922516], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17410 loss: 0.8049 iter time (s): 4.159 samples/sec: 30.773
g0238:  iteration    17410/10000000 | consumed samples:      2228480 | consumed tokens:   4563927040 | elapsed time per iteration (ms): 4192.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.549772E-01 | loss scale: 131072.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.528 | tokens per gpu per second (tgs): 1953.790 | TFLOPs: 15.72 |
g0220: [2024-08-10 02:29:45,189] [INFO] [logging.py:96:log_dist] [Rank 0] step=17420, skipped=22, lr=[0.00019998696194404214, 0.00019998696194404214], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17420 loss: 0.8867 iter time (s): 4.141 samples/sec: 30.911
g0238:  iteration    17420/10000000 | consumed samples:      2229760 | consumed tokens:   4566548480 | elapsed time per iteration (ms): 4173.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.573566E-01 | loss scale: 131072.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.669 | tokens per gpu per second (tgs): 1962.831 | TFLOPs: 15.80 |
g0220: [2024-08-10 02:30:26,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=17430, skipped=22, lr=[0.00019998691826582417, 0.00019998691826582417], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17430 loss: 0.8520 iter time (s): 4.088 samples/sec: 31.313
g0238:  iteration    17430/10000000 | consumed samples:      2231040 | consumed tokens:   4569169920 | elapsed time per iteration (ms): 4120.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.617172E-01 | loss scale: 131072.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.066 | tokens per gpu per second (tgs): 1988.250 | TFLOPs: 16.00 |
g0220: [2024-08-10 02:31:06,253] [INFO] [logging.py:96:log_dist] [Rank 0] step=17440, skipped=22, lr=[0.00019998687451457125, 0.00019998687451457125], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17440 loss: 0.8363 iter time (s): 3.954 samples/sec: 32.375
g0238:  iteration    17440/10000000 | consumed samples:      2232320 | consumed tokens:   4571791360 | elapsed time per iteration (ms): 3986.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.534304E-01 | loss scale: 131072.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.111 | tokens per gpu per second (tgs): 2055.093 | TFLOPs: 16.54 |
g0220: [2024-08-10 02:31:47,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=17450, skipped=22, lr=[0.00019998683069028352, 0.00019998683069028352], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17450 loss: 0.8654 iter time (s): 4.095 samples/sec: 31.259
g0238:  iteration    17450/10000000 | consumed samples:      2233600 | consumed tokens:   4574412800 | elapsed time per iteration (ms): 4127.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.581206E-01 | loss scale: 131072.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.009 | tokens per gpu per second (tgs): 1984.546 | TFLOPs: 15.97 |
g0220: [2024-08-10 02:32:29,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=17460, skipped=22, lr=[0.0001999867867929609, 0.0001999867867929609], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17460 loss: 0.8474 iter time (s): 4.183 samples/sec: 30.597
g0238:  iteration    17460/10000000 | consumed samples:      2234880 | consumed tokens:   4577034240 | elapsed time per iteration (ms): 4216.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.418205E-01 | loss scale: 131072.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.355 | tokens per gpu per second (tgs): 1942.702 | TFLOPs: 15.63 |
g0220: [2024-08-10 02:33:12,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=17470, skipped=22, lr=[0.0001999867428226035, 0.0001999867428226035], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17470 loss: 0.8790 iter time (s): 4.293 samples/sec: 29.818
g0238:  iteration    17470/10000000 | consumed samples:      2236160 | consumed tokens:   4579655680 | elapsed time per iteration (ms): 4325.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.528616E-01 | loss scale: 131072.0 | grad norm: 0.236 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.592 | tokens per gpu per second (tgs): 1893.857 | TFLOPs: 15.24 |
g0220: [2024-08-10 02:33:55,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=17480, skipped=22, lr=[0.0001999866987792113, 0.0001999866987792113], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17480 loss: 0.8585 iter time (s): 4.210 samples/sec: 30.401
g0238:  iteration    17480/10000000 | consumed samples:      2237440 | consumed tokens:   4582277120 | elapsed time per iteration (ms): 4257.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.448543E-01 | loss scale: 131072.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.064 | tokens per gpu per second (tgs): 1924.103 | TFLOPs: 15.48 |
g0220: [2024-08-10 02:34:33,858] [INFO] [logging.py:96:log_dist] [Rank 0] step=17490, skipped=22, lr=[0.00019998665466278438, 0.00019998665466278438], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17490 loss: 0.8480 iter time (s): 3.800 samples/sec: 33.686
g0238:  iteration    17490/10000000 | consumed samples:      2238720 | consumed tokens:   4584898560 | elapsed time per iteration (ms): 3832.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.557839E-01 | loss scale: 131072.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 33.396 | tokens per gpu per second (tgs): 2137.357 | TFLOPs: 17.20 |
g0220: [2024-08-10 02:35:14,422] [INFO] [logging.py:96:log_dist] [Rank 0] step=17500, skipped=22, lr=[0.00019998661047332275, 0.00019998661047332275], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17500 loss: 0.8603 iter time (s): 4.023 samples/sec: 31.814
g0238:  iteration    17500/10000000 | consumed samples:      2240000 | consumed tokens:   4587520000 | elapsed time per iteration (ms): 4056.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.563803E-01 | loss scale: 131072.0 | grad norm: 0.268 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.555 | tokens per gpu per second (tgs): 2019.507 | TFLOPs: 16.25 |
g0220: [2024-08-10 02:35:55,505] [INFO] [logging.py:96:log_dist] [Rank 0] step=17510, skipped=22, lr=[0.00019998656621082644, 0.00019998656621082644], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17510 loss: 0.8654 iter time (s): 4.075 samples/sec: 31.411
g0238:  iteration    17510/10000000 | consumed samples:      2241280 | consumed tokens:   4590141440 | elapsed time per iteration (ms): 4108.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.497293E-01 | loss scale: 131072.0 | grad norm: 0.243 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.155 | tokens per gpu per second (tgs): 1993.937 | TFLOPs: 16.05 |
g0220: [2024-08-10 02:36:37,918] [INFO] [logging.py:96:log_dist] [Rank 0] step=17520, skipped=22, lr=[0.00019998652187529553, 0.00019998652187529553], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17520 loss: 0.8376 iter time (s): 4.208 samples/sec: 30.422
g0238:  iteration    17520/10000000 | consumed samples:      2242560 | consumed tokens:   4592762880 | elapsed time per iteration (ms): 4241.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.531290E-01 | loss scale: 131072.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.181 | tokens per gpu per second (tgs): 1931.608 | TFLOPs: 15.54 |
g0220: [2024-08-10 02:37:19,184] [INFO] [logging.py:96:log_dist] [Rank 0] step=17530, skipped=22, lr=[0.00019998647746672998, 0.00019998647746672998], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17530 loss: 0.8650 iter time (s): 4.093 samples/sec: 31.273
g0238:  iteration    17530/10000000 | consumed samples:      2243840 | consumed tokens:   4595384320 | elapsed time per iteration (ms): 4126.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.519716E-01 | loss scale: 131072.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.020 | tokens per gpu per second (tgs): 1985.250 | TFLOPs: 15.98 |
g0220: [2024-08-10 02:38:00,757] [INFO] [logging.py:96:log_dist] [Rank 0] step=17540, skipped=22, lr=[0.00019998643298512988, 0.00019998643298512988], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17540 loss: 0.8691 iter time (s): 4.123 samples/sec: 31.043
g0238:  iteration    17540/10000000 | consumed samples:      2245120 | consumed tokens:   4598005760 | elapsed time per iteration (ms): 4157.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.621755E-01 | loss scale: 131072.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.789 | tokens per gpu per second (tgs): 1970.474 | TFLOPs: 15.86 |
g0220: [2024-08-10 02:38:41,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=17550, skipped=22, lr=[0.0001999863884304952, 0.0001999863884304952], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17550 loss: 0.8514 iter time (s): 4.043 samples/sec: 31.657
g0238:  iteration    17550/10000000 | consumed samples:      2246400 | consumed tokens:   4600627200 | elapsed time per iteration (ms): 4077.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.539343E-01 | loss scale: 131072.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.392 | tokens per gpu per second (tgs): 2009.082 | TFLOPs: 16.17 |
g0220: [2024-08-10 02:39:22,389] [INFO] [logging.py:96:log_dist] [Rank 0] step=17560, skipped=22, lr=[0.00019998634380282606, 0.00019998634380282606], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17560 loss: 0.8629 iter time (s): 4.053 samples/sec: 31.583
g0238:  iteration    17560/10000000 | consumed samples:      2247680 | consumed tokens:   4603248640 | elapsed time per iteration (ms): 4085.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.688521E-01 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.328 | tokens per gpu per second (tgs): 2005.014 | TFLOPs: 16.13 |
g0220: [2024-08-10 02:40:02,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=17570, skipped=22, lr=[0.00019998629910212244, 0.00019998629910212244], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17570 loss: 0.8912 iter time (s): 3.946 samples/sec: 32.439
g0238:  iteration    17570/10000000 | consumed samples:      2248960 | consumed tokens:   4605870080 | elapsed time per iteration (ms): 3978.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.670326E-01 | loss scale: 131072.0 | grad norm: 0.244 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.170 | tokens per gpu per second (tgs): 2058.893 | TFLOPs: 16.57 |
g0220: [2024-08-10 02:40:43,951] [INFO] [logging.py:96:log_dist] [Rank 0] step=17580, skipped=22, lr=[0.0001999862543283844, 0.0001999862543283844], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17580 loss: 0.8515 iter time (s): 4.145 samples/sec: 30.883
g0238:  iteration    17580/10000000 | consumed samples:      2250240 | consumed tokens:   4608491520 | elapsed time per iteration (ms): 4177.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.631028E-01 | loss scale: 131072.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.642 | tokens per gpu per second (tgs): 1961.060 | TFLOPs: 15.78 |
g0220: [2024-08-10 02:41:25,409] [INFO] [logging.py:96:log_dist] [Rank 0] step=17590, skipped=22, lr=[0.00019998620948161197, 0.00019998620948161197], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17590 loss: 0.8604 iter time (s): 4.113 samples/sec: 31.120
g0238:  iteration    17590/10000000 | consumed samples:      2251520 | consumed tokens:   4611112960 | elapsed time per iteration (ms): 4145.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.508141E-01 | loss scale: 131072.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.875 | tokens per gpu per second (tgs): 1976.020 | TFLOPs: 15.90 |
g0220: [2024-08-10 02:42:09,195] [INFO] [logging.py:96:log_dist] [Rank 0] step=17600, skipped=22, lr=[0.00019998616456180516, 0.00019998616456180516], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17600 loss: 0.8396 iter time (s): 4.346 samples/sec: 29.452
g0238:  iteration    17600/10000000 | consumed samples:      2252800 | consumed tokens:   4613734400 | elapsed time per iteration (ms): 4378.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.532802E-01 | loss scale: 131072.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.233 | tokens per gpu per second (tgs): 1870.905 | TFLOPs: 15.06 |
g0220: [2024-08-10 02:42:49,166] [INFO] [logging.py:96:log_dist] [Rank 0] step=17610, skipped=22, lr=[0.000199986119568964, 0.000199986119568964], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17610 loss: 0.8723 iter time (s): 3.965 samples/sec: 32.286
g0238:  iteration    17610/10000000 | consumed samples:      2254080 | consumed tokens:   4616355840 | elapsed time per iteration (ms): 3997.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.550155E-01 | loss scale: 131072.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.023 | tokens per gpu per second (tgs): 2049.500 | TFLOPs: 16.49 |
g0220: [2024-08-10 02:43:29,839] [INFO] [logging.py:96:log_dist] [Rank 0] step=17620, skipped=22, lr=[0.0001999860745030886, 0.0001999860745030886], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17620 loss: 0.8517 iter time (s): 4.035 samples/sec: 31.726
g0238:  iteration    17620/10000000 | consumed samples:      2255360 | consumed tokens:   4618977280 | elapsed time per iteration (ms): 4067.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.595407E-01 | loss scale: 131072.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.471 | tokens per gpu per second (tgs): 2014.151 | TFLOPs: 16.21 |
g0220: [2024-08-10 02:44:11,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=17630, skipped=22, lr=[0.0001999860293641789, 0.0001999860293641789], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17630 loss: 0.8627 iter time (s): 4.117 samples/sec: 31.088
g0238:  iteration    17630/10000000 | consumed samples:      2256640 | consumed tokens:   4621598720 | elapsed time per iteration (ms): 4150.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.597342E-01 | loss scale: 131072.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.842 | tokens per gpu per second (tgs): 1973.913 | TFLOPs: 15.88 |
g0220: [2024-08-10 02:44:52,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=17640, skipped=22, lr=[0.00019998598415223498, 0.00019998598415223498], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17640 loss: 0.8482 iter time (s): 4.081 samples/sec: 31.363
g0238:  iteration    17640/10000000 | consumed samples:      2257920 | consumed tokens:   4624220160 | elapsed time per iteration (ms): 4114.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.524211E-01 | loss scale: 131072.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.112 | tokens per gpu per second (tgs): 1991.191 | TFLOPs: 16.02 |
g0220: [2024-08-10 02:45:33,551] [INFO] [logging.py:96:log_dist] [Rank 0] step=17650, skipped=22, lr=[0.00019998593886725688, 0.00019998593886725688], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17650 loss: 0.8454 iter time (s): 4.074 samples/sec: 31.418
g0238:  iteration    17650/10000000 | consumed samples:      2259200 | consumed tokens:   4626841600 | elapsed time per iteration (ms): 4106.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.579741E-01 | loss scale: 131072.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.167 | tokens per gpu per second (tgs): 1994.668 | TFLOPs: 16.05 |
g0220: [2024-08-10 02:46:16,138] [INFO] [logging.py:96:log_dist] [Rank 0] step=17660, skipped=22, lr=[0.0001999858935092446, 0.0001999858935092446], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17660 loss: 0.8651 iter time (s): 4.226 samples/sec: 30.288
g0238:  iteration    17660/10000000 | consumed samples:      2260480 | consumed tokens:   4629463040 | elapsed time per iteration (ms): 4258.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.533040E-01 | loss scale: 131072.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.056 | tokens per gpu per second (tgs): 1923.575 | TFLOPs: 15.48 |
g0220: [2024-08-10 02:46:58,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=17670, skipped=22, lr=[0.00019998584807819823, 0.00019998584807819823], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17670 loss: 0.8287 iter time (s): 4.223 samples/sec: 30.308
g0238:  iteration    17670/10000000 | consumed samples:      2261760 | consumed tokens:   4632084480 | elapsed time per iteration (ms): 4255.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.483253E-01 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.076 | tokens per gpu per second (tgs): 1924.842 | TFLOPs: 15.49 |
g0220: [2024-08-10 02:47:41,927] [INFO] [logging.py:96:log_dist] [Rank 0] step=17680, skipped=22, lr=[0.00019998580257411778, 0.00019998580257411778], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17680 loss: 0.8495 iter time (s): 4.289 samples/sec: 29.843
g0238:  iteration    17680/10000000 | consumed samples:      2263040 | consumed tokens:   4634705920 | elapsed time per iteration (ms): 4322.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.591704E-01 | loss scale: 131072.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.610 | tokens per gpu per second (tgs): 1895.033 | TFLOPs: 15.25 |
g0220: [2024-08-10 02:48:23,056] [INFO] [logging.py:96:log_dist] [Rank 0] step=17690, skipped=22, lr=[0.00019998575699700324, 0.00019998575699700324], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17690 loss: 0.8606 iter time (s): 4.080 samples/sec: 31.376
g0238:  iteration    17690/10000000 | consumed samples:      2264320 | consumed tokens:   4637327360 | elapsed time per iteration (ms): 4112.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.533069E-01 | loss scale: 131072.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.122 | tokens per gpu per second (tgs): 1991.783 | TFLOPs: 16.03 |
g0220: [2024-08-10 02:49:06,454] [INFO] [logging.py:96:log_dist] [Rank 0] step=17700, skipped=22, lr=[0.00019998571134685473, 0.00019998571134685473], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17700 loss: 0.8464 iter time (s): 4.307 samples/sec: 29.717
g0238:  iteration    17700/10000000 | consumed samples:      2265600 | consumed tokens:   4639948800 | elapsed time per iteration (ms): 4339.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.530173E-01 | loss scale: 131072.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.495 | tokens per gpu per second (tgs): 1887.672 | TFLOPs: 15.19 |
g0220: [2024-08-10 02:49:47,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=17710, skipped=22, lr=[0.0001999856656236722, 0.0001999856656236722], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17710 loss: 0.8556 iter time (s): 4.064 samples/sec: 31.494
g0238:  iteration    17710/10000000 | consumed samples:      2266880 | consumed tokens:   4642570240 | elapsed time per iteration (ms): 4098.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.647162E-01 | loss scale: 131072.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.234 | tokens per gpu per second (tgs): 1998.976 | TFLOPs: 16.09 |
g0220: [2024-08-10 02:50:29,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=17720, skipped=22, lr=[0.00019998561982745576, 0.00019998561982745576], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17720 loss: 0.8344 iter time (s): 4.161 samples/sec: 30.762
g0238:  iteration    17720/10000000 | consumed samples:      2268160 | consumed tokens:   4645191680 | elapsed time per iteration (ms): 4193.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.489710E-01 | loss scale: 131072.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.522 | tokens per gpu per second (tgs): 1953.431 | TFLOPs: 15.72 |
g0220: [2024-08-10 02:51:09,247] [INFO] [logging.py:96:log_dist] [Rank 0] step=17730, skipped=22, lr=[0.0001999855739582054, 0.0001999855739582054], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17730 loss: 0.8459 iter time (s): 3.954 samples/sec: 32.371
g0238:  iteration    17730/10000000 | consumed samples:      2269440 | consumed tokens:   4647813120 | elapsed time per iteration (ms): 3987.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.632351E-01 | loss scale: 131072.0 | grad norm: 0.246 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.100 | tokens per gpu per second (tgs): 2054.413 | TFLOPs: 16.53 |
g0220: [2024-08-10 02:51:49,880] [INFO] [logging.py:96:log_dist] [Rank 0] step=17740, skipped=22, lr=[0.00019998552801592115, 0.00019998552801592115], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17740 loss: 0.8685 iter time (s): 4.031 samples/sec: 31.758
g0238:  iteration    17740/10000000 | consumed samples:      2270720 | consumed tokens:   4650434560 | elapsed time per iteration (ms): 4063.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.578099E-01 | loss scale: 131072.0 | grad norm: 0.261 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.501 | tokens per gpu per second (tgs): 2016.075 | TFLOPs: 16.22 |
g0220: [2024-08-10 02:52:31,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=17750, skipped=22, lr=[0.00019998548200060305, 0.00019998548200060305], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17750 loss: 0.8534 iter time (s): 4.169 samples/sec: 30.704
g0238:  iteration    17750/10000000 | consumed samples:      2272000 | consumed tokens:   4653056000 | elapsed time per iteration (ms): 4202.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.511985E-01 | loss scale: 131072.0 | grad norm: 0.248 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.462 | tokens per gpu per second (tgs): 1949.541 | TFLOPs: 15.69 |
g0220: [2024-08-10 02:53:12,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=17760, skipped=22, lr=[0.0001999854359122512, 0.0001999854359122512], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17760 loss: 0.8873 iter time (s): 4.076 samples/sec: 31.404
g0238:  iteration    17760/10000000 | consumed samples:      2273280 | consumed tokens:   4655677440 | elapsed time per iteration (ms): 4109.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.549962E-01 | loss scale: 131072.0 | grad norm: 0.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.146 | tokens per gpu per second (tgs): 1993.316 | TFLOPs: 16.04 |
g0220: [2024-08-10 02:53:54,612] [INFO] [logging.py:96:log_dist] [Rank 0] step=17770, skipped=22, lr=[0.00019998538975086552, 0.00019998538975086552], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17770 loss: 0.8238 iter time (s): 4.128 samples/sec: 31.007
g0238:  iteration    17770/10000000 | consumed samples:      2274560 | consumed tokens:   4658298880 | elapsed time per iteration (ms): 4161.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.554412E-01 | loss scale: 131072.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.758 | tokens per gpu per second (tgs): 1968.487 | TFLOPs: 15.84 |
g0220: [2024-08-10 02:54:34,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=17780, skipped=22, lr=[0.00019998534351644613, 0.00019998534351644613], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17780 loss: 0.8290 iter time (s): 3.955 samples/sec: 32.363
g0238:  iteration    17780/10000000 | consumed samples:      2275840 | consumed tokens:   4660920320 | elapsed time per iteration (ms): 3987.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.434872E-01 | loss scale: 131072.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.099 | tokens per gpu per second (tgs): 2054.342 | TFLOPs: 16.53 |
g0220: [2024-08-10 02:55:14,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=17790, skipped=22, lr=[0.00019998529720899306, 0.00019998529720899306], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17790 loss: 0.8458 iter time (s): 3.956 samples/sec: 32.355
g0238:  iteration    17790/10000000 | consumed samples:      2277120 | consumed tokens:   4663541760 | elapsed time per iteration (ms): 3988.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.464770E-01 | loss scale: 131072.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.089 | tokens per gpu per second (tgs): 2053.678 | TFLOPs: 16.53 |
g0220: [2024-08-10 02:55:55,862] [INFO] [logging.py:96:log_dist] [Rank 0] step=17800, skipped=22, lr=[0.00019998525082850628, 0.00019998525082850628], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17800 loss: 0.8427 iter time (s): 4.115 samples/sec: 31.106
g0238:  iteration    17800/10000000 | consumed samples:      2278400 | consumed tokens:   4666163200 | elapsed time per iteration (ms): 4148.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.581787E-01 | loss scale: 131072.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.856 | tokens per gpu per second (tgs): 1974.803 | TFLOPs: 15.89 |
g0220: [2024-08-10 02:56:38,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=17810, skipped=22, lr=[0.00019998520437498592, 0.00019998520437498592], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17810 loss: 0.8837 iter time (s): 4.255 samples/sec: 30.082
g0238:  iteration    17810/10000000 | consumed samples:      2279680 | consumed tokens:   4668784640 | elapsed time per iteration (ms): 4289.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.527081E-01 | loss scale: 131072.0 | grad norm: 0.220 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.843 | tokens per gpu per second (tgs): 1909.932 | TFLOPs: 15.37 |
g0220: [2024-08-10 02:57:19,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=17820, skipped=22, lr=[0.000199985157848432, 0.000199985157848432], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17820 loss: 0.8722 iter time (s): 4.088 samples/sec: 31.313
g0238:  iteration    17820/10000000 | consumed samples:      2280960 | consumed tokens:   4671406080 | elapsed time per iteration (ms): 4120.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.485751E-01 | loss scale: 131072.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.061 | tokens per gpu per second (tgs): 1987.904 | TFLOPs: 16.00 |
g0220: [2024-08-10 02:58:01,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=17830, skipped=22, lr=[0.00019998511124884444, 0.00019998511124884444], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17830 loss: 0.8397 iter time (s): 4.135 samples/sec: 30.955
g0238:  iteration    17830/10000000 | consumed samples:      2282240 | consumed tokens:   4674027520 | elapsed time per iteration (ms): 4168.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.600493E-01 | loss scale: 131072.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.706 | tokens per gpu per second (tgs): 1965.209 | TFLOPs: 15.81 |
g0220: [2024-08-10 02:58:42,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=17840, skipped=22, lr=[0.0001999850645762234, 0.0001999850645762234], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17840 loss: 0.8582 iter time (s): 4.006 samples/sec: 31.949
g0238:  iteration    17840/10000000 | consumed samples:      2283520 | consumed tokens:   4676648960 | elapsed time per iteration (ms): 4039.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.485751E-01 | loss scale: 131072.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.686 | tokens per gpu per second (tgs): 2027.889 | TFLOPs: 16.32 |
g0220: [2024-08-10 02:59:21,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=17850, skipped=22, lr=[0.0001999850178305689, 0.0001999850178305689], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17850 loss: 0.8116 iter time (s): 3.926 samples/sec: 32.600
g0238:  iteration    17850/10000000 | consumed samples:      2284800 | consumed tokens:   4679270400 | elapsed time per iteration (ms): 3959.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.487628E-01 | loss scale: 131072.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.326 | tokens per gpu per second (tgs): 2068.881 | TFLOPs: 16.65 |
g0220: [2024-08-10 03:00:02,435] [INFO] [logging.py:96:log_dist] [Rank 0] step=17860, skipped=22, lr=[0.00019998497101188093, 0.00019998497101188093], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17860 loss: 0.8349 iter time (s): 4.047 samples/sec: 31.631
g0238:  iteration    17860/10000000 | consumed samples:      2286080 | consumed tokens:   4681891840 | elapsed time per iteration (ms): 4079.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.463468E-01 | loss scale: 131072.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.379 | tokens per gpu per second (tgs): 2008.242 | TFLOPs: 16.16 |
g0220: [2024-08-10 03:00:41,403] [INFO] [logging.py:96:log_dist] [Rank 0] step=17870, skipped=22, lr=[0.00019998492412015955, 0.00019998492412015955], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17870 loss: 0.8401 iter time (s): 3.864 samples/sec: 33.124
g0238:  iteration    17870/10000000 | consumed samples:      2287360 | consumed tokens:   4684513280 | elapsed time per iteration (ms): 3896.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.537307E-01 | loss scale: 131072.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.847 | tokens per gpu per second (tgs): 2102.206 | TFLOPs: 16.92 |
g0220: [2024-08-10 03:01:21,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=17880, skipped=22, lr=[0.0001999848771554048, 0.0001999848771554048], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17880 loss: 0.8583 iter time (s): 3.980 samples/sec: 32.161
g0238:  iteration    17880/10000000 | consumed samples:      2288640 | consumed tokens:   4687134720 | elapsed time per iteration (ms): 4012.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.422911E-01 | loss scale: 131072.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.901 | tokens per gpu per second (tgs): 2041.678 | TFLOPs: 16.43 |
g0220: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-10 03:01:34,147] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 03:01:34,147] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 03:01:34,147] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-10 03:01:34,146] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 03:01:34,147] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-10 03:01:34,147] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 03:02:02,909] [INFO] [logging.py:96:log_dist] [Rank 0] step=17890, skipped=22, lr=[0.00019998483011761668, 0.00019998483011761668], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17890 loss: 0.8489 iter time (s): 4.105 samples/sec: 31.179
g0238:  iteration    17890/10000000 | consumed samples:      2289920 | consumed tokens:   4689756160 | elapsed time per iteration (ms): 4138.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.464932E-01 | loss scale: 262144.0 | grad norm: 0.409 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.927 | tokens per gpu per second (tgs): 1979.340 | TFLOPs: 15.93 |
g0220: [2024-08-10 03:02:43,100] [INFO] [logging.py:96:log_dist] [Rank 0] step=17900, skipped=22, lr=[0.0001999847830067953, 0.0001999847830067953], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17900 loss: 0.8549 iter time (s): 3.986 samples/sec: 32.115
g0238:  iteration    17900/10000000 | consumed samples:      2291200 | consumed tokens:   4692377600 | elapsed time per iteration (ms): 4018.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.483272E-01 | loss scale: 262144.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.853 | tokens per gpu per second (tgs): 2038.608 | TFLOPs: 16.41 |
g0220: [2024-08-10 03:03:23,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=17910, skipped=22, lr=[0.00019998473582294064, 0.00019998473582294064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17910 loss: 0.8752 iter time (s): 3.991 samples/sec: 32.071
g0238:  iteration    17910/10000000 | consumed samples:      2292480 | consumed tokens:   4694999040 | elapsed time per iteration (ms): 4024.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.606874E-01 | loss scale: 262144.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.809 | tokens per gpu per second (tgs): 2035.773 | TFLOPs: 16.38 |
g0220: [2024-08-10 03:04:04,674] [INFO] [logging.py:96:log_dist] [Rank 0] step=17920, skipped=22, lr=[0.00019998468856605276, 0.00019998468856605276], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17920 loss: 0.8469 iter time (s): 4.099 samples/sec: 31.225
g0238:  iteration    17920/10000000 | consumed samples:      2293760 | consumed tokens:   4697620480 | elapsed time per iteration (ms): 4135.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.360028E-01 | loss scale: 262144.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.953 | tokens per gpu per second (tgs): 1980.966 | TFLOPs: 15.94 |
g0220: [2024-08-10 03:04:44,740] [INFO] [logging.py:96:log_dist] [Rank 0] step=17930, skipped=22, lr=[0.00019998464123613166, 0.00019998464123613166], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17930 loss: 0.8141 iter time (s): 3.972 samples/sec: 32.225
g0238:  iteration    17930/10000000 | consumed samples:      2295040 | consumed tokens:   4700241920 | elapsed time per iteration (ms): 4004.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.517040E-01 | loss scale: 262144.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.962 | tokens per gpu per second (tgs): 2045.538 | TFLOPs: 16.46 |
g0220: [2024-08-10 03:05:26,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=17940, skipped=22, lr=[0.00019998459383317743, 0.00019998459383317743], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17940 loss: 0.8074 iter time (s): 4.144 samples/sec: 30.888
g0238:  iteration    17940/10000000 | consumed samples:      2296320 | consumed tokens:   4702863360 | elapsed time per iteration (ms): 4177.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.381918E-01 | loss scale: 262144.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.643 | tokens per gpu per second (tgs): 1961.138 | TFLOPs: 15.78 |
g0220: [2024-08-10 03:06:07,045] [INFO] [logging.py:96:log_dist] [Rank 0] step=17950, skipped=22, lr=[0.00019998454635719007, 0.00019998454635719007], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17950 loss: 0.8237 iter time (s): 4.018 samples/sec: 31.855
g0238:  iteration    17950/10000000 | consumed samples:      2297600 | consumed tokens:   4705484800 | elapsed time per iteration (ms): 4053.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.435478E-01 | loss scale: 262144.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.580 | tokens per gpu per second (tgs): 2021.142 | TFLOPs: 16.26 |
g0220: [2024-08-10 03:06:49,491] [INFO] [logging.py:96:log_dist] [Rank 0] step=17960, skipped=22, lr=[0.00019998449880816963, 0.00019998449880816963], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17960 loss: 0.8592 iter time (s): 4.211 samples/sec: 30.394
g0238:  iteration    17960/10000000 | consumed samples:      2298880 | consumed tokens:   4708106240 | elapsed time per iteration (ms): 4244.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.612653E-01 | loss scale: 262144.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.157 | tokens per gpu per second (tgs): 1930.038 | TFLOPs: 15.53 |
g0220: [2024-08-10 03:07:31,537] [INFO] [logging.py:96:log_dist] [Rank 0] step=17970, skipped=22, lr=[0.00019998445118611614, 0.00019998445118611614], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17970 loss: 0.8546 iter time (s): 4.171 samples/sec: 30.685
g0238:  iteration    17970/10000000 | consumed samples:      2300160 | consumed tokens:   4710727680 | elapsed time per iteration (ms): 4204.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.495203E-01 | loss scale: 262144.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.444 | tokens per gpu per second (tgs): 1948.389 | TFLOPs: 15.68 |
g0220: [2024-08-10 03:08:12,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=17980, skipped=22, lr=[0.00019998440349102962, 0.00019998440349102962], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17980 loss: 0.8450 iter time (s): 4.085 samples/sec: 31.333
g0238:  iteration    17980/10000000 | consumed samples:      2301440 | consumed tokens:   4713349120 | elapsed time per iteration (ms): 4122.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.502550E-01 | loss scale: 262144.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.050 | tokens per gpu per second (tgs): 1987.209 | TFLOPs: 15.99 |
g0220: [2024-08-10 03:08:52,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=17990, skipped=22, lr=[0.00019998435572291011, 0.00019998435572291011], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 17990 loss: 0.8575 iter time (s): 3.932 samples/sec: 32.557
g0238:  iteration    17990/10000000 | consumed samples:      2302720 | consumed tokens:   4715970560 | elapsed time per iteration (ms): 3968.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.347498E-01 | loss scale: 262144.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.258 | tokens per gpu per second (tgs): 2064.498 | TFLOPs: 16.61 |
g0220: [2024-08-10 03:09:34,341] [INFO] [logging.py:96:log_dist] [Rank 0] step=18000, skipped=22, lr=[0.0001999843078817577, 0.0001999843078817577], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18000 loss: 0.8582 iter time (s): 4.153 samples/sec: 30.823
g0238:  iteration    18000/10000000 | consumed samples:      2304000 | consumed tokens:   4718592000 | elapsed time per iteration (ms): 4189.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.526264E-01 | loss scale: 262144.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.550 | tokens per gpu per second (tgs): 1955.172 | TFLOPs: 15.73 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 18000 | lm loss value: 8.505194E-01 | lm loss PPL: 2.340862E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   18000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-10 03:16:03,317] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step18000 is about to be saved!
g0238: [2024-08-10 03:16:03,324] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0238: [2024-08-10 03:16:03,324] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0238: [2024-08-10 03:16:03,324] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0234: [2024-08-10 03:16:03,326] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0234: [2024-08-10 03:16:03,326] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0234: [2024-08-10 03:16:03,326] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0225: [2024-08-10 03:16:03,327] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0225: [2024-08-10 03:16:03,327] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0220: [2024-08-10 03:16:03,327] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0220: [2024-08-10 03:16:03,327] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0225: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0236: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0237: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0237: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0220: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0236: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0236: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0237: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0233: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0233: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0235: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0235: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0235: [2024-08-10 03:16:03,329] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0233: [2024-08-10 03:16:03,328] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0238: [2024-08-10 03:16:03,352] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_23-model_00-model_states.pt...
g0237: [2024-08-10 03:16:03,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_20-model_00-model_states.pt...
g0234: [2024-08-10 03:16:03,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_11-model_00-model_states.pt...
g0236: [2024-08-10 03:16:03,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_17-model_00-model_states.pt...
g0235: [2024-08-10 03:16:03,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_14-model_00-model_states.pt...
g0225: [2024-08-10 03:16:03,365] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_05-model_00-model_states.pt...
g0233: [2024-08-10 03:16:03,366] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_08-model_00-model_states.pt...
g0220: [2024-08-10 03:16:03,382] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_01-model_00-model_states.pt...
g0236: [2024-08-10 03:16:03,515] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_17-model_00-model_states.pt.
g0225: [2024-08-10 03:16:03,521] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_05-model_00-model_states.pt.
g0236: [2024-08-10 03:16:03,554] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_18-model_00-model_states.pt...
g0225: [2024-08-10 03:16:03,560] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_06-model_00-model_states.pt...
g0233: [2024-08-10 03:16:03,565] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_08-model_00-model_states.pt.
g0234: [2024-08-10 03:16:03,571] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_11-model_00-model_states.pt.
g0235: [2024-08-10 03:16:03,592] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_14-model_00-model_states.pt.
g0233: [2024-08-10 03:16:03,604] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_09-model_00-model_states.pt...
g0234: [2024-08-10 03:16:03,611] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_12-model_00-model_states.pt...
g0235: [2024-08-10 03:16:03,630] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_15-model_00-model_states.pt...
g0220: [2024-08-10 03:16:03,647] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_01-model_00-model_states.pt.
g0220: [2024-08-10 03:16:03,670] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_02-model_00-model_states.pt...
g0236: [2024-08-10 03:16:03,679] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_18-model_00-model_states.pt.
g0238: [2024-08-10 03:16:03,689] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_23-model_00-model_states.pt.
g0238: [2024-08-10 03:16:03,690] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_24-model_00-model_states.pt...
g0238: [2024-08-10 03:16:03,692] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_24-model_00-model_states.pt.
g0236: [2024-08-10 03:16:03,714] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_19-model_00-model_states.pt...
g0238: [2024-08-10 03:16:03,738] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_25-model_00-model_states.pt...
g0225: [2024-08-10 03:16:03,746] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_06-model_00-model_states.pt.
g0233: [2024-08-10 03:16:03,749] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_09-model_00-model_states.pt.
g0235: [2024-08-10 03:16:03,762] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_15-model_00-model_states.pt.
g0234: [2024-08-10 03:16:03,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_12-model_00-model_states.pt.
g0225: [2024-08-10 03:16:03,781] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_07-model_00-model_states.pt...
g0233: [2024-08-10 03:16:03,784] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_10-model_00-model_states.pt...
g0235: [2024-08-10 03:16:03,796] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_16-model_00-model_states.pt...
g0234: [2024-08-10 03:16:03,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_13-model_00-model_states.pt...
g0236: [2024-08-10 03:16:03,847] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_19-model_00-model_states.pt.
g0236: [2024-08-10 03:16:03,849] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_05_model_states.pt...
g0233: [2024-08-10 03:16:03,900] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_10-model_00-model_states.pt.
g0233: [2024-08-10 03:16:03,901] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_02_model_states.pt...
g0225: [2024-08-10 03:16:03,942] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_07-model_00-model_states.pt.
g0225: [2024-08-10 03:16:03,944] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_01_model_states.pt...
g0234: [2024-08-10 03:16:03,948] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_13-model_00-model_states.pt.
g0234: [2024-08-10 03:16:03,950] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_03_model_states.pt...
g0237: [2024-08-10 03:16:03,964] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_20-model_00-model_states.pt.
g0220: [2024-08-10 03:16:03,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_02-model_00-model_states.pt.
g0220: [2024-08-10 03:16:04,001] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_03-model_00-model_states.pt...
g0237: [2024-08-10 03:16:04,002] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_21-model_00-model_states.pt...
g0238: [2024-08-10 03:16:04,006] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_25-model_00-model_states.pt.
g0238: [2024-08-10 03:16:04,007] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_07_model_states.pt...
g0235: [2024-08-10 03:16:04,042] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_16-model_00-model_states.pt.
g0235: [2024-08-10 03:16:04,044] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_04_model_states.pt...
g0220: [2024-08-10 03:16:04,111] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_03-model_00-model_states.pt.
g0220: [2024-08-10 03:16:04,141] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_04-model_00-model_states.pt...
g0237: [2024-08-10 03:16:04,202] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_21-model_00-model_states.pt.
g0220: [2024-08-10 03:16:04,235] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_04-model_00-model_states.pt.
g0237: [2024-08-10 03:16:04,236] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_22-model_00-model_states.pt...
g0220: [2024-08-10 03:16:04,236] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_00_model_states.pt
g0220: [2024-08-10 03:16:04,237] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_00_model_states.pt...
g0237: [2024-08-10 03:16:04,335] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/layer_22-model_00-model_states.pt.
g0237: [2024-08-10 03:16:04,338] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_06_model_states.pt...
g0236: [2024-08-10 03:16:06,206] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_05_model_states.pt.
g0236: [2024-08-10 03:16:06,206] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0225: [2024-08-10 03:16:06,347] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_01_model_states.pt.
g0225: [2024-08-10 03:16:06,347] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0238: [2024-08-10 03:16:06,380] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_07_model_states.pt.
g0238: [2024-08-10 03:16:06,380] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0234: [2024-08-10 03:16:06,406] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_03_model_states.pt.
g0234: [2024-08-10 03:16:06,407] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0233: [2024-08-10 03:16:06,425] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_02_model_states.pt.
g0233: [2024-08-10 03:16:06,426] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0235: [2024-08-10 03:16:06,433] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_04_model_states.pt.
g0235: [2024-08-10 03:16:06,433] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0237: [2024-08-10 03:16:06,732] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_06_model_states.pt.
g0237: [2024-08-10 03:16:06,733] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0220: [2024-08-10 03:16:08,640] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step18000/mp_rank_00_model_states.pt.
g0220: [2024-08-10 03:16:08,641] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18000 is ready now!
g0220:   successfully saved checkpoint at iteration   18000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 4.19, Latency(second): 5.38
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (5379.84, 5380.29)
g0220: [2024-08-10 03:16:51,009] [INFO] [logging.py:96:log_dist] [Rank 0] step=18010, skipped=22, lr=[0.00019998425996757237, 0.00019998425996757237], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18010 loss: 0.8497 iter time (s): 4.202 samples/sec: 30.463
g0238:  iteration    18010/10000000 | consumed samples:      2305280 | consumed tokens:   4721213440 | elapsed time per iteration (ms): 43666.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.501923E-01 | loss scale: 262144.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.931 | tokens per gpu per second (tgs): 187.603 | TFLOPs: 1.51 |
g0220: [2024-08-10 03:17:32,442] [INFO] [logging.py:96:log_dist] [Rank 0] step=18020, skipped=22, lr=[0.0001999842119803542, 0.0001999842119803542], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18020 loss: 0.7886 iter time (s): 4.111 samples/sec: 31.138
g0238:  iteration    18020/10000000 | consumed samples:      2306560 | consumed tokens:   4723834880 | elapsed time per iteration (ms): 4143.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.496663E-01 | loss scale: 262144.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.894 | tokens per gpu per second (tgs): 1977.198 | TFLOPs: 15.91 |
g0220: [2024-08-10 03:18:14,267] [INFO] [logging.py:96:log_dist] [Rank 0] step=18030, skipped=22, lr=[0.0001999841639201032, 0.0001999841639201032], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18030 loss: 0.8506 iter time (s): 4.150 samples/sec: 30.843
g0238:  iteration    18030/10000000 | consumed samples:      2307840 | consumed tokens:   4726456320 | elapsed time per iteration (ms): 4182.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.459394E-01 | loss scale: 262144.0 | grad norm: 0.251 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.603 | tokens per gpu per second (tgs): 1958.622 | TFLOPs: 15.76 |
g0220: [2024-08-10 03:18:55,380] [INFO] [logging.py:96:log_dist] [Rank 0] step=18040, skipped=22, lr=[0.00019998411578681937, 0.00019998411578681937], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18040 loss: 0.8424 iter time (s): 4.079 samples/sec: 31.383
g0238:  iteration    18040/10000000 | consumed samples:      2309120 | consumed tokens:   4729077760 | elapsed time per iteration (ms): 4111.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.587643E-01 | loss scale: 262144.0 | grad norm: 0.238 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.133 | tokens per gpu per second (tgs): 1992.540 | TFLOPs: 16.03 |
g0220: [2024-08-10 03:19:36,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=18050, skipped=22, lr=[0.00019998406758050283, 0.00019998406758050283], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18050 loss: 0.8573 iter time (s): 4.073 samples/sec: 31.430
g0238:  iteration    18050/10000000 | consumed samples:      2310400 | consumed tokens:   4731699200 | elapsed time per iteration (ms): 4105.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.530717E-01 | loss scale: 262144.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.181 | tokens per gpu per second (tgs): 1995.571 | TFLOPs: 16.06 |
g0220: [2024-08-10 03:20:18,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=18060, skipped=22, lr=[0.00019998401930115356, 0.00019998401930115356], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18060 loss: 0.8460 iter time (s): 4.160 samples/sec: 30.770
g0238:  iteration    18060/10000000 | consumed samples:      2311680 | consumed tokens:   4734320640 | elapsed time per iteration (ms): 4192.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.472509E-01 | loss scale: 262144.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.531 | tokens per gpu per second (tgs): 1953.982 | TFLOPs: 15.72 |
g0220: [2024-08-10 03:20:59,791] [INFO] [logging.py:96:log_dist] [Rank 0] step=18070, skipped=22, lr=[0.0001999839709487716, 0.0001999839709487716], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18070 loss: 0.8244 iter time (s): 4.111 samples/sec: 31.138
g0238:  iteration    18070/10000000 | consumed samples:      2312960 | consumed tokens:   4736942080 | elapsed time per iteration (ms): 4143.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.489462E-01 | loss scale: 262144.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.892 | tokens per gpu per second (tgs): 1977.089 | TFLOPs: 15.91 |
g0220: [2024-08-10 03:21:42,081] [INFO] [logging.py:96:log_dist] [Rank 0] step=18080, skipped=22, lr=[0.000199983922523357, 0.000199983922523357], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18080 loss: 0.8348 iter time (s): 4.196 samples/sec: 30.503
g0238:  iteration    18080/10000000 | consumed samples:      2314240 | consumed tokens:   4739563520 | elapsed time per iteration (ms): 4229.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.381333E-01 | loss scale: 262144.0 | grad norm: 0.270 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.267 | tokens per gpu per second (tgs): 1937.108 | TFLOPs: 15.59 |
g0220: [2024-08-10 03:22:22,019] [INFO] [logging.py:96:log_dist] [Rank 0] step=18090, skipped=22, lr=[0.0001999838740249098, 0.0001999838740249098], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18090 loss: 0.8472 iter time (s): 3.961 samples/sec: 32.316
g0238:  iteration    18090/10000000 | consumed samples:      2315520 | consumed tokens:   4742184960 | elapsed time per iteration (ms): 3993.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.511006E-01 | loss scale: 262144.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.050 | tokens per gpu per second (tgs): 2051.218 | TFLOPs: 16.51 |
g0220: [2024-08-10 03:23:03,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=18100, skipped=22, lr=[0.00019998382545343004, 0.00019998382545343004], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18100 loss: 0.8410 iter time (s): 4.133 samples/sec: 30.974
g0238:  iteration    18100/10000000 | consumed samples:      2316800 | consumed tokens:   4744806400 | elapsed time per iteration (ms): 4165.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.358995E-01 | loss scale: 262144.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.732 | tokens per gpu per second (tgs): 1966.834 | TFLOPs: 15.83 |
g0220: [2024-08-10 03:23:43,289] [INFO] [logging.py:96:log_dist] [Rank 0] step=18110, skipped=22, lr=[0.00019998377680891774, 0.00019998377680891774], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18110 loss: 0.8475 iter time (s): 3.929 samples/sec: 32.580
g0238:  iteration    18110/10000000 | consumed samples:      2318080 | consumed tokens:   4747427840 | elapsed time per iteration (ms): 3962.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.442710E-01 | loss scale: 262144.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.307 | tokens per gpu per second (tgs): 2067.651 | TFLOPs: 16.64 |
g0220: [2024-08-10 03:24:25,194] [INFO] [logging.py:96:log_dist] [Rank 0] step=18120, skipped=22, lr=[0.00019998372809137293, 0.00019998372809137293], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18120 loss: 0.8755 iter time (s): 4.158 samples/sec: 30.787
g0238:  iteration    18120/10000000 | consumed samples:      2319360 | consumed tokens:   4750049280 | elapsed time per iteration (ms): 4190.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.499194E-01 | loss scale: 262144.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.545 | tokens per gpu per second (tgs): 1954.886 | TFLOPs: 15.73 |
g0220: [2024-08-10 03:25:07,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=18130, skipped=22, lr=[0.0001999836793007957, 0.0001999836793007957], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18130 loss: 0.8182 iter time (s): 4.180 samples/sec: 30.624
g0238:  iteration    18130/10000000 | consumed samples:      2320640 | consumed tokens:   4752670720 | elapsed time per iteration (ms): 4212.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.411964E-01 | loss scale: 262144.0 | grad norm: 0.243 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.383 | tokens per gpu per second (tgs): 1944.536 | TFLOPs: 15.65 |
g0220: [2024-08-10 03:25:47,502] [INFO] [logging.py:96:log_dist] [Rank 0] step=18140, skipped=22, lr=[0.00019998363043718606, 0.00019998363043718606], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18140 loss: 0.8606 iter time (s): 3.985 samples/sec: 32.120
g0238:  iteration    18140/10000000 | consumed samples:      2321920 | consumed tokens:   4755292160 | elapsed time per iteration (ms): 4017.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.459336E-01 | loss scale: 262144.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.858 | tokens per gpu per second (tgs): 2038.897 | TFLOPs: 16.41 |
g0220: [2024-08-10 03:26:28,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=18150, skipped=22, lr=[0.000199983581500544, 0.000199983581500544], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18150 loss: 0.8584 iter time (s): 4.114 samples/sec: 31.112
g0238:  iteration    18150/10000000 | consumed samples:      2323200 | consumed tokens:   4757913600 | elapsed time per iteration (ms): 4147.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.525463E-01 | loss scale: 262144.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.866 | tokens per gpu per second (tgs): 1975.397 | TFLOPs: 15.90 |
g0220: [2024-08-10 03:27:11,321] [INFO] [logging.py:96:log_dist] [Rank 0] step=18160, skipped=22, lr=[0.0001999835324908696, 0.0001999835324908696], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18160 loss: 0.8506 iter time (s): 4.202 samples/sec: 30.458
g0238:  iteration    18160/10000000 | consumed samples:      2324480 | consumed tokens:   4760535040 | elapsed time per iteration (ms): 4234.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.486365E-01 | loss scale: 262144.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.226 | tokens per gpu per second (tgs): 1934.439 | TFLOPs: 15.57 |
g0220: [2024-08-10 03:27:53,863] [INFO] [logging.py:96:log_dist] [Rank 0] step=18170, skipped=22, lr=[0.00019998348340816293, 0.00019998348340816293], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18170 loss: 0.8747 iter time (s): 4.220 samples/sec: 30.332
g0238:  iteration    18170/10000000 | consumed samples:      2325760 | consumed tokens:   4763156480 | elapsed time per iteration (ms): 4254.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.575742E-01 | loss scale: 262144.0 | grad norm: 0.303 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.088 | tokens per gpu per second (tgs): 1925.614 | TFLOPs: 15.50 |
g0220: [2024-08-10 03:28:35,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=18180, skipped=22, lr=[0.00019998343425242394, 0.00019998343425242394], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18180 loss: 0.8937 iter time (s): 4.111 samples/sec: 31.138
g0238:  iteration    18180/10000000 | consumed samples:      2327040 | consumed tokens:   4765777920 | elapsed time per iteration (ms): 4143.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.586674E-01 | loss scale: 262144.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.893 | tokens per gpu per second (tgs): 1977.126 | TFLOPs: 15.91 |
g0220: [2024-08-10 03:29:16,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=18190, skipped=22, lr=[0.00019998338502365276, 0.00019998338502365276], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18190 loss: 0.8476 iter time (s): 4.098 samples/sec: 31.235
g0238:  iteration    18190/10000000 | consumed samples:      2328320 | consumed tokens:   4768399360 | elapsed time per iteration (ms): 4130.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.481075E-01 | loss scale: 262144.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.986 | tokens per gpu per second (tgs): 1983.129 | TFLOPs: 15.96 |
g0220: [2024-08-10 03:29:58,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=18200, skipped=22, lr=[0.0001999833357218494, 0.0001999833357218494], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18200 loss: 0.8421 iter time (s): 4.161 samples/sec: 30.761
g0238:  iteration    18200/10000000 | consumed samples:      2329600 | consumed tokens:   4771020800 | elapsed time per iteration (ms): 4193.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.533595E-01 | loss scale: 262144.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.521 | tokens per gpu per second (tgs): 1953.329 | TFLOPs: 15.72 |
g0220: [2024-08-10 03:30:39,459] [INFO] [logging.py:96:log_dist] [Rank 0] step=18210, skipped=22, lr=[0.00019998328634701385, 0.00019998328634701385], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18210 loss: 0.8328 iter time (s): 4.059 samples/sec: 31.537
g0238:  iteration    18210/10000000 | consumed samples:      2330880 | consumed tokens:   4773642240 | elapsed time per iteration (ms): 4091.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.459169E-01 | loss scale: 262144.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.285 | tokens per gpu per second (tgs): 2002.216 | TFLOPs: 16.11 |
g0220: [2024-08-10 03:31:20,060] [INFO] [logging.py:96:log_dist] [Rank 0] step=18220, skipped=22, lr=[0.0001999832368991462, 0.0001999832368991462], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18220 loss: 0.8283 iter time (s): 4.027 samples/sec: 31.787
g0238:  iteration    18220/10000000 | consumed samples:      2332160 | consumed tokens:   4776263680 | elapsed time per iteration (ms): 4060.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.379817E-01 | loss scale: 262144.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.527 | tokens per gpu per second (tgs): 2017.712 | TFLOPs: 16.24 |
g0220: [2024-08-10 03:32:00,659] [INFO] [logging.py:96:log_dist] [Rank 0] step=18230, skipped=22, lr=[0.00019998318737824645, 0.00019998318737824645], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18230 loss: 0.8290 iter time (s): 4.027 samples/sec: 31.788
g0238:  iteration    18230/10000000 | consumed samples:      2333440 | consumed tokens:   4778885120 | elapsed time per iteration (ms): 4059.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.484495E-01 | loss scale: 262144.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.528 | tokens per gpu per second (tgs): 2017.785 | TFLOPs: 16.24 |
g0220: [2024-08-10 03:32:42,020] [INFO] [logging.py:96:log_dist] [Rank 0] step=18240, skipped=22, lr=[0.00019998313778431468, 0.00019998313778431468], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18240 loss: 0.8479 iter time (s): 4.103 samples/sec: 31.195
g0238:  iteration    18240/10000000 | consumed samples:      2334720 | consumed tokens:   4781506560 | elapsed time per iteration (ms): 4136.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.446402E-01 | loss scale: 262144.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.946 | tokens per gpu per second (tgs): 1980.546 | TFLOPs: 15.94 |
g0220: [2024-08-10 03:33:22,998] [INFO] [logging.py:96:log_dist] [Rank 0] step=18250, skipped=22, lr=[0.0001999830881173509, 0.0001999830881173509], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18250 loss: 0.8287 iter time (s): 4.065 samples/sec: 31.490
g0238:  iteration    18250/10000000 | consumed samples:      2336000 | consumed tokens:   4784128000 | elapsed time per iteration (ms): 4098.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.438203E-01 | loss scale: 262144.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.232 | tokens per gpu per second (tgs): 1998.852 | TFLOPs: 16.09 |
g0220: [2024-08-10 03:34:04,962] [INFO] [logging.py:96:log_dist] [Rank 0] step=18260, skipped=22, lr=[0.00019998303837735518, 0.00019998303837735518], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18260 loss: 0.8534 iter time (s): 4.163 samples/sec: 30.749
g0238:  iteration    18260/10000000 | consumed samples:      2337280 | consumed tokens:   4786749440 | elapsed time per iteration (ms): 4196.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.531644E-01 | loss scale: 262144.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.505 | tokens per gpu per second (tgs): 1952.339 | TFLOPs: 15.71 |
g0220: [2024-08-10 03:34:45,557] [INFO] [logging.py:96:log_dist] [Rank 0] step=18270, skipped=22, lr=[0.00019998298856432754, 0.00019998298856432754], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18270 loss: 0.8324 iter time (s): 4.027 samples/sec: 31.787
g0238:  iteration    18270/10000000 | consumed samples:      2338560 | consumed tokens:   4789370880 | elapsed time per iteration (ms): 4059.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.454879E-01 | loss scale: 262144.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.533 | tokens per gpu per second (tgs): 2018.124 | TFLOPs: 16.24 |
g0220: [2024-08-10 03:35:26,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=18280, skipped=22, lr=[0.000199982938678268, 0.000199982938678268], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18280 loss: 0.8319 iter time (s): 4.054 samples/sec: 31.573
g0238:  iteration    18280/10000000 | consumed samples:      2339840 | consumed tokens:   4791992320 | elapsed time per iteration (ms): 4086.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.517285E-01 | loss scale: 262144.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.322 | tokens per gpu per second (tgs): 2004.597 | TFLOPs: 16.13 |
g0220: [2024-08-10 03:36:08,902] [INFO] [logging.py:96:log_dist] [Rank 0] step=18290, skipped=22, lr=[0.00019998288871917662, 0.00019998288871917662], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18290 loss: 0.8342 iter time (s): 4.215 samples/sec: 30.366
g0238:  iteration    18290/10000000 | consumed samples:      2341120 | consumed tokens:   4794613760 | elapsed time per iteration (ms): 4247.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.446003E-01 | loss scale: 262144.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.133 | tokens per gpu per second (tgs): 1928.511 | TFLOPs: 15.52 |
g0220: [2024-08-10 03:36:51,493] [INFO] [logging.py:96:log_dist] [Rank 0] step=18300, skipped=22, lr=[0.0001999828386870534, 0.0001999828386870534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18300 loss: 0.8234 iter time (s): 4.227 samples/sec: 30.284
g0238:  iteration    18300/10000000 | consumed samples:      2342400 | consumed tokens:   4797235200 | elapsed time per iteration (ms): 4259.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.377092E-01 | loss scale: 262144.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.053 | tokens per gpu per second (tgs): 1923.411 | TFLOPs: 15.48 |
g0220: [2024-08-10 03:37:32,324] [INFO] [logging.py:96:log_dist] [Rank 0] step=18310, skipped=22, lr=[0.00019998278858189843, 0.00019998278858189843], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18310 loss: 0.8447 iter time (s): 4.050 samples/sec: 31.602
g0238:  iteration    18310/10000000 | consumed samples:      2343680 | consumed tokens:   4799856640 | elapsed time per iteration (ms): 4083.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.503448E-01 | loss scale: 262144.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.349 | tokens per gpu per second (tgs): 2006.324 | TFLOPs: 16.15 |
g0220: [2024-08-10 03:38:12,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=18320, skipped=22, lr=[0.0001999827384037117, 0.0001999827384037117], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18320 loss: 0.8138 iter time (s): 3.984 samples/sec: 32.132
g0238:  iteration    18320/10000000 | consumed samples:      2344960 | consumed tokens:   4802478080 | elapsed time per iteration (ms): 4015.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.400400E-01 | loss scale: 262144.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.874 | tokens per gpu per second (tgs): 2039.918 | TFLOPs: 16.42 |
g0220: [2024-08-10 03:38:53,322] [INFO] [logging.py:96:log_dist] [Rank 0] step=18330, skipped=22, lr=[0.00019998268815249332, 0.00019998268815249332], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18330 loss: 0.8648 iter time (s): 4.038 samples/sec: 31.697
g0238:  iteration    18330/10000000 | consumed samples:      2346240 | consumed tokens:   4805099520 | elapsed time per iteration (ms): 4083.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.477055E-01 | loss scale: 262144.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.343 | tokens per gpu per second (tgs): 2005.942 | TFLOPs: 16.14 |
g0220: [2024-08-10 03:39:33,384] [INFO] [logging.py:96:log_dist] [Rank 0] step=18340, skipped=22, lr=[0.00019998263782824326, 0.00019998263782824326], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18340 loss: 0.8421 iter time (s): 3.974 samples/sec: 32.212
g0238:  iteration    18340/10000000 | consumed samples:      2347520 | consumed tokens:   4807720960 | elapsed time per iteration (ms): 4006.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.482213E-01 | loss scale: 262144.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.950 | tokens per gpu per second (tgs): 2044.808 | TFLOPs: 16.45 |
g0220: [2024-08-10 03:40:14,944] [INFO] [logging.py:96:log_dist] [Rank 0] step=18350, skipped=22, lr=[0.0001999825874309616, 0.0001999825874309616], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18350 loss: 0.8552 iter time (s): 4.123 samples/sec: 31.046
g0238:  iteration    18350/10000000 | consumed samples:      2348800 | consumed tokens:   4810342400 | elapsed time per iteration (ms): 4156.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.494293E-01 | loss scale: 262144.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.799 | tokens per gpu per second (tgs): 1971.143 | TFLOPs: 15.86 |
g0220: [2024-08-10 03:40:57,275] [INFO] [logging.py:96:log_dist] [Rank 0] step=18360, skipped=22, lr=[0.00019998253696064833, 0.00019998253696064833], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18360 loss: 0.8453 iter time (s): 4.200 samples/sec: 30.475
g0238:  iteration    18360/10000000 | consumed samples:      2350080 | consumed tokens:   4812963840 | elapsed time per iteration (ms): 4233.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.367947E-01 | loss scale: 262144.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.238 | tokens per gpu per second (tgs): 1935.244 | TFLOPs: 15.57 |
g0220: [2024-08-10 03:41:38,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=18370, skipped=22, lr=[0.00019998248641730354, 0.00019998248641730354], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18370 loss: 0.8255 iter time (s): 4.113 samples/sec: 31.124
g0238:  iteration    18370/10000000 | consumed samples:      2351360 | consumed tokens:   4815585280 | elapsed time per iteration (ms): 4145.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.455527E-01 | loss scale: 262144.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.879 | tokens per gpu per second (tgs): 1976.267 | TFLOPs: 15.90 |
g0220: [2024-08-10 03:42:19,423] [INFO] [logging.py:96:log_dist] [Rank 0] step=18380, skipped=22, lr=[0.00019998243580092724, 0.00019998243580092724], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18380 loss: 0.8285 iter time (s): 4.037 samples/sec: 31.707
g0238:  iteration    18380/10000000 | consumed samples:      2352640 | consumed tokens:   4818206720 | elapsed time per iteration (ms): 4069.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.479358E-01 | loss scale: 262144.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.453 | tokens per gpu per second (tgs): 2012.968 | TFLOPs: 16.20 |
g0220: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 03:42:31,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 03:42:31,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-10 03:42:31,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 03:42:31,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-10 03:42:31,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-10 03:42:31,798] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 03:42:31,799] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-10 03:43:00,051] [INFO] [logging.py:96:log_dist] [Rank 0] step=18390, skipped=22, lr=[0.0001999823851115195, 0.0001999823851115195], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18390 loss: 0.8588 iter time (s): 4.030 samples/sec: 31.763
g0238:  iteration    18390/10000000 | consumed samples:      2353920 | consumed tokens:   4820828160 | elapsed time per iteration (ms): 4063.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.438794E-01 | loss scale: 524288.0 | grad norm: 0.251 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.501 | tokens per gpu per second (tgs): 2016.050 | TFLOPs: 16.22 |
g0238: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18390
g0220: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 18390
g0237: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: Grad overflow on iteration 18390
g0234: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 18390
g0233: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 18390
g0225: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 18390
g0237: Grad overflow on iteration 18390
g0233: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: Grad overflow on iteration 18390
g0235: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 18390
g0233: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 18390
g0235: Grad overflow on iteration 18390
g0233: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 18390
g0235: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 18390
g0238: Grad overflow on iteration 18390
g0237: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 18390
g0225: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 18390
g0235: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 18390
g0236: Grad overflow on iteration 18390
g0233: Grad overflow on iteration 18390
g0235: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: Grad overflow on iteration 18390
g0233: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: Grad overflow on iteration 18390
g0233: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: Grad overflow on iteration 18390
g0237: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: Grad overflow on iteration 18390
g0238: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: Grad overflow on iteration 18390
g0234: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: Grad overflow on iteration 18390
g0238: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 18390
g0236: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-10 03:43:04,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 18390
g0236: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: Grad overflow on iteration 18390
g0234: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: Grad overflow on iteration 18390
g0220: Grad overflow on iteration 18390
g0220: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 18390
g0220: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-10 03:43:04,199] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-10 03:43:04,200] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0236: [2024-08-10 03:43:04,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-10 03:43:04,200] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18390
g0238: [2024-08-10 03:43:04,200] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-10 03:43:41,443] [INFO] [logging.py:96:log_dist] [Rank 0] step=18400, skipped=23, lr=[0.00019998233434908032, 0.00019998233434908032], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18400 loss: 0.8032 iter time (s): 4.105 samples/sec: 31.179
g0238:  iteration    18400/10000000 | consumed samples:      2355200 | consumed tokens:   4823449600 | elapsed time per iteration (ms): 4138.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.363985E-01 | loss scale: 262144.0 | grad norm: 0.329 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.929 | tokens per gpu per second (tgs): 1979.437 | TFLOPs: 15.93 |
g0225: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 18402
g0225: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 18402
g0225: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 18402
g0220: Grad overflow on iteration 18402
g0234: Grad overflow on iteration 18402
g0234: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 18402
g0233: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: Grad overflow on iteration 18402
g0234: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 18402
g0237: Grad overflow on iteration 18402
g0233: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: Grad overflow on iteration 18402
g0233: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: Grad overflow on iteration 18402
g0234: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 18402
g0237: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: Grad overflow on iteration 18402
g0233: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 18402
g0237: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: Grad overflow on iteration 18402
g0220: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18402
g0235: Grad overflow on iteration 18402
g0238: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18402
g0237: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 03:43:54,033] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0235: Grad overflow on iteration 18402
g0238: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 18402
g0235: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: Grad overflow on iteration 18402
g0235: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 18402
g0236: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: Grad overflow on iteration 18402
g0236: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: Grad overflow on iteration 18402
g0236: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-10 03:43:54,032] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 18402
g0225: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 18402
g0234: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 18402
g0233: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 18402
g0236: Grad overflow on iteration 18402
g0235: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18402
g0237: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 18402
g0237: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 18402
g0220: [2024-08-10 03:43:54,033] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 18403
g0220: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 18403
g0233: Grad overflow on iteration 18403
g0234: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 18403
g0233: Grad overflow on iteration 18403
g0234: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18403
g0236: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 18403
g0238: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18403
g0236: Grad overflow on iteration 18403
g0238: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18403
g0236: Grad overflow on iteration 18403
g0238: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: Grad overflow on iteration 18403
g0237: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 18403
g0234: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: Grad overflow on iteration 18403
g0237: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 18403
g0237: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: Grad overflow on iteration 18403
g0235: Grad overflow on iteration 18403
g0237: Grad overflow on iteration 18403
g0233: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 18403
g0225: Grad overflow on iteration 18403
g0236: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 18403
g0236: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 18403
g0234: Grad overflow on iteration 18403
g0233: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 18403
g0233: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: Grad overflow on iteration 18403
g0235: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18403
g0234: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: Grad overflow on iteration 18403
g0238: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: Grad overflow on iteration 18403
g0220: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: Grad overflow on iteration 18403
g0225: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 18403
g0225: Grad overflow on iteration 18403
g0220: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: Grad overflow on iteration 18403
g0225: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 03:43:57,878] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 03:43:57,879] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 03:43:57,879] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
g0220: [2024-08-10 03:44:22,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=18410, skipped=25, lr=[0.00019998228351360975, 0.00019998228351360975], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18410 loss: 0.8778 iter time (s): 4.048 samples/sec: 31.620
g0238:  iteration    18410/10000000 | consumed samples:      2356480 | consumed tokens:   4826071040 | elapsed time per iteration (ms): 4080.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.584049E-01 | loss scale: 65536.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.366 | tokens per gpu per second (tgs): 2007.441 | TFLOPs: 16.15 |
g0220: [2024-08-10 03:45:04,780] [INFO] [logging.py:96:log_dist] [Rank 0] step=18420, skipped=25, lr=[0.00019998223260510786, 0.00019998223260510786], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18420 loss: 0.8336 iter time (s): 4.220 samples/sec: 30.333
g0238:  iteration    18420/10000000 | consumed samples:      2357760 | consumed tokens:   4828692480 | elapsed time per iteration (ms): 4252.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.390996E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.097 | tokens per gpu per second (tgs): 1926.194 | TFLOPs: 15.50 |
g0220: [2024-08-10 03:45:46,744] [INFO] [logging.py:96:log_dist] [Rank 0] step=18430, skipped=25, lr=[0.00019998218162357463, 0.00019998218162357463], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18430 loss: 0.8576 iter time (s): 4.164 samples/sec: 30.742
g0238:  iteration    18430/10000000 | consumed samples:      2359040 | consumed tokens:   4831313920 | elapsed time per iteration (ms): 4196.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.359885E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.503 | tokens per gpu per second (tgs): 1952.207 | TFLOPs: 15.71 |
g0220: [2024-08-10 03:46:29,203] [INFO] [logging.py:96:log_dist] [Rank 0] step=18440, skipped=25, lr=[0.00019998213056901016, 0.00019998213056901016], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18440 loss: 0.8035 iter time (s): 4.213 samples/sec: 30.381
g0238:  iteration    18440/10000000 | consumed samples:      2360320 | consumed tokens:   4833935360 | elapsed time per iteration (ms): 4246.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.434860E-01 | loss scale: 65536.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.146 | tokens per gpu per second (tgs): 1929.329 | TFLOPs: 15.53 |
g0220: [2024-08-10 03:47:10,776] [INFO] [logging.py:96:log_dist] [Rank 0] step=18450, skipped=25, lr=[0.00019998207944141444, 0.00019998207944141444], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18450 loss: 0.8413 iter time (s): 4.124 samples/sec: 31.041
g0238:  iteration    18450/10000000 | consumed samples:      2361600 | consumed tokens:   4836556800 | elapsed time per iteration (ms): 4157.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.526378E-01 | loss scale: 65536.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.790 | tokens per gpu per second (tgs): 1970.568 | TFLOPs: 15.86 |
g0220: [2024-08-10 03:47:52,198] [INFO] [logging.py:96:log_dist] [Rank 0] step=18460, skipped=25, lr=[0.00019998202824078755, 0.00019998202824078755], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18460 loss: 0.8361 iter time (s): 4.110 samples/sec: 31.147
g0238:  iteration    18460/10000000 | consumed samples:      2362880 | consumed tokens:   4839178240 | elapsed time per iteration (ms): 4142.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.490908E-01 | loss scale: 65536.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.901 | tokens per gpu per second (tgs): 1977.677 | TFLOPs: 15.91 |
g0220: [2024-08-10 03:48:33,351] [INFO] [logging.py:96:log_dist] [Rank 0] step=18470, skipped=25, lr=[0.00019998197696712952, 0.00019998197696712952], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18470 loss: 0.8768 iter time (s): 4.083 samples/sec: 31.352
g0238:  iteration    18470/10000000 | consumed samples:      2364160 | consumed tokens:   4841799680 | elapsed time per iteration (ms): 4115.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.445340E-01 | loss scale: 65536.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.104 | tokens per gpu per second (tgs): 1990.666 | TFLOPs: 16.02 |
g0220: [2024-08-10 03:49:14,445] [INFO] [logging.py:96:log_dist] [Rank 0] step=18480, skipped=25, lr=[0.00019998192562044036, 0.00019998192562044036], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18480 loss: 0.8899 iter time (s): 4.077 samples/sec: 31.397
g0238:  iteration    18480/10000000 | consumed samples:      2365440 | consumed tokens:   4844421120 | elapsed time per iteration (ms): 4109.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.547765E-01 | loss scale: 65536.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.147 | tokens per gpu per second (tgs): 1993.423 | TFLOPs: 16.04 |
g0220: [2024-08-10 03:49:55,992] [INFO] [logging.py:96:log_dist] [Rank 0] step=18490, skipped=25, lr=[0.00019998187420072015, 0.00019998187420072015], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18490 loss: 0.8384 iter time (s): 4.122 samples/sec: 31.052
g0238:  iteration    18490/10000000 | consumed samples:      2366720 | consumed tokens:   4847042560 | elapsed time per iteration (ms): 4154.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.437604E-01 | loss scale: 65536.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.809 | tokens per gpu per second (tgs): 1971.786 | TFLOPs: 15.87 |
g0220: [2024-08-10 03:50:35,742] [INFO] [logging.py:96:log_dist] [Rank 0] step=18500, skipped=25, lr=[0.0001999818227079689, 0.0001999818227079689], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18500 loss: 0.8154 iter time (s): 3.943 samples/sec: 32.466
g0238:  iteration    18500/10000000 | consumed samples:      2368000 | consumed tokens:   4849664000 | elapsed time per iteration (ms): 3975.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.419783E-01 | loss scale: 65536.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.200 | tokens per gpu per second (tgs): 2060.802 | TFLOPs: 16.58 |
g0220: [2024-08-10 03:51:17,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=18510, skipped=25, lr=[0.00019998177114218665, 0.00019998177114218665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18510 loss: 0.8384 iter time (s): 4.133 samples/sec: 30.970
g0238:  iteration    18510/10000000 | consumed samples:      2369280 | consumed tokens:   4852285440 | elapsed time per iteration (ms): 4165.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.531042E-01 | loss scale: 65536.0 | grad norm: 0.214 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.730 | tokens per gpu per second (tgs): 1966.736 | TFLOPs: 15.83 |
g0220: [2024-08-10 03:51:57,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=18520, skipped=25, lr=[0.00019998171950337347, 0.00019998171950337347], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18520 loss: 0.8761 iter time (s): 4.007 samples/sec: 31.943
g0238:  iteration    18520/10000000 | consumed samples:      2370560 | consumed tokens:   4854906880 | elapsed time per iteration (ms): 4039.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.442132E-01 | loss scale: 65536.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.685 | tokens per gpu per second (tgs): 2027.817 | TFLOPs: 16.32 |
g0220: [2024-08-10 03:52:37,696] [INFO] [logging.py:96:log_dist] [Rank 0] step=18530, skipped=25, lr=[0.00019998166779152936, 0.00019998166779152936], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18530 loss: 0.8149 iter time (s): 3.958 samples/sec: 32.342
g0238:  iteration    18530/10000000 | consumed samples:      2371840 | consumed tokens:   4857528320 | elapsed time per iteration (ms): 3990.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.378352E-01 | loss scale: 65536.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.080 | tokens per gpu per second (tgs): 2053.096 | TFLOPs: 16.52 |
g0220: [2024-08-10 03:53:17,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=18540, skipped=25, lr=[0.00019998161600665439, 0.00019998161600665439], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18540 loss: 0.8248 iter time (s): 3.983 samples/sec: 32.137
g0238:  iteration    18540/10000000 | consumed samples:      2373120 | consumed tokens:   4860149760 | elapsed time per iteration (ms): 4015.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.442422E-01 | loss scale: 65536.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.878 | tokens per gpu per second (tgs): 2040.164 | TFLOPs: 16.42 |
g0220: [2024-08-10 03:54:00,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=18550, skipped=25, lr=[0.00019998156414874859, 0.00019998156414874859], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18550 loss: 0.8242 iter time (s): 4.251 samples/sec: 30.113
g0238:  iteration    18550/10000000 | consumed samples:      2374400 | consumed tokens:   4862771200 | elapsed time per iteration (ms): 4283.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.420129E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.885 | tokens per gpu per second (tgs): 1912.611 | TFLOPs: 15.39 |
g0220: [2024-08-10 03:54:41,264] [INFO] [logging.py:96:log_dist] [Rank 0] step=18560, skipped=25, lr=[0.00019998151221781198, 0.00019998151221781198], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18560 loss: 0.8626 iter time (s): 4.026 samples/sec: 31.797
g0238:  iteration    18560/10000000 | consumed samples:      2375680 | consumed tokens:   4865392640 | elapsed time per iteration (ms): 4058.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.464082E-01 | loss scale: 65536.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.536 | tokens per gpu per second (tgs): 2018.307 | TFLOPs: 16.24 |
g0220: [2024-08-10 03:55:20,515] [INFO] [logging.py:96:log_dist] [Rank 0] step=18570, skipped=25, lr=[0.00019998146021384463, 0.00019998146021384463], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18570 loss: 0.8434 iter time (s): 3.892 samples/sec: 32.891
g0238:  iteration    18570/10000000 | consumed samples:      2376960 | consumed tokens:   4868014080 | elapsed time per iteration (ms): 3924.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.493367E-01 | loss scale: 65536.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.616 | tokens per gpu per second (tgs): 2087.401 | TFLOPs: 16.80 |
g0220: [2024-08-10 03:56:00,417] [INFO] [logging.py:96:log_dist] [Rank 0] step=18580, skipped=25, lr=[0.0001999814081368466, 0.0001999814081368466], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18580 loss: 0.8379 iter time (s): 3.958 samples/sec: 32.341
g0238:  iteration    18580/10000000 | consumed samples:      2378240 | consumed tokens:   4870635520 | elapsed time per iteration (ms): 3990.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.534892E-01 | loss scale: 65536.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.079 | tokens per gpu per second (tgs): 2053.033 | TFLOPs: 16.52 |
g0220: [2024-08-10 03:56:41,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=18590, skipped=25, lr=[0.00019998135598681786, 0.00019998135598681786], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18590 loss: 0.8589 iter time (s): 4.071 samples/sec: 31.443
g0238:  iteration    18590/10000000 | consumed samples:      2379520 | consumed tokens:   4873256960 | elapsed time per iteration (ms): 4103.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.397926E-01 | loss scale: 65536.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.193 | tokens per gpu per second (tgs): 1996.360 | TFLOPs: 16.07 |
g0220: [2024-08-10 03:57:22,949] [INFO] [logging.py:96:log_dist] [Rank 0] step=18600, skipped=25, lr=[0.00019998130376375849, 0.00019998130376375849], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18600 loss: 0.8459 iter time (s): 4.116 samples/sec: 31.095
g0238:  iteration    18600/10000000 | consumed samples:      2380800 | consumed tokens:   4875878400 | elapsed time per iteration (ms): 4149.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.427608E-01 | loss scale: 65536.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.845 | tokens per gpu per second (tgs): 1974.089 | TFLOPs: 15.89 |
g0233: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 18603
g0233: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 18603
g0233: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 18603
g0233: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 18603
g0235: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 18603
g0220: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 18603
g0220: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 18603
g0220: Grad overflow on iteration 18603
g0233: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 18603
g0235: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 18603
g0237: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 18603
g0220: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: Grad overflow on iteration 18603
g0220: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18603
g0235: Grad overflow on iteration 18603
g0220: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: Grad overflow on iteration 18603
g0225: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 18603
g0220: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: Grad overflow on iteration 18603
g0237: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18603
g0238: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18603
g0225: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: Grad overflow on iteration 18603
g0237: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: Grad overflow on iteration 18603
g0225: Grad overflow on iteration 18603
g0236: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: Grad overflow on iteration 18603
g0237: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 18603
g0225: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 18603
g0225: Grad overflow on iteration 18603
g0238: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: Grad overflow on iteration 18603
g0225: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: Grad overflow on iteration 18603
g0236: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: Grad overflow on iteration 18603
g0234: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 18603
g0236: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 03:57:39,870] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0234: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 03:57:39,869] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 18603
g0234: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 18603
g0238: [2024-08-10 03:57:39,870] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 03:58:04,131] [INFO] [logging.py:96:log_dist] [Rank 0] step=18610, skipped=26, lr=[0.00019998125146766853, 0.00019998125146766853], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18610 loss: 0.8677 iter time (s): 4.085 samples/sec: 31.332
g0238:  iteration    18610/10000000 | consumed samples:      2382080 | consumed tokens:   4878499840 | elapsed time per iteration (ms): 4118.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.525537E-01 | loss scale: 32768.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.082 | tokens per gpu per second (tgs): 1989.271 | TFLOPs: 16.01 |
g0220: [2024-08-10 03:58:45,733] [INFO] [logging.py:96:log_dist] [Rank 0] step=18620, skipped=26, lr=[0.00019998119909854804, 0.00019998119909854804], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18620 loss: 0.7898 iter time (s): 4.127 samples/sec: 31.012
g0238:  iteration    18620/10000000 | consumed samples:      2383360 | consumed tokens:   4881121280 | elapsed time per iteration (ms): 4160.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.273113E-01 | loss scale: 32768.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.768 | tokens per gpu per second (tgs): 1969.140 | TFLOPs: 15.85 |
g0220: [2024-08-10 03:59:25,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=18630, skipped=26, lr=[0.00019998114665639702, 0.00019998114665639702], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18630 loss: 0.8251 iter time (s): 3.921 samples/sec: 32.645
g0238:  iteration    18630/10000000 | consumed samples:      2384640 | consumed tokens:   4883742720 | elapsed time per iteration (ms): 3953.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.373206E-01 | loss scale: 32768.0 | grad norm: 0.241 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.375 | tokens per gpu per second (tgs): 2072.025 | TFLOPs: 16.67 |
g0220: [2024-08-10 04:00:07,137] [INFO] [logging.py:96:log_dist] [Rank 0] step=18640, skipped=26, lr=[0.00019998109414121553, 0.00019998109414121553], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18640 loss: 0.8368 iter time (s): 4.154 samples/sec: 30.814
g0238:  iteration    18640/10000000 | consumed samples:      2385920 | consumed tokens:   4886364160 | elapsed time per iteration (ms): 4186.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.409346E-01 | loss scale: 32768.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.572 | tokens per gpu per second (tgs): 1956.604 | TFLOPs: 15.75 |
g0220: [2024-08-10 04:00:48,638] [INFO] [logging.py:96:log_dist] [Rank 0] step=18650, skipped=26, lr=[0.00019998104155300361, 0.00019998104155300361], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18650 loss: 0.8438 iter time (s): 4.117 samples/sec: 31.091
g0238:  iteration    18650/10000000 | consumed samples:      2387200 | consumed tokens:   4888985600 | elapsed time per iteration (ms): 4150.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.488829E-01 | loss scale: 32768.0 | grad norm: 0.234 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.843 | tokens per gpu per second (tgs): 1973.978 | TFLOPs: 15.88 |
g0220: [2024-08-10 04:01:28,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=18660, skipped=26, lr=[0.0001999809888917613, 0.0001999809888917613], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18660 loss: 0.8691 iter time (s): 3.991 samples/sec: 32.070
g0238:  iteration    18660/10000000 | consumed samples:      2388480 | consumed tokens:   4891607040 | elapsed time per iteration (ms): 4023.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.454659E-01 | loss scale: 32768.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.811 | tokens per gpu per second (tgs): 2035.897 | TFLOPs: 16.38 |
g0220: [2024-08-10 04:02:10,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=18670, skipped=26, lr=[0.00019998093615748865, 0.00019998093615748865], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18670 loss: 0.8332 iter time (s): 4.116 samples/sec: 31.099
g0238:  iteration    18670/10000000 | consumed samples:      2389760 | consumed tokens:   4894228480 | elapsed time per iteration (ms): 4148.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.419232E-01 | loss scale: 32768.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.854 | tokens per gpu per second (tgs): 1974.645 | TFLOPs: 15.89 |
g0220: [2024-08-10 04:02:52,127] [INFO] [logging.py:96:log_dist] [Rank 0] step=18680, skipped=26, lr=[0.0001999808833501857, 0.0001999808833501857], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18680 loss: 0.8546 iter time (s): 4.144 samples/sec: 30.887
g0238:  iteration    18680/10000000 | consumed samples:      2391040 | consumed tokens:   4896849920 | elapsed time per iteration (ms): 4176.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.458563E-01 | loss scale: 32768.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.648 | tokens per gpu per second (tgs): 1961.462 | TFLOPs: 15.78 |
g0220: [2024-08-10 04:03:33,498] [INFO] [logging.py:96:log_dist] [Rank 0] step=18690, skipped=26, lr=[0.00019998083046985247, 0.00019998083046985247], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18690 loss: 0.8159 iter time (s): 4.104 samples/sec: 31.186
g0238:  iteration    18690/10000000 | consumed samples:      2392320 | consumed tokens:   4899471360 | elapsed time per iteration (ms): 4137.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.381956E-01 | loss scale: 32768.0 | grad norm: 0.173 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.939 | tokens per gpu per second (tgs): 1980.105 | TFLOPs: 15.93 |
g0220: [2024-08-10 04:04:15,698] [INFO] [logging.py:96:log_dist] [Rank 0] step=18700, skipped=26, lr=[0.000199980777516489, 0.000199980777516489], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18700 loss: 0.8230 iter time (s): 4.187 samples/sec: 30.573
g0238:  iteration    18700/10000000 | consumed samples:      2393600 | consumed tokens:   4902092800 | elapsed time per iteration (ms): 4219.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.320721E-01 | loss scale: 32768.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.332 | tokens per gpu per second (tgs): 1941.259 | TFLOPs: 15.62 |
g0220: [2024-08-10 04:04:57,014] [INFO] [logging.py:96:log_dist] [Rank 0] step=18710, skipped=26, lr=[0.00019998072449009535, 0.00019998072449009535], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18710 loss: 0.8604 iter time (s): 4.099 samples/sec: 31.229
g0238:  iteration    18710/10000000 | consumed samples:      2394880 | consumed tokens:   4904714240 | elapsed time per iteration (ms): 4131.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.615273E-01 | loss scale: 32768.0 | grad norm: 0.290 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.980 | tokens per gpu per second (tgs): 1982.729 | TFLOPs: 15.96 |
g0220: [2024-08-10 04:05:37,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=18720, skipped=26, lr=[0.00019998067139067154, 0.00019998067139067154], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18720 loss: 0.8609 iter time (s): 4.028 samples/sec: 31.781
g0238:  iteration    18720/10000000 | consumed samples:      2396160 | consumed tokens:   4907335680 | elapsed time per iteration (ms): 4060.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.531226E-01 | loss scale: 32768.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.524 | tokens per gpu per second (tgs): 2017.539 | TFLOPs: 16.24 |
g0220: [2024-08-10 04:06:19,436] [INFO] [logging.py:96:log_dist] [Rank 0] step=18730, skipped=26, lr=[0.00019998061821821763, 0.00019998061821821763], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18730 loss: 0.8153 iter time (s): 4.149 samples/sec: 30.852
g0238:  iteration    18730/10000000 | consumed samples:      2397440 | consumed tokens:   4909957120 | elapsed time per iteration (ms): 4181.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.424804E-01 | loss scale: 32768.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.609 | tokens per gpu per second (tgs): 1958.976 | TFLOPs: 15.76 |
g0220: [2024-08-10 04:07:01,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=18740, skipped=26, lr=[0.00019998056497273368, 0.00019998056497273368], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18740 loss: 0.8131 iter time (s): 4.131 samples/sec: 30.983
g0238:  iteration    18740/10000000 | consumed samples:      2398720 | consumed tokens:   4912578560 | elapsed time per iteration (ms): 4164.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.348651E-01 | loss scale: 32768.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.739 | tokens per gpu per second (tgs): 1967.288 | TFLOPs: 15.83 |
g0220: [2024-08-10 04:07:42,825] [INFO] [logging.py:96:log_dist] [Rank 0] step=18750, skipped=26, lr=[0.00019998051165421972, 0.00019998051165421972], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18750 loss: 0.8394 iter time (s): 4.142 samples/sec: 30.901
g0238:  iteration    18750/10000000 | consumed samples:      2400000 | consumed tokens:   4915200000 | elapsed time per iteration (ms): 4174.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.401732E-01 | loss scale: 32768.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.661 | tokens per gpu per second (tgs): 1962.330 | TFLOPs: 15.79 |
g0220: [2024-08-10 04:08:24,576] [INFO] [logging.py:96:log_dist] [Rank 0] step=18760, skipped=26, lr=[0.00019998045826267574, 0.00019998045826267574], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18760 loss: 0.8656 iter time (s): 4.143 samples/sec: 30.898
g0238:  iteration    18760/10000000 | consumed samples:      2401280 | consumed tokens:   4917821440 | elapsed time per iteration (ms): 4175.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.449006E-01 | loss scale: 32768.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.657 | tokens per gpu per second (tgs): 1962.033 | TFLOPs: 15.79 |
g0220: [2024-08-10 04:09:05,373] [INFO] [logging.py:96:log_dist] [Rank 0] step=18770, skipped=26, lr=[0.00019998040479810182, 0.00019998040479810182], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18770 loss: 0.8300 iter time (s): 4.046 samples/sec: 31.632
g0238:  iteration    18770/10000000 | consumed samples:      2402560 | consumed tokens:   4920442880 | elapsed time per iteration (ms): 4082.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.480567E-01 | loss scale: 32768.0 | grad norm: 0.265 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.356 | tokens per gpu per second (tgs): 2006.807 | TFLOPs: 16.15 |
g0220: [2024-08-10 04:09:47,985] [INFO] [logging.py:96:log_dist] [Rank 0] step=18780, skipped=26, lr=[0.00019998035126049803, 0.00019998035126049803], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18780 loss: 0.8399 iter time (s): 4.226 samples/sec: 30.289
g0238:  iteration    18780/10000000 | consumed samples:      2403840 | consumed tokens:   4923064320 | elapsed time per iteration (ms): 4258.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.453882E-01 | loss scale: 32768.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.056 | tokens per gpu per second (tgs): 1923.608 | TFLOPs: 15.48 |
g0220: [2024-08-10 04:10:29,401] [INFO] [logging.py:96:log_dist] [Rank 0] step=18790, skipped=26, lr=[0.00019998029764986436, 0.00019998029764986436], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18790 loss: 0.8510 iter time (s): 4.109 samples/sec: 31.153
g0238:  iteration    18790/10000000 | consumed samples:      2405120 | consumed tokens:   4925685760 | elapsed time per iteration (ms): 4141.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.400145E-01 | loss scale: 32768.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.906 | tokens per gpu per second (tgs): 1978.000 | TFLOPs: 15.92 |
g0220: [2024-08-10 04:11:11,001] [INFO] [logging.py:96:log_dist] [Rank 0] step=18800, skipped=26, lr=[0.00019998024396620089, 0.00019998024396620089], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18800 loss: 0.8161 iter time (s): 4.122 samples/sec: 31.055
g0238:  iteration    18800/10000000 | consumed samples:      2406400 | consumed tokens:   4928307200 | elapsed time per iteration (ms): 4160.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.387881E-01 | loss scale: 32768.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.769 | tokens per gpu per second (tgs): 1969.245 | TFLOPs: 15.85 |
g0220: [2024-08-10 04:11:51,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=18810, skipped=26, lr=[0.00019998019020950762, 0.00019998019020950762], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18810 loss: 0.8606 iter time (s): 4.050 samples/sec: 31.608
g0238:  iteration    18810/10000000 | consumed samples:      2407680 | consumed tokens:   4930928640 | elapsed time per iteration (ms): 4082.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.393626E-01 | loss scale: 32768.0 | grad norm: 0.249 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.352 | tokens per gpu per second (tgs): 2006.509 | TFLOPs: 16.15 |
g0220: [2024-08-10 04:12:31,850] [INFO] [logging.py:96:log_dist] [Rank 0] step=18820, skipped=26, lr=[0.0001999801363797846, 0.0001999801363797846], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18820 loss: 0.8446 iter time (s): 3.970 samples/sec: 32.244
g0238:  iteration    18820/10000000 | consumed samples:      2408960 | consumed tokens:   4933550080 | elapsed time per iteration (ms): 4002.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.456833E-01 | loss scale: 32768.0 | grad norm: 0.216 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.983 | tokens per gpu per second (tgs): 2046.917 | TFLOPs: 16.47 |
g0220: [2024-08-10 04:13:13,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=18830, skipped=26, lr=[0.00019998008247703192, 0.00019998008247703192], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18830 loss: 0.8369 iter time (s): 4.140 samples/sec: 30.921
g0238:  iteration    18830/10000000 | consumed samples:      2410240 | consumed tokens:   4936171520 | elapsed time per iteration (ms): 4178.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.618912E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.633 | tokens per gpu per second (tgs): 1960.530 | TFLOPs: 15.78 |
g0220: [2024-08-10 04:13:56,587] [INFO] [logging.py:96:log_dist] [Rank 0] step=18840, skipped=26, lr=[0.00019998002850124956, 0.00019998002850124956], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18840 loss: 0.8295 iter time (s): 4.260 samples/sec: 30.049
g0238:  iteration    18840/10000000 | consumed samples:      2411520 | consumed tokens:   4938792960 | elapsed time per iteration (ms): 4298.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.415516E-01 | loss scale: 32768.0 | grad norm: 0.257 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.780 | tokens per gpu per second (tgs): 1905.914 | TFLOPs: 15.34 |
g0220: [2024-08-10 04:14:38,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=18850, skipped=26, lr=[0.00019997997445243764, 0.00019997997445243764], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18850 loss: 0.8709 iter time (s): 4.164 samples/sec: 30.736
g0238:  iteration    18850/10000000 | consumed samples:      2412800 | consumed tokens:   4941414400 | elapsed time per iteration (ms): 4198.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.477240E-01 | loss scale: 32768.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.486 | tokens per gpu per second (tgs): 1951.081 | TFLOPs: 15.70 |
g0220: [2024-08-10 04:15:18,522] [INFO] [logging.py:96:log_dist] [Rank 0] step=18860, skipped=26, lr=[0.0001999799203305961, 0.0001999799203305961], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18860 loss: 0.8231 iter time (s): 3.958 samples/sec: 32.342
g0238:  iteration    18860/10000000 | consumed samples:      2414080 | consumed tokens:   4944035840 | elapsed time per iteration (ms): 3991.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.419592E-01 | loss scale: 32768.0 | grad norm: 0.233 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.066 | tokens per gpu per second (tgs): 2052.234 | TFLOPs: 16.51 |
g0220: [2024-08-10 04:15:59,508] [INFO] [logging.py:96:log_dist] [Rank 0] step=18870, skipped=26, lr=[0.00019997986613572505, 0.00019997986613572505], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18870 loss: 0.8473 iter time (s): 4.065 samples/sec: 31.485
g0238:  iteration    18870/10000000 | consumed samples:      2415360 | consumed tokens:   4946657280 | elapsed time per iteration (ms): 4098.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.468304E-01 | loss scale: 32768.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.231 | tokens per gpu per second (tgs): 1998.799 | TFLOPs: 16.08 |
g0220: [2024-08-10 04:16:40,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=18880, skipped=26, lr=[0.00019997981186782452, 0.00019997981186782452], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18880 loss: 0.8388 iter time (s): 4.114 samples/sec: 31.116
g0238:  iteration    18880/10000000 | consumed samples:      2416640 | consumed tokens:   4949278720 | elapsed time per iteration (ms): 4146.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.393772E-01 | loss scale: 32768.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.873 | tokens per gpu per second (tgs): 1975.844 | TFLOPs: 15.90 |
g0220: [2024-08-10 04:17:22,878] [INFO] [logging.py:96:log_dist] [Rank 0] step=18890, skipped=26, lr=[0.00019997975752689452, 0.00019997975752689452], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18890 loss: 0.8427 iter time (s): 4.158 samples/sec: 30.784
g0238:  iteration    18890/10000000 | consumed samples:      2417920 | consumed tokens:   4951900160 | elapsed time per iteration (ms): 4191.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.428426E-01 | loss scale: 32768.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.542 | tokens per gpu per second (tgs): 1954.681 | TFLOPs: 15.73 |
g0220: [2024-08-10 04:18:04,140] [INFO] [logging.py:96:log_dist] [Rank 0] step=18900, skipped=26, lr=[0.00019997970311293512, 0.00019997970311293512], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18900 loss: 0.8644 iter time (s): 4.094 samples/sec: 31.268
g0238:  iteration    18900/10000000 | consumed samples:      2419200 | consumed tokens:   4954521600 | elapsed time per iteration (ms): 4126.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.450170E-01 | loss scale: 32768.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.023 | tokens per gpu per second (tgs): 1985.449 | TFLOPs: 15.98 |
g0220: [2024-08-10 04:18:46,177] [INFO] [logging.py:96:log_dist] [Rank 0] step=18910, skipped=26, lr=[0.00019997964862594641, 0.00019997964862594641], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18910 loss: 0.8190 iter time (s): 4.171 samples/sec: 30.686
g0238:  iteration    18910/10000000 | consumed samples:      2420480 | consumed tokens:   4957143040 | elapsed time per iteration (ms): 4203.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.388988E-01 | loss scale: 32768.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.449 | tokens per gpu per second (tgs): 1948.713 | TFLOPs: 15.68 |
g0220: [2024-08-10 04:19:28,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=18920, skipped=26, lr=[0.00019997959406592834, 0.00019997959406592834], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18920 loss: 0.8493 iter time (s): 4.161 samples/sec: 30.765
g0238:  iteration    18920/10000000 | consumed samples:      2421760 | consumed tokens:   4959764480 | elapsed time per iteration (ms): 4192.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.414502E-01 | loss scale: 32768.0 | grad norm: 0.331 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.529 | tokens per gpu per second (tgs): 1953.827 | TFLOPs: 15.72 |
g0220: [2024-08-10 04:20:09,058] [INFO] [logging.py:96:log_dist] [Rank 0] step=18930, skipped=26, lr=[0.00019997953943288102, 0.00019997953943288102], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18930 loss: 0.8267 iter time (s): 4.062 samples/sec: 31.512
g0238:  iteration    18930/10000000 | consumed samples:      2423040 | consumed tokens:   4962385920 | elapsed time per iteration (ms): 4095.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.513159E-01 | loss scale: 32768.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.255 | tokens per gpu per second (tgs): 2000.346 | TFLOPs: 16.10 |
g0220: [2024-08-10 04:20:49,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=18940, skipped=26, lr=[0.0001999794847268044, 0.0001999794847268044], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18940 loss: 0.8628 iter time (s): 4.049 samples/sec: 31.609
g0238:  iteration    18940/10000000 | consumed samples:      2424320 | consumed tokens:   4965007360 | elapsed time per iteration (ms): 4082.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.321026E-01 | loss scale: 32768.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.353 | tokens per gpu per second (tgs): 2006.571 | TFLOPs: 16.15 |
g0220: [2024-08-10 04:21:30,842] [INFO] [logging.py:96:log_dist] [Rank 0] step=18950, skipped=26, lr=[0.00019997942994769865, 0.00019997942994769865], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18950 loss: 0.8367 iter time (s): 4.063 samples/sec: 31.503
g0238:  iteration    18950/10000000 | consumed samples:      2425600 | consumed tokens:   4967628800 | elapsed time per iteration (ms): 4095.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.457655E-01 | loss scale: 32768.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.252 | tokens per gpu per second (tgs): 2000.117 | TFLOPs: 16.10 |
g0220: [2024-08-10 04:22:11,988] [INFO] [logging.py:96:log_dist] [Rank 0] step=18960, skipped=26, lr=[0.00019997937509556374, 0.00019997937509556374], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18960 loss: 0.8164 iter time (s): 4.081 samples/sec: 31.363
g0238:  iteration    18960/10000000 | consumed samples:      2426880 | consumed tokens:   4970250240 | elapsed time per iteration (ms): 4114.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.300014E-01 | loss scale: 32768.0 | grad norm: 0.232 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.107 | tokens per gpu per second (tgs): 1990.825 | TFLOPs: 16.02 |
g0220: [2024-08-10 04:22:52,814] [INFO] [logging.py:96:log_dist] [Rank 0] step=18970, skipped=26, lr=[0.0001999793201703997, 0.0001999793201703997], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18970 loss: 0.8343 iter time (s): 4.049 samples/sec: 31.613
g0238:  iteration    18970/10000000 | consumed samples:      2428160 | consumed tokens:   4972871680 | elapsed time per iteration (ms): 4082.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.382250E-01 | loss scale: 32768.0 | grad norm: 0.254 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.355 | tokens per gpu per second (tgs): 2006.715 | TFLOPs: 16.15 |
g0220: [2024-08-10 04:23:35,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=18980, skipped=26, lr=[0.00019997926517220663, 0.00019997926517220663], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18980 loss: 0.8431 iter time (s): 4.237 samples/sec: 30.208
g0238:  iteration    18980/10000000 | consumed samples:      2429440 | consumed tokens:   4975493120 | elapsed time per iteration (ms): 4270.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.437320E-01 | loss scale: 32768.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.973 | tokens per gpu per second (tgs): 1918.294 | TFLOPs: 15.44 |
g0220: [2024-08-10 04:24:16,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=18990, skipped=26, lr=[0.0001999792101009845, 0.0001999792101009845], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 18990 loss: 0.8305 iter time (s): 4.018 samples/sec: 31.853
g0238:  iteration    18990/10000000 | consumed samples:      2430720 | consumed tokens:   4978114560 | elapsed time per iteration (ms): 4051.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.375970E-01 | loss scale: 32768.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.592 | tokens per gpu per second (tgs): 2021.876 | TFLOPs: 16.27 |
g0220: [2024-08-10 04:24:57,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=19000, skipped=26, lr=[0.0001999791549567334, 0.0001999791549567334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19000 loss: 0.8487 iter time (s): 4.103 samples/sec: 31.199
g0238:  iteration    19000/10000000 | consumed samples:      2432000 | consumed tokens:   4980736000 | elapsed time per iteration (ms): 4135.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.351861E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.952 | tokens per gpu per second (tgs): 1980.958 | TFLOPs: 15.94 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 19000 | lm loss value: 8.418427E-01 | lm loss PPL: 2.320639E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   19000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-10 04:31:30,075] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step19000 is about to be saved!
g0220: [2024-08-10 04:31:30,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0220: [2024-08-10 04:31:30,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0220: [2024-08-10 04:31:30,080] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0238: [2024-08-10 04:31:30,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0238: [2024-08-10 04:31:30,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0238: [2024-08-10 04:31:30,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0233: [2024-08-10 04:31:30,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0233: [2024-08-10 04:31:30,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0233: [2024-08-10 04:31:30,081] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0235: [2024-08-10 04:31:30,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0235: [2024-08-10 04:31:30,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0237: [2024-08-10 04:31:30,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0237: [2024-08-10 04:31:30,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0235: [2024-08-10 04:31:30,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0236: [2024-08-10 04:31:30,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0236: [2024-08-10 04:31:30,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0236: [2024-08-10 04:31:30,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0237: [2024-08-10 04:31:30,084] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0234: [2024-08-10 04:31:30,085] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0234: [2024-08-10 04:31:30,085] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0234: [2024-08-10 04:31:30,085] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0225: [2024-08-10 04:31:30,086] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0225: [2024-08-10 04:31:30,086] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0225: [2024-08-10 04:31:30,086] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0238: [2024-08-10 04:31:30,104] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_23-model_00-model_states.pt...
g0233: [2024-08-10 04:31:30,117] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_08-model_00-model_states.pt...
g0237: [2024-08-10 04:31:30,121] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_20-model_00-model_states.pt...
g0236: [2024-08-10 04:31:30,122] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_17-model_00-model_states.pt...
g0235: [2024-08-10 04:31:30,122] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_14-model_00-model_states.pt...
g0234: [2024-08-10 04:31:30,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_11-model_00-model_states.pt...
g0225: [2024-08-10 04:31:30,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_05-model_00-model_states.pt...
g0220: [2024-08-10 04:31:30,133] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_01-model_00-model_states.pt...
g0237: [2024-08-10 04:31:30,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_20-model_00-model_states.pt.
g0236: [2024-08-10 04:31:30,231] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_17-model_00-model_states.pt.
g0235: [2024-08-10 04:31:30,262] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_14-model_00-model_states.pt.
g0237: [2024-08-10 04:31:30,267] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_21-model_00-model_states.pt...
g0234: [2024-08-10 04:31:30,267] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_11-model_00-model_states.pt.
g0236: [2024-08-10 04:31:30,270] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_18-model_00-model_states.pt...
g0235: [2024-08-10 04:31:30,301] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_15-model_00-model_states.pt...
g0234: [2024-08-10 04:31:30,307] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_12-model_00-model_states.pt...
g0233: [2024-08-10 04:31:30,334] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_08-model_00-model_states.pt.
g0225: [2024-08-10 04:31:30,344] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_05-model_00-model_states.pt.
g0238: [2024-08-10 04:31:30,358] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_23-model_00-model_states.pt.
g0238: [2024-08-10 04:31:30,359] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_24-model_00-model_states.pt...
g0238: [2024-08-10 04:31:30,364] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_24-model_00-model_states.pt.
g0220: [2024-08-10 04:31:30,367] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_01-model_00-model_states.pt.
g0233: [2024-08-10 04:31:30,372] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_09-model_00-model_states.pt...
g0236: [2024-08-10 04:31:30,377] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_18-model_00-model_states.pt.
g0225: [2024-08-10 04:31:30,384] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_06-model_00-model_states.pt...
g0220: [2024-08-10 04:31:30,387] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_02-model_00-model_states.pt...
g0238: [2024-08-10 04:31:30,410] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_25-model_00-model_states.pt...
g0236: [2024-08-10 04:31:30,412] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_19-model_00-model_states.pt...
g0237: [2024-08-10 04:31:30,421] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_21-model_00-model_states.pt.
g0235: [2024-08-10 04:31:30,448] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_15-model_00-model_states.pt.
g0237: [2024-08-10 04:31:30,454] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_22-model_00-model_states.pt...
g0234: [2024-08-10 04:31:30,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_12-model_00-model_states.pt.
g0235: [2024-08-10 04:31:30,483] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_16-model_00-model_states.pt...
g0233: [2024-08-10 04:31:30,496] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_09-model_00-model_states.pt.
g0234: [2024-08-10 04:31:30,510] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_13-model_00-model_states.pt...
g0233: [2024-08-10 04:31:30,531] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_10-model_00-model_states.pt...
g0225: [2024-08-10 04:31:30,532] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_06-model_00-model_states.pt.
g0220: [2024-08-10 04:31:30,559] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_02-model_00-model_states.pt.
g0238: [2024-08-10 04:31:30,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_25-model_00-model_states.pt.
g0238: [2024-08-10 04:31:30,564] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_07_model_states.pt...
g0225: [2024-08-10 04:31:30,567] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_07-model_00-model_states.pt...
g0237: [2024-08-10 04:31:30,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_22-model_00-model_states.pt.
g0237: [2024-08-10 04:31:30,576] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_06_model_states.pt...
g0236: [2024-08-10 04:31:30,577] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_19-model_00-model_states.pt.
g0236: [2024-08-10 04:31:30,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_05_model_states.pt...
g0220: [2024-08-10 04:31:30,591] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_03-model_00-model_states.pt...
g0234: [2024-08-10 04:31:30,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_13-model_00-model_states.pt.
g0235: [2024-08-10 04:31:30,630] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_16-model_00-model_states.pt.
g0234: [2024-08-10 04:31:30,632] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_03_model_states.pt...
g0235: [2024-08-10 04:31:30,632] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_04_model_states.pt...
g0225: [2024-08-10 04:31:30,679] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_07-model_00-model_states.pt.
g0225: [2024-08-10 04:31:30,681] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_01_model_states.pt...
g0233: [2024-08-10 04:31:30,724] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_10-model_00-model_states.pt.
g0233: [2024-08-10 04:31:30,725] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_02_model_states.pt...
g0220: [2024-08-10 04:31:30,768] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_03-model_00-model_states.pt.
g0220: [2024-08-10 04:31:30,792] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_04-model_00-model_states.pt...
g0220: [2024-08-10 04:31:30,954] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/layer_04-model_00-model_states.pt.
g0220: [2024-08-10 04:31:30,956] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_00_model_states.pt
g0220: [2024-08-10 04:31:30,956] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_00_model_states.pt...
g0238: [2024-08-10 04:31:32,716] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_07_model_states.pt.
g0238: [2024-08-10 04:31:32,717] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0237: [2024-08-10 04:31:32,940] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_06_model_states.pt.
g0237: [2024-08-10 04:31:32,941] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0235: [2024-08-10 04:31:33,013] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_04_model_states.pt.
g0225: [2024-08-10 04:31:33,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_01_model_states.pt.
g0235: [2024-08-10 04:31:33,014] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0225: [2024-08-10 04:31:33,014] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0234: [2024-08-10 04:31:33,055] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_03_model_states.pt.
g0234: [2024-08-10 04:31:33,055] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0233: [2024-08-10 04:31:33,228] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_02_model_states.pt.
g0233: [2024-08-10 04:31:33,229] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0236: [2024-08-10 04:31:33,676] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_05_model_states.pt.
g0236: [2024-08-10 04:31:33,677] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0220: [2024-08-10 04:31:34,688] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step19000/mp_rank_00_model_states.pt.
g0220: [2024-08-10 04:31:34,689] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19000 is ready now!
g0220:   successfully saved checkpoint at iteration   19000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 4.87, Latency(second): 4.629
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (4628.56, 4628.73)
g0220: [2024-08-10 04:32:17,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=19010, skipped=26, lr=[0.00019997909973945335, 0.00019997909973945335], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19010 loss: 0.8508 iter time (s): 4.246 samples/sec: 30.145
g0238:  iteration    19010/10000000 | consumed samples:      2433280 | consumed tokens:   4983357440 | elapsed time per iteration (ms): 44005.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.396033E-01 | loss scale: 32768.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.909 | tokens per gpu per second (tgs): 186.160 | TFLOPs: 1.50 |
g0220: [2024-08-10 04:32:58,854] [INFO] [logging.py:96:log_dist] [Rank 0] step=19020, skipped=26, lr=[0.00019997904444914443, 0.00019997904444914443], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19020 loss: 0.8414 iter time (s): 4.109 samples/sec: 31.154
g0238:  iteration    19020/10000000 | consumed samples:      2434560 | consumed tokens:   4985978880 | elapsed time per iteration (ms): 4141.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.309248E-01 | loss scale: 32768.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.908 | tokens per gpu per second (tgs): 1978.125 | TFLOPs: 15.92 |
g0220: [2024-08-10 04:33:39,266] [INFO] [logging.py:96:log_dist] [Rank 0] step=19030, skipped=26, lr=[0.00019997898908580665, 0.00019997898908580665], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19030 loss: 0.8280 iter time (s): 4.008 samples/sec: 31.936
g0238:  iteration    19030/10000000 | consumed samples:      2435840 | consumed tokens:   4988600320 | elapsed time per iteration (ms): 4041.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.491041E-01 | loss scale: 32768.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.674 | tokens per gpu per second (tgs): 2027.167 | TFLOPs: 16.31 |
g0220: [2024-08-10 04:34:20,861] [INFO] [logging.py:96:log_dist] [Rank 0] step=19040, skipped=26, lr=[0.00019997893364944006, 0.00019997893364944006], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19040 loss: 0.8607 iter time (s): 4.127 samples/sec: 31.016
g0238:  iteration    19040/10000000 | consumed samples:      2437120 | consumed tokens:   4991221760 | elapsed time per iteration (ms): 4159.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.432619E-01 | loss scale: 32768.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.772 | tokens per gpu per second (tgs): 1969.431 | TFLOPs: 15.85 |
g0220: [2024-08-10 04:35:03,572] [INFO] [logging.py:96:log_dist] [Rank 0] step=19050, skipped=26, lr=[0.0001999788781400447, 0.0001999788781400447], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19050 loss: 0.8333 iter time (s): 4.238 samples/sec: 30.202
g0238:  iteration    19050/10000000 | consumed samples:      2438400 | consumed tokens:   4993843200 | elapsed time per iteration (ms): 4271.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.400123E-01 | loss scale: 32768.0 | grad norm: 0.302 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.969 | tokens per gpu per second (tgs): 1918.021 | TFLOPs: 15.43 |
g0220: [2024-08-10 04:35:44,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=19060, skipped=26, lr=[0.00019997882255762058, 0.00019997882255762058], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19060 loss: 0.8646 iter time (s): 4.069 samples/sec: 31.457
g0238:  iteration    19060/10000000 | consumed samples:      2439680 | consumed tokens:   4996464640 | elapsed time per iteration (ms): 4102.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.422903E-01 | loss scale: 32768.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.203 | tokens per gpu per second (tgs): 1996.997 | TFLOPs: 16.07 |
g0220: [2024-08-10 04:36:24,728] [INFO] [logging.py:96:log_dist] [Rank 0] step=19070, skipped=26, lr=[0.00019997876690216782, 0.00019997876690216782], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19070 loss: 0.8355 iter time (s): 3.981 samples/sec: 32.154
g0238:  iteration    19070/10000000 | consumed samples:      2440960 | consumed tokens:   4999086080 | elapsed time per iteration (ms): 4013.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.422313E-01 | loss scale: 32768.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.893 | tokens per gpu per second (tgs): 2041.164 | TFLOPs: 16.43 |
g0220: [2024-08-10 04:37:06,090] [INFO] [logging.py:96:log_dist] [Rank 0] step=19080, skipped=26, lr=[0.0001999787111736864, 0.0001999787111736864], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19080 loss: 0.8454 iter time (s): 4.104 samples/sec: 31.192
g0238:  iteration    19080/10000000 | consumed samples:      2442240 | consumed tokens:   5001707520 | elapsed time per iteration (ms): 4136.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.378698E-01 | loss scale: 32768.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.947 | tokens per gpu per second (tgs): 1980.580 | TFLOPs: 15.94 |
g0220: [2024-08-10 04:37:47,518] [INFO] [logging.py:96:log_dist] [Rank 0] step=19090, skipped=26, lr=[0.00019997865537217636, 0.00019997865537217636], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19090 loss: 0.8551 iter time (s): 4.110 samples/sec: 31.145
g0238:  iteration    19090/10000000 | consumed samples:      2443520 | consumed tokens:   5004328960 | elapsed time per iteration (ms): 4142.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.333805E-01 | loss scale: 32768.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.897 | tokens per gpu per second (tgs): 1977.406 | TFLOPs: 15.91 |
g0220: [2024-08-10 04:38:27,421] [INFO] [logging.py:96:log_dist] [Rank 0] step=19100, skipped=26, lr=[0.0001999785994976378, 0.0001999785994976378], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19100 loss: 0.8515 iter time (s): 3.958 samples/sec: 32.343
g0238:  iteration    19100/10000000 | consumed samples:      2444800 | consumed tokens:   5006950400 | elapsed time per iteration (ms): 3990.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.360050E-01 | loss scale: 32768.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.078 | tokens per gpu per second (tgs): 2052.988 | TFLOPs: 16.52 |
g0220: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 04:38:48,263] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 04:38:48,263] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 04:38:48,262] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 04:38:48,263] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 04:38:48,263] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 04:38:48,263] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 04:38:48,263] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 04:38:48,263] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 04:38:48,263] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 04:39:07,154] [INFO] [logging.py:96:log_dist] [Rank 0] step=19110, skipped=26, lr=[0.00019997854355007072, 0.00019997854355007072], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19110 loss: 0.8360 iter time (s): 3.941 samples/sec: 32.482
g0238:  iteration    19110/10000000 | consumed samples:      2446080 | consumed tokens:   5009571840 | elapsed time per iteration (ms): 3973.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.462829E-01 | loss scale: 65536.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.213 | tokens per gpu per second (tgs): 2061.607 | TFLOPs: 16.59 |
g0220: [2024-08-10 04:39:47,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=19120, skipped=26, lr=[0.00019997848752947517, 0.00019997848752947517], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19120 loss: 0.8471 iter time (s): 4.034 samples/sec: 31.728
g0238:  iteration    19120/10000000 | consumed samples:      2447360 | consumed tokens:   5012193280 | elapsed time per iteration (ms): 4067.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.492258E-01 | loss scale: 65536.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.467 | tokens per gpu per second (tgs): 2013.870 | TFLOPs: 16.21 |
g0220: [2024-08-10 04:40:28,715] [INFO] [logging.py:96:log_dist] [Rank 0] step=19130, skipped=26, lr=[0.00019997843143585117, 0.00019997843143585117], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19130 loss: 0.8501 iter time (s): 4.055 samples/sec: 31.563
g0238:  iteration    19130/10000000 | consumed samples:      2448640 | consumed tokens:   5014814720 | elapsed time per iteration (ms): 4088.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.406885E-01 | loss scale: 65536.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.311 | tokens per gpu per second (tgs): 2003.892 | TFLOPs: 16.13 |
g0220: [2024-08-10 04:41:11,028] [INFO] [logging.py:96:log_dist] [Rank 0] step=19140, skipped=26, lr=[0.00019997837526919882, 0.00019997837526919882], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19140 loss: 0.8660 iter time (s): 4.198 samples/sec: 30.492
g0238:  iteration    19140/10000000 | consumed samples:      2449920 | consumed tokens:   5017436160 | elapsed time per iteration (ms): 4231.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.424927E-01 | loss scale: 65536.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.251 | tokens per gpu per second (tgs): 1936.083 | TFLOPs: 15.58 |
g0220: [2024-08-10 04:41:50,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=19150, skipped=26, lr=[0.00019997831902951812, 0.00019997831902951812], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19150 loss: 0.8712 iter time (s): 3.964 samples/sec: 32.291
g0238:  iteration    19150/10000000 | consumed samples:      2451200 | consumed tokens:   5020057600 | elapsed time per iteration (ms): 3996.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.488310E-01 | loss scale: 65536.0 | grad norm: 0.230 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.025 | tokens per gpu per second (tgs): 2049.573 | TFLOPs: 16.49 |
g0220: [2024-08-10 04:42:33,671] [INFO] [logging.py:96:log_dist] [Rank 0] step=19160, skipped=26, lr=[0.00019997826271680912, 0.00019997826271680912], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19160 loss: 0.8319 iter time (s): 4.234 samples/sec: 30.229
g0238:  iteration    19160/10000000 | consumed samples:      2452480 | consumed tokens:   5022679040 | elapsed time per iteration (ms): 4267.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.428555E-01 | loss scale: 65536.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.996 | tokens per gpu per second (tgs): 1919.717 | TFLOPs: 15.45 |
g0220: [2024-08-10 04:43:15,046] [INFO] [logging.py:96:log_dist] [Rank 0] step=19170, skipped=26, lr=[0.00019997820633107187, 0.00019997820633107187], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19170 loss: 0.8063 iter time (s): 4.105 samples/sec: 31.183
g0238:  iteration    19170/10000000 | consumed samples:      2453760 | consumed tokens:   5025300480 | elapsed time per iteration (ms): 4137.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.338508E-01 | loss scale: 65536.0 | grad norm: 0.237 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.937 | tokens per gpu per second (tgs): 1979.955 | TFLOPs: 15.93 |
g0220: [2024-08-10 04:43:55,288] [INFO] [logging.py:96:log_dist] [Rank 0] step=19180, skipped=26, lr=[0.0001999781498723064, 0.0001999781498723064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19180 loss: 0.8236 iter time (s): 3.979 samples/sec: 32.172
g0238:  iteration    19180/10000000 | consumed samples:      2455040 | consumed tokens:   5027921920 | elapsed time per iteration (ms): 4024.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.390674E-01 | loss scale: 65536.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.805 | tokens per gpu per second (tgs): 2035.545 | TFLOPs: 16.38 |
g0220: [2024-08-10 04:44:35,137] [INFO] [logging.py:96:log_dist] [Rank 0] step=19190, skipped=26, lr=[0.00019997809334051277, 0.00019997809334051277], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19190 loss: 0.8373 iter time (s): 3.951 samples/sec: 32.394
g0238:  iteration    19190/10000000 | consumed samples:      2456320 | consumed tokens:   5030543360 | elapsed time per iteration (ms): 3984.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.375255E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.122 | tokens per gpu per second (tgs): 2055.812 | TFLOPs: 16.54 |
g0220: [2024-08-10 04:45:15,496] [INFO] [logging.py:96:log_dist] [Rank 0] step=19200, skipped=26, lr=[0.00019997803673569103, 0.00019997803673569103], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19200 loss: 0.8343 iter time (s): 4.003 samples/sec: 31.973
g0238:  iteration    19200/10000000 | consumed samples:      2457600 | consumed tokens:   5033164800 | elapsed time per iteration (ms): 4035.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.393507E-01 | loss scale: 65536.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.716 | tokens per gpu per second (tgs): 2029.842 | TFLOPs: 16.33 |
g0220: [2024-08-10 04:45:56,240] [INFO] [logging.py:96:log_dist] [Rank 0] step=19210, skipped=26, lr=[0.0001999779800578412, 0.0001999779800578412], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19210 loss: 0.8178 iter time (s): 4.042 samples/sec: 31.669
g0238:  iteration    19210/10000000 | consumed samples:      2458880 | consumed tokens:   5035786240 | elapsed time per iteration (ms): 4074.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.193224E-01 | loss scale: 65536.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.416 | tokens per gpu per second (tgs): 2010.623 | TFLOPs: 16.18 |
g0220: [2024-08-10 04:46:38,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=19220, skipped=26, lr=[0.00019997792330696333, 0.00019997792330696333], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19220 loss: 0.7911 iter time (s): 4.183 samples/sec: 30.603
g0238:  iteration    19220/10000000 | consumed samples:      2460160 | consumed tokens:   5038407680 | elapsed time per iteration (ms): 4215.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.306354E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.363 | tokens per gpu per second (tgs): 1943.245 | TFLOPs: 15.64 |
g0220: [2024-08-10 04:47:18,869] [INFO] [logging.py:96:log_dist] [Rank 0] step=19230, skipped=26, lr=[0.00019997786648305748, 0.00019997786648305748], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19230 loss: 0.8494 iter time (s): 4.014 samples/sec: 31.888
g0238:  iteration    19230/10000000 | consumed samples:      2461440 | consumed tokens:   5041029120 | elapsed time per iteration (ms): 4047.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.391823E-01 | loss scale: 65536.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.626 | tokens per gpu per second (tgs): 2024.054 | TFLOPs: 16.29 |
g0220: [2024-08-10 04:47:59,497] [INFO] [logging.py:96:log_dist] [Rank 0] step=19240, skipped=26, lr=[0.0001999778095861237, 0.0001999778095861237], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19240 loss: 0.8335 iter time (s): 4.030 samples/sec: 31.764
g0238:  iteration    19240/10000000 | consumed samples:      2462720 | consumed tokens:   5043650560 | elapsed time per iteration (ms): 4062.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.385462E-01 | loss scale: 65536.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.508 | tokens per gpu per second (tgs): 2016.491 | TFLOPs: 16.23 |
g0220: [2024-08-10 04:48:39,205] [INFO] [logging.py:96:log_dist] [Rank 0] step=19250, skipped=26, lr=[0.00019997775261616196, 0.00019997775261616196], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19250 loss: 0.8730 iter time (s): 3.938 samples/sec: 32.501
g0238:  iteration    19250/10000000 | consumed samples:      2464000 | consumed tokens:   5046272000 | elapsed time per iteration (ms): 3971.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.450747E-01 | loss scale: 65536.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.233 | tokens per gpu per second (tgs): 2062.932 | TFLOPs: 16.60 |
g0220: [2024-08-10 04:49:20,513] [INFO] [logging.py:96:log_dist] [Rank 0] step=19260, skipped=26, lr=[0.00019997769557317238, 0.00019997769557317238], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19260 loss: 0.8480 iter time (s): 4.098 samples/sec: 31.237
g0238:  iteration    19260/10000000 | consumed samples:      2465280 | consumed tokens:   5048893440 | elapsed time per iteration (ms): 4130.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.399555E-01 | loss scale: 65536.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.987 | tokens per gpu per second (tgs): 1983.193 | TFLOPs: 15.96 |
g0220: [2024-08-10 04:50:00,602] [INFO] [logging.py:96:log_dist] [Rank 0] step=19270, skipped=26, lr=[0.000199977638457155, 0.000199977638457155], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19270 loss: 0.8370 iter time (s): 3.976 samples/sec: 32.190
g0238:  iteration    19270/10000000 | consumed samples:      2466560 | consumed tokens:   5051514880 | elapsed time per iteration (ms): 4008.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.393567E-01 | loss scale: 65536.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.929 | tokens per gpu per second (tgs): 2043.477 | TFLOPs: 16.44 |
g0220: [2024-08-10 04:50:41,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=19280, skipped=26, lr=[0.00019997758126810984, 0.00019997758126810984], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19280 loss: 0.8400 iter time (s): 4.077 samples/sec: 31.396
g0238:  iteration    19280/10000000 | consumed samples:      2467840 | consumed tokens:   5054136320 | elapsed time per iteration (ms): 4109.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.387128E-01 | loss scale: 65536.0 | grad norm: 0.231 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.147 | tokens per gpu per second (tgs): 1993.389 | TFLOPs: 16.04 |
g0220: [2024-08-10 04:51:22,870] [INFO] [logging.py:96:log_dist] [Rank 0] step=19290, skipped=26, lr=[0.00019997752400603694, 0.00019997752400603694], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19290 loss: 0.8660 iter time (s): 4.085 samples/sec: 31.335
g0238:  iteration    19290/10000000 | consumed samples:      2469120 | consumed tokens:   5056757760 | elapsed time per iteration (ms): 4117.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.369961E-01 | loss scale: 65536.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.089 | tokens per gpu per second (tgs): 1989.668 | TFLOPs: 16.01 |
g0220: [2024-08-10 04:52:04,661] [INFO] [logging.py:96:log_dist] [Rank 0] step=19300, skipped=26, lr=[0.00019997746667093638, 0.00019997746667093638], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19300 loss: 0.8279 iter time (s): 4.147 samples/sec: 30.869
g0238:  iteration    19300/10000000 | consumed samples:      2470400 | consumed tokens:   5059379200 | elapsed time per iteration (ms): 4179.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.338293E-01 | loss scale: 65536.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.628 | tokens per gpu per second (tgs): 1960.198 | TFLOPs: 15.77 |
g0220: [2024-08-10 04:52:44,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=19310, skipped=26, lr=[0.00019997740926280816, 0.00019997740926280816], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19310 loss: 0.8233 iter time (s): 3.939 samples/sec: 32.492
g0238:  iteration    19310/10000000 | consumed samples:      2471680 | consumed tokens:   5062000640 | elapsed time per iteration (ms): 3972.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.370411E-01 | loss scale: 65536.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.221 | tokens per gpu per second (tgs): 2062.160 | TFLOPs: 16.59 |
g0220: [2024-08-10 04:53:25,633] [INFO] [logging.py:96:log_dist] [Rank 0] step=19320, skipped=26, lr=[0.00019997735178165236, 0.00019997735178165236], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19320 loss: 0.8156 iter time (s): 4.091 samples/sec: 31.288
g0238:  iteration    19320/10000000 | consumed samples:      2472960 | consumed tokens:   5064622080 | elapsed time per iteration (ms): 4124.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.326530E-01 | loss scale: 65536.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.032 | tokens per gpu per second (tgs): 1986.071 | TFLOPs: 15.98 |
g0220: [2024-08-10 04:54:06,350] [INFO] [logging.py:96:log_dist] [Rank 0] step=19330, skipped=26, lr=[0.000199977294227469, 0.000199977294227469], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19330 loss: 0.8298 iter time (s): 4.039 samples/sec: 31.695
g0238:  iteration    19330/10000000 | consumed samples:      2474240 | consumed tokens:   5067243520 | elapsed time per iteration (ms): 4071.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.425528E-01 | loss scale: 65536.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.436 | tokens per gpu per second (tgs): 2011.926 | TFLOPs: 16.19 |
g0220: [2024-08-10 04:54:47,520] [INFO] [logging.py:96:log_dist] [Rank 0] step=19340, skipped=26, lr=[0.00019997723660025808, 0.00019997723660025808], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19340 loss: 0.8131 iter time (s): 4.083 samples/sec: 31.349
g0238:  iteration    19340/10000000 | consumed samples:      2475520 | consumed tokens:   5069864960 | elapsed time per iteration (ms): 4116.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.419545E-01 | loss scale: 65536.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.093 | tokens per gpu per second (tgs): 1989.932 | TFLOPs: 16.01 |
g0220: [2024-08-10 04:55:29,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=19350, skipped=26, lr=[0.00019997717890001977, 0.00019997717890001977], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19350 loss: 0.8221 iter time (s): 4.191 samples/sec: 30.541
g0238:  iteration    19350/10000000 | consumed samples:      2476800 | consumed tokens:   5072486400 | elapsed time per iteration (ms): 4224.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.383317E-01 | loss scale: 65536.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.302 | tokens per gpu per second (tgs): 1939.338 | TFLOPs: 15.61 |
g0220: [2024-08-10 04:56:12,216] [INFO] [logging.py:96:log_dist] [Rank 0] step=19360, skipped=26, lr=[0.000199977121126754, 0.000199977121126754], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19360 loss: 0.8013 iter time (s): 4.213 samples/sec: 30.385
g0238:  iteration    19360/10000000 | consumed samples:      2478080 | consumed tokens:   5075107840 | elapsed time per iteration (ms): 4245.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.309287E-01 | loss scale: 65536.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.150 | tokens per gpu per second (tgs): 1929.601 | TFLOPs: 15.53 |
g0220: [2024-08-10 04:56:51,915] [INFO] [logging.py:96:log_dist] [Rank 0] step=19370, skipped=26, lr=[0.00019997706328046087, 0.00019997706328046087], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19370 loss: 0.8102 iter time (s): 3.937 samples/sec: 32.515
g0238:  iteration    19370/10000000 | consumed samples:      2479360 | consumed tokens:   5077729280 | elapsed time per iteration (ms): 3970.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.207409E-01 | loss scale: 65536.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.238 | tokens per gpu per second (tgs): 2063.208 | TFLOPs: 16.60 |
g0220: [2024-08-10 04:57:33,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=19380, skipped=26, lr=[0.00019997700536114043, 0.00019997700536114043], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19380 loss: 0.8371 iter time (s): 4.171 samples/sec: 30.690
g0238:  iteration    19380/10000000 | consumed samples:      2480640 | consumed tokens:   5080350720 | elapsed time per iteration (ms): 4203.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.472721E-01 | loss scale: 65536.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.452 | tokens per gpu per second (tgs): 1948.934 | TFLOPs: 15.68 |
g0220: [2024-08-10 04:58:15,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=19390, skipped=26, lr=[0.00019997694736879266, 0.00019997694736879266], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19390 loss: 0.8219 iter time (s): 4.101 samples/sec: 31.211
g0238:  iteration    19390/10000000 | consumed samples:      2481920 | consumed tokens:   5082972160 | elapsed time per iteration (ms): 4133.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.321598E-01 | loss scale: 65536.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.965 | tokens per gpu per second (tgs): 1981.732 | TFLOPs: 15.95 |
g0220: [2024-08-10 04:58:57,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=19400, skipped=26, lr=[0.00019997688930341765, 0.00019997688930341765], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19400 loss: 0.8573 iter time (s): 4.164 samples/sec: 30.741
g0238:  iteration    19400/10000000 | consumed samples:      2483200 | consumed tokens:   5085593600 | elapsed time per iteration (ms): 4196.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.409671E-01 | loss scale: 65536.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.500 | tokens per gpu per second (tgs): 1952.008 | TFLOPs: 15.71 |
g0220: [2024-08-10 04:59:36,102] [INFO] [logging.py:96:log_dist] [Rank 0] step=19410, skipped=26, lr=[0.00019997683116501546, 0.00019997683116501546], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19410 loss: 0.8497 iter time (s): 3.852 samples/sec: 33.233
g0238:  iteration    19410/10000000 | consumed samples:      2484480 | consumed tokens:   5088215040 | elapsed time per iteration (ms): 3884.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.400187E-01 | loss scale: 65536.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.954 | tokens per gpu per second (tgs): 2109.039 | TFLOPs: 16.97 |
g0220: [2024-08-10 05:00:15,832] [INFO] [logging.py:96:log_dist] [Rank 0] step=19420, skipped=26, lr=[0.00019997677295358613, 0.00019997677295358613], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19420 loss: 0.8468 iter time (s): 3.940 samples/sec: 32.485
g0238:  iteration    19420/10000000 | consumed samples:      2485760 | consumed tokens:   5090836480 | elapsed time per iteration (ms): 3973.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.292737E-01 | loss scale: 65536.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.218 | tokens per gpu per second (tgs): 2061.937 | TFLOPs: 16.59 |
g0220: [2024-08-10 05:00:57,948] [INFO] [logging.py:96:log_dist] [Rank 0] step=19430, skipped=26, lr=[0.00019997671466912969, 0.00019997671466912969], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19430 loss: 0.8403 iter time (s): 4.179 samples/sec: 30.630
g0238:  iteration    19430/10000000 | consumed samples:      2487040 | consumed tokens:   5093457920 | elapsed time per iteration (ms): 4211.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.326653E-01 | loss scale: 65536.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.392 | tokens per gpu per second (tgs): 1945.100 | TFLOPs: 15.65 |
g0220: [2024-08-10 05:01:39,237] [INFO] [logging.py:96:log_dist] [Rank 0] step=19440, skipped=26, lr=[0.00019997665631164615, 0.00019997665631164615], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19440 loss: 0.8221 iter time (s): 4.096 samples/sec: 31.248
g0238:  iteration    19440/10000000 | consumed samples:      2488320 | consumed tokens:   5096079360 | elapsed time per iteration (ms): 4129.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.422491E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.000 | tokens per gpu per second (tgs): 1984.022 | TFLOPs: 15.97 |
g0220: [2024-08-10 05:02:20,807] [INFO] [logging.py:96:log_dist] [Rank 0] step=19450, skipped=26, lr=[0.00019997659788113564, 0.00019997659788113564], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19450 loss: 0.8101 iter time (s): 4.124 samples/sec: 31.038
g0238:  iteration    19450/10000000 | consumed samples:      2489600 | consumed tokens:   5098700800 | elapsed time per iteration (ms): 4156.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.301604E-01 | loss scale: 65536.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.793 | tokens per gpu per second (tgs): 1970.733 | TFLOPs: 15.86 |
g0220: [2024-08-10 05:03:02,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=19460, skipped=26, lr=[0.00019997653937759815, 0.00019997653937759815], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19460 loss: 0.8126 iter time (s): 4.186 samples/sec: 30.576
g0238:  iteration    19460/10000000 | consumed samples:      2490880 | consumed tokens:   5101322240 | elapsed time per iteration (ms): 4218.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.303572E-01 | loss scale: 65536.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.341 | tokens per gpu per second (tgs): 1941.815 | TFLOPs: 15.63 |
g0220: [2024-08-10 05:03:44,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=19470, skipped=26, lr=[0.0001999764808010337, 0.0001999764808010337], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19470 loss: 0.8622 iter time (s): 4.146 samples/sec: 30.872
g0238:  iteration    19470/10000000 | consumed samples:      2492160 | consumed tokens:   5103943680 | elapsed time per iteration (ms): 4181.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.318873E-01 | loss scale: 65536.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.614 | tokens per gpu per second (tgs): 1959.294 | TFLOPs: 15.77 |
g0220: [2024-08-10 05:04:25,308] [INFO] [logging.py:96:log_dist] [Rank 0] step=19480, skipped=26, lr=[0.0001999764221514424, 0.0001999764221514424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19480 loss: 0.8442 iter time (s): 4.017 samples/sec: 31.862
g0238:  iteration    19480/10000000 | consumed samples:      2493440 | consumed tokens:   5106565120 | elapsed time per iteration (ms): 4050.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.373596E-01 | loss scale: 65536.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.603 | tokens per gpu per second (tgs): 2022.588 | TFLOPs: 16.28 |
g0220: [2024-08-10 05:05:06,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=19490, skipped=26, lr=[0.00019997636342882424, 0.00019997636342882424], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19490 loss: 0.8355 iter time (s): 4.071 samples/sec: 31.444
g0238:  iteration    19490/10000000 | consumed samples:      2494720 | consumed tokens:   5109186560 | elapsed time per iteration (ms): 4103.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.446946E-01 | loss scale: 65536.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.197 | tokens per gpu per second (tgs): 1996.592 | TFLOPs: 16.07 |
g0220: [2024-08-10 05:05:47,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=19500, skipped=26, lr=[0.0001999763046331793, 0.0001999763046331793], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19500 loss: 0.8851 iter time (s): 4.132 samples/sec: 30.978
g0238:  iteration    19500/10000000 | consumed samples:      2496000 | consumed tokens:   5111808000 | elapsed time per iteration (ms): 4164.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.303882E-01 | loss scale: 65536.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.736 | tokens per gpu per second (tgs): 1967.111 | TFLOPs: 15.83 |
g0220: [2024-08-10 05:06:30,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=19510, skipped=26, lr=[0.0001999762457645076, 0.0001999762457645076], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19510 loss: 0.8253 iter time (s): 4.197 samples/sec: 30.497
g0238:  iteration    19510/10000000 | consumed samples:      2497280 | consumed tokens:   5114429440 | elapsed time per iteration (ms): 4230.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.366917E-01 | loss scale: 65536.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.260 | tokens per gpu per second (tgs): 1936.623 | TFLOPs: 15.58 |
g0220: [2024-08-10 05:07:10,663] [INFO] [logging.py:96:log_dist] [Rank 0] step=19520, skipped=26, lr=[0.0001999761868228092, 0.0001999761868228092], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19520 loss: 0.8516 iter time (s): 4.005 samples/sec: 31.958
g0238:  iteration    19520/10000000 | consumed samples:      2498560 | consumed tokens:   5117050880 | elapsed time per iteration (ms): 4037.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.330029E-01 | loss scale: 65536.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.699 | tokens per gpu per second (tgs): 2028.761 | TFLOPs: 16.33 |
g0220: [2024-08-10 05:07:50,591] [INFO] [logging.py:96:log_dist] [Rank 0] step=19530, skipped=26, lr=[0.00019997612780808414, 0.00019997612780808414], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19530 loss: 0.8221 iter time (s): 3.960 samples/sec: 32.321
g0238:  iteration    19530/10000000 | consumed samples:      2499840 | consumed tokens:   5119672320 | elapsed time per iteration (ms): 3992.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.248312E-01 | loss scale: 65536.0 | grad norm: 0.207 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.058 | tokens per gpu per second (tgs): 2051.680 | TFLOPs: 16.51 |
g0220: [2024-08-10 05:08:31,173] [INFO] [logging.py:96:log_dist] [Rank 0] step=19540, skipped=26, lr=[0.0001999760687203325, 0.0001999760687203325], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19540 loss: 0.8025 iter time (s): 4.026 samples/sec: 31.796
g0238:  iteration    19540/10000000 | consumed samples:      2501120 | consumed tokens:   5122293760 | elapsed time per iteration (ms): 4058.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.284576E-01 | loss scale: 65536.0 | grad norm: 0.218 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.542 | tokens per gpu per second (tgs): 2018.663 | TFLOPs: 16.24 |
g0220: [2024-08-10 05:09:12,569] [INFO] [logging.py:96:log_dist] [Rank 0] step=19550, skipped=26, lr=[0.00019997600955955425, 0.00019997600955955425], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19550 loss: 0.8225 iter time (s): 4.107 samples/sec: 31.164
g0238:  iteration    19550/10000000 | consumed samples:      2502400 | consumed tokens:   5124915200 | elapsed time per iteration (ms): 4139.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.287147E-01 | loss scale: 65536.0 | grad norm: 0.204 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.921 | tokens per gpu per second (tgs): 1978.956 | TFLOPs: 15.92 |
g0220: [2024-08-10 05:09:55,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=19560, skipped=26, lr=[0.00019997595032574949, 0.00019997595032574949], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19560 loss: 0.8539 iter time (s): 4.221 samples/sec: 30.328
g0238:  iteration    19560/10000000 | consumed samples:      2503680 | consumed tokens:   5127536640 | elapsed time per iteration (ms): 4253.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.313807E-01 | loss scale: 65536.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.097 | tokens per gpu per second (tgs): 1926.181 | TFLOPs: 15.50 |
g0220: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 19563
g0220: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 19563
g0220: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 19563
g0225: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 19563
g0220: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 19563
g0225: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 19563
g0220: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 19563
g0235: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 19563
g0234: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 19563
g0233: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 19563
g0233: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 19563
g0234: Grad overflow on iteration 19563
g0220: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: Grad overflow on iteration 19563
g0234: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 19563
g0235: Grad overflow on iteration 19563
g0233: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 19563
g0234: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 19563
g0237: Grad overflow on iteration 19563
g0234: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 19563
g0237: Grad overflow on iteration 19563
g0238: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 19563
g0234: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: Grad overflow on iteration 19563
g0237: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: Grad overflow on iteration 19563
g0237: Grad overflow on iteration 19563
g0235: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 19563
g0237: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 05:10:12,392] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0236: Grad overflow on iteration 19563
g0234: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 19563
g0220: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 19563
g0236: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: Grad overflow on iteration 19563
g0236: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: Grad overflow on iteration 19563
g0238: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 05:10:12,391] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 19563
g0238: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 19563
g0236: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 05:10:12,392] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 05:10:37,532] [INFO] [logging.py:96:log_dist] [Rank 0] step=19570, skipped=27, lr=[0.00019997589101891826, 0.00019997589101891826], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19570 loss: 0.8228 iter time (s): 4.211 samples/sec: 30.400
g0238:  iteration    19570/10000000 | consumed samples:      2504960 | consumed tokens:   5130158080 | elapsed time per iteration (ms): 4243.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.348927E-01 | loss scale: 32768.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.166 | tokens per gpu per second (tgs): 1930.596 | TFLOPs: 15.54 |
g0220: [2024-08-10 05:11:18,749] [INFO] [logging.py:96:log_dist] [Rank 0] step=19580, skipped=27, lr=[0.00019997583163906062, 0.00019997583163906062], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19580 loss: 0.8329 iter time (s): 4.089 samples/sec: 31.305
g0238:  iteration    19580/10000000 | consumed samples:      2506240 | consumed tokens:   5132779520 | elapsed time per iteration (ms): 4121.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.386089E-01 | loss scale: 32768.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.055 | tokens per gpu per second (tgs): 1987.501 | TFLOPs: 15.99 |
g0220: [2024-08-10 05:11:59,765] [INFO] [logging.py:96:log_dist] [Rank 0] step=19590, skipped=27, lr=[0.0001999757721861766, 0.0001999757721861766], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19590 loss: 0.8326 iter time (s): 4.069 samples/sec: 31.457
g0238:  iteration    19590/10000000 | consumed samples:      2507520 | consumed tokens:   5135400960 | elapsed time per iteration (ms): 4101.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.360836E-01 | loss scale: 32768.0 | grad norm: 0.311 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.207 | tokens per gpu per second (tgs): 1997.241 | TFLOPs: 16.07 |
g0220: [2024-08-10 05:12:40,708] [INFO] [logging.py:96:log_dist] [Rank 0] step=19600, skipped=27, lr=[0.0001999757126602662, 0.0001999757126602662], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19600 loss: 0.8342 iter time (s): 4.061 samples/sec: 31.516
g0238:  iteration    19600/10000000 | consumed samples:      2508800 | consumed tokens:   5138022400 | elapsed time per iteration (ms): 4094.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.329306E-01 | loss scale: 32768.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.263 | tokens per gpu per second (tgs): 2000.806 | TFLOPs: 16.10 |
g0220: [2024-08-10 05:13:22,223] [INFO] [logging.py:96:log_dist] [Rank 0] step=19610, skipped=27, lr=[0.00019997565306132954, 0.00019997565306132954], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19610 loss: 0.8459 iter time (s): 4.119 samples/sec: 31.077
g0238:  iteration    19610/10000000 | consumed samples:      2510080 | consumed tokens:   5140643840 | elapsed time per iteration (ms): 4151.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.442020E-01 | loss scale: 32768.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.832 | tokens per gpu per second (tgs): 1973.254 | TFLOPs: 15.88 |
g0220: [2024-08-10 05:14:04,260] [INFO] [logging.py:96:log_dist] [Rank 0] step=19620, skipped=27, lr=[0.00019997559338936664, 0.00019997559338936664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19620 loss: 0.8578 iter time (s): 4.171 samples/sec: 30.689
g0238:  iteration    19620/10000000 | consumed samples:      2511360 | consumed tokens:   5143265280 | elapsed time per iteration (ms): 4205.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.332971E-01 | loss scale: 32768.0 | grad norm: 0.243 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.440 | tokens per gpu per second (tgs): 1948.166 | TFLOPs: 15.68 |
g0220: [2024-08-10 05:14:45,024] [INFO] [logging.py:96:log_dist] [Rank 0] step=19630, skipped=27, lr=[0.0001999755336443775, 0.0001999755336443775], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19630 loss: 0.8418 iter time (s): 4.042 samples/sec: 31.667
g0238:  iteration    19630/10000000 | consumed samples:      2512640 | consumed tokens:   5145886720 | elapsed time per iteration (ms): 4074.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.245487E-01 | loss scale: 32768.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.412 | tokens per gpu per second (tgs): 2010.375 | TFLOPs: 16.18 |
g0220: [2024-08-10 05:15:26,647] [INFO] [logging.py:96:log_dist] [Rank 0] step=19640, skipped=27, lr=[0.00019997547382636223, 0.00019997547382636223], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19640 loss: 0.8586 iter time (s): 4.130 samples/sec: 30.994
g0238:  iteration    19640/10000000 | consumed samples:      2513920 | consumed tokens:   5148508160 | elapsed time per iteration (ms): 4162.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.323749E-01 | loss scale: 32768.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.753 | tokens per gpu per second (tgs): 1968.177 | TFLOPs: 15.84 |
g0220: [2024-08-10 05:16:08,991] [INFO] [logging.py:96:log_dist] [Rank 0] step=19650, skipped=27, lr=[0.00019997541393532086, 0.00019997541393532086], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19650 loss: 0.8480 iter time (s): 4.199 samples/sec: 30.484
g0238:  iteration    19650/10000000 | consumed samples:      2515200 | consumed tokens:   5151129600 | elapsed time per iteration (ms): 4234.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.304728E-01 | loss scale: 32768.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.228 | tokens per gpu per second (tgs): 1934.601 | TFLOPs: 15.57 |
g0220: [2024-08-10 05:16:52,254] [INFO] [logging.py:96:log_dist] [Rank 0] step=19660, skipped=27, lr=[0.0001999753539712534, 0.0001999753539712534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19660 loss: 0.8410 iter time (s): 4.293 samples/sec: 29.819
g0238:  iteration    19660/10000000 | consumed samples:      2516480 | consumed tokens:   5153751040 | elapsed time per iteration (ms): 4326.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.391615E-01 | loss scale: 32768.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.587 | tokens per gpu per second (tgs): 1893.582 | TFLOPs: 15.24 |
g0220: [2024-08-10 05:17:32,717] [INFO] [logging.py:96:log_dist] [Rank 0] step=19670, skipped=27, lr=[0.00019997529393415994, 0.00019997529393415994], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19670 loss: 0.8155 iter time (s): 4.014 samples/sec: 31.892
g0238:  iteration    19670/10000000 | consumed samples:      2517760 | consumed tokens:   5156372480 | elapsed time per iteration (ms): 4046.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.350513E-01 | loss scale: 32768.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.634 | tokens per gpu per second (tgs): 2024.600 | TFLOPs: 16.29 |
g0220: [2024-08-10 05:18:14,461] [INFO] [logging.py:96:log_dist] [Rank 0] step=19680, skipped=27, lr=[0.00019997523382404053, 0.00019997523382404053], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19680 loss: 0.8036 iter time (s): 4.136 samples/sec: 30.951
g0238:  iteration    19680/10000000 | consumed samples:      2519040 | consumed tokens:   5158993920 | elapsed time per iteration (ms): 4174.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.235828E-01 | loss scale: 32768.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.660 | tokens per gpu per second (tgs): 1962.226 | TFLOPs: 15.79 |
g0220: [2024-08-10 05:18:58,094] [INFO] [logging.py:96:log_dist] [Rank 0] step=19690, skipped=27, lr=[0.00019997517364089517, 0.00019997517364089517], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19690 loss: 0.8342 iter time (s): 4.327 samples/sec: 29.581
g0238:  iteration    19690/10000000 | consumed samples:      2520320 | consumed tokens:   5161615360 | elapsed time per iteration (ms): 4363.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.339838E-01 | loss scale: 32768.0 | grad norm: 0.222 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.332 | tokens per gpu per second (tgs): 1877.227 | TFLOPs: 15.11 |
g0220: [2024-08-10 05:19:39,338] [INFO] [logging.py:96:log_dist] [Rank 0] step=19700, skipped=27, lr=[0.00019997511338472391, 0.00019997511338472391], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19700 loss: 0.8261 iter time (s): 4.086 samples/sec: 31.326
g0238:  iteration    19700/10000000 | consumed samples:      2521600 | consumed tokens:   5164236800 | elapsed time per iteration (ms): 4123.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.346731E-01 | loss scale: 32768.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.043 | tokens per gpu per second (tgs): 1986.755 | TFLOPs: 15.99 |
g0220: [2024-08-10 05:20:20,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=19710, skipped=27, lr=[0.00019997505305552687, 0.00019997505305552687], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19710 loss: 0.8281 iter time (s): 4.075 samples/sec: 31.408
g0238:  iteration    19710/10000000 | consumed samples:      2522880 | consumed tokens:   5166858240 | elapsed time per iteration (ms): 4109.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.313036E-01 | loss scale: 32768.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.145 | tokens per gpu per second (tgs): 1993.276 | TFLOPs: 16.04 |
g0220: [2024-08-10 05:21:03,238] [INFO] [logging.py:96:log_dist] [Rank 0] step=19720, skipped=27, lr=[0.00019997499265330402, 0.00019997499265330402], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19720 loss: 0.8109 iter time (s): 4.247 samples/sec: 30.137
g0238:  iteration    19720/10000000 | consumed samples:      2524160 | consumed tokens:   5169479680 | elapsed time per iteration (ms): 4280.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.379106E-01 | loss scale: 32768.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.906 | tokens per gpu per second (tgs): 1913.996 | TFLOPs: 15.40 |
g0220: [2024-08-10 05:21:44,302] [INFO] [logging.py:96:log_dist] [Rank 0] step=19730, skipped=27, lr=[0.00019997493217805543, 0.00019997493217805543], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19730 loss: 0.8169 iter time (s): 4.074 samples/sec: 31.420
g0238:  iteration    19730/10000000 | consumed samples:      2525440 | consumed tokens:   5172101120 | elapsed time per iteration (ms): 4106.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.369835E-01 | loss scale: 32768.0 | grad norm: 0.217 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.170 | tokens per gpu per second (tgs): 1994.910 | TFLOPs: 16.05 |
g0220: [2024-08-10 05:22:24,758] [INFO] [logging.py:96:log_dist] [Rank 0] step=19740, skipped=27, lr=[0.00019997487162978116, 0.00019997487162978116], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19740 loss: 0.8628 iter time (s): 4.013 samples/sec: 31.898
g0238:  iteration    19740/10000000 | consumed samples:      2526720 | consumed tokens:   5174722560 | elapsed time per iteration (ms): 4045.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.335470E-01 | loss scale: 32768.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.639 | tokens per gpu per second (tgs): 2024.884 | TFLOPs: 16.29 |
g0220: [2024-08-10 05:23:05,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=19750, skipped=27, lr=[0.00019997481100848122, 0.00019997481100848122], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19750 loss: 0.8416 iter time (s): 4.039 samples/sec: 31.694
g0238:  iteration    19750/10000000 | consumed samples:      2528000 | consumed tokens:   5177344000 | elapsed time per iteration (ms): 4071.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.372282E-01 | loss scale: 32768.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.436 | tokens per gpu per second (tgs): 2011.897 | TFLOPs: 16.19 |
g0220: [2024-08-10 05:23:46,434] [INFO] [logging.py:96:log_dist] [Rank 0] step=19760, skipped=27, lr=[0.0001999747503141557, 0.0001999747503141557], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19760 loss: 0.8240 iter time (s): 4.063 samples/sec: 31.501
g0238:  iteration    19760/10000000 | consumed samples:      2529280 | consumed tokens:   5179965440 | elapsed time per iteration (ms): 4095.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.329424E-01 | loss scale: 32768.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.252 | tokens per gpu per second (tgs): 2000.096 | TFLOPs: 16.10 |
g0220: [2024-08-10 05:24:27,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=19770, skipped=27, lr=[0.0001999746895468046, 0.0001999746895468046], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19770 loss: 0.8509 iter time (s): 4.096 samples/sec: 31.250
g0238:  iteration    19770/10000000 | consumed samples:      2530560 | consumed tokens:   5182586880 | elapsed time per iteration (ms): 4128.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.267705E-01 | loss scale: 32768.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.003 | tokens per gpu per second (tgs): 1984.162 | TFLOPs: 15.97 |
g0220: [2024-08-10 05:25:09,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=19780, skipped=27, lr=[0.00019997462870642802, 0.00019997462870642802], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19780 loss: 0.8399 iter time (s): 4.109 samples/sec: 31.153
g0238:  iteration    19780/10000000 | consumed samples:      2531840 | consumed tokens:   5185208320 | elapsed time per iteration (ms): 4141.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.215736E-01 | loss scale: 32768.0 | grad norm: 0.826 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.904 | tokens per gpu per second (tgs): 1977.835 | TFLOPs: 15.92 |
g0220: [2024-08-10 05:25:50,898] [INFO] [logging.py:96:log_dist] [Rank 0] step=19790, skipped=27, lr=[0.00019997456779302598, 0.00019997456779302598], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19790 loss: 0.8504 iter time (s): 4.143 samples/sec: 30.896
g0238:  iteration    19790/10000000 | consumed samples:      2533120 | consumed tokens:   5187829760 | elapsed time per iteration (ms): 4175.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.335336E-01 | loss scale: 32768.0 | grad norm: 0.346 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.654 | tokens per gpu per second (tgs): 1961.845 | TFLOPs: 15.79 |
g0220: [2024-08-10 05:26:31,176] [INFO] [logging.py:96:log_dist] [Rank 0] step=19800, skipped=27, lr=[0.0001999745068065985, 0.0001999745068065985], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19800 loss: 0.8341 iter time (s): 3.994 samples/sec: 32.051
g0238:  iteration    19800/10000000 | consumed samples:      2534400 | consumed tokens:   5190451200 | elapsed time per iteration (ms): 4027.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.363564E-01 | loss scale: 32768.0 | grad norm: 0.663 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.779 | tokens per gpu per second (tgs): 2033.863 | TFLOPs: 16.37 |
g0220: [2024-08-10 05:27:11,485] [INFO] [logging.py:96:log_dist] [Rank 0] step=19810, skipped=27, lr=[0.00019997444574714572, 0.00019997444574714572], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19810 loss: 0.8593 iter time (s): 3.998 samples/sec: 32.016
g0238:  iteration    19810/10000000 | consumed samples:      2535680 | consumed tokens:   5193072640 | elapsed time per iteration (ms): 4031.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.384354E-01 | loss scale: 32768.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.753 | tokens per gpu per second (tgs): 2032.212 | TFLOPs: 16.35 |
g0220: [2024-08-10 05:27:52,748] [INFO] [logging.py:96:log_dist] [Rank 0] step=19820, skipped=27, lr=[0.00019997438461466757, 0.00019997438461466757], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19820 loss: 0.8420 iter time (s): 4.093 samples/sec: 31.269
g0238:  iteration    19820/10000000 | consumed samples:      2536960 | consumed tokens:   5195694080 | elapsed time per iteration (ms): 4126.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.453541E-01 | loss scale: 32768.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.021 | tokens per gpu per second (tgs): 1985.348 | TFLOPs: 15.98 |
g0220: [2024-08-10 05:28:33,694] [INFO] [logging.py:96:log_dist] [Rank 0] step=19830, skipped=27, lr=[0.00019997432340916417, 0.00019997432340916417], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19830 loss: 0.8571 iter time (s): 4.062 samples/sec: 31.512
g0238:  iteration    19830/10000000 | consumed samples:      2538240 | consumed tokens:   5198315520 | elapsed time per iteration (ms): 4094.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.378060E-01 | loss scale: 32768.0 | grad norm: 0.228 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.260 | tokens per gpu per second (tgs): 2000.662 | TFLOPs: 16.10 |
g0220: [2024-08-10 05:29:14,853] [INFO] [logging.py:96:log_dist] [Rank 0] step=19840, skipped=27, lr=[0.00019997426213063554, 0.00019997426213063554], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19840 loss: 0.8150 iter time (s): 4.083 samples/sec: 31.349
g0238:  iteration    19840/10000000 | consumed samples:      2539520 | consumed tokens:   5200936960 | elapsed time per iteration (ms): 4115.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.375346E-01 | loss scale: 32768.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.100 | tokens per gpu per second (tgs): 1990.424 | TFLOPs: 16.02 |
g0220: [2024-08-10 05:29:54,934] [INFO] [logging.py:96:log_dist] [Rank 0] step=19850, skipped=27, lr=[0.0001999742007790817, 0.0001999742007790817], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19850 loss: 0.8046 iter time (s): 3.975 samples/sec: 32.198
g0238:  iteration    19850/10000000 | consumed samples:      2540800 | consumed tokens:   5203558400 | elapsed time per iteration (ms): 4008.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.364294E-01 | loss scale: 32768.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.936 | tokens per gpu per second (tgs): 2043.908 | TFLOPs: 16.45 |
g0220: [2024-08-10 05:30:36,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=19860, skipped=27, lr=[0.00019997413935450277, 0.00019997413935450277], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19860 loss: 0.8440 iter time (s): 4.133 samples/sec: 30.969
g0238:  iteration    19860/10000000 | consumed samples:      2542080 | consumed tokens:   5206179840 | elapsed time per iteration (ms): 4166.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.334785E-01 | loss scale: 32768.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.723 | tokens per gpu per second (tgs): 1966.296 | TFLOPs: 15.82 |
g0220: [2024-08-10 05:31:17,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=19870, skipped=27, lr=[0.00019997407785689878, 0.00019997407785689878], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19870 loss: 0.8259 iter time (s): 4.052 samples/sec: 31.588
g0238:  iteration    19870/10000000 | consumed samples:      2543360 | consumed tokens:   5208801280 | elapsed time per iteration (ms): 4085.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.308514E-01 | loss scale: 32768.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.333 | tokens per gpu per second (tgs): 2005.301 | TFLOPs: 16.14 |
g0220: [2024-08-10 05:32:00,109] [INFO] [logging.py:96:log_dist] [Rank 0] step=19880, skipped=27, lr=[0.00019997401628626968, 0.00019997401628626968], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19880 loss: 0.8077 iter time (s): 4.233 samples/sec: 30.241
g0238:  iteration    19880/10000000 | consumed samples:      2544640 | consumed tokens:   5211422720 | elapsed time per iteration (ms): 4266.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.221533E-01 | loss scale: 32768.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.004 | tokens per gpu per second (tgs): 1920.258 | TFLOPs: 15.45 |
g0220: [2024-08-10 05:32:41,817] [INFO] [logging.py:96:log_dist] [Rank 0] step=19890, skipped=27, lr=[0.00019997395464261565, 0.00019997395464261565], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19890 loss: 0.8173 iter time (s): 4.138 samples/sec: 30.934
g0238:  iteration    19890/10000000 | consumed samples:      2545920 | consumed tokens:   5214044160 | elapsed time per iteration (ms): 4170.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.303158E-01 | loss scale: 32768.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.690 | tokens per gpu per second (tgs): 1964.168 | TFLOPs: 15.81 |
g0220: [2024-08-10 05:33:23,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=19900, skipped=27, lr=[0.00019997389292593666, 0.00019997389292593666], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19900 loss: 0.8043 iter time (s): 4.147 samples/sec: 30.869
g0238:  iteration    19900/10000000 | consumed samples:      2547200 | consumed tokens:   5216665600 | elapsed time per iteration (ms): 4179.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.365643E-01 | loss scale: 32768.0 | grad norm: 0.267 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.624 | tokens per gpu per second (tgs): 1959.908 | TFLOPs: 15.77 |
g0220: [2024-08-10 05:34:04,346] [INFO] [logging.py:96:log_dist] [Rank 0] step=19910, skipped=27, lr=[0.00019997383113623277, 0.00019997383113623277], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19910 loss: 0.8387 iter time (s): 4.040 samples/sec: 31.681
g0238:  iteration    19910/10000000 | consumed samples:      2548480 | consumed tokens:   5219287040 | elapsed time per iteration (ms): 4073.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.272095E-01 | loss scale: 32768.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.425 | tokens per gpu per second (tgs): 2011.189 | TFLOPs: 16.18 |
g0220: [2024-08-10 05:34:46,180] [INFO] [logging.py:96:log_dist] [Rank 0] step=19920, skipped=27, lr=[0.00019997376927350407, 0.00019997376927350407], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19920 loss: 0.8184 iter time (s): 4.150 samples/sec: 30.845
g0238:  iteration    19920/10000000 | consumed samples:      2549760 | consumed tokens:   5221908480 | elapsed time per iteration (ms): 4183.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.270204E-01 | loss scale: 32768.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.598 | tokens per gpu per second (tgs): 1958.276 | TFLOPs: 15.76 |
g0220: [2024-08-10 05:35:26,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=19930, skipped=27, lr=[0.00019997370733775054, 0.00019997370733775054], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19930 loss: 0.7792 iter time (s): 3.956 samples/sec: 32.354
g0238:  iteration    19930/10000000 | consumed samples:      2551040 | consumed tokens:   5224529920 | elapsed time per iteration (ms): 3988.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.253895E-01 | loss scale: 32768.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.090 | tokens per gpu per second (tgs): 2053.735 | TFLOPs: 16.53 |
g0220: [2024-08-10 05:36:07,013] [INFO] [logging.py:96:log_dist] [Rank 0] step=19940, skipped=27, lr=[0.00019997364532897227, 0.00019997364532897227], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19940 loss: 0.8152 iter time (s): 4.062 samples/sec: 31.515
g0238:  iteration    19940/10000000 | consumed samples:      2552320 | consumed tokens:   5227151360 | elapsed time per iteration (ms): 4094.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.345938E-01 | loss scale: 32768.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.259 | tokens per gpu per second (tgs): 2000.576 | TFLOPs: 16.10 |
g0220: [2024-08-10 05:36:47,356] [INFO] [logging.py:96:log_dist] [Rank 0] step=19950, skipped=27, lr=[0.0001999735832471693, 0.0001999735832471693], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19950 loss: 0.8273 iter time (s): 4.001 samples/sec: 31.993
g0238:  iteration    19950/10000000 | consumed samples:      2553600 | consumed tokens:   5229772800 | elapsed time per iteration (ms): 4033.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.290593E-01 | loss scale: 32768.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.731 | tokens per gpu per second (tgs): 2030.785 | TFLOPs: 16.34 |
g0220: [2024-08-10 05:37:28,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=19960, skipped=27, lr=[0.00019997352109234166, 0.00019997352109234166], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19960 loss: 0.8258 iter time (s): 4.092 samples/sec: 31.279
g0238:  iteration    19960/10000000 | consumed samples:      2554880 | consumed tokens:   5232394240 | elapsed time per iteration (ms): 4124.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.233250E-01 | loss scale: 32768.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.032 | tokens per gpu per second (tgs): 1986.016 | TFLOPs: 15.98 |
g0220: [2024-08-10 05:38:09,903] [INFO] [logging.py:96:log_dist] [Rank 0] step=19970, skipped=27, lr=[0.00019997345886448942, 0.00019997345886448942], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19970 loss: 0.8053 iter time (s): 4.097 samples/sec: 31.243
g0238:  iteration    19970/10000000 | consumed samples:      2556160 | consumed tokens:   5235015680 | elapsed time per iteration (ms): 4129.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.262146E-01 | loss scale: 32768.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.994 | tokens per gpu per second (tgs): 1983.603 | TFLOPs: 15.96 |
g0220: [2024-08-10 05:38:50,414] [INFO] [logging.py:96:log_dist] [Rank 0] step=19980, skipped=27, lr=[0.00019997339656361261, 0.00019997339656361261], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19980 loss: 0.8146 iter time (s): 4.018 samples/sec: 31.857
g0238:  iteration    19980/10000000 | consumed samples:      2557440 | consumed tokens:   5237637120 | elapsed time per iteration (ms): 4051.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.221308E-01 | loss scale: 32768.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.597 | tokens per gpu per second (tgs): 2022.184 | TFLOPs: 16.27 |
g0220: [2024-08-10 05:39:30,961] [INFO] [logging.py:96:log_dist] [Rank 0] step=19990, skipped=27, lr=[0.0001999733341897113, 0.0001999733341897113], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 19990 loss: 0.8296 iter time (s): 4.022 samples/sec: 31.829
g0238:  iteration    19990/10000000 | consumed samples:      2558720 | consumed tokens:   5240258560 | elapsed time per iteration (ms): 4054.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.410283E-01 | loss scale: 32768.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.568 | tokens per gpu per second (tgs): 2020.335 | TFLOPs: 16.26 |
g0220: [2024-08-10 05:40:10,764] [INFO] [logging.py:96:log_dist] [Rank 0] step=20000, skipped=27, lr=[0.00019997327174278555, 0.00019997327174278555], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20000 loss: 0.8272 iter time (s): 3.947 samples/sec: 32.427
g0238:  iteration    20000/10000000 | consumed samples:      2560000 | consumed tokens:   5242880000 | elapsed time per iteration (ms): 3980.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.306079E-01 | loss scale: 32768.0 | grad norm: 0.209 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.159 | tokens per gpu per second (tgs): 2058.158 | TFLOPs: 16.56 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 20000 | lm loss value: 8.304504E-01 | lm loss PPL: 2.294352E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   20000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-10 05:46:42,627] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step20000 is about to be saved!
g0238: [2024-08-10 05:46:42,633] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0238: [2024-08-10 05:46:42,633] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0220: [2024-08-10 05:46:42,634] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0238: [2024-08-10 05:46:42,634] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0220: [2024-08-10 05:46:42,634] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0220: [2024-08-10 05:46:42,634] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0234: [2024-08-10 05:46:42,635] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0234: [2024-08-10 05:46:42,635] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0234: [2024-08-10 05:46:42,635] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0236: [2024-08-10 05:46:42,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0236: [2024-08-10 05:46:42,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0236: [2024-08-10 05:46:42,636] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0237: [2024-08-10 05:46:42,637] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0237: [2024-08-10 05:46:42,637] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0237: [2024-08-10 05:46:42,637] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0233: [2024-08-10 05:46:42,637] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0233: [2024-08-10 05:46:42,637] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0233: [2024-08-10 05:46:42,637] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0235: [2024-08-10 05:46:42,638] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0235: [2024-08-10 05:46:42,638] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0235: [2024-08-10 05:46:42,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0225: [2024-08-10 05:46:42,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0225: [2024-08-10 05:46:42,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0225: [2024-08-10 05:46:42,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0238: [2024-08-10 05:46:42,657] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_23-model_00-model_states.pt...
g0234: [2024-08-10 05:46:42,673] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_11-model_00-model_states.pt...
g0237: [2024-08-10 05:46:42,673] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_20-model_00-model_states.pt...
g0236: [2024-08-10 05:46:42,674] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_17-model_00-model_states.pt...
g0233: [2024-08-10 05:46:42,675] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_08-model_00-model_states.pt...
g0235: [2024-08-10 05:46:42,676] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_14-model_00-model_states.pt...
g0225: [2024-08-10 05:46:42,678] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_05-model_00-model_states.pt...
g0220: [2024-08-10 05:46:42,687] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_01-model_00-model_states.pt...
g0237: [2024-08-10 05:46:42,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_20-model_00-model_states.pt.
g0238: [2024-08-10 05:46:42,794] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_23-model_00-model_states.pt.
g0238: [2024-08-10 05:46:42,795] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_24-model_00-model_states.pt...
g0238: [2024-08-10 05:46:42,797] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_24-model_00-model_states.pt.
g0234: [2024-08-10 05:46:42,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_11-model_00-model_states.pt.
g0235: [2024-08-10 05:46:42,808] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_14-model_00-model_states.pt.
g0237: [2024-08-10 05:46:42,817] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_21-model_00-model_states.pt...
g0236: [2024-08-10 05:46:42,819] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_17-model_00-model_states.pt.
g0238: [2024-08-10 05:46:42,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_25-model_00-model_states.pt...
g0234: [2024-08-10 05:46:42,844] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_12-model_00-model_states.pt...
g0235: [2024-08-10 05:46:42,846] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_15-model_00-model_states.pt...
g0225: [2024-08-10 05:46:42,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_05-model_00-model_states.pt.
g0236: [2024-08-10 05:46:42,858] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_18-model_00-model_states.pt...
g0225: [2024-08-10 05:46:42,891] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_06-model_00-model_states.pt...
g0233: [2024-08-10 05:46:42,895] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_08-model_00-model_states.pt.
g0233: [2024-08-10 05:46:42,933] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_09-model_00-model_states.pt...
g0235: [2024-08-10 05:46:42,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_15-model_00-model_states.pt.
g0234: [2024-08-10 05:46:42,982] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_12-model_00-model_states.pt.
g0235: [2024-08-10 05:46:42,984] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_16-model_00-model_states.pt...
g0237: [2024-08-10 05:46:43,015] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_21-model_00-model_states.pt.
g0234: [2024-08-10 05:46:43,017] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_13-model_00-model_states.pt...
g0238: [2024-08-10 05:46:43,038] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_25-model_00-model_states.pt.
g0238: [2024-08-10 05:46:43,039] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_07_model_states.pt...
g0225: [2024-08-10 05:46:43,047] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_06-model_00-model_states.pt.
g0237: [2024-08-10 05:46:43,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_22-model_00-model_states.pt...
g0225: [2024-08-10 05:46:43,082] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_07-model_00-model_states.pt...
g0236: [2024-08-10 05:46:43,087] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_18-model_00-model_states.pt.
g0220: [2024-08-10 05:46:43,117] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_01-model_00-model_states.pt.
g0236: [2024-08-10 05:46:43,122] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_19-model_00-model_states.pt...
g0235: [2024-08-10 05:46:43,143] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_16-model_00-model_states.pt.
g0235: [2024-08-10 05:46:43,145] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_04_model_states.pt...
g0220: [2024-08-10 05:46:43,146] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_02-model_00-model_states.pt...
g0234: [2024-08-10 05:46:43,160] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_13-model_00-model_states.pt.
g0234: [2024-08-10 05:46:43,162] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_03_model_states.pt...
g0225: [2024-08-10 05:46:43,207] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_07-model_00-model_states.pt.
g0225: [2024-08-10 05:46:43,209] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_01_model_states.pt...
g0236: [2024-08-10 05:46:43,213] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_19-model_00-model_states.pt.
g0236: [2024-08-10 05:46:43,215] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_05_model_states.pt...
g0220: [2024-08-10 05:46:43,235] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_02-model_00-model_states.pt.
g0220: [2024-08-10 05:46:43,264] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_03-model_00-model_states.pt...
g0220: [2024-08-10 05:46:43,401] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_03-model_00-model_states.pt.
g0220: [2024-08-10 05:46:43,431] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_04-model_00-model_states.pt...
g0237: [2024-08-10 05:46:43,463] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_22-model_00-model_states.pt.
g0237: [2024-08-10 05:46:43,465] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_06_model_states.pt...
g0220: [2024-08-10 05:46:43,517] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_04-model_00-model_states.pt.
g0220: [2024-08-10 05:46:43,518] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_00_model_states.pt
g0220: [2024-08-10 05:46:43,519] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_00_model_states.pt...
g0233: [2024-08-10 05:46:43,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_09-model_00-model_states.pt.
g0233: [2024-08-10 05:46:43,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_10-model_00-model_states.pt...
g0233: [2024-08-10 05:46:43,836] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/layer_10-model_00-model_states.pt.
g0233: [2024-08-10 05:46:43,837] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_02_model_states.pt...
g0238: [2024-08-10 05:46:44,908] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_07_model_states.pt.
g0238: [2024-08-10 05:46:44,909] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0235: [2024-08-10 05:46:45,452] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_04_model_states.pt.
g0235: [2024-08-10 05:46:45,452] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0225: [2024-08-10 05:46:45,624] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_01_model_states.pt.
g0225: [2024-08-10 05:46:45,625] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0234: [2024-08-10 05:46:45,711] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_03_model_states.pt.
g0234: [2024-08-10 05:46:45,711] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0237: [2024-08-10 05:46:45,796] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_06_model_states.pt.
g0237: [2024-08-10 05:46:45,797] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0233: [2024-08-10 05:46:46,505] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_02_model_states.pt.
g0233: [2024-08-10 05:46:46,506] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0236: [2024-08-10 05:46:46,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_05_model_states.pt.
g0236: [2024-08-10 05:46:46,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0220: [2024-08-10 05:46:47,086] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step20000/mp_rank_00_model_states.pt.
g0220: [2024-08-10 05:46:47,086] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20000 is ready now!
g0220:   successfully saved checkpoint at iteration   20000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 4.99, Latency(second): 4.516
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (4515.58, 4515.89)
g0220: [2024-08-10 05:47:28,928] [INFO] [logging.py:96:log_dist] [Rank 0] step=20010, skipped=27, lr=[0.00019997320922283534, 0.00019997320922283534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20010 loss: 0.8108 iter time (s): 4.150 samples/sec: 30.840
g0238:  iteration    20010/10000000 | consumed samples:      2561280 | consumed tokens:   5245501440 | elapsed time per iteration (ms): 43816.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.310727E-01 | loss scale: 32768.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.921 | tokens per gpu per second (tgs): 186.962 | TFLOPs: 1.50 |
g0220: [2024-08-10 05:48:09,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=20020, skipped=27, lr=[0.00019997314662986078, 0.00019997314662986078], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20020 loss: 0.8512 iter time (s): 4.064 samples/sec: 31.496
g0238:  iteration    20020/10000000 | consumed samples:      2562560 | consumed tokens:   5248122880 | elapsed time per iteration (ms): 4096.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.280580E-01 | loss scale: 32768.0 | grad norm: 0.215 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.245 | tokens per gpu per second (tgs): 1999.690 | TFLOPs: 16.09 |
g0220: [2024-08-10 05:48:52,386] [INFO] [logging.py:96:log_dist] [Rank 0] step=20030, skipped=27, lr=[0.0001999730839638619, 0.0001999730839638619], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20030 loss: 0.8367 iter time (s): 4.204 samples/sec: 30.451
g0238:  iteration    20030/10000000 | consumed samples:      2563840 | consumed tokens:   5250744320 | elapsed time per iteration (ms): 4249.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.388965E-01 | loss scale: 32768.0 | grad norm: 0.200 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.123 | tokens per gpu per second (tgs): 1927.895 | TFLOPs: 15.51 |
g0220: [2024-08-10 05:49:34,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=20040, skipped=27, lr=[0.00019997302122483874, 0.00019997302122483874], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20040 loss: 0.8431 iter time (s): 4.190 samples/sec: 30.552
g0238:  iteration    20040/10000000 | consumed samples:      2565120 | consumed tokens:   5253365760 | elapsed time per iteration (ms): 4221.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.331970E-01 | loss scale: 32768.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.319 | tokens per gpu per second (tgs): 1940.394 | TFLOPs: 15.61 |
g0220: [2024-08-10 05:50:16,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=20050, skipped=27, lr=[0.00019997295841279138, 0.00019997295841279138], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20050 loss: 0.8392 iter time (s): 4.170 samples/sec: 30.692
g0238:  iteration    20050/10000000 | consumed samples:      2566400 | consumed tokens:   5255987200 | elapsed time per iteration (ms): 4203.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.356551E-01 | loss scale: 32768.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.451 | tokens per gpu per second (tgs): 1948.861 | TFLOPs: 15.68 |
g0220: [2024-08-10 05:50:56,613] [INFO] [logging.py:96:log_dist] [Rank 0] step=20060, skipped=27, lr=[0.00019997289552771983, 0.00019997289552771983], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20060 loss: 0.8407 iter time (s): 3.965 samples/sec: 32.286
g0238:  iteration    20060/10000000 | consumed samples:      2567680 | consumed tokens:   5258608640 | elapsed time per iteration (ms): 3997.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.285599E-01 | loss scale: 32768.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.022 | tokens per gpu per second (tgs): 2049.412 | TFLOPs: 16.49 |
g0220: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0225: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0238: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 05:51:16,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0233: [2024-08-10 05:51:16,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0235: [2024-08-10 05:51:16,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0234: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 05:51:16,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0236: [2024-08-10 05:51:16,784] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 05:51:16,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 05:51:16,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 05:51:16,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0237: [2024-08-10 05:51:16,785] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 32768.0 to 65536.0
g0220: [2024-08-10 05:51:37,303] [INFO] [logging.py:96:log_dist] [Rank 0] step=20070, skipped=27, lr=[0.00019997283256962417, 0.00019997283256962417], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20070 loss: 0.8761 iter time (s): 4.036 samples/sec: 31.711
g0238:  iteration    20070/10000000 | consumed samples:      2568960 | consumed tokens:   5261230080 | elapsed time per iteration (ms): 4069.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.370907E-01 | loss scale: 65536.0 | grad norm: 0.467 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.457 | tokens per gpu per second (tgs): 2013.256 | TFLOPs: 16.20 |
g0220: [2024-08-10 05:52:18,972] [INFO] [logging.py:96:log_dist] [Rank 0] step=20080, skipped=27, lr=[0.00019997276953850443, 0.00019997276953850443], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20080 loss: 0.8308 iter time (s): 4.134 samples/sec: 30.962
g0238:  iteration    20080/10000000 | consumed samples:      2570240 | consumed tokens:   5263851520 | elapsed time per iteration (ms): 4167.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.234223E-01 | loss scale: 65536.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.718 | tokens per gpu per second (tgs): 1965.926 | TFLOPs: 15.82 |
g0220: [2024-08-10 05:53:00,213] [INFO] [logging.py:96:log_dist] [Rank 0] step=20090, skipped=27, lr=[0.00019997270643436064, 0.00019997270643436064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20090 loss: 0.8291 iter time (s): 4.091 samples/sec: 31.287
g0238:  iteration    20090/10000000 | consumed samples:      2571520 | consumed tokens:   5266472960 | elapsed time per iteration (ms): 4124.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.288587E-01 | loss scale: 65536.0 | grad norm: 0.240 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.038 | tokens per gpu per second (tgs): 1986.413 | TFLOPs: 15.98 |
g0220: [2024-08-10 05:53:42,154] [INFO] [logging.py:96:log_dist] [Rank 0] step=20100, skipped=27, lr=[0.00019997264325719288, 0.00019997264325719288], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20100 loss: 0.8237 iter time (s): 4.162 samples/sec: 30.758
g0238:  iteration    20100/10000000 | consumed samples:      2572800 | consumed tokens:   5269094400 | elapsed time per iteration (ms): 4194.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.278407E-01 | loss scale: 65536.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.519 | tokens per gpu per second (tgs): 1953.242 | TFLOPs: 15.72 |
g0220: [2024-08-10 05:54:25,482] [INFO] [logging.py:96:log_dist] [Rank 0] step=20110, skipped=27, lr=[0.0001999725800070012, 0.0001999725800070012], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20110 loss: 0.8208 iter time (s): 4.300 samples/sec: 29.765
g0238:  iteration    20110/10000000 | consumed samples:      2574080 | consumed tokens:   5271715840 | elapsed time per iteration (ms): 4332.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.259357E-01 | loss scale: 65536.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.542 | tokens per gpu per second (tgs): 1890.710 | TFLOPs: 15.21 |
g0220: [2024-08-10 05:55:06,296] [INFO] [logging.py:96:log_dist] [Rank 0] step=20120, skipped=27, lr=[0.00019997251668378563, 0.00019997251668378563], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20120 loss: 0.8418 iter time (s): 4.049 samples/sec: 31.615
g0238:  iteration    20120/10000000 | consumed samples:      2575360 | consumed tokens:   5274337280 | elapsed time per iteration (ms): 4081.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.380903E-01 | loss scale: 65536.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.361 | tokens per gpu per second (tgs): 2007.130 | TFLOPs: 16.15 |
g0220: [2024-08-10 05:55:46,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=20130, skipped=27, lr=[0.0001999724532875462, 0.0001999724532875462], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20130 loss: 0.8210 iter time (s): 4.029 samples/sec: 31.772
g0238:  iteration    20130/10000000 | consumed samples:      2576640 | consumed tokens:   5276958720 | elapsed time per iteration (ms): 4061.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.154572E-01 | loss scale: 65536.0 | grad norm: 0.246 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.518 | tokens per gpu per second (tgs): 2017.167 | TFLOPs: 16.23 |
g0220: [2024-08-10 05:56:28,500] [INFO] [logging.py:96:log_dist] [Rank 0] step=20140, skipped=27, lr=[0.00019997238981828302, 0.00019997238981828302], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20140 loss: 0.8197 iter time (s): 4.126 samples/sec: 31.020
g0238:  iteration    20140/10000000 | consumed samples:      2577920 | consumed tokens:   5279580160 | elapsed time per iteration (ms): 4159.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.328341E-01 | loss scale: 65536.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.774 | tokens per gpu per second (tgs): 1969.544 | TFLOPs: 15.85 |
g0220: [2024-08-10 05:57:09,439] [INFO] [logging.py:96:log_dist] [Rank 0] step=20150, skipped=27, lr=[0.00019997232627599605, 0.00019997232627599605], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20150 loss: 0.8632 iter time (s): 4.061 samples/sec: 31.516
g0238:  iteration    20150/10000000 | consumed samples:      2579200 | consumed tokens:   5282201600 | elapsed time per iteration (ms): 4093.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.372740E-01 | loss scale: 65536.0 | grad norm: 0.246 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.266 | tokens per gpu per second (tgs): 2001.037 | TFLOPs: 16.10 |
g0220: [2024-08-10 05:57:50,605] [INFO] [logging.py:96:log_dist] [Rank 0] step=20160, skipped=27, lr=[0.00019997226266068547, 0.00019997226266068547], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20160 loss: 0.8322 iter time (s): 4.084 samples/sec: 31.344
g0238:  iteration    20160/10000000 | consumed samples:      2580480 | consumed tokens:   5284823040 | elapsed time per iteration (ms): 4116.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.302303E-01 | loss scale: 65536.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.093 | tokens per gpu per second (tgs): 1989.957 | TFLOPs: 16.01 |
g0220: [2024-08-10 05:58:30,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=20170, skipped=27, lr=[0.00019997219897235118, 0.00019997219897235118], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20170 loss: 0.8405 iter time (s): 3.916 samples/sec: 32.684
g0238:  iteration    20170/10000000 | consumed samples:      2581760 | consumed tokens:   5287444480 | elapsed time per iteration (ms): 3949.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.353035E-01 | loss scale: 65536.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.412 | tokens per gpu per second (tgs): 2074.374 | TFLOPs: 16.69 |
g0220: [2024-08-10 05:59:10,413] [INFO] [logging.py:96:log_dist] [Rank 0] step=20180, skipped=27, lr=[0.00019997213521099334, 0.00019997213521099334], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20180 loss: 0.8461 iter time (s): 3.999 samples/sec: 32.009
g0238:  iteration    20180/10000000 | consumed samples:      2583040 | consumed tokens:   5290065920 | elapsed time per iteration (ms): 4031.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.256436E-01 | loss scale: 65536.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.748 | tokens per gpu per second (tgs): 2031.896 | TFLOPs: 16.35 |
g0220: [2024-08-10 05:59:51,983] [INFO] [logging.py:96:log_dist] [Rank 0] step=20190, skipped=27, lr=[0.00019997207137661193, 0.00019997207137661193], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20190 loss: 0.8122 iter time (s): 4.124 samples/sec: 31.041
g0238:  iteration    20190/10000000 | consumed samples:      2584320 | consumed tokens:   5292687360 | elapsed time per iteration (ms): 4156.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.297070E-01 | loss scale: 65536.0 | grad norm: 0.272 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.792 | tokens per gpu per second (tgs): 1970.678 | TFLOPs: 15.86 |
g0220: [2024-08-10 06:00:33,706] [INFO] [logging.py:96:log_dist] [Rank 0] step=20200, skipped=27, lr=[0.00019997200746920703, 0.00019997200746920703], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20200 loss: 0.8264 iter time (s): 4.139 samples/sec: 30.922
g0238:  iteration    20200/10000000 | consumed samples:      2585600 | consumed tokens:   5295308800 | elapsed time per iteration (ms): 4172.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.291909E-01 | loss scale: 65536.0 | grad norm: 0.239 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.681 | tokens per gpu per second (tgs): 1963.571 | TFLOPs: 15.80 |
g0220: [2024-08-10 06:01:15,432] [INFO] [logging.py:96:log_dist] [Rank 0] step=20210, skipped=27, lr=[0.0001999719434887787, 0.0001999719434887787], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20210 loss: 0.8253 iter time (s): 4.139 samples/sec: 30.923
g0238:  iteration    20210/10000000 | consumed samples:      2586880 | consumed tokens:   5297930240 | elapsed time per iteration (ms): 4172.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.195687E-01 | loss scale: 65536.0 | grad norm: 0.168 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.675 | tokens per gpu per second (tgs): 1963.172 | TFLOPs: 15.80 |
g0220: [2024-08-10 06:01:55,856] [INFO] [logging.py:96:log_dist] [Rank 0] step=20220, skipped=27, lr=[0.00019997187943532698, 0.00019997187943532698], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20220 loss: 0.8018 iter time (s): 4.009 samples/sec: 31.925
g0238:  iteration    20220/10000000 | consumed samples:      2588160 | consumed tokens:   5300551680 | elapsed time per iteration (ms): 4042.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.351155E-01 | loss scale: 65536.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.665 | tokens per gpu per second (tgs): 2026.589 | TFLOPs: 16.31 |
g0220: [2024-08-10 06:02:37,785] [INFO] [logging.py:96:log_dist] [Rank 0] step=20230, skipped=27, lr=[0.0001999718153088519, 0.0001999718153088519], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20230 loss: 0.8431 iter time (s): 4.160 samples/sec: 30.767
g0238:  iteration    20230/10000000 | consumed samples:      2589440 | consumed tokens:   5303173120 | elapsed time per iteration (ms): 4192.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.316044E-01 | loss scale: 65536.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.528 | tokens per gpu per second (tgs): 1953.807 | TFLOPs: 15.72 |
g0220: [2024-08-10 06:03:18,809] [INFO] [logging.py:96:log_dist] [Rank 0] step=20240, skipped=27, lr=[0.00019997175110935355, 0.00019997175110935355], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20240 loss: 0.8435 iter time (s): 4.070 samples/sec: 31.453
g0238:  iteration    20240/10000000 | consumed samples:      2590720 | consumed tokens:   5305794560 | elapsed time per iteration (ms): 4102.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.399875E-01 | loss scale: 65536.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.201 | tokens per gpu per second (tgs): 1996.887 | TFLOPs: 16.07 |
g0220: [2024-08-10 06:03:58,272] [INFO] [logging.py:96:log_dist] [Rank 0] step=20250, skipped=27, lr=[0.00019997168683683194, 0.00019997168683683194], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20250 loss: 0.8112 iter time (s): 3.913 samples/sec: 32.708
g0238:  iteration    20250/10000000 | consumed samples:      2592000 | consumed tokens:   5308416000 | elapsed time per iteration (ms): 3946.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.322501E-01 | loss scale: 65536.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.436 | tokens per gpu per second (tgs): 2075.886 | TFLOPs: 16.71 |
g0220: [2024-08-10 06:04:38,700] [INFO] [logging.py:96:log_dist] [Rank 0] step=20260, skipped=27, lr=[0.0001999716224912871, 0.0001999716224912871], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20260 loss: 0.8495 iter time (s): 4.009 samples/sec: 31.925
g0238:  iteration    20260/10000000 | consumed samples:      2593280 | consumed tokens:   5311037440 | elapsed time per iteration (ms): 4042.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.257511E-01 | loss scale: 65536.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.661 | tokens per gpu per second (tgs): 2026.298 | TFLOPs: 16.31 |
g0220: [2024-08-10 06:05:19,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=20270, skipped=27, lr=[0.00019997155807271915, 0.00019997155807271915], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20270 loss: 0.8013 iter time (s): 4.013 samples/sec: 31.894
g0238:  iteration    20270/10000000 | consumed samples:      2594560 | consumed tokens:   5313658880 | elapsed time per iteration (ms): 4047.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.211760E-01 | loss scale: 65536.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.628 | tokens per gpu per second (tgs): 2024.200 | TFLOPs: 16.29 |
g0220: [2024-08-10 06:06:01,068] [INFO] [logging.py:96:log_dist] [Rank 0] step=20280, skipped=27, lr=[0.0001999714935811281, 0.0001999714935811281], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20280 loss: 0.8280 iter time (s): 4.157 samples/sec: 30.792
g0238:  iteration    20280/10000000 | consumed samples:      2595840 | consumed tokens:   5316280320 | elapsed time per iteration (ms): 4189.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.432910E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.551 | tokens per gpu per second (tgs): 1955.261 | TFLOPs: 15.73 |
g0220: [2024-08-10 06:06:41,039] [INFO] [logging.py:96:log_dist] [Rank 0] step=20290, skipped=27, lr=[0.000199971429016514, 0.000199971429016514], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20290 loss: 0.8304 iter time (s): 3.964 samples/sec: 32.292
g0238:  iteration    20290/10000000 | consumed samples:      2597120 | consumed tokens:   5318901760 | elapsed time per iteration (ms): 3997.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.382112E-01 | loss scale: 65536.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.023 | tokens per gpu per second (tgs): 2049.499 | TFLOPs: 16.49 |
g0220: [2024-08-10 06:07:22,762] [INFO] [logging.py:96:log_dist] [Rank 0] step=20300, skipped=27, lr=[0.0001999713643788769, 0.0001999713643788769], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20300 loss: 0.8573 iter time (s): 4.139 samples/sec: 30.925
g0238:  iteration    20300/10000000 | consumed samples:      2598400 | consumed tokens:   5321523200 | elapsed time per iteration (ms): 4172.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.304198E-01 | loss scale: 65536.0 | grad norm: 0.487 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.679 | tokens per gpu per second (tgs): 1963.457 | TFLOPs: 15.80 |
g0220: [2024-08-10 06:08:05,023] [INFO] [logging.py:96:log_dist] [Rank 0] step=20310, skipped=27, lr=[0.00019997129966821685, 0.00019997129966821685], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20310 loss: 0.8462 iter time (s): 4.193 samples/sec: 30.529
g0238:  iteration    20310/10000000 | consumed samples:      2599680 | consumed tokens:   5324144640 | elapsed time per iteration (ms): 4226.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.234960E-01 | loss scale: 65536.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.288 | tokens per gpu per second (tgs): 1938.453 | TFLOPs: 15.60 |
g0220: [2024-08-10 06:08:47,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=20320, skipped=27, lr=[0.0001999712348845339, 0.0001999712348845339], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20320 loss: 0.8146 iter time (s): 4.165 samples/sec: 30.731
g0238:  iteration    20320/10000000 | consumed samples:      2600960 | consumed tokens:   5326766080 | elapsed time per iteration (ms): 4198.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.153043E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.489 | tokens per gpu per second (tgs): 1951.313 | TFLOPs: 15.70 |
g0220: [2024-08-10 06:09:27,974] [INFO] [logging.py:96:log_dist] [Rank 0] step=20330, skipped=27, lr=[0.0001999711700278281, 0.0001999711700278281], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20330 loss: 0.8378 iter time (s): 4.064 samples/sec: 31.495
g0238:  iteration    20330/10000000 | consumed samples:      2602240 | consumed tokens:   5329387520 | elapsed time per iteration (ms): 4096.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.384442E-01 | loss scale: 65536.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.243 | tokens per gpu per second (tgs): 1999.561 | TFLOPs: 16.09 |
g0220: [2024-08-10 06:10:10,535] [INFO] [logging.py:96:log_dist] [Rank 0] step=20340, skipped=27, lr=[0.00019997110509809948, 0.00019997110509809948], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20340 loss: 0.8250 iter time (s): 4.224 samples/sec: 30.306
g0238:  iteration    20340/10000000 | consumed samples:      2603520 | consumed tokens:   5332008960 | elapsed time per iteration (ms): 4256.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.367966E-01 | loss scale: 65536.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.074 | tokens per gpu per second (tgs): 1924.748 | TFLOPs: 15.49 |
g0220: [2024-08-10 06:10:51,071] [INFO] [logging.py:96:log_dist] [Rank 0] step=20350, skipped=27, lr=[0.00019997104009534814, 0.00019997104009534814], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20350 loss: 0.8290 iter time (s): 4.021 samples/sec: 31.831
g0238:  iteration    20350/10000000 | consumed samples:      2604800 | consumed tokens:   5334630400 | elapsed time per iteration (ms): 4053.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.303232E-01 | loss scale: 65536.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.576 | tokens per gpu per second (tgs): 2020.863 | TFLOPs: 16.26 |
g0220: [2024-08-10 06:11:32,441] [INFO] [logging.py:96:log_dist] [Rank 0] step=20360, skipped=27, lr=[0.0001999709750195741, 0.0001999709750195741], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20360 loss: 0.8405 iter time (s): 4.104 samples/sec: 31.188
g0238:  iteration    20360/10000000 | consumed samples:      2606080 | consumed tokens:   5337251840 | elapsed time per iteration (ms): 4136.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.232350E-01 | loss scale: 65536.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.941 | tokens per gpu per second (tgs): 1980.216 | TFLOPs: 15.94 |
g0220: [2024-08-10 06:12:12,713] [INFO] [logging.py:96:log_dist] [Rank 0] step=20370, skipped=27, lr=[0.0001999709098707774, 0.0001999709098707774], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20370 loss: 0.8250 iter time (s): 3.994 samples/sec: 32.045
g0238:  iteration    20370/10000000 | consumed samples:      2607360 | consumed tokens:   5339873280 | elapsed time per iteration (ms): 4027.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.229156E-01 | loss scale: 65536.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.785 | tokens per gpu per second (tgs): 2034.230 | TFLOPs: 16.37 |
g0220: [2024-08-10 06:12:54,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=20380, skipped=27, lr=[0.00019997084464895811, 0.00019997084464895811], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20380 loss: 0.8162 iter time (s): 4.157 samples/sec: 30.793
g0238:  iteration    20380/10000000 | consumed samples:      2608640 | consumed tokens:   5342494720 | elapsed time per iteration (ms): 4189.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.242031E-01 | loss scale: 65536.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.553 | tokens per gpu per second (tgs): 1955.369 | TFLOPs: 15.74 |
g0220: [2024-08-10 06:13:34,562] [INFO] [logging.py:96:log_dist] [Rank 0] step=20390, skipped=27, lr=[0.00019997077935411626, 0.00019997077935411626], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20390 loss: 0.7985 iter time (s): 3.963 samples/sec: 32.300
g0238:  iteration    20390/10000000 | consumed samples:      2609920 | consumed tokens:   5345116160 | elapsed time per iteration (ms): 3995.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.197872E-01 | loss scale: 65536.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.037 | tokens per gpu per second (tgs): 2050.366 | TFLOPs: 16.50 |
g0220: [2024-08-10 06:14:15,452] [INFO] [logging.py:96:log_dist] [Rank 0] step=20400, skipped=27, lr=[0.0001999707139862519, 0.0001999707139862519], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20400 loss: 0.8178 iter time (s): 4.056 samples/sec: 31.555
g0238:  iteration    20400/10000000 | consumed samples:      2611200 | consumed tokens:   5347737600 | elapsed time per iteration (ms): 4089.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.245662E-01 | loss scale: 65536.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.304 | tokens per gpu per second (tgs): 2003.441 | TFLOPs: 16.12 |
g0220: [2024-08-10 06:14:55,639] [INFO] [logging.py:96:log_dist] [Rank 0] step=20410, skipped=27, lr=[0.00019997064854536511, 0.00019997064854536511], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20410 loss: 0.8336 iter time (s): 3.986 samples/sec: 32.112
g0238:  iteration    20410/10000000 | consumed samples:      2612480 | consumed tokens:   5350359040 | elapsed time per iteration (ms): 4018.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.259770E-01 | loss scale: 65536.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.850 | tokens per gpu per second (tgs): 2038.414 | TFLOPs: 16.40 |
g0220: [2024-08-10 06:15:36,137] [INFO] [logging.py:96:log_dist] [Rank 0] step=20420, skipped=27, lr=[0.00019997058303145593, 0.00019997058303145593], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20420 loss: 0.8434 iter time (s): 4.017 samples/sec: 31.864
g0238:  iteration    20420/10000000 | consumed samples:      2613760 | consumed tokens:   5352980480 | elapsed time per iteration (ms): 4049.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.342979E-01 | loss scale: 65536.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.607 | tokens per gpu per second (tgs): 2022.836 | TFLOPs: 16.28 |
g0220: [2024-08-10 06:16:17,307] [INFO] [logging.py:96:log_dist] [Rank 0] step=20430, skipped=27, lr=[0.00019997051744452437, 0.00019997051744452437], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20430 loss: 0.8083 iter time (s): 4.084 samples/sec: 31.339
g0238:  iteration    20430/10000000 | consumed samples:      2615040 | consumed tokens:   5355601920 | elapsed time per iteration (ms): 4116.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.184101E-01 | loss scale: 65536.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.091 | tokens per gpu per second (tgs): 1989.828 | TFLOPs: 16.01 |
g0220: [2024-08-10 06:16:59,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=20440, skipped=27, lr=[0.00019997045178457055, 0.00019997045178457055], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20440 loss: 0.8270 iter time (s): 4.170 samples/sec: 30.694
g0238:  iteration    20440/10000000 | consumed samples:      2616320 | consumed tokens:   5358223360 | elapsed time per iteration (ms): 4202.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.187970E-01 | loss scale: 65536.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.457 | tokens per gpu per second (tgs): 1949.222 | TFLOPs: 15.69 |
g0220: [2024-08-10 06:17:41,270] [INFO] [logging.py:96:log_dist] [Rank 0] step=20450, skipped=27, lr=[0.00019997038605159444, 0.00019997038605159444], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20450 loss: 0.8012 iter time (s): 4.161 samples/sec: 30.764
g0238:  iteration    20450/10000000 | consumed samples:      2617600 | consumed tokens:   5360844800 | elapsed time per iteration (ms): 4193.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.177633E-01 | loss scale: 65536.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.523 | tokens per gpu per second (tgs): 1953.459 | TFLOPs: 15.72 |
g0220: [2024-08-10 06:18:23,050] [INFO] [logging.py:96:log_dist] [Rank 0] step=20460, skipped=27, lr=[0.00019997032024559618, 0.00019997032024559618], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20460 loss: 0.8015 iter time (s): 4.145 samples/sec: 30.877
g0238:  iteration    20460/10000000 | consumed samples:      2618880 | consumed tokens:   5363466240 | elapsed time per iteration (ms): 4178.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.207720E-01 | loss scale: 65536.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.636 | tokens per gpu per second (tgs): 1960.712 | TFLOPs: 15.78 |
g0220: [2024-08-10 06:19:05,365] [INFO] [logging.py:96:log_dist] [Rank 0] step=20470, skipped=27, lr=[0.00019997025436657573, 0.00019997025436657573], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20470 loss: 0.8391 iter time (s): 4.199 samples/sec: 30.486
g0238:  iteration    20470/10000000 | consumed samples:      2620160 | consumed tokens:   5366087680 | elapsed time per iteration (ms): 4233.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.320363E-01 | loss scale: 65536.0 | grad norm: 0.229 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.236 | tokens per gpu per second (tgs): 1935.098 | TFLOPs: 15.57 |
g0220: [2024-08-10 06:19:46,042] [INFO] [logging.py:96:log_dist] [Rank 0] step=20480, skipped=27, lr=[0.00019997018841453321, 0.00019997018841453321], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20480 loss: 0.8337 iter time (s): 4.033 samples/sec: 31.738
g0238:  iteration    20480/10000000 | consumed samples:      2621440 | consumed tokens:   5368709120 | elapsed time per iteration (ms): 4065.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.222301E-01 | loss scale: 65536.0 | grad norm: 0.211 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.483 | tokens per gpu per second (tgs): 2014.919 | TFLOPs: 16.21 |
g0220: [2024-08-10 06:20:27,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=20490, skipped=27, lr=[0.00019997012238946862, 0.00019997012238946862], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20490 loss: 0.8143 iter time (s): 4.160 samples/sec: 30.766
g0238:  iteration    20490/10000000 | consumed samples:      2622720 | consumed tokens:   5371330560 | elapsed time per iteration (ms): 4192.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.242307E-01 | loss scale: 65536.0 | grad norm: 0.221 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.528 | tokens per gpu per second (tgs): 1953.805 | TFLOPs: 15.72 |
g0220: [2024-08-10 06:21:10,021] [INFO] [logging.py:96:log_dist] [Rank 0] step=20500, skipped=27, lr=[0.0001999700562913821, 0.0001999700562913821], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20500 loss: 0.8237 iter time (s): 4.171 samples/sec: 30.690
g0238:  iteration    20500/10000000 | consumed samples:      2624000 | consumed tokens:   5373952000 | elapsed time per iteration (ms): 4205.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.283553E-01 | loss scale: 65536.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.439 | tokens per gpu per second (tgs): 1948.096 | TFLOPs: 15.68 |
g0220: [2024-08-10 06:21:50,790] [INFO] [logging.py:96:log_dist] [Rank 0] step=20510, skipped=27, lr=[0.00019996999012027357, 0.00019996999012027357], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20510 loss: 0.8598 iter time (s): 4.044 samples/sec: 31.655
g0238:  iteration    20510/10000000 | consumed samples:      2625280 | consumed tokens:   5376573440 | elapsed time per iteration (ms): 4076.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.334197E-01 | loss scale: 65536.0 | grad norm: 0.192 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.397 | tokens per gpu per second (tgs): 2009.436 | TFLOPs: 16.17 |
g0220: [2024-08-10 06:22:31,385] [INFO] [logging.py:96:log_dist] [Rank 0] step=20520, skipped=27, lr=[0.00019996992387614316, 0.00019996992387614316], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20520 loss: 0.8627 iter time (s): 4.027 samples/sec: 31.786
g0238:  iteration    20520/10000000 | consumed samples:      2626560 | consumed tokens:   5379194880 | elapsed time per iteration (ms): 4059.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.311148E-01 | loss scale: 65536.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.531 | tokens per gpu per second (tgs): 2017.993 | TFLOPs: 16.24 |
g0220: [2024-08-10 06:23:13,369] [INFO] [logging.py:96:log_dist] [Rank 0] step=20530, skipped=27, lr=[0.0001999698575589909, 0.0001999698575589909], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20530 loss: 0.8158 iter time (s): 4.163 samples/sec: 30.747
g0238:  iteration    20530/10000000 | consumed samples:      2627840 | consumed tokens:   5381816320 | elapsed time per iteration (ms): 4199.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.182636E-01 | loss scale: 65536.0 | grad norm: 0.227 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.482 | tokens per gpu per second (tgs): 1950.829 | TFLOPs: 15.70 |
g0220: [2024-08-10 06:23:54,634] [INFO] [logging.py:96:log_dist] [Rank 0] step=20540, skipped=27, lr=[0.00019996979116881688, 0.00019996979116881688], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20540 loss: 0.8333 iter time (s): 4.090 samples/sec: 31.298
g0238:  iteration    20540/10000000 | consumed samples:      2629120 | consumed tokens:   5384437760 | elapsed time per iteration (ms): 4125.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.253797E-01 | loss scale: 65536.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.026 | tokens per gpu per second (tgs): 1985.634 | TFLOPs: 15.98 |
g0220: [2024-08-10 06:24:36,059] [INFO] [logging.py:96:log_dist] [Rank 0] step=20550, skipped=27, lr=[0.00019996972470562113, 0.00019996972470562113], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20550 loss: 0.8340 iter time (s): 4.107 samples/sec: 31.166
g0238:  iteration    20550/10000000 | consumed samples:      2630400 | consumed tokens:   5387059200 | elapsed time per iteration (ms): 4142.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.320511E-01 | loss scale: 65536.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.900 | tokens per gpu per second (tgs): 1977.596 | TFLOPs: 15.91 |
g0220: [2024-08-10 06:25:15,808] [INFO] [logging.py:96:log_dist] [Rank 0] step=20560, skipped=27, lr=[0.00019996965816940367, 0.00019996965816940367], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20560 loss: 0.8069 iter time (s): 3.939 samples/sec: 32.493
g0238:  iteration    20560/10000000 | consumed samples:      2631680 | consumed tokens:   5389680640 | elapsed time per iteration (ms): 3974.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.328652E-01 | loss scale: 65536.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.202 | tokens per gpu per second (tgs): 2060.942 | TFLOPs: 16.58 |
g0238: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0234: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0238: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0225: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0235: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0237: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0236: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0238: [2024-08-10 06:25:36,524] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0233: [2024-08-10 06:25:36,525] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 65536.0 to 131072.0
g0220: [2024-08-10 06:25:57,366] [INFO] [logging.py:96:log_dist] [Rank 0] step=20570, skipped=27, lr=[0.00019996959156016455, 0.00019996959156016455], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20570 loss: 0.8431 iter time (s): 4.122 samples/sec: 31.052
g0238:  iteration    20570/10000000 | consumed samples:      2632960 | consumed tokens:   5392302080 | elapsed time per iteration (ms): 4155.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.333413E-01 | loss scale: 131072.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.801 | tokens per gpu per second (tgs): 1971.268 | TFLOPs: 15.86 |
g0220: [2024-08-10 06:26:37,319] [INFO] [logging.py:96:log_dist] [Rank 0] step=20580, skipped=27, lr=[0.00019996952487790387, 0.00019996952487790387], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20580 loss: 0.8448 iter time (s): 3.963 samples/sec: 32.300
g0238:  iteration    20580/10000000 | consumed samples:      2634240 | consumed tokens:   5394923520 | elapsed time per iteration (ms): 3995.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.235037E-01 | loss scale: 131072.0 | grad norm: 0.219 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.037 | tokens per gpu per second (tgs): 2050.396 | TFLOPs: 16.50 |
g0220: [2024-08-10 06:27:18,370] [INFO] [logging.py:96:log_dist] [Rank 0] step=20590, skipped=27, lr=[0.00019996945812262166, 0.00019996945812262166], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20590 loss: 0.7988 iter time (s): 4.073 samples/sec: 31.429
g0238:  iteration    20590/10000000 | consumed samples:      2635520 | consumed tokens:   5397544960 | elapsed time per iteration (ms): 4105.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.197379E-01 | loss scale: 131072.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.180 | tokens per gpu per second (tgs): 1995.536 | TFLOPs: 16.06 |
g0220: [2024-08-10 06:28:00,679] [INFO] [logging.py:96:log_dist] [Rank 0] step=20600, skipped=27, lr=[0.00019996939129431795, 0.00019996939129431795], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20600 loss: 0.8303 iter time (s): 4.198 samples/sec: 30.488
g0238:  iteration    20600/10000000 | consumed samples:      2636800 | consumed tokens:   5400166400 | elapsed time per iteration (ms): 4230.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.467380E-01 | loss scale: 131072.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.254 | tokens per gpu per second (tgs): 1936.275 | TFLOPs: 15.58 |
g0220: [2024-08-10 06:28:41,910] [INFO] [logging.py:96:log_dist] [Rank 0] step=20610, skipped=27, lr=[0.0001999693243929928, 0.0001999693243929928], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20610 loss: 0.8505 iter time (s): 4.090 samples/sec: 31.292
g0238:  iteration    20610/10000000 | consumed samples:      2638080 | consumed tokens:   5402787840 | elapsed time per iteration (ms): 4123.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.350688E-01 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.041 | tokens per gpu per second (tgs): 1986.594 | TFLOPs: 15.99 |
g0220: [2024-08-10 06:29:22,584] [INFO] [logging.py:96:log_dist] [Rank 0] step=20620, skipped=27, lr=[0.0001999692574186463, 0.0001999692574186463], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20620 loss: 0.8427 iter time (s): 4.033 samples/sec: 31.734
g0238:  iteration    20620/10000000 | consumed samples:      2639360 | consumed tokens:   5405409280 | elapsed time per iteration (ms): 4066.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.323802E-01 | loss scale: 131072.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.474 | tokens per gpu per second (tgs): 2014.331 | TFLOPs: 16.21 |
g0220: [2024-08-10 06:30:03,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=20630, skipped=27, lr=[0.00019996919037127846, 0.00019996919037127846], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20630 loss: 0.8350 iter time (s): 4.037 samples/sec: 31.703
g0238:  iteration    20630/10000000 | consumed samples:      2640640 | consumed tokens:   5408030720 | elapsed time per iteration (ms): 4069.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.197588E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.451 | tokens per gpu per second (tgs): 2012.836 | TFLOPs: 16.20 |
g0220: [2024-08-10 06:30:45,122] [INFO] [logging.py:96:log_dist] [Rank 0] step=20640, skipped=27, lr=[0.00019996912325088932, 0.00019996912325088932], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20640 loss: 0.8051 iter time (s): 4.152 samples/sec: 30.832
g0238:  iteration    20640/10000000 | consumed samples:      2641920 | consumed tokens:   5410652160 | elapsed time per iteration (ms): 4184.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.233245E-01 | loss scale: 131072.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.592 | tokens per gpu per second (tgs): 1957.900 | TFLOPs: 15.76 |
g0220: [2024-08-10 06:31:26,199] [INFO] [logging.py:96:log_dist] [Rank 0] step=20650, skipped=27, lr=[0.00019996905605747898, 0.00019996905605747898], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20650 loss: 0.8142 iter time (s): 4.074 samples/sec: 31.415
g0238:  iteration    20650/10000000 | consumed samples:      2643200 | consumed tokens:   5413273600 | elapsed time per iteration (ms): 4107.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.156061E-01 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.163 | tokens per gpu per second (tgs): 1994.405 | TFLOPs: 16.05 |
g0220: [2024-08-10 06:32:07,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=20660, skipped=27, lr=[0.0001999689887910475, 0.0001999689887910475], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20660 loss: 0.8253 iter time (s): 4.141 samples/sec: 30.911
g0238:  iteration    20660/10000000 | consumed samples:      2644480 | consumed tokens:   5415895040 | elapsed time per iteration (ms): 4173.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.205935E-01 | loss scale: 131072.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.671 | tokens per gpu per second (tgs): 1962.943 | TFLOPs: 15.80 |
g0220: [2024-08-10 06:32:49,916] [INFO] [logging.py:96:log_dist] [Rank 0] step=20670, skipped=27, lr=[0.00019996892145159482, 0.00019996892145159482], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20670 loss: 0.8334 iter time (s): 4.165 samples/sec: 30.729
g0238:  iteration    20670/10000000 | consumed samples:      2645760 | consumed tokens:   5418516480 | elapsed time per iteration (ms): 4198.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.253883E-01 | loss scale: 131072.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.488 | tokens per gpu per second (tgs): 1951.252 | TFLOPs: 15.70 |
g0220: [2024-08-10 06:33:32,357] [INFO] [logging.py:96:log_dist] [Rank 0] step=20680, skipped=27, lr=[0.00019996885403912113, 0.00019996885403912113], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20680 loss: 0.7890 iter time (s): 4.211 samples/sec: 30.396
g0238:  iteration    20680/10000000 | consumed samples:      2647040 | consumed tokens:   5421137920 | elapsed time per iteration (ms): 4244.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.205521E-01 | loss scale: 131072.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.159 | tokens per gpu per second (tgs): 1930.194 | TFLOPs: 15.53 |
g0220: [2024-08-10 06:34:14,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=20690, skipped=27, lr=[0.00019996878655362637, 0.00019996878655362637], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20690 loss: 0.8055 iter time (s): 4.167 samples/sec: 30.714
g0238:  iteration    20690/10000000 | consumed samples:      2648320 | consumed tokens:   5423759360 | elapsed time per iteration (ms): 4201.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.197201E-01 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.469 | tokens per gpu per second (tgs): 1950.017 | TFLOPs: 15.69 |
g0220: [2024-08-10 06:34:55,167] [INFO] [logging.py:96:log_dist] [Rank 0] step=20700, skipped=27, lr=[0.0001999687189951107, 0.0001999687189951107], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20700 loss: 0.8207 iter time (s): 4.047 samples/sec: 31.627
g0238:  iteration    20700/10000000 | consumed samples:      2649600 | consumed tokens:   5426380800 | elapsed time per iteration (ms): 4080.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.232985E-01 | loss scale: 131072.0 | grad norm: 0.174 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.372 | tokens per gpu per second (tgs): 2007.837 | TFLOPs: 16.16 |
g0220: [2024-08-10 06:35:34,283] [INFO] [logging.py:96:log_dist] [Rank 0] step=20710, skipped=27, lr=[0.00019996865136357407, 0.00019996865136357407], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20710 loss: 0.8363 iter time (s): 3.879 samples/sec: 32.997
g0238:  iteration    20710/10000000 | consumed samples:      2650880 | consumed tokens:   5429002240 | elapsed time per iteration (ms): 3911.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.277971E-01 | loss scale: 131072.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.724 | tokens per gpu per second (tgs): 2094.331 | TFLOPs: 16.85 |
g0220: [2024-08-10 06:36:14,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=20720, skipped=27, lr=[0.0001999685836590166, 0.0001999685836590166], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20720 loss: 0.8187 iter time (s): 4.033 samples/sec: 31.736
g0238:  iteration    20720/10000000 | consumed samples:      2652160 | consumed tokens:   5431623680 | elapsed time per iteration (ms): 4065.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.225331E-01 | loss scale: 131072.0 | grad norm: 0.223 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.484 | tokens per gpu per second (tgs): 2014.986 | TFLOPs: 16.21 |
g0220: [2024-08-10 06:36:56,704] [INFO] [logging.py:96:log_dist] [Rank 0] step=20730, skipped=27, lr=[0.00019996851588143828, 0.00019996851588143828], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20730 loss: 0.8192 iter time (s): 4.144 samples/sec: 30.889
g0238:  iteration    20730/10000000 | consumed samples:      2653440 | consumed tokens:   5434245120 | elapsed time per iteration (ms): 4176.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.286983E-01 | loss scale: 131072.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.648 | tokens per gpu per second (tgs): 1961.445 | TFLOPs: 15.78 |
g0220: [2024-08-10 06:37:38,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=20740, skipped=27, lr=[0.00019996844803083926, 0.00019996844803083926], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20740 loss: 0.8322 iter time (s): 4.194 samples/sec: 30.523
g0238:  iteration    20740/10000000 | consumed samples:      2654720 | consumed tokens:   5436866560 | elapsed time per iteration (ms): 4226.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.226483E-01 | loss scale: 131072.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.289 | tokens per gpu per second (tgs): 1938.478 | TFLOPs: 15.60 |
g0220: [2024-08-10 06:38:19,570] [INFO] [logging.py:96:log_dist] [Rank 0] step=20750, skipped=27, lr=[0.00019996838010721948, 0.00019996838010721948], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20750 loss: 0.7891 iter time (s): 4.027 samples/sec: 31.784
g0238:  iteration    20750/10000000 | consumed samples:      2656000 | consumed tokens:   5439488000 | elapsed time per iteration (ms): 4060.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.207338E-01 | loss scale: 131072.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.522 | tokens per gpu per second (tgs): 2017.413 | TFLOPs: 16.23 |
g0220: [2024-08-10 06:38:59,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=20760, skipped=27, lr=[0.0001999683121105791, 0.0001999683121105791], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20760 loss: 0.8028 iter time (s): 3.998 samples/sec: 32.017
g0238:  iteration    20760/10000000 | consumed samples:      2657280 | consumed tokens:   5442109440 | elapsed time per iteration (ms): 4032.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.281890E-01 | loss scale: 131072.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.739 | tokens per gpu per second (tgs): 2031.275 | TFLOPs: 16.35 |
g0220: [2024-08-10 06:39:40,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=20770, skipped=27, lr=[0.00019996824404091808, 0.00019996824404091808], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20770 loss: 0.8130 iter time (s): 4.064 samples/sec: 31.496
g0238:  iteration    20770/10000000 | consumed samples:      2658560 | consumed tokens:   5444730880 | elapsed time per iteration (ms): 4097.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.237579E-01 | loss scale: 131072.0 | grad norm: 0.343 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.241 | tokens per gpu per second (tgs): 1999.446 | TFLOPs: 16.09 |
g0220: [2024-08-10 06:40:22,586] [INFO] [logging.py:96:log_dist] [Rank 0] step=20780, skipped=27, lr=[0.00019996817589823654, 0.00019996817589823654], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20780 loss: 0.8227 iter time (s): 4.137 samples/sec: 30.937
g0238:  iteration    20780/10000000 | consumed samples:      2659840 | consumed tokens:   5447352320 | elapsed time per iteration (ms): 4171.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.251884E-01 | loss scale: 131072.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.685 | tokens per gpu per second (tgs): 1963.816 | TFLOPs: 15.80 |
g0220: [2024-08-10 06:41:03,804] [INFO] [logging.py:96:log_dist] [Rank 0] step=20790, skipped=27, lr=[0.0001999681076825345, 0.0001999681076825345], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20790 loss: 0.8084 iter time (s): 4.089 samples/sec: 31.300
g0238:  iteration    20790/10000000 | consumed samples:      2661120 | consumed tokens:   5449973760 | elapsed time per iteration (ms): 4121.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.317270E-01 | loss scale: 131072.0 | grad norm: 0.226 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.054 | tokens per gpu per second (tgs): 1987.473 | TFLOPs: 15.99 |
g0220: [2024-08-10 06:41:44,415] [INFO] [logging.py:96:log_dist] [Rank 0] step=20800, skipped=27, lr=[0.000199968039393812, 0.000199968039393812], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20800 loss: 0.8370 iter time (s): 4.029 samples/sec: 31.773
g0238:  iteration    20800/10000000 | consumed samples:      2662400 | consumed tokens:   5452595200 | elapsed time per iteration (ms): 4061.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.241179E-01 | loss scale: 131072.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.519 | tokens per gpu per second (tgs): 2017.203 | TFLOPs: 16.23 |
g0220: [2024-08-10 06:42:24,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=20810, skipped=27, lr=[0.00019996797103206908, 0.00019996797103206908], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20810 loss: 0.8464 iter time (s): 3.957 samples/sec: 32.346
g0238:  iteration    20810/10000000 | consumed samples:      2663680 | consumed tokens:   5455216640 | elapsed time per iteration (ms): 3990.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.316640E-01 | loss scale: 131072.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.079 | tokens per gpu per second (tgs): 2053.028 | TFLOPs: 16.52 |
g0220: [2024-08-10 06:43:04,243] [INFO] [logging.py:96:log_dist] [Rank 0] step=20820, skipped=27, lr=[0.00019996790259730585, 0.00019996790259730585], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20820 loss: 0.8190 iter time (s): 3.960 samples/sec: 32.321
g0238:  iteration    20820/10000000 | consumed samples:      2664960 | consumed tokens:   5457838080 | elapsed time per iteration (ms): 3992.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.301895E-01 | loss scale: 131072.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.060 | tokens per gpu per second (tgs): 2051.821 | TFLOPs: 16.51 |
g0220: [2024-08-10 06:43:44,035] [INFO] [logging.py:96:log_dist] [Rank 0] step=20830, skipped=27, lr=[0.0001999678340895223, 0.0001999678340895223], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20830 loss: 0.8396 iter time (s): 3.947 samples/sec: 32.433
g0238:  iteration    20830/10000000 | consumed samples:      2666240 | consumed tokens:   5460459520 | elapsed time per iteration (ms): 3979.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.154428E-01 | loss scale: 131072.0 | grad norm: 0.252 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.168 | tokens per gpu per second (tgs): 2058.727 | TFLOPs: 16.57 |
g0220: [2024-08-10 06:44:24,564] [INFO] [logging.py:96:log_dist] [Rank 0] step=20840, skipped=27, lr=[0.00019996776550871857, 0.00019996776550871857], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20840 loss: 0.8572 iter time (s): 4.020 samples/sec: 31.844
g0238:  iteration    20840/10000000 | consumed samples:      2667520 | consumed tokens:   5463080960 | elapsed time per iteration (ms): 4052.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.344004E-01 | loss scale: 131072.0 | grad norm: 0.260 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.583 | tokens per gpu per second (tgs): 2021.282 | TFLOPs: 16.27 |
g0220: [2024-08-10 06:45:05,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=20850, skipped=27, lr=[0.00019996769685489462, 0.00019996769685489462], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20850 loss: 0.8025 iter time (s): 4.076 samples/sec: 31.402
g0238:  iteration    20850/10000000 | consumed samples:      2668800 | consumed tokens:   5465702400 | elapsed time per iteration (ms): 4108.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.222091E-01 | loss scale: 131072.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.154 | tokens per gpu per second (tgs): 1993.852 | TFLOPs: 16.04 |
g0220: [2024-08-10 06:45:46,544] [INFO] [logging.py:96:log_dist] [Rank 0] step=20860, skipped=27, lr=[0.00019996762812805055, 0.00019996762812805055], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20860 loss: 0.8337 iter time (s): 4.057 samples/sec: 31.552
g0238:  iteration    20860/10000000 | consumed samples:      2670080 | consumed tokens:   5468323840 | elapsed time per iteration (ms): 4089.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.249138E-01 | loss scale: 131072.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.300 | tokens per gpu per second (tgs): 2003.231 | TFLOPs: 16.12 |
g0220: [2024-08-10 06:46:27,037] [INFO] [logging.py:96:log_dist] [Rank 0] step=20870, skipped=27, lr=[0.00019996755932818642, 0.00019996755932818642], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20870 loss: 0.8272 iter time (s): 4.017 samples/sec: 31.866
g0238:  iteration    20870/10000000 | consumed samples:      2671360 | consumed tokens:   5470945280 | elapsed time per iteration (ms): 4049.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.291126E-01 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.610 | tokens per gpu per second (tgs): 2023.041 | TFLOPs: 16.28 |
g0220: [2024-08-10 06:47:08,963] [INFO] [logging.py:96:log_dist] [Rank 0] step=20880, skipped=27, lr=[0.00019996749045530224, 0.00019996749045530224], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20880 loss: 0.8196 iter time (s): 4.160 samples/sec: 30.773
g0238:  iteration    20880/10000000 | consumed samples:      2672640 | consumed tokens:   5473566720 | elapsed time per iteration (ms): 4192.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.247187E-01 | loss scale: 131072.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.528 | tokens per gpu per second (tgs): 1953.824 | TFLOPs: 15.72 |
g0220: [2024-08-10 06:47:49,827] [INFO] [logging.py:96:log_dist] [Rank 0] step=20890, skipped=27, lr=[0.0001999674215093981, 0.0001999674215093981], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20890 loss: 0.8221 iter time (s): 4.053 samples/sec: 31.580
g0238:  iteration    20890/10000000 | consumed samples:      2673920 | consumed tokens:   5476188160 | elapsed time per iteration (ms): 4086.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.264378E-01 | loss scale: 131072.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.326 | tokens per gpu per second (tgs): 2004.854 | TFLOPs: 16.13 |
g0220: [2024-08-10 06:48:32,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=20900, skipped=27, lr=[0.00019996735249047407, 0.00019996735249047407], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20900 loss: 0.8535 iter time (s): 4.236 samples/sec: 30.219
g0238:  iteration    20900/10000000 | consumed samples:      2675200 | consumed tokens:   5478809600 | elapsed time per iteration (ms): 4268.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.301412E-01 | loss scale: 131072.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.988 | tokens per gpu per second (tgs): 1919.239 | TFLOPs: 15.44 |
g0220: [2024-08-10 06:49:14,287] [INFO] [logging.py:96:log_dist] [Rank 0] step=20910, skipped=27, lr=[0.00019996728339853013, 0.00019996728339853013], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20910 loss: 0.8769 iter time (s): 4.145 samples/sec: 30.880
g0238:  iteration    20910/10000000 | consumed samples:      2676480 | consumed tokens:   5481431040 | elapsed time per iteration (ms): 4177.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.297135E-01 | loss scale: 131072.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.637 | tokens per gpu per second (tgs): 1960.777 | TFLOPs: 15.78 |
g0220: [2024-08-10 06:49:54,738] [INFO] [logging.py:96:log_dist] [Rank 0] step=20920, skipped=27, lr=[0.0001999672142335664, 0.0001999672142335664], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20920 loss: 0.8095 iter time (s): 4.013 samples/sec: 31.899
g0238:  iteration    20920/10000000 | consumed samples:      2677760 | consumed tokens:   5484052480 | elapsed time per iteration (ms): 4044.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.352543E-01 | loss scale: 131072.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.645 | tokens per gpu per second (tgs): 2025.299 | TFLOPs: 16.30 |
g0220: [2024-08-10 06:50:34,475] [INFO] [logging.py:96:log_dist] [Rank 0] step=20930, skipped=27, lr=[0.00019996714499558293, 0.00019996714499558293], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20930 loss: 0.8062 iter time (s): 3.941 samples/sec: 32.479
g0238:  iteration    20930/10000000 | consumed samples:      2679040 | consumed tokens:   5486673920 | elapsed time per iteration (ms): 3973.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.168856E-01 | loss scale: 131072.0 | grad norm: 0.212 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.212 | tokens per gpu per second (tgs): 2061.593 | TFLOPs: 16.59 |
g0220: [2024-08-10 06:51:14,884] [INFO] [logging.py:96:log_dist] [Rank 0] step=20940, skipped=27, lr=[0.0001999670756845797, 0.0001999670756845797], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20940 loss: 0.8183 iter time (s): 4.009 samples/sec: 31.931
g0238:  iteration    20940/10000000 | consumed samples:      2680320 | consumed tokens:   5489295360 | elapsed time per iteration (ms): 4040.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.223298E-01 | loss scale: 131072.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.676 | tokens per gpu per second (tgs): 2027.257 | TFLOPs: 16.31 |
g0220: [2024-08-10 06:51:55,250] [INFO] [logging.py:96:log_dist] [Rank 0] step=20950, skipped=27, lr=[0.00019996700630055687, 0.00019996700630055687], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20950 loss: 0.7938 iter time (s): 4.004 samples/sec: 31.968
g0238:  iteration    20950/10000000 | consumed samples:      2681600 | consumed tokens:   5491916800 | elapsed time per iteration (ms): 4036.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.203749E-01 | loss scale: 131072.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.710 | tokens per gpu per second (tgs): 2029.433 | TFLOPs: 16.33 |
g0220: [2024-08-10 06:52:36,394] [INFO] [logging.py:96:log_dist] [Rank 0] step=20960, skipped=27, lr=[0.00019996693684351446, 0.00019996693684351446], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20960 loss: 0.8144 iter time (s): 4.082 samples/sec: 31.356
g0238:  iteration    20960/10000000 | consumed samples:      2682880 | consumed tokens:   5494538240 | elapsed time per iteration (ms): 4114.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.175601E-01 | loss scale: 131072.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.111 | tokens per gpu per second (tgs): 1991.104 | TFLOPs: 16.02 |
g0220: [2024-08-10 06:53:18,331] [INFO] [logging.py:96:log_dist] [Rank 0] step=20970, skipped=27, lr=[0.00019996686731345246, 0.00019996686731345246], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20970 loss: 0.8234 iter time (s): 4.161 samples/sec: 30.759
g0238:  iteration    20970/10000000 | consumed samples:      2684160 | consumed tokens:   5497159680 | elapsed time per iteration (ms): 4193.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.268399E-01 | loss scale: 131072.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.522 | tokens per gpu per second (tgs): 1953.412 | TFLOPs: 15.72 |
g0220: [2024-08-10 06:53:59,595] [INFO] [logging.py:96:log_dist] [Rank 0] step=20980, skipped=27, lr=[0.00019996679771037097, 0.00019996679771037097], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20980 loss: 0.8214 iter time (s): 4.080 samples/sec: 31.372
g0238:  iteration    20980/10000000 | consumed samples:      2685440 | consumed tokens:   5499781120 | elapsed time per iteration (ms): 4126.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.098572E-01 | loss scale: 131072.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.019 | tokens per gpu per second (tgs): 1985.209 | TFLOPs: 15.98 |
g0220: [2024-08-10 06:54:39,371] [INFO] [logging.py:96:log_dist] [Rank 0] step=20990, skipped=27, lr=[0.00019996672803427007, 0.00019996672803427007], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 20990 loss: 0.8116 iter time (s): 3.945 samples/sec: 32.447
g0238:  iteration    20990/10000000 | consumed samples:      2686720 | consumed tokens:   5502402560 | elapsed time per iteration (ms): 3977.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.165131E-01 | loss scale: 131072.0 | grad norm: 0.187 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.181 | tokens per gpu per second (tgs): 2059.558 | TFLOPs: 16.57 |
g0220: [2024-08-10 06:55:19,334] [INFO] [logging.py:96:log_dist] [Rank 0] step=21000, skipped=27, lr=[0.00019996665828514974, 0.00019996665828514974], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21000 loss: 0.8420 iter time (s): 3.964 samples/sec: 32.291
g0238:  iteration    21000/10000000 | consumed samples:      2688000 | consumed tokens:   5505024000 | elapsed time per iteration (ms): 3996.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.298301E-01 | loss scale: 131072.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.030 | tokens per gpu per second (tgs): 2049.930 | TFLOPs: 16.50 |
g0238: -------------------------------------------------------------------------------------------------
g0238:  validation loss at iteration 21000 | lm loss value: 8.200764E-01 | lm loss PPL: 2.270673E+00 | 
g0238: -------------------------------------------------------------------------------------------------
g0220: saving checkpoint at iteration   21000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: [2024-08-10 07:01:46,821] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint global_step21000 is about to be saved!
g0220: [2024-08-10 07:01:46,826] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0220: [2024-08-10 07:01:46,827] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0220: [2024-08-10 07:01:46,828] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0238: [2024-08-10 07:01:46,828] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0238: [2024-08-10 07:01:46,828] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0238: [2024-08-10 07:01:46,828] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0234: [2024-08-10 07:01:46,829] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0234: [2024-08-10 07:01:46,829] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0234: [2024-08-10 07:01:46,829] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0235: [2024-08-10 07:01:46,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0235: [2024-08-10 07:01:46,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0235: [2024-08-10 07:01:46,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0237: [2024-08-10 07:01:46,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0237: [2024-08-10 07:01:46,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0237: [2024-08-10 07:01:46,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0236: [2024-08-10 07:01:46,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0236: [2024-08-10 07:01:46,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0225: [2024-08-10 07:01:46,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0225: [2024-08-10 07:01:46,832] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0236: [2024-08-10 07:01:46,833] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0233: [2024-08-10 07:01:46,833] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0225: [2024-08-10 07:01:46,833] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0233: [2024-08-10 07:01:46,833] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0233: [2024-08-10 07:01:46,833] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0238: [2024-08-10 07:01:46,857] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt...
g0220: [2024-08-10 07:01:46,860] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt...
g0234: [2024-08-10 07:01:46,867] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt...
g0235: [2024-08-10 07:01:46,869] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt...
g0237: [2024-08-10 07:01:46,869] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt...
g0233: [2024-08-10 07:01:46,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt...
g0225: [2024-08-10 07:01:46,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt...
g0236: [2024-08-10 07:01:46,870] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt...
g0233: [2024-08-10 07:01:46,995] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_08-model_00-model_states.pt.
g0234: [2024-08-10 07:01:47,013] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_11-model_00-model_states.pt.
g0233: [2024-08-10 07:01:47,032] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt...
g0236: [2024-08-10 07:01:47,048] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_17-model_00-model_states.pt.
g0237: [2024-08-10 07:01:47,053] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_20-model_00-model_states.pt.
g0234: [2024-08-10 07:01:47,053] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt...
g0238: [2024-08-10 07:01:47,059] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_23-model_00-model_states.pt.
g0238: [2024-08-10 07:01:47,060] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt...
g0238: [2024-08-10 07:01:47,062] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_24-model_00-model_states.pt.
g0225: [2024-08-10 07:01:47,079] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_05-model_00-model_states.pt.
g0236: [2024-08-10 07:01:47,087] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt...
g0237: [2024-08-10 07:01:47,091] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt...
g0238: [2024-08-10 07:01:47,115] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt...
g0225: [2024-08-10 07:01:47,118] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt...
g0220: [2024-08-10 07:01:47,169] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_01-model_00-model_states.pt.
g0220: [2024-08-10 07:01:47,190] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt...
g0235: [2024-08-10 07:01:47,215] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_14-model_00-model_states.pt.
g0236: [2024-08-10 07:01:47,225] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_18-model_00-model_states.pt.
g0234: [2024-08-10 07:01:47,229] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_12-model_00-model_states.pt.
g0238: [2024-08-10 07:01:47,244] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_25-model_00-model_states.pt.
g0238: [2024-08-10 07:01:47,245] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_07_model_states.pt...
g0235: [2024-08-10 07:01:47,254] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt...
g0236: [2024-08-10 07:01:47,261] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt...
g0234: [2024-08-10 07:01:47,264] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt...
g0225: [2024-08-10 07:01:47,305] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_06-model_00-model_states.pt.
g0225: [2024-08-10 07:01:47,340] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt...
g0234: [2024-08-10 07:01:47,360] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_13-model_00-model_states.pt.
g0234: [2024-08-10 07:01:47,362] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_03_model_states.pt...
g0235: [2024-08-10 07:01:47,372] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_15-model_00-model_states.pt.
g0220: [2024-08-10 07:01:47,379] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_02-model_00-model_states.pt.
g0220: [2024-08-10 07:01:47,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt...
g0235: [2024-08-10 07:01:47,406] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt...
g0236: [2024-08-10 07:01:47,453] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_19-model_00-model_states.pt.
g0236: [2024-08-10 07:01:47,455] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_05_model_states.pt...
g0233: [2024-08-10 07:01:47,468] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_09-model_00-model_states.pt.
g0225: [2024-08-10 07:01:47,495] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_07-model_00-model_states.pt.
g0225: [2024-08-10 07:01:47,497] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_01_model_states.pt...
g0220: [2024-08-10 07:01:47,500] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_03-model_00-model_states.pt.
g0233: [2024-08-10 07:01:47,502] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt...
g0237: [2024-08-10 07:01:47,505] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_21-model_00-model_states.pt.
g0220: [2024-08-10 07:01:47,519] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt...
g0237: [2024-08-10 07:01:47,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt...
g0235: [2024-08-10 07:01:47,542] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_16-model_00-model_states.pt.
g0235: [2024-08-10 07:01:47,544] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_04_model_states.pt...
g0233: [2024-08-10 07:01:47,617] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_10-model_00-model_states.pt.
g0233: [2024-08-10 07:01:47,618] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_02_model_states.pt...
g0220: [2024-08-10 07:01:47,625] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_04-model_00-model_states.pt.
g0220: [2024-08-10 07:01:47,626] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt
g0220: [2024-08-10 07:01:47,627] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt...
g0237: [2024-08-10 07:01:47,657] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/layer_22-model_00-model_states.pt.
g0237: [2024-08-10 07:01:47,658] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_06_model_states.pt...
g0238: [2024-08-10 07:01:49,175] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_07_model_states.pt.
g0238: [2024-08-10 07:01:49,176] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0236: [2024-08-10 07:01:49,933] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_05_model_states.pt.
g0236: [2024-08-10 07:01:49,933] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0235: [2024-08-10 07:01:50,030] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_04_model_states.pt.
g0235: [2024-08-10 07:01:50,030] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0225: [2024-08-10 07:01:50,043] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_01_model_states.pt.
g0225: [2024-08-10 07:01:50,043] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0237: [2024-08-10 07:01:50,192] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_06_model_states.pt.
g0237: [2024-08-10 07:01:50,193] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0233: [2024-08-10 07:01:50,222] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_02_model_states.pt.
g0233: [2024-08-10 07:01:50,222] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0234: [2024-08-10 07:01:50,348] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_03_model_states.pt.
g0234: [2024-08-10 07:01:50,348] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0220: [2024-08-10 07:01:51,506] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase/global_step21000/mp_rank_00_model_states.pt.
g0220: [2024-08-10 07:01:51,507] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21000 is ready now!
g0220:   successfully saved checkpoint at iteration   21000 to /groups/gcf51099/crypto_llm/models/0.latin_wikipedia_poly_010000_1234_True/checkpoint/gpt_1.1B_tok300B_lr2.0e-4_min1.0e-5_w3000M_d300B_cosine_gbs128_mbs1_g32_pp8_seed1234_rebase
g0220: Checkpoint Save GB: 22.521, GB/Sec: 4.79, Latency(second): 4.704
g0238: (min, max) time across ranks (ms):
g0238:     save-checkpoint ................................: (4703.11, 4704.24)
g0220: [2024-08-10 07:02:34,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=21010, skipped=27, lr=[0.00019996658846301013, 0.00019996658846301013], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21010 loss: 0.8129 iter time (s): 4.219 samples/sec: 30.342
g0238:  iteration    21010/10000000 | consumed samples:      2689280 | consumed tokens:   5507645440 | elapsed time per iteration (ms): 43467.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.154826E-01 | loss scale: 131072.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 2.945 | tokens per gpu per second (tgs): 188.464 | TFLOPs: 1.52 |
g0220: [2024-08-10 07:03:14,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=21020, skipped=27, lr=[0.0001999665185678512, 0.0001999665185678512], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21020 loss: 0.7950 iter time (s): 3.990 samples/sec: 32.082
g0238:  iteration    21020/10000000 | consumed samples:      2690560 | consumed tokens:   5510266880 | elapsed time per iteration (ms): 4023.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.153230E-01 | loss scale: 131072.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.817 | tokens per gpu per second (tgs): 2036.269 | TFLOPs: 16.39 |
g0220: [2024-08-10 07:03:56,458] [INFO] [logging.py:96:log_dist] [Rank 0] step=21030, skipped=27, lr=[0.00019996644859967308, 0.00019996644859967308], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21030 loss: 0.8394 iter time (s): 4.189 samples/sec: 30.554
g0238:  iteration    21030/10000000 | consumed samples:      2691840 | consumed tokens:   5512888320 | elapsed time per iteration (ms): 4222.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.179133E-01 | loss scale: 131072.0 | grad norm: 0.175 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.316 | tokens per gpu per second (tgs): 1940.224 | TFLOPs: 15.61 |
g0220: [2024-08-10 07:04:37,301] [INFO] [logging.py:96:log_dist] [Rank 0] step=21040, skipped=27, lr=[0.00019996637855847576, 0.00019996637855847576], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21040 loss: 0.8458 iter time (s): 4.052 samples/sec: 31.589
g0238:  iteration    21040/10000000 | consumed samples:      2693120 | consumed tokens:   5515509760 | elapsed time per iteration (ms): 4084.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.204759E-01 | loss scale: 131072.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.338 | tokens per gpu per second (tgs): 2005.645 | TFLOPs: 16.14 |
g0220: [2024-08-10 07:05:18,483] [INFO] [logging.py:96:log_dist] [Rank 0] step=21050, skipped=27, lr=[0.00019996630844425938, 0.00019996630844425938], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21050 loss: 0.8185 iter time (s): 4.086 samples/sec: 31.329
g0238:  iteration    21050/10000000 | consumed samples:      2694400 | consumed tokens:   5518131200 | elapsed time per iteration (ms): 4118.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.263309E-01 | loss scale: 131072.0 | grad norm: 0.203 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.076 | tokens per gpu per second (tgs): 1988.862 | TFLOPs: 16.00 |
g0220: [2024-08-10 07:06:00,877] [INFO] [logging.py:96:log_dist] [Rank 0] step=21060, skipped=27, lr=[0.00019996623825702388, 0.00019996623825702388], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21060 loss: 0.8029 iter time (s): 4.206 samples/sec: 30.431
g0238:  iteration    21060/10000000 | consumed samples:      2695680 | consumed tokens:   5520752640 | elapsed time per iteration (ms): 4238.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.089327E-01 | loss scale: 131072.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.199 | tokens per gpu per second (tgs): 1932.751 | TFLOPs: 15.55 |
g0233: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0236: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0233: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0225: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0235: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 07:06:21,781] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0237: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0238: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0234: [2024-08-10 07:06:21,782] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 131072.0 to 262144.0
g0220: [2024-08-10 07:06:42,376] [INFO] [logging.py:96:log_dist] [Rank 0] step=21070, skipped=27, lr=[0.0001999661679967694, 0.0001999661679967694], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21070 loss: 0.7695 iter time (s): 4.117 samples/sec: 31.089
g0238:  iteration    21070/10000000 | consumed samples:      2696960 | consumed tokens:   5523374080 | elapsed time per iteration (ms): 4149.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.117142E-01 | loss scale: 262144.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.845 | tokens per gpu per second (tgs): 1974.076 | TFLOPs: 15.89 |
g0220: [2024-08-10 07:07:23,630] [INFO] [logging.py:96:log_dist] [Rank 0] step=21080, skipped=27, lr=[0.00019996609766349599, 0.00019996609766349599], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21080 loss: 0.8582 iter time (s): 4.093 samples/sec: 31.276
g0238:  iteration    21080/10000000 | consumed samples:      2698240 | consumed tokens:   5525995520 | elapsed time per iteration (ms): 4125.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.303274E-01 | loss scale: 262144.0 | grad norm: 0.188 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.027 | tokens per gpu per second (tgs): 1985.712 | TFLOPs: 15.98 |
g0220: [2024-08-10 07:08:05,294] [INFO] [logging.py:96:log_dist] [Rank 0] step=21090, skipped=27, lr=[0.00019996602725720364, 0.00019996602725720364], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21090 loss: 0.8696 iter time (s): 4.134 samples/sec: 30.962
g0238:  iteration    21090/10000000 | consumed samples:      2699520 | consumed tokens:   5528616960 | elapsed time per iteration (ms): 4166.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.256424E-01 | loss scale: 262144.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.722 | tokens per gpu per second (tgs): 1966.221 | TFLOPs: 15.82 |
g0220: [2024-08-10 07:08:47,114] [INFO] [logging.py:96:log_dist] [Rank 0] step=21100, skipped=27, lr=[0.00019996595677789246, 0.00019996595677789246], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21100 loss: 0.8053 iter time (s): 4.149 samples/sec: 30.849
g0238:  iteration    21100/10000000 | consumed samples:      2700800 | consumed tokens:   5531238400 | elapsed time per iteration (ms): 4182.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.250438E-01 | loss scale: 262144.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.608 | tokens per gpu per second (tgs): 1958.892 | TFLOPs: 15.76 |
g0220: [2024-08-10 07:09:29,284] [INFO] [logging.py:96:log_dist] [Rank 0] step=21110, skipped=27, lr=[0.00019996588622556247, 0.00019996588622556247], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21110 loss: 0.8110 iter time (s): 4.184 samples/sec: 30.590
g0238:  iteration    21110/10000000 | consumed samples:      2702080 | consumed tokens:   5533859840 | elapsed time per iteration (ms): 4217.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.200476E-01 | loss scale: 262144.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.353 | tokens per gpu per second (tgs): 1942.608 | TFLOPs: 15.63 |
g0220: [2024-08-10 07:10:10,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=21120, skipped=27, lr=[0.00019996581560021375, 0.00019996581560021375], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21120 loss: 0.8510 iter time (s): 4.111 samples/sec: 31.138
g0238:  iteration    21120/10000000 | consumed samples:      2703360 | consumed tokens:   5536481280 | elapsed time per iteration (ms): 4144.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.203215E-01 | loss scale: 262144.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.888 | tokens per gpu per second (tgs): 1976.803 | TFLOPs: 15.91 |
g0220: [2024-08-10 07:10:51,141] [INFO] [logging.py:96:log_dist] [Rank 0] step=21130, skipped=27, lr=[0.00019996574490184637, 0.00019996574490184637], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21130 loss: 0.7782 iter time (s): 4.008 samples/sec: 31.937
g0238:  iteration    21130/10000000 | consumed samples:      2704640 | consumed tokens:   5539102720 | elapsed time per iteration (ms): 4041.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.303444E-01 | loss scale: 262144.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.671 | tokens per gpu per second (tgs): 2026.938 | TFLOPs: 16.31 |
g0220: [2024-08-10 07:11:32,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=21140, skipped=27, lr=[0.00019996567413046036, 0.00019996567413046036], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21140 loss: 0.8234 iter time (s): 4.108 samples/sec: 31.159
g0238:  iteration    21140/10000000 | consumed samples:      2705920 | consumed tokens:   5541724160 | elapsed time per iteration (ms): 4140.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.301016E-01 | loss scale: 262144.0 | grad norm: 0.198 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.915 | tokens per gpu per second (tgs): 1978.528 | TFLOPs: 15.92 |
g0220: [2024-08-10 07:12:13,224] [INFO] [logging.py:96:log_dist] [Rank 0] step=21150, skipped=27, lr=[0.00019996560328605574, 0.00019996560328605574], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21150 loss: 0.7900 iter time (s): 4.035 samples/sec: 31.722
g0238:  iteration    21150/10000000 | consumed samples:      2707200 | consumed tokens:   5544345600 | elapsed time per iteration (ms): 4067.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.090653E-01 | loss scale: 262144.0 | grad norm: 0.170 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.466 | tokens per gpu per second (tgs): 2013.815 | TFLOPs: 16.21 |
g0220: [2024-08-10 07:12:54,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=21160, skipped=27, lr=[0.00019996553236863266, 0.00019996553236863266], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21160 loss: 0.7989 iter time (s): 4.105 samples/sec: 31.179
g0238:  iteration    21160/10000000 | consumed samples:      2708480 | consumed tokens:   5546967040 | elapsed time per iteration (ms): 4138.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.300989E-01 | loss scale: 262144.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.931 | tokens per gpu per second (tgs): 1979.597 | TFLOPs: 15.93 |
g0220: [2024-08-10 07:13:37,099] [INFO] [logging.py:96:log_dist] [Rank 0] step=21170, skipped=27, lr=[0.00019996546137819105, 0.00019996546137819105], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21170 loss: 0.8446 iter time (s): 4.216 samples/sec: 30.361
g0238:  iteration    21170/10000000 | consumed samples:      2709760 | consumed tokens:   5549588480 | elapsed time per iteration (ms): 4249.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.171117E-01 | loss scale: 262144.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.122 | tokens per gpu per second (tgs): 1927.832 | TFLOPs: 15.51 |
g0220: [2024-08-10 07:14:17,980] [INFO] [logging.py:96:log_dist] [Rank 0] step=21180, skipped=27, lr=[0.00019996539031473108, 0.00019996539031473108], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21180 loss: 0.8359 iter time (s): 4.055 samples/sec: 31.563
g0238:  iteration    21180/10000000 | consumed samples:      2711040 | consumed tokens:   5552209920 | elapsed time per iteration (ms): 4088.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.304578E-01 | loss scale: 262144.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.311 | tokens per gpu per second (tgs): 2003.928 | TFLOPs: 16.13 |
g0220: [2024-08-10 07:15:00,578] [INFO] [logging.py:96:log_dist] [Rank 0] step=21190, skipped=27, lr=[0.00019996531917825273, 0.00019996531917825273], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21190 loss: 0.8220 iter time (s): 4.227 samples/sec: 30.278
g0238:  iteration    21190/10000000 | consumed samples:      2712320 | consumed tokens:   5554831360 | elapsed time per iteration (ms): 4259.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.230370E-01 | loss scale: 262144.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.048 | tokens per gpu per second (tgs): 1923.058 | TFLOPs: 15.48 |
g0220: [2024-08-10 07:15:41,882] [INFO] [logging.py:96:log_dist] [Rank 0] step=21200, skipped=27, lr=[0.00019996524796875607, 0.00019996524796875607], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21200 loss: 0.8463 iter time (s): 4.098 samples/sec: 31.236
g0238:  iteration    21200/10000000 | consumed samples:      2713600 | consumed tokens:   5557452800 | elapsed time per iteration (ms): 4130.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.140345E-01 | loss scale: 262144.0 | grad norm: 0.224 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.991 | tokens per gpu per second (tgs): 1983.411 | TFLOPs: 15.96 |
g0220: [2024-08-10 07:16:23,930] [INFO] [logging.py:96:log_dist] [Rank 0] step=21210, skipped=27, lr=[0.00019996517668624116, 0.00019996517668624116], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21210 loss: 0.8158 iter time (s): 4.172 samples/sec: 30.682
g0238:  iteration    21210/10000000 | consumed samples:      2714880 | consumed tokens:   5560074240 | elapsed time per iteration (ms): 4204.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.255078E-01 | loss scale: 262144.0 | grad norm: 0.171 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.442 | tokens per gpu per second (tgs): 1948.257 | TFLOPs: 15.68 |
g0220: [2024-08-10 07:17:04,555] [INFO] [logging.py:96:log_dist] [Rank 0] step=21220, skipped=27, lr=[0.0001999651053307081, 0.0001999651053307081], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21220 loss: 0.8043 iter time (s): 4.030 samples/sec: 31.764
g0238:  iteration    21220/10000000 | consumed samples:      2716160 | consumed tokens:   5562695680 | elapsed time per iteration (ms): 4062.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.211981E-01 | loss scale: 262144.0 | grad norm: 0.191 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.508 | tokens per gpu per second (tgs): 2016.515 | TFLOPs: 16.23 |
g0220: [2024-08-10 07:17:47,012] [INFO] [logging.py:96:log_dist] [Rank 0] step=21230, skipped=27, lr=[0.00019996503390215688, 0.00019996503390215688], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21230 loss: 0.7889 iter time (s): 4.213 samples/sec: 30.382
g0238:  iteration    21230/10000000 | consumed samples:      2717440 | consumed tokens:   5565317120 | elapsed time per iteration (ms): 4245.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.121731E-01 | loss scale: 262144.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.148 | tokens per gpu per second (tgs): 1929.472 | TFLOPs: 15.53 |
g0220: [2024-08-10 07:18:30,298] [INFO] [logging.py:96:log_dist] [Rank 0] step=21240, skipped=27, lr=[0.00019996496240058757, 0.00019996496240058757], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21240 loss: 0.8116 iter time (s): 4.296 samples/sec: 29.793
g0238:  iteration    21240/10000000 | consumed samples:      2718720 | consumed tokens:   5567938560 | elapsed time per iteration (ms): 4328.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.299050E-01 | loss scale: 262144.0 | grad norm: 0.206 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.571 | tokens per gpu per second (tgs): 1892.521 | TFLOPs: 15.23 |
g0220: [2024-08-10 07:19:10,822] [INFO] [logging.py:96:log_dist] [Rank 0] step=21250, skipped=27, lr=[0.00019996489082600024, 0.00019996489082600024], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21250 loss: 0.8225 iter time (s): 4.020 samples/sec: 31.841
g0238:  iteration    21250/10000000 | consumed samples:      2720000 | consumed tokens:   5570560000 | elapsed time per iteration (ms): 4052.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.301657E-01 | loss scale: 262144.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.586 | tokens per gpu per second (tgs): 2021.532 | TFLOPs: 16.27 |
g0220: [2024-08-10 07:19:51,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=21260, skipped=27, lr=[0.00019996481917839494, 0.00019996481917839494], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21260 loss: 0.7734 iter time (s): 4.055 samples/sec: 31.565
g0238:  iteration    21260/10000000 | consumed samples:      2721280 | consumed tokens:   5573181440 | elapsed time per iteration (ms): 4087.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.198574E-01 | loss scale: 262144.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.315 | tokens per gpu per second (tgs): 2004.157 | TFLOPs: 16.13 |
g0220: [2024-08-10 07:20:31,608] [INFO] [logging.py:96:log_dist] [Rank 0] step=21270, skipped=27, lr=[0.00019996474745777174, 0.00019996474745777174], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21270 loss: 0.8174 iter time (s): 3.959 samples/sec: 32.333
g0238:  iteration    21270/10000000 | consumed samples:      2722560 | consumed tokens:   5575802880 | elapsed time per iteration (ms): 3991.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.193516E-01 | loss scale: 262144.0 | grad norm: 0.256 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.071 | tokens per gpu per second (tgs): 2052.569 | TFLOPs: 16.52 |
g0220: [2024-08-10 07:21:13,396] [INFO] [logging.py:96:log_dist] [Rank 0] step=21280, skipped=27, lr=[0.00019996467566413064, 0.00019996467566413064], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21280 loss: 0.8060 iter time (s): 4.146 samples/sec: 30.872
g0238:  iteration    21280/10000000 | consumed samples:      2723840 | consumed tokens:   5578424320 | elapsed time per iteration (ms): 4178.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.242421E-01 | loss scale: 262144.0 | grad norm: 0.199 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.631 | tokens per gpu per second (tgs): 1960.381 | TFLOPs: 15.78 |
g0220: [2024-08-10 07:21:54,833] [INFO] [logging.py:96:log_dist] [Rank 0] step=21290, skipped=27, lr=[0.00019996460379747175, 0.00019996460379747175], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21290 loss: 0.7981 iter time (s): 4.111 samples/sec: 31.140
g0238:  iteration    21290/10000000 | consumed samples:      2725120 | consumed tokens:   5581045760 | elapsed time per iteration (ms): 4143.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.102001E-01 | loss scale: 262144.0 | grad norm: 0.202 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.891 | tokens per gpu per second (tgs): 1976.996 | TFLOPs: 15.91 |
g0220: [2024-08-10 07:22:36,800] [INFO] [logging.py:96:log_dist] [Rank 0] step=21300, skipped=27, lr=[0.00019996453185779512, 0.00019996453185779512], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21300 loss: 0.7962 iter time (s): 4.164 samples/sec: 30.736
g0238:  iteration    21300/10000000 | consumed samples:      2726400 | consumed tokens:   5583667200 | elapsed time per iteration (ms): 4196.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.240129E-01 | loss scale: 262144.0 | grad norm: 0.184 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.499 | tokens per gpu per second (tgs): 1951.926 | TFLOPs: 15.71 |
g0220: [2024-08-10 07:23:21,592] [INFO] [logging.py:96:log_dist] [Rank 0] step=21310, skipped=27, lr=[0.00019996445984510078, 0.00019996445984510078], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21310 loss: 0.8148 iter time (s): 4.446 samples/sec: 28.787
g0238:  iteration    21310/10000000 | consumed samples:      2727680 | consumed tokens:   5586288640 | elapsed time per iteration (ms): 4479.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.253231E-01 | loss scale: 262144.0 | grad norm: 0.172 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 28.578 | tokens per gpu per second (tgs): 1828.971 | TFLOPs: 14.72 |
g0220: [2024-08-10 07:24:02,022] [INFO] [logging.py:96:log_dist] [Rank 0] step=21320, skipped=27, lr=[0.00019996438775938885, 0.00019996438775938885], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21320 loss: 0.8195 iter time (s): 4.010 samples/sec: 31.918
g0238:  iteration    21320/10000000 | consumed samples:      2728960 | consumed tokens:   5588910080 | elapsed time per iteration (ms): 4043.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.260702E-01 | loss scale: 262144.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.660 | tokens per gpu per second (tgs): 2026.222 | TFLOPs: 16.31 |
g0220: [2024-08-10 07:24:42,676] [INFO] [logging.py:96:log_dist] [Rank 0] step=21330, skipped=27, lr=[0.00019996431560065925, 0.00019996431560065925], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21330 loss: 0.8190 iter time (s): 4.031 samples/sec: 31.755
g0238:  iteration    21330/10000000 | consumed samples:      2730240 | consumed tokens:   5591531520 | elapsed time per iteration (ms): 4065.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.204367E-01 | loss scale: 262144.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.486 | tokens per gpu per second (tgs): 2015.076 | TFLOPs: 16.22 |
g0220: [2024-08-10 07:25:23,115] [INFO] [logging.py:96:log_dist] [Rank 0] step=21340, skipped=27, lr=[0.0001999642433689122, 0.0001999642433689122], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21340 loss: 0.8138 iter time (s): 4.011 samples/sec: 31.910
g0238:  iteration    21340/10000000 | consumed samples:      2731520 | consumed tokens:   5594152960 | elapsed time per iteration (ms): 4043.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.203108E-01 | loss scale: 262144.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.653 | tokens per gpu per second (tgs): 2025.807 | TFLOPs: 16.30 |
g0220: [2024-08-10 07:26:04,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=21350, skipped=27, lr=[0.00019996417106414764, 0.00019996417106414764], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21350 loss: 0.8150 iter time (s): 4.117 samples/sec: 31.092
g0238:  iteration    21350/10000000 | consumed samples:      2732800 | consumed tokens:   5596774400 | elapsed time per iteration (ms): 4151.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.259628E-01 | loss scale: 262144.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.832 | tokens per gpu per second (tgs): 1973.276 | TFLOPs: 15.88 |
g0220: [2024-08-10 07:26:46,511] [INFO] [logging.py:96:log_dist] [Rank 0] step=21360, skipped=27, lr=[0.00019996409868636565, 0.00019996409868636565], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21360 loss: 0.8076 iter time (s): 4.155 samples/sec: 30.804
g0238:  iteration    21360/10000000 | consumed samples:      2734080 | consumed tokens:   5599395840 | elapsed time per iteration (ms): 4188.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.125509E-01 | loss scale: 262144.0 | grad norm: 0.167 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.561 | tokens per gpu per second (tgs): 1955.887 | TFLOPs: 15.74 |
g0220: [2024-08-10 07:27:27,488] [INFO] [logging.py:96:log_dist] [Rank 0] step=21370, skipped=27, lr=[0.0001999640262355663, 0.0001999640262355663], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21370 loss: 0.8181 iter time (s): 4.065 samples/sec: 31.491
g0238:  iteration    21370/10000000 | consumed samples:      2735360 | consumed tokens:   5602017280 | elapsed time per iteration (ms): 4097.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.180895E-01 | loss scale: 262144.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.238 | tokens per gpu per second (tgs): 1999.260 | TFLOPs: 16.09 |
g0220: [2024-08-10 07:28:08,610] [INFO] [logging.py:96:log_dist] [Rank 0] step=21380, skipped=27, lr=[0.00019996395371174965, 0.00019996395371174965], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21380 loss: 0.8071 iter time (s): 4.079 samples/sec: 31.378
g0238:  iteration    21380/10000000 | consumed samples:      2736640 | consumed tokens:   5604638720 | elapsed time per iteration (ms): 4114.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.239181E-01 | loss scale: 262144.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.108 | tokens per gpu per second (tgs): 1990.896 | TFLOPs: 16.02 |
g0220: [2024-08-10 07:28:49,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=21390, skipped=27, lr=[0.00019996388111491578, 0.00019996388111491578], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21390 loss: 0.8105 iter time (s): 4.069 samples/sec: 31.454
g0238:  iteration    21390/10000000 | consumed samples:      2737920 | consumed tokens:   5607260160 | elapsed time per iteration (ms): 4104.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.224080E-01 | loss scale: 262144.0 | grad norm: 0.273 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.184 | tokens per gpu per second (tgs): 1995.762 | TFLOPs: 16.06 |
g0220: [2024-08-10 07:29:31,282] [INFO] [logging.py:96:log_dist] [Rank 0] step=21400, skipped=27, lr=[0.0001999638084450647, 0.0001999638084450647], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21400 loss: 0.8475 iter time (s): 4.113 samples/sec: 31.117
g0238:  iteration    21400/10000000 | consumed samples:      2739200 | consumed tokens:   5609881600 | elapsed time per iteration (ms): 4161.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.219792E-01 | loss scale: 262144.0 | grad norm: 0.177 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.760 | tokens per gpu per second (tgs): 1968.631 | TFLOPs: 15.84 |
g0220: [2024-08-10 07:30:12,670] [INFO] [logging.py:96:log_dist] [Rank 0] step=21410, skipped=27, lr=[0.00019996373570219645, 0.00019996373570219645], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21410 loss: 0.8058 iter time (s): 4.102 samples/sec: 31.204
g0238:  iteration    21410/10000000 | consumed samples:      2740480 | consumed tokens:   5612503040 | elapsed time per iteration (ms): 4136.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.179345E-01 | loss scale: 262144.0 | grad norm: 0.194 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.946 | tokens per gpu per second (tgs): 1980.549 | TFLOPs: 15.94 |
g0220: [2024-08-10 07:30:54,390] [INFO] [logging.py:96:log_dist] [Rank 0] step=21420, skipped=27, lr=[0.00019996366288631113, 0.00019996366288631113], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21420 loss: 0.8119 iter time (s): 4.140 samples/sec: 30.920
g0238:  iteration    21420/10000000 | consumed samples:      2741760 | consumed tokens:   5615124480 | elapsed time per iteration (ms): 4172.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.254035E-01 | loss scale: 262144.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.680 | tokens per gpu per second (tgs): 1963.536 | TFLOPs: 15.80 |
g0220: [2024-08-10 07:31:35,489] [INFO] [logging.py:96:log_dist] [Rank 0] step=21430, skipped=27, lr=[0.00019996358999740878, 0.00019996358999740878], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21430 loss: 0.7692 iter time (s): 4.077 samples/sec: 31.392
g0238:  iteration    21430/10000000 | consumed samples:      2743040 | consumed tokens:   5617745920 | elapsed time per iteration (ms): 4109.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.196102E-01 | loss scale: 262144.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.145 | tokens per gpu per second (tgs): 1993.253 | TFLOPs: 16.04 |
g0220: [2024-08-10 07:32:16,970] [INFO] [logging.py:96:log_dist] [Rank 0] step=21440, skipped=27, lr=[0.0001999635170354895, 0.0001999635170354895], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21440 loss: 0.8087 iter time (s): 4.115 samples/sec: 31.105
g0238:  iteration    21440/10000000 | consumed samples:      2744320 | consumed tokens:   5620367360 | elapsed time per iteration (ms): 4148.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.247335E-01 | loss scale: 262144.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.857 | tokens per gpu per second (tgs): 1974.876 | TFLOPs: 15.89 |
g0220: [2024-08-10 07:32:57,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=21450, skipped=27, lr=[0.00019996344400055328, 0.00019996344400055328], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21450 loss: 0.7939 iter time (s): 4.025 samples/sec: 31.803
g0238:  iteration    21450/10000000 | consumed samples:      2745600 | consumed tokens:   5622988800 | elapsed time per iteration (ms): 4057.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.163807E-01 | loss scale: 262144.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.545 | tokens per gpu per second (tgs): 2018.868 | TFLOPs: 16.25 |
g0220: [2024-08-10 07:33:39,278] [INFO] [logging.py:96:log_dist] [Rank 0] step=21460, skipped=27, lr=[0.00019996337089260018, 0.00019996337089260018], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21460 loss: 0.8239 iter time (s): 4.140 samples/sec: 30.916
g0238:  iteration    21460/10000000 | consumed samples:      2746880 | consumed tokens:   5625610240 | elapsed time per iteration (ms): 4173.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.220537E-01 | loss scale: 262144.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.673 | tokens per gpu per second (tgs): 1963.101 | TFLOPs: 15.80 |
g0220: [2024-08-10 07:34:21,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=21470, skipped=27, lr=[0.00019996329771163027, 0.00019996329771163027], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21470 loss: 0.8145 iter time (s): 4.226 samples/sec: 30.288
g0238:  iteration    21470/10000000 | consumed samples:      2748160 | consumed tokens:   5628231680 | elapsed time per iteration (ms): 4258.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.251232E-01 | loss scale: 262144.0 | grad norm: 0.190 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.057 | tokens per gpu per second (tgs): 1923.621 | TFLOPs: 15.48 |
g0220: [2024-08-10 07:35:03,259] [INFO] [logging.py:96:log_dist] [Rank 0] step=21480, skipped=27, lr=[0.00019996322445764366, 0.00019996322445764366], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21480 loss: 0.8391 iter time (s): 4.106 samples/sec: 31.172
g0238:  iteration    21480/10000000 | consumed samples:      2749440 | consumed tokens:   5630853120 | elapsed time per iteration (ms): 4139.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.318054E-01 | loss scale: 262144.0 | grad norm: 0.182 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.922 | tokens per gpu per second (tgs): 1978.999 | TFLOPs: 15.93 |
g0220: [2024-08-10 07:35:43,546] [INFO] [logging.py:96:log_dist] [Rank 0] step=21490, skipped=27, lr=[0.0001999631511306403, 0.0001999631511306403], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21490 loss: 0.7995 iter time (s): 3.996 samples/sec: 32.035
g0238:  iteration    21490/10000000 | consumed samples:      2750720 | consumed tokens:   5633474560 | elapsed time per iteration (ms): 4028.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.195380E-01 | loss scale: 262144.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.771 | tokens per gpu per second (tgs): 2033.349 | TFLOPs: 16.36 |
g0220: [2024-08-10 07:36:24,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=21500, skipped=27, lr=[0.00019996307773062033, 0.00019996307773062033], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21500 loss: 0.8357 iter time (s): 4.096 samples/sec: 31.248
g0238:  iteration    21500/10000000 | consumed samples:      2752000 | consumed tokens:   5636096000 | elapsed time per iteration (ms): 4128.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.145113E-01 | loss scale: 262144.0 | grad norm: 0.181 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.002 | tokens per gpu per second (tgs): 1984.147 | TFLOPs: 15.97 |
g0220: [2024-08-10 07:37:08,139] [INFO] [logging.py:96:log_dist] [Rank 0] step=21510, skipped=27, lr=[0.0001999630042575838, 0.0001999630042575838], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21510 loss: 0.8425 iter time (s): 4.297 samples/sec: 29.785
g0238:  iteration    21510/10000000 | consumed samples:      2753280 | consumed tokens:   5638717440 | elapsed time per iteration (ms): 4330.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.187506E-01 | loss scale: 262144.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.558 | tokens per gpu per second (tgs): 1891.738 | TFLOPs: 15.22 |
g0220: [2024-08-10 07:37:51,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=21520, skipped=27, lr=[0.00019996293071153075, 0.00019996293071153075], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21520 loss: 0.8265 iter time (s): 4.291 samples/sec: 29.831
g0238:  iteration    21520/10000000 | consumed samples:      2754560 | consumed tokens:   5641338880 | elapsed time per iteration (ms): 4323.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.194973E-01 | loss scale: 262144.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.603 | tokens per gpu per second (tgs): 1894.569 | TFLOPs: 15.25 |
g0220: [2024-08-10 07:38:31,802] [INFO] [logging.py:96:log_dist] [Rank 0] step=21530, skipped=27, lr=[0.00019996285709246125, 0.00019996285709246125], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21530 loss: 0.8296 iter time (s): 4.010 samples/sec: 31.923
g0238:  iteration    21530/10000000 | consumed samples:      2755840 | consumed tokens:   5643960320 | elapsed time per iteration (ms): 4042.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.232331E-01 | loss scale: 262144.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.664 | tokens per gpu per second (tgs): 2026.499 | TFLOPs: 16.31 |
g0220: [2024-08-10 07:39:12,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=21540, skipped=27, lr=[0.0001999627834003753, 0.0001999627834003753], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21540 loss: 0.8065 iter time (s): 4.029 samples/sec: 31.768
g0238:  iteration    21540/10000000 | consumed samples:      2757120 | consumed tokens:   5646581760 | elapsed time per iteration (ms): 4062.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.146059E-01 | loss scale: 262144.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.506 | tokens per gpu per second (tgs): 2016.362 | TFLOPs: 16.23 |
g0220: [2024-08-10 07:39:52,779] [INFO] [logging.py:96:log_dist] [Rank 0] step=21550, skipped=27, lr=[0.00019996270963527302, 0.00019996270963527302], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21550 loss: 0.7956 iter time (s): 4.002 samples/sec: 31.984
g0238:  iteration    21550/10000000 | consumed samples:      2758400 | consumed tokens:   5649203200 | elapsed time per iteration (ms): 4034.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.173895E-01 | loss scale: 262144.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.724 | tokens per gpu per second (tgs): 2030.347 | TFLOPs: 16.34 |
g0220: [2024-08-10 07:40:33,905] [INFO] [logging.py:96:log_dist] [Rank 0] step=21560, skipped=27, lr=[0.00019996263579715443, 0.00019996263579715443], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21560 loss: 0.8554 iter time (s): 4.079 samples/sec: 31.378
g0238:  iteration    21560/10000000 | consumed samples:      2759680 | consumed tokens:   5651824640 | elapsed time per iteration (ms): 4112.6 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.204502E-01 | loss scale: 262144.0 | grad norm: 0.293 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.124 | tokens per gpu per second (tgs): 1991.945 | TFLOPs: 16.03 |
g0220: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0237: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0220: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0233: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0225: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0235: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0234: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0235: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0234: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0225: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0236: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0237: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:352:_update_scale] No Grad overflow for 500 iterations
g0238: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0236: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0238: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0233: [2024-08-10 07:40:54,577] [INFO] [fused_optimizer.py:353:_update_scale] Increasing dynamic loss scale from 262144.0 to 524288.0
g0220: [2024-08-10 07:41:15,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=21570, skipped=27, lr=[0.00019996256188601963, 0.00019996256188601963], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21570 loss: 0.8400 iter time (s): 4.129 samples/sec: 31.000
g0238:  iteration    21570/10000000 | consumed samples:      2760960 | consumed tokens:   5654446080 | elapsed time per iteration (ms): 4161.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.249427E-01 | loss scale: 524288.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.759 | tokens per gpu per second (tgs): 1968.568 | TFLOPs: 15.84 |
g0220: [2024-08-10 07:41:55,397] [INFO] [logging.py:96:log_dist] [Rank 0] step=21580, skipped=27, lr=[0.00019996248790186863, 0.00019996248790186863], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21580 loss: 0.8253 iter time (s): 3.955 samples/sec: 32.361
g0238:  iteration    21580/10000000 | consumed samples:      2762240 | consumed tokens:   5657067520 | elapsed time per iteration (ms): 3987.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.166337E-01 | loss scale: 524288.0 | grad norm: 0.183 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.097 | tokens per gpu per second (tgs): 2054.229 | TFLOPs: 16.53 |
g0233: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 21580
g0237: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 21580
g0233: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 21580
g0237: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 21580
g0233: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 21580
g0225: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 21580
g0234: Grad overflow on iteration 21580
g0234: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 21580
g0234: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 21580
g0235: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 21580
g0237: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0237: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: Grad overflow on iteration 21580
g0225: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 21580
g0238: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21580
g0236: Grad overflow on iteration 21580
g0238: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21580
g0236: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 21580
g0238: Grad overflow on iteration 21580
g0236: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: Grad overflow on iteration 21580
g0236: Grad overflow on iteration 21580
g0225: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 21580
g0220: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0238: Grad overflow on iteration 21580
g0234: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: Grad overflow on iteration 21580
g0236: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 21580
g0233: Grad overflow on iteration 21580
g0235: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 21580
g0225: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: Grad overflow on iteration 21580
g0220: Grad overflow on iteration 21580
g0233: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0225: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 21580
g0220: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 21580
g0235: Grad overflow on iteration 21580
g0220: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0234: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: Grad overflow on iteration 21580
g0238: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0233: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0236: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0235: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 21580
g0235: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-10 07:41:59,657] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 524288.0, reducing to 262144.0
g0235: [2024-08-10 07:41:59,657] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 524288.0 to 262144.0
g0220: [2024-08-10 07:42:37,316] [INFO] [logging.py:96:log_dist] [Rank 0] step=21590, skipped=28, lr=[0.00019996241384470152, 0.00019996241384470152], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21590 loss: 0.8107 iter time (s): 4.159 samples/sec: 30.776
g0238:  iteration    21590/10000000 | consumed samples:      2763520 | consumed tokens:   5659688960 | elapsed time per iteration (ms): 4191.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.132310E-01 | loss scale: 262144.0 | grad norm: 0.186 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.536 | tokens per gpu per second (tgs): 1954.295 | TFLOPs: 15.73 |
g0220: [2024-08-10 07:43:17,895] [INFO] [logging.py:96:log_dist] [Rank 0] step=21600, skipped=28, lr=[0.00019996233971451832, 0.00019996233971451832], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21600 loss: 0.8197 iter time (s): 4.025 samples/sec: 31.798
g0238:  iteration    21600/10000000 | consumed samples:      2764800 | consumed tokens:   5662310400 | elapsed time per iteration (ms): 4057.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.219572E-01 | loss scale: 262144.0 | grad norm: 0.185 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.543 | tokens per gpu per second (tgs): 2018.761 | TFLOPs: 16.25 |
g0233: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 21604
g0233: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 21604
g0233: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 21604
g0233: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 21604
g0233: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 21604
g0235: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 21604
g0235: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 21604
g0235: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 21604
g0235: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0235: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 21604
g0220: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 21604
g0236: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 21604
g0220: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 21604
g0233: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 21604
g0237: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21604
g0238: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21604
g0237: Grad overflow on iteration 21604
g0238: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: Grad overflow on iteration 21604
g0238: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21604
g0234: Grad overflow on iteration 21604
g0225: Grad overflow on iteration 21604
g0238: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: Grad overflow on iteration 21604
g0234: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 21604
g0236: Grad overflow on iteration 21604
g0236: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 21604
g0236: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0236: Grad overflow on iteration 21604
g0237: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: Grad overflow on iteration 21604
g0236: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 21604
g0220: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 21604
g0234: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0238: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21604
g0220: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0234: Grad overflow on iteration 21604
g0225: Grad overflow on iteration 21604
g0234: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 21604
g0234: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-10 07:43:39,322] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0225: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0237: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0220: [2024-08-10 07:43:39,323] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 262144.0, reducing to 131072.0
g0238: [2024-08-10 07:43:39,323] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 262144.0 to 131072.0
g0233: [2024-08-10 07:43:43,388] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 21605
g0233: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 21605
g0233: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 21605
g0233: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 07:43:43,388] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 21605
g0237: [2024-08-10 07:43:43,388] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 07:43:43,388] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 21605
g0220: Grad overflow on iteration 21605
g0237: Grad overflow on iteration 21605
g0220: [2024-08-10 07:43:43,388] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21605
g0220: Grad overflow on iteration 21605
g0238: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 21605
g0237: [2024-08-10 07:43:43,388] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21605
g0220: [2024-08-10 07:43:43,388] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 07:43:43,388] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: Grad overflow on iteration 21605
g0235: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-10 07:43:43,388] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: Grad overflow on iteration 21605
g0237: Grad overflow on iteration 21605
g0225: Grad overflow on iteration 21605
g0238: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 21605
g0235: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 21605
g0220: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21605
g0234: [2024-08-10 07:43:43,388] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 21605
g0234: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 21605
g0234: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 21605
g0238: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: Grad overflow on iteration 21605
g0233: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: Grad overflow on iteration 21605
g0225: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 21605
g0225: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0225: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0237: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 21605
g0237: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0235: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 21605
g0238: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21605
g0236: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 21605
g0236: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 21605
g0220: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 21605
g0237: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 21605
g0234: Grad overflow on iteration 21605
g0220: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0236: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0234: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0220: [2024-08-10 07:43:43,389] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 131072.0, reducing to 65536.0
g0237: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0238: [2024-08-10 07:43:43,389] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 131072.0 to 65536.0
g0233: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21606
g0233: Grad overflow on iteration 21606
g0238: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: Grad overflow on iteration 21606
g0220: Grad overflow on iteration 21606
g0238: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: Grad overflow on iteration 21606
g0237: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 21606
g0220: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 21606
g0220: Grad overflow on iteration 21606
g0225: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0238: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: Grad overflow on iteration 21606
g0233: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: Grad overflow on iteration 21606
g0233: Grad overflow on iteration 21606
g0220: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0235: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: Grad overflow on iteration 21606
g0225: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0225: Grad overflow on iteration 21606
g0236: Grad overflow on iteration 21606
g0225: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: Grad overflow on iteration 21606
g0233: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: Grad overflow on iteration 21606
g0236: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0233: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: Grad overflow on iteration 21606
g0220: Grad overflow on iteration 21606
g0233: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: Grad overflow on iteration 21606
g0235: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 21606
g0225: Grad overflow on iteration 21606
g0237: Grad overflow on iteration 21606
g0225: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0235: Grad overflow on iteration 21606
g0235: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: Grad overflow on iteration 21606
g0236: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 21606
g0236: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0236: Grad overflow on iteration 21606
g0235: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0225: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0233: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 21606
g0235: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: Grad overflow on iteration 21606
g0234: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 21606
g0234: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0234: Grad overflow on iteration 21606
g0237: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0236: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0234: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0237: Grad overflow on iteration 21606
g0234: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0237: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:344:_update_scale] 
g0220: [2024-08-10 07:43:47,579] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0238: Grad overflow on iteration 21606
g0220: [2024-08-10 07:43:47,579] [INFO] [logging.py:96:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 65536.0, reducing to 32768.0
g0238: [2024-08-10 07:43:47,580] [INFO] [fused_optimizer.py:345:_update_scale] Reducing dynamic loss scale from 65536.0 to 32768.0
g0220: [2024-08-10 07:44:00,594] [INFO] [logging.py:96:log_dist] [Rank 0] step=21610, skipped=31, lr=[0.00019996226551131913, 0.00019996226551131913], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21610 loss: 0.8065 iter time (s): 4.237 samples/sec: 30.213
g0238:  iteration    21610/10000000 | consumed samples:      2766080 | consumed tokens:   5664931840 | elapsed time per iteration (ms): 4269.8 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.194390E-01 | loss scale: 32768.0 | grad norm: 0.205 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.978 | tokens per gpu per second (tgs): 1918.591 | TFLOPs: 15.44 |
g0220: [2024-08-10 07:44:41,618] [INFO] [logging.py:96:log_dist] [Rank 0] step=21620, skipped=31, lr=[0.00019996219123510396, 0.00019996219123510396], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21620 loss: 0.8153 iter time (s): 4.069 samples/sec: 31.458
g0238:  iteration    21620/10000000 | consumed samples:      2767360 | consumed tokens:   5667553280 | elapsed time per iteration (ms): 4102.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.207865E-01 | loss scale: 32768.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.201 | tokens per gpu per second (tgs): 1996.870 | TFLOPs: 16.07 |
g0220: [2024-08-10 07:45:21,900] [INFO] [logging.py:96:log_dist] [Rank 0] step=21630, skipped=31, lr=[0.00019996211688587292, 0.00019996211688587292], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21630 loss: 0.7889 iter time (s): 3.996 samples/sec: 32.035
g0238:  iteration    21630/10000000 | consumed samples:      2768640 | consumed tokens:   5670174720 | elapsed time per iteration (ms): 4028.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.160625E-01 | loss scale: 32768.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.776 | tokens per gpu per second (tgs): 2033.636 | TFLOPs: 16.37 |
g0220: [2024-08-10 07:46:01,242] [INFO] [logging.py:96:log_dist] [Rank 0] step=21640, skipped=31, lr=[0.00019996204246362605, 0.00019996204246362605], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21640 loss: 0.8052 iter time (s): 3.901 samples/sec: 32.811
g0238:  iteration    21640/10000000 | consumed samples:      2769920 | consumed tokens:   5672796160 | elapsed time per iteration (ms): 3934.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.143501E-01 | loss scale: 32768.0 | grad norm: 0.195 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.536 | tokens per gpu per second (tgs): 2082.284 | TFLOPs: 16.76 |
g0220: [2024-08-10 07:46:43,794] [INFO] [logging.py:96:log_dist] [Rank 0] step=21650, skipped=31, lr=[0.0001999619679683634, 0.0001999619679683634], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21650 loss: 0.7758 iter time (s): 4.222 samples/sec: 30.314
g0238:  iteration    21650/10000000 | consumed samples:      2771200 | consumed tokens:   5675417600 | elapsed time per iteration (ms): 4255.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.176593E-01 | loss scale: 32768.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.081 | tokens per gpu per second (tgs): 1925.185 | TFLOPs: 15.49 |
g0220: [2024-08-10 07:47:25,367] [INFO] [logging.py:96:log_dist] [Rank 0] step=21660, skipped=31, lr=[0.000199961893400085, 0.000199961893400085], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21660 loss: 0.7876 iter time (s): 4.124 samples/sec: 31.041
g0238:  iteration    21660/10000000 | consumed samples:      2772480 | consumed tokens:   5678039040 | elapsed time per iteration (ms): 4157.2 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.075620E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.790 | tokens per gpu per second (tgs): 1970.565 | TFLOPs: 15.86 |
g0220: [2024-08-10 07:48:06,798] [INFO] [logging.py:96:log_dist] [Rank 0] step=21670, skipped=31, lr=[0.00019996181875879096, 0.00019996181875879096], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21670 loss: 0.8444 iter time (s): 4.111 samples/sec: 31.139
g0238:  iteration    21670/10000000 | consumed samples:      2773760 | consumed tokens:   5680660480 | elapsed time per iteration (ms): 4143.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.109058E-01 | loss scale: 32768.0 | grad norm: 0.176 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.893 | tokens per gpu per second (tgs): 1977.156 | TFLOPs: 15.91 |
g0220: [2024-08-10 07:48:46,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=21680, skipped=31, lr=[0.00019996174404448126, 0.00019996174404448126], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21680 loss: 0.7966 iter time (s): 3.942 samples/sec: 32.472
g0238:  iteration    21680/10000000 | consumed samples:      2775040 | consumed tokens:   5683281920 | elapsed time per iteration (ms): 3974.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.107112E-01 | loss scale: 32768.0 | grad norm: 0.193 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.205 | tokens per gpu per second (tgs): 2061.127 | TFLOPs: 16.59 |
g0220: [2024-08-10 07:49:28,695] [INFO] [logging.py:96:log_dist] [Rank 0] step=21690, skipped=31, lr=[0.00019996166925715606, 0.00019996166925715606], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21690 loss: 0.8237 iter time (s): 4.182 samples/sec: 30.605
g0238:  iteration    21690/10000000 | consumed samples:      2776320 | consumed tokens:   5685903360 | elapsed time per iteration (ms): 4215.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.157670E-01 | loss scale: 32768.0 | grad norm: 0.189 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.368 | tokens per gpu per second (tgs): 1943.534 | TFLOPs: 15.64 |
g0220: [2024-08-10 07:50:09,509] [INFO] [logging.py:96:log_dist] [Rank 0] step=21700, skipped=31, lr=[0.00019996159439681534, 0.00019996159439681534], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21700 loss: 0.8534 iter time (s): 4.048 samples/sec: 31.619
g0238:  iteration    21700/10000000 | consumed samples:      2777600 | consumed tokens:   5688524800 | elapsed time per iteration (ms): 4081.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.138547E-01 | loss scale: 32768.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.362 | tokens per gpu per second (tgs): 2007.183 | TFLOPs: 16.15 |
g0220: [2024-08-10 07:50:52,829] [INFO] [logging.py:96:log_dist] [Rank 0] step=21710, skipped=31, lr=[0.0001999615194634592, 0.0001999615194634592], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21710 loss: 0.7985 iter time (s): 4.300 samples/sec: 29.770
g0238:  iteration    21710/10000000 | consumed samples:      2778880 | consumed tokens:   5691146240 | elapsed time per iteration (ms): 4332.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.158953E-01 | loss scale: 32768.0 | grad norm: 0.179 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 29.548 | tokens per gpu per second (tgs): 1891.050 | TFLOPs: 15.22 |
g0220: [2024-08-10 07:51:33,950] [INFO] [logging.py:96:log_dist] [Rank 0] step=21720, skipped=31, lr=[0.00019996144445708766, 0.00019996144445708766], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21720 loss: 0.8078 iter time (s): 4.080 samples/sec: 31.376
g0238:  iteration    21720/10000000 | consumed samples:      2780160 | consumed tokens:   5693767680 | elapsed time per iteration (ms): 4112.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.214727E-01 | loss scale: 32768.0 | grad norm: 0.208 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.128 | tokens per gpu per second (tgs): 1992.185 | TFLOPs: 16.03 |
g0220: [2024-08-10 07:52:12,929] [INFO] [logging.py:96:log_dist] [Rank 0] step=21730, skipped=31, lr=[0.0001999613693777008, 0.0001999613693777008], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21730 loss: 0.8341 iter time (s): 3.865 samples/sec: 33.114
g0238:  iteration    21730/10000000 | consumed samples:      2781440 | consumed tokens:   5696389120 | elapsed time per iteration (ms): 3897.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.268658E-01 | loss scale: 32768.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.838 | tokens per gpu per second (tgs): 2101.628 | TFLOPs: 16.91 |
g0220: [2024-08-10 07:52:54,291] [INFO] [logging.py:96:log_dist] [Rank 0] step=21740, skipped=31, lr=[0.0001999612942252987, 0.0001999612942252987], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21740 loss: 0.8392 iter time (s): 4.103 samples/sec: 31.197
g0238:  iteration    21740/10000000 | consumed samples:      2782720 | consumed tokens:   5699010560 | elapsed time per iteration (ms): 4136.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.276807E-01 | loss scale: 32768.0 | grad norm: 0.197 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.944 | tokens per gpu per second (tgs): 1980.416 | TFLOPs: 15.94 |
g0220: [2024-08-10 07:53:36,781] [INFO] [logging.py:96:log_dist] [Rank 0] step=21750, skipped=31, lr=[0.00019996121899988137, 0.00019996121899988137], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21750 loss: 0.8099 iter time (s): 4.216 samples/sec: 30.364
g0238:  iteration    21750/10000000 | consumed samples:      2784000 | consumed tokens:   5701632000 | elapsed time per iteration (ms): 4248.7 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.248539E-01 | loss scale: 32768.0 | grad norm: 0.225 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.127 | tokens per gpu per second (tgs): 1928.140 | TFLOPs: 15.52 |
g0220: [2024-08-10 07:54:19,447] [INFO] [logging.py:96:log_dist] [Rank 0] step=21760, skipped=31, lr=[0.0001999611437014489, 0.0001999611437014489], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21760 loss: 0.7877 iter time (s): 4.234 samples/sec: 30.235
g0238:  iteration    21760/10000000 | consumed samples:      2785280 | consumed tokens:   5704253440 | elapsed time per iteration (ms): 4266.5 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.079220E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.001 | tokens per gpu per second (tgs): 1920.055 | TFLOPs: 15.45 |
g0220: [2024-08-10 07:55:01,561] [INFO] [logging.py:96:log_dist] [Rank 0] step=21770, skipped=31, lr=[0.00019996106833000134, 0.00019996106833000134], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21770 loss: 0.8107 iter time (s): 4.179 samples/sec: 30.632
g0238:  iteration    21770/10000000 | consumed samples:      2786560 | consumed tokens:   5706874880 | elapsed time per iteration (ms): 4211.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.234711E-01 | loss scale: 32768.0 | grad norm: 0.210 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.394 | tokens per gpu per second (tgs): 1945.188 | TFLOPs: 15.65 |
g0220: [2024-08-10 07:55:40,729] [INFO] [logging.py:96:log_dist] [Rank 0] step=21780, skipped=31, lr=[0.00019996099288553876, 0.00019996099288553876], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21780 loss: 0.8256 iter time (s): 3.884 samples/sec: 32.955
g0238:  iteration    21780/10000000 | consumed samples:      2787840 | consumed tokens:   5709496320 | elapsed time per iteration (ms): 3917.0 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.227650E-01 | loss scale: 32768.0 | grad norm: 0.213 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 32.678 | tokens per gpu per second (tgs): 2091.401 | TFLOPs: 16.83 |
g0220: [2024-08-10 07:56:22,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=21790, skipped=31, lr=[0.00019996091736806118, 0.00019996091736806118], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21790 loss: 0.7984 iter time (s): 4.100 samples/sec: 31.218
g0238:  iteration    21790/10000000 | consumed samples:      2789120 | consumed tokens:   5712117760 | elapsed time per iteration (ms): 4133.3 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.143168E-01 | loss scale: 32768.0 | grad norm: 0.180 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.968 | tokens per gpu per second (tgs): 1981.939 | TFLOPs: 15.95 |
g0220: [2024-08-10 07:57:03,719] [INFO] [logging.py:96:log_dist] [Rank 0] step=21800, skipped=31, lr=[0.00019996084177756871, 0.00019996084177756871], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21800 loss: 0.8281 iter time (s): 4.132 samples/sec: 30.978
g0238:  iteration    21800/10000000 | consumed samples:      2790400 | consumed tokens:   5714739200 | elapsed time per iteration (ms): 4165.4 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.205773E-01 | loss scale: 32768.0 | grad norm: 0.201 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.729 | tokens per gpu per second (tgs): 1966.655 | TFLOPs: 15.83 |
g0220: [2024-08-10 07:57:45,337] [INFO] [logging.py:96:log_dist] [Rank 0] step=21810, skipped=31, lr=[0.00019996076611406135, 0.00019996076611406135], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21810 loss: 0.8180 iter time (s): 4.129 samples/sec: 30.998
g0238:  iteration    21810/10000000 | consumed samples:      2791680 | consumed tokens:   5717360640 | elapsed time per iteration (ms): 4161.9 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.192356E-01 | loss scale: 32768.0 | grad norm: 0.178 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 30.755 | tokens per gpu per second (tgs): 1968.317 | TFLOPs: 15.84 |
g0220: [2024-08-10 07:58:25,360] [INFO] [logging.py:96:log_dist] [Rank 0] step=21820, skipped=31, lr=[0.00019996069037753924, 0.00019996069037753924], mom=[(0.9, 0.95), (0.9, 0.95)]
g0220: steps: 21820 loss: 0.7799 iter time (s): 3.969 samples/sec: 32.246
g0238:  iteration    21820/10000000 | consumed samples:      2792960 | consumed tokens:   5719982080 | elapsed time per iteration (ms): 4002.1 | learning rate: 2.000E-04 | global batch size:   128 | lm loss: 8.113207E-01 | loss scale: 32768.0 | grad norm: 0.196 | num zeros: 0.0 | actual seqlen:  2048 | number of skipped iterations:   0 | number of nan iterations:   0 | samples per second: 31.983 | tokens per gpu per second (tgs): 2046.903 | TFLOPs: 16.47 |
